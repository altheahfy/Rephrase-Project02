#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BasicFivePatternHandler ãƒ‡ãƒãƒƒã‚°ç‰ˆ
"""

import spacy
from typing import Dict, Any, List, Optional

class DebugBasicFivePatternHandler:
    """5æ–‡å‹ç”¨ãƒ‡ãƒãƒƒã‚°ç‰ˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.nlp = spacy.load('en_core_web_sm')
        
        # 5æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©
        self.patterns = {
            'SV': ['S', 'V'],                    # ç¬¬1æ–‡å‹
            'SVC': ['S', 'V', 'C1'],             # ç¬¬2æ–‡å‹  
            'SVO': ['S', 'V', 'O1'],             # ç¬¬3æ–‡å‹
            'SVOO': ['S', 'V', 'O1', 'O2'],      # ç¬¬4æ–‡å‹
            'SVOC': ['S', 'V', 'O1', 'C2']       # ç¬¬5æ–‡å‹
        }
        
        # æ–‡å‹åˆ¤å®šç”¨å‹•è©åˆ†é¡
        self.verb_types = {
            'linking': ['be', 'seem', 'become', 'appear', 'look', 'sound', 'feel', 'taste', 'smell', 'remain', 'stay', 'turn', 'grow'],
            'transitive': ['love', 'like', 'see', 'hear', 'make', 'take', 'give', 'send', 'show', 'read', 'play', 'study'],
            'ditransitive': ['give', 'send', 'show', 'tell', 'teach', 'buy', 'make', 'get', 'find', 'offer'],
            'causative': ['make', 'let', 'have', 'get', 'help', 'see', 'hear', 'watch', 'call', 'find', 'consider']
        }
    
    def process(self, text: str) -> Dict[str, Any]:
        """å‡¦ç†ãƒ¡ã‚¤ãƒ³ï¼ˆãƒ‡ãƒãƒƒã‚°ç‰ˆï¼‰"""
        print(f"ğŸ” ãƒ‡ãƒãƒƒã‚°å‡¦ç†é–‹å§‹: {text}")
        
        try:
            doc = self.nlp(text)
            print(f"ğŸ“Š Tokenè©³ç´°:")
            for i, token in enumerate(doc):
                print(f"  [{i}] {token.text:>8} | {token.pos_:>8} | {token.tag_:>12} | {token.lemma_}")
            
            # åŸºæœ¬è¦ç´ æŠ½å‡º
            elements = self._extract_basic_elements_debug(doc)
            print(f"ğŸ“‹ æŠ½å‡ºè¦ç´ : {elements}")
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³è­˜åˆ¥
            pattern = self._identify_pattern(elements, doc)
            print(f"ğŸ¯ è­˜åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³: {pattern}")
            
            return {
                'success': True,
                'slots': elements,
                'pattern_type': pattern,
                'debug_info': {
                    'tokens': [(t.text, t.pos_, t.lemma_) for t in doc]
                }
            }
            
        except Exception as e:
            print(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'success': False,
                'error': str(e),
                'slots': {}
            }
    
    def _extract_basic_elements_debug(self, doc) -> Dict[str, str]:
        """åŸºæœ¬è¦ç´ æŠ½å‡ºï¼ˆãƒ‡ãƒãƒƒã‚°ç‰ˆï¼‰"""
        print(f"ğŸ”§ åŸºæœ¬è¦ç´ æŠ½å‡ºé–‹å§‹")
        elements = {}
        
        # ä¸»èªå€™è£œï¼ˆæ–‡é ­ã®åè©å¥å…¨ä½“ã‚’æŠ½å‡ºï¼‰
        subject_tokens = []
        for token in doc:
            print(f"  ä¸»èªå‡¦ç†: {token.text} ({token.pos_})")
            if token.pos_ in ['DET', 'ADJ', 'NOUN', 'PRON', 'PROPN']:
                subject_tokens.append(token.text)
                print(f"    â†’ ä¸»èªå€™è£œã«è¿½åŠ ")
            elif token.pos_ in ['VERB', 'AUX']:
                print(f"    â†’ å‹•è©ç™ºè¦‹ã€ä¸»èªçµ‚äº†")
                break  # å‹•è©ã«åˆ°é”ã—ãŸã‚‰ä¸»èªçµ‚äº†
            elif subject_tokens:  # ä¸»èªå€™è£œãŒã‚ã‚Šã€å‹•è©ä»¥å¤–ã«åˆ°é”ã—ãŸã‚‰çµ‚äº†
                print(f"    â†’ ä¸»èªçµ‚äº†")
                break
        
        if subject_tokens:
            elements['S'] = ' '.join(subject_tokens)
            print(f"âœ… ä¸»èª: {elements['S']}")
        
        # å‹•è©æŠ½å‡ºã¨ä½ç½®ç‰¹å®š
        verb_idx = None
        for i, token in enumerate(doc):
            print(f"  å‹•è©å‡¦ç†: {token.text} ({token.pos_}, lemma={token.lemma_})")
            if token.pos_ == 'VERB' and not token.lemma_ in ['be']:
                elements['V'] = token.text
                verb_idx = i
                print(f"    â†’ ãƒ¡ã‚¤ãƒ³å‹•è©ç™ºè¦‹: {token.text} (ä½ç½®{i})")
                break
            elif token.pos_ == 'AUX' and token.lemma_ == 'be':
                elements['V'] = token.text
                verb_idx = i
                print(f"    â†’ beå‹•è©ç™ºè¦‹: {token.text} (ä½ç½®{i})")
                break
        
        if verb_idx is None:
            print(f"âŒ å‹•è©ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return elements
        
        # å‹•è©å¾Œã®è¦ç´ ã‚’è©³ç´°åˆ†æ
        post_verb_tokens = [token for token in doc[verb_idx + 1:] if token.pos_ != 'PUNCT']
        print(f"ğŸ“ å‹•è©å¾Œãƒˆãƒ¼ã‚¯ãƒ³: {[t.text for t in post_verb_tokens]}")
        
        if not post_verb_tokens:
            print(f"â„¹ï¸ å‹•è©å¾Œè¦ç´ ãªã—")
            return elements
        
        # å‹•è©ã®ç¨®é¡åˆ¤å®šï¼ˆé€£çµå‹•è©ã‹ã©ã†ã‹ï¼‰
        verb_lemma = doc[verb_idx].lemma_
        is_linking_verb = verb_lemma in self.verb_types['linking']
        print(f"ğŸ”— å‹•è©åˆ¤å®š: {verb_lemma} â†’ é€£çµå‹•è©: {is_linking_verb}")
        
        # è¦ç´ åˆ†é¡
        elements_found = []
        current_phrase = []
        
        for token in post_verb_tokens:
            print(f"  å¾Œå‡¦ç†: {token.text} ({token.pos_})")
            if token.pos_ in ['DET', 'ADJ', 'NOUN', 'PRON', 'PROPN']:
                current_phrase.append(token.text)
                print(f"    â†’ å¥ã«è¿½åŠ : {current_phrase}")
            elif current_phrase:  # å¥ã®çµ‚äº†
                phrase_text = ' '.join(current_phrase)
                print(f"    â†’ å¥çµ‚äº†: {phrase_text}")
                
                # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®POSã§åˆ¤å®š
                last_token_pos = None
                for t in post_verb_tokens:
                    if t.text == current_phrase[-1]:
                        last_token_pos = t.pos_
                        break
                
                print(f"    â†’ æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³POS: {last_token_pos}")
                
                if last_token_pos == 'ADJ':
                    elements_found.append(('ADJ', phrase_text))
                    print(f"    â†’ ADJè¦ç´ ã¨ã—ã¦è¨˜éŒ²")
                else:
                    elements_found.append(('NOUN', phrase_text))
                    print(f"    â†’ NOUNè¦ç´ ã¨ã—ã¦è¨˜éŒ²")
                current_phrase = []
        
        # æœ€å¾Œã®å¥ã‚’å‡¦ç†
        if current_phrase:
            phrase_text = ' '.join(current_phrase)
            print(f"  æœ€çµ‚å¥å‡¦ç†: {phrase_text}")
            
            last_token_pos = None
            for t in post_verb_tokens:
                if t.text == current_phrase[-1]:
                    last_token_pos = t.pos_
                    break
            
            print(f"    â†’ æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³POS: {last_token_pos}")
            
            if last_token_pos == 'ADJ':
                elements_found.append(('ADJ', phrase_text))
                print(f"    â†’ ADJè¦ç´ ã¨ã—ã¦è¨˜éŒ²")
            else:
                elements_found.append(('NOUN', phrase_text))
                print(f"    â†’ NOUNè¦ç´ ã¨ã—ã¦è¨˜éŒ²")
        
        print(f"ğŸ“Š ç™ºè¦‹è¦ç´ : {elements_found}")
        
        # æ–‡å‹åˆ¤å®šã¨ã‚¹ãƒ­ãƒƒãƒˆé…ç½®
        noun_count = len([e for e in elements_found if e[0] == 'NOUN'])
        adj_count = len([e for e in elements_found if e[0] == 'ADJ'])
        
        print(f"ğŸ“ˆ è¦ç´ æ•°: NOUN={noun_count}, ADJ={adj_count}")
        print(f"ğŸ”— é€£çµå‹•è©: {is_linking_verb}")
        
        if is_linking_verb and adj_count > 0:
            # ç¬¬2æ–‡å‹: é€£çµå‹•è© + å½¢å®¹è©
            adj_elements = [e[1] for e in elements_found if e[0] == 'ADJ']
            elements['C1'] = adj_elements[0]
            print(f"âœ… ç¬¬2æ–‡å‹(å½¢å®¹è©è£œèª): C1={elements['C1']}")
            
        elif is_linking_verb and noun_count > 0:
            # ç¬¬2æ–‡å‹: é€£çµå‹•è© + åè©ï¼ˆè£œèªï¼‰
            noun_elements = [e[1] for e in elements_found if e[0] == 'NOUN']
            elements['C1'] = noun_elements[0]
            print(f"âœ… ç¬¬2æ–‡å‹(åè©è£œèª): C1={elements['C1']}")
            
        elif noun_count == 1 and adj_count == 0:
            # ç¬¬3æ–‡å‹: ä»–å‹•è© + ç›®çš„èª
            noun_elements = [e[1] for e in elements_found if e[0] == 'NOUN']
            elements['O1'] = noun_elements[0]
            print(f"âœ… ç¬¬3æ–‡å‹: O1={elements['O1']}")
            
        else:
            print(f"â„¹ï¸ ãã®ä»–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå®Ÿè£…ä¸­ï¼‰")
        
        print(f"ğŸ¯ æœ€çµ‚è¦ç´ : {elements}")
        return elements
    
    def _identify_pattern(self, elements: Dict[str, str], doc) -> Optional[str]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³è­˜åˆ¥"""
        if 'S' in elements and 'V' in elements:
            if 'C1' in elements:
                return 'SVC'  # ç¬¬2æ–‡å‹
            elif 'O1' in elements:
                if 'O2' in elements:
                    return 'SVOO'  # ç¬¬4æ–‡å‹
                elif 'C2' in elements:
                    return 'SVOC'  # ç¬¬5æ–‡å‹
                else:
                    return 'SVO'  # ç¬¬3æ–‡å‹
            else:
                return 'SV'   # ç¬¬1æ–‡å‹
        return None

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
if __name__ == "__main__":
    handler = DebugBasicFivePatternHandler()
    result = handler.process("She looks happy.")
    print(f"\nğŸ¯ æœ€çµ‚çµæœ: {result}")
