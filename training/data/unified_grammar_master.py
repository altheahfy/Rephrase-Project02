"""
ğŸ”§ Unified Grammar Master System
çµ±åˆæ–‡æ³•æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ  - Phase 1-4å®Œå…¨çµ±åˆç‰ˆ

å¯¾å¿œãƒ‘ã‚¿ãƒ¼ãƒ³: 40æ§‹æ–‡ (72.7%ã‚«ãƒãƒ¬ãƒƒã‚¸)
- Phase 1: å€’ç½®æ§‹æ–‡ (6ãƒ‘ã‚¿ãƒ¼ãƒ³)
- Phase 2: æ™‚åˆ¶-ã‚¢ã‚¹ãƒšã‚¯ãƒˆ (15ãƒ‘ã‚¿ãƒ¼ãƒ³) 
- Phase 3: å¼·èª¿æ§‹æ–‡ (8ãƒ‘ã‚¿ãƒ¼ãƒ³)
- Phase 4: é«˜åº¦æ§‹æ–‡ (11ãƒ‘ã‚¿ãƒ¼ãƒ³)
"""

import spacy
import re
from enum import Enum
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple, Union, Any
import json

# å„Phaseã‹ã‚‰å¿…è¦ãªã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
class GrammarType(Enum):
    """çµ±åˆæ–‡æ³•ã‚¿ã‚¤ãƒ—"""
    # Phase 1: å€’ç½®
    NEGATIVE_INVERSION = "negative_inversion"
    CONDITIONAL_INVERSION = "conditional_inversion"
    ONLY_INVERSION = "only_inversion"
    ADVERBIAL_INVERSION = "adverbial_inversion"
    SO_NEITHER_INVERSION = "so_neither_inversion"
    EMPHATIC_INVERSION = "emphatic_inversion"
    
    # Phase 2: æ™‚åˆ¶-ã‚¢ã‚¹ãƒšã‚¯ãƒˆ
    SIMPLE_PRESENT = "simple_present"
    SIMPLE_PAST = "simple_past"
    SIMPLE_FUTURE = "simple_future"
    PRESENT_PERFECT = "present_perfect"
    PAST_PERFECT = "past_perfect"
    FUTURE_PERFECT = "future_perfect"
    PRESENT_PROGRESSIVE = "present_progressive"
    PAST_PROGRESSIVE = "past_progressive"
    FUTURE_PROGRESSIVE = "future_progressive"
    PRESENT_PERFECT_PROGRESSIVE = "present_perfect_progressive"
    PAST_PASSIVE = "past_passive"
    PRESENT_PASSIVE = "present_passive"
    CONDITIONAL = "conditional"
    SUBJUNCTIVE = "subjunctive"
    MODAL_CONSTRUCTIONS = "modal_constructions"
    
    # Phase 3: å¼·èª¿
    IT_CLEFT = "it_cleft"
    PSEUDO_CLEFT = "pseudo_cleft"
    DO_EMPHASIS = "do_emphasis"
    EXCLAMATION_EMPHASIS = "exclamation_emphasis"
    REPETITION_EMPHASIS = "repetition_emphasis"
    FRONTING_EMPHASIS = "fronting_emphasis"
    ADVERB_EMPHASIS = "adverb_emphasis"
    INTENSIFIER_EMPHASIS = "intensifier_emphasis"
    
    # Phase 4: é«˜åº¦æ§‹æ–‡
    VP_ELLIPSIS = "vp_ellipsis"
    NP_ELLIPSIS = "np_ellipsis"
    IT_EXTRAPOSITION = "it_extraposition"
    COMPARATIVE_CONSTRUCTIONS = "comparative_constructions"
    SUPERLATIVE = "superlative"
    EXISTENTIAL_THERE = "existential_there"
    REAL_CONDITIONAL = "real_conditional"
    UNREAL_CONDITIONAL = "unreal_conditional"
    CONCESSIVE = "concessive"
    CORRELATIVE = "correlative"
    PARTICIPLE_ABSOLUTE = "participle_absolute"
    
    # åŸºæœ¬
    SV_PATTERN = "sv_pattern"                    # ç¬¬1æ–‡å‹
    SVC_PATTERN = "svc_pattern"                  # ç¬¬2æ–‡å‹  
    SVO_PATTERN = "svo_pattern"                  # ç¬¬3æ–‡å‹
    SVOO_PATTERN = "svoo_pattern"                # ç¬¬4æ–‡å‹
    SVOC_PATTERN = "svoc_pattern"                # ç¬¬5æ–‡å‹
    BASIC_STRUCTURE = "basic_structure"
    NO_SPECIAL_GRAMMAR = "no_special_grammar"

@dataclass
class GrammarAnalysisResult:
    """çµ±åˆæ–‡æ³•è§£æçµæœ"""
    sentence: str
    detected_patterns: List[Dict[str, Any]]
    primary_grammar: GrammarType
    complexity_score: float
    confidence: float
    components: Dict[str, Any]
    explanation: str
    phase_results: Dict[str, Any]
    rephrase_slots: Dict[str, Any]

class UnifiedGrammarMaster:
    """çµ±åˆæ–‡æ³•ãƒã‚¹ã‚¿ãƒ¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        print("ğŸ”§ Unified Grammar Master åˆæœŸåŒ–ä¸­...")
        
        # spaCyãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        try:
            self.nlp = spacy.load("en_core_web_sm")
            print("âœ… spaCyè‹±èªãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
        except OSError:
            print("âŒ spaCyè‹±èªãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            self.nlp = None
            return
        
        # Phaseåˆ¥æ¤œå‡ºå™¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–
        self._init_phase0_basic_patterns()  # 5æ–‡å‹è¿½åŠ 
        self._init_phase1_inversion()
        self._init_phase2_tense_aspect()
        self._init_phase3_emphasis()
        self._init_phase4_advanced()
        
        # å„ªå…ˆé †ä½è¨­å®šï¼ˆè¤‡é›‘åº¦é †ï¼‰
        self.detection_priority = [
            'phase4',  # é«˜åº¦æ§‹æ–‡ï¼ˆæœ€å„ªå…ˆï¼‰
            'phase3',  # å¼·èª¿æ§‹æ–‡
            'phase1',  # å€’ç½®æ§‹æ–‡  
            'phase2',  # æ™‚åˆ¶-ã‚¢ã‚¹ãƒšã‚¯ãƒˆ
            'phase0'   # åŸºæœ¬5æ–‡å‹ï¼ˆæœ€å¾Œï¼‰
        ]
        
        print("âœ… çµ±åˆæ–‡æ³•ãƒã‚¹ã‚¿ãƒ¼æº–å‚™å®Œäº† - 45ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œ (5æ–‡å‹å«ã‚€)")

    def _init_phase0_basic_patterns(self):
        """Phase 0: åŸºæœ¬5æ–‡å‹åˆæœŸåŒ–"""
        # é€£çµå‹•è©ï¼ˆç¬¬2æ–‡å‹ãƒ»ç¬¬5æ–‡å‹ï¼‰
        self.linking_verbs = ['be', 'am', 'is', 'are', 'was', 'were', 'being', 'been',
                            'seem', 'appear', 'become', 'get', 'feel', 'look', 'sound',
                            'taste', 'smell', 'remain', 'stay', 'turn', 'grow', 'prove']
        
        # æˆä¸å‹•è©ï¼ˆç¬¬4æ–‡å‹ï¼‰
        self.ditransitive_verbs = ['give', 'send', 'tell', 'show', 'teach', 'buy', 'make',
                                 'get', 'bring', 'take', 'hand', 'pass', 'lend', 'offer',
                                 'promise', 'write', 'read', 'sing', 'play', 'cook']
        
        # ç¬¬5æ–‡å‹å‹•è©ï¼ˆSVOCï¼‰
        self.svoc_verbs = ['make', 'keep', 'leave', 'find', 'call', 'name', 'consider',
                         'think', 'believe', 'elect', 'choose', 'paint', 'drive',
                         'turn', 'get', 'have', 'let', 'help', 'see', 'hear', 'watch']

    def _init_phase1_inversion(self):
        """Phase 1: å€’ç½®æ§‹æ–‡åˆæœŸåŒ–"""
        self.inversion_triggers = {
            'negative': ['never', 'seldom', 'rarely', 'hardly', 'scarcely', 'barely', 'not only', 'under no circumstances', 'at no time', 'in no way'],
            'conditional': ['should', 'were', 'had'],
            'only': ['only when', 'only if', 'only after', 'only then', 'only by'],
            'adverbial': ['here', 'there', 'away', 'down', 'up', 'in', 'out', 'off'],
            'so_neither': ['so', 'neither', 'nor'],
            'emphatic': ['such', 'so great', 'so important']
        }

    def _init_phase2_tense_aspect(self):
        """Phase 2: æ™‚åˆ¶-ã‚¢ã‚¹ãƒšã‚¯ãƒˆåˆæœŸåŒ–"""
        self.tense_patterns = {
            'simple_present': r'\b(am|is|are|do|does|have|has)\b(?!\s+\w+ing|\s+\w+ed|\s+been)',
            'simple_past': r'\b(was|were|did|had)\b(?!\s+\w+ing|\s+been)',
            'simple_future': r'\b(will|shall)\s+\w+',
            'present_perfect': r'\b(have|has)\s+\w*ed\b|\b(have|has)\s+\w+en\b',
            'past_perfect': r'\bhad\s+\w*ed\b|\bhad\s+\w+en\b',
            'future_perfect': r'\bwill\s+have\s+\w*ed\b|\bwill\s+have\s+\w+en\b',
            'present_progressive': r'\b(am|is|are)\s+\w+ing\b',
            'past_progressive': r'\b(was|were)\s+\w+ing\b',
            'future_progressive': r'\bwill\s+be\s+\w+ing\b',
            'present_perfect_progressive': r'\b(have|has)\s+been\s+\w+ing\b',
            'past_passive': r'\b(was|were)\s+\w*ed\b|\b(was|were)\s+\w+en\b',
            'present_passive': r'\b(am|is|are)\s+\w*ed\b|\b(am|is|are)\s+\w+en\b'
        }

    def _init_phase3_emphasis(self):
        """Phase 3: å¼·èª¿æ§‹æ–‡åˆæœŸåŒ–"""
        self.emphasis_patterns = {
            'it_cleft': r'^It\s+(is|was)\s+(.+?)\s+(who|that|which)\s+(.+)$',
            'pseudo_cleft': r'^(What|Where|When|Why|How|All)\s+(.+?)\s+(is|was)\s+(.+)$',
            'do_emphasis': r'\b(do|does|did)\s+(\w+)\b',
            'exclamation': r'^(What|How|Such)\s+(.+?)!$',
            'repetition': r'\b(\w+),\s*\1\b',
            'fronting': r'^(This|That|Here|There|Never|Rarely)\s+'
        }

    def _init_phase4_advanced(self):
        """Phase 4: é«˜åº¦æ§‹æ–‡åˆæœŸåŒ–"""
        self.advanced_patterns = {
            'ellipsis': r'(.+?)\s+(can|could|will|would|did|does|do)\s+(\w+),?\s+and\s+(.+?)\s+(can|could|will|would|did|does|do)\s+(too|also)',
            'vp_ellipsis': r'(.+?)\s+(can|could|will|would|should|must|might|may)\s+(\w+),?\s+and\s+(.+?)\s+(can|could|will|would|should|must|might|may)\s+(too|also)',
            'np_ellipsis': r'(.+?)\s+(one|ones|some|many|few|several)\s*\.?$',
            'extraposition': r'^It\s+(is|was|seems|appears|happens)\s+(.+?)\s+(that|to)\s+(.+)$',
            'it_extraposition': r'^It\s+(is|was)\s+(clear|obvious|important|necessary|easy|hard|difficult)\s+(that|to)',
            'comparative': r'\b(more|less|\w+er)\s+than\b|\bas\s+\w+\s+as\b|\bthe\s+(most|\w+est)\b',
            'existential': r'^(There|Here)\s+(is|are|was|were|will\s+be|has\s+been|have\s+been)\s+',
            'conditional': r'^(If|When|Unless|Provided)\s+(.+?),\s*(.+)$',
            'real_conditional': r'^If\s+(.+?),\s*(.+?)\s+(will|can|may|might)\s+(.+)$',
            'unreal_conditional': r'^If\s+(.+?)\s+(were|had|could|would|might)\s+(.+?),\s*(.+)$',
            'concessive': r'^(Although|Though|Even\s+though|Despite|However)\s+',
            'correlative': r'^(Both|Either|Neither|Not\s+only)\s+(.+?)\s+(and|or|nor|but\s+also)\s+',
            'participle': r'^(Having|Being|Done|Finished)\s+(.+?),\s*(.+)$'
        }

    def analyze_sentence(self, sentence: str) -> GrammarAnalysisResult:
        """çµ±åˆæ–‡æ³•è§£æã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
        if not self.nlp:
            return self._create_error_result(sentence, "spaCyãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        
        print(f"ğŸ” çµ±åˆè§£æé–‹å§‹: {sentence}")
        doc = self.nlp(sentence)
        
        # Phaseåˆ¥è§£æçµæœã‚’ä¿å­˜
        phase_results = {}
        detected_patterns = []
        
        # å„ªå…ˆé †ä½ã«å¾“ã£ã¦å„Phaseã‚’å®Ÿè¡Œ
        for phase in self.detection_priority:
            if phase == 'phase4':
                result = self._analyze_phase4_advanced(sentence, doc)
            elif phase == 'phase3':
                result = self._analyze_phase3_emphasis(sentence, doc)
            elif phase == 'phase1':
                result = self._analyze_phase1_inversion(sentence, doc)
            elif phase == 'phase2':
                result = self._analyze_phase2_tense_aspect(sentence, doc)
            elif phase == 'phase0':
                result = self._analyze_phase0_basic_patterns(sentence, doc)
            
            phase_results[phase] = result
            if result['detected']:
                detected_patterns.extend(result['patterns'])
        
        # æœ€ã‚‚é‡è¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®š
        primary_grammar = self._determine_primary_grammar(detected_patterns)
        
        # è¤‡é›‘åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
        complexity_score = self._calculate_complexity(detected_patterns)
        
        # ç·åˆä¿¡é ¼åº¦è¨ˆç®—
        confidence = self._calculate_overall_confidence(detected_patterns)
        
        # Rephraseã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ
        rephrase_slots = self._generate_rephrase_slots(sentence, detected_patterns, doc)
        
        return GrammarAnalysisResult(
            sentence=sentence,
            detected_patterns=detected_patterns,
            primary_grammar=primary_grammar,
            complexity_score=complexity_score,
            confidence=confidence,
            components=self._extract_components(sentence, detected_patterns, doc),
            explanation=self._generate_explanation(detected_patterns, primary_grammar),
            phase_results=phase_results,
            rephrase_slots=rephrase_slots
        )

    def _analyze_phase1_inversion(self, sentence: str, doc) -> Dict[str, Any]:
        """Phase 1: å€’ç½®æ§‹æ–‡è§£æ"""
        patterns = []
        
        for inversion_type, triggers in self.inversion_triggers.items():
            for trigger in triggers:
                if sentence.lower().startswith(trigger.lower()):
                    patterns.append({
                        'type': f'{inversion_type}_inversion',
                        'trigger': trigger,
                        'confidence': 0.85,
                        'phase': 1,
                        'complexity': 4
                    })
                    break
        
        return {
            'detected': len(patterns) > 0,
            'patterns': patterns,
            'phase': 'inversion'
        }

    def _analyze_phase2_tense_aspect(self, sentence: str, doc) -> Dict[str, Any]:
        """Phase 2: æ™‚åˆ¶-ã‚¢ã‚¹ãƒšã‚¯ãƒˆè§£æ"""
        patterns = []
        
        for tense_type, pattern in self.tense_patterns.items():
            if re.search(pattern, sentence, re.IGNORECASE):
                patterns.append({
                    'type': tense_type,
                    'pattern': pattern,
                    'confidence': 0.90,
                    'phase': 2,
                    'complexity': 2
                })
        
        return {
            'detected': len(patterns) > 0,
            'patterns': patterns,
            'phase': 'tense_aspect'
        }

    def _analyze_phase3_emphasis(self, sentence: str, doc) -> Dict[str, Any]:
        """Phase 3: å¼·èª¿æ§‹æ–‡è§£æ"""
        patterns = []
        
        for emphasis_type, pattern in self.emphasis_patterns.items():
            match = re.search(pattern, sentence, re.IGNORECASE)
            if match:
                patterns.append({
                    'type': emphasis_type,
                    'matched_groups': match.groups(),
                    'confidence': 0.88,
                    'phase': 3,
                    'complexity': 4
                })
        
        return {
            'detected': len(patterns) > 0,
            'patterns': patterns,
            'phase': 'emphasis'
        }

    def _analyze_phase4_advanced(self, sentence: str, doc) -> Dict[str, Any]:
        """Phase 4: é«˜åº¦æ§‹æ–‡è§£æ"""
        patterns = []
        
        for advanced_type, pattern in self.advanced_patterns.items():
            match = re.search(pattern, sentence, re.IGNORECASE)
            if match:
                patterns.append({
                    'type': advanced_type,
                    'matched_groups': match.groups(),
                    'confidence': 0.82,
                    'phase': 4,
                    'complexity': 5
                })
        
        return {
            'detected': len(patterns) > 0,
            'patterns': patterns,
            'phase': 'advanced'
        }

    def _analyze_phase0_basic_patterns(self, sentence: str, doc) -> Dict[str, Any]:
        """Phase 0: åŸºæœ¬5æ–‡å‹è§£æ"""
        patterns = []
        
        # æ–‡ã®æ§‹é€ è§£æ
        root_verb = None
        subject = None
        objects = []
        complements = []
        
        for token in doc:
            if token.dep_ == 'ROOT' and token.pos_ == 'VERB':
                root_verb = token
            elif token.dep_ == 'nsubj':
                subject = token
            elif token.dep_ in ['dobj', 'pobj']:
                objects.append(token)
            elif token.dep_ in ['attr', 'acomp', 'xcomp']:
                complements.append(token)
        
        if not root_verb or not subject:
            return {'detected': False, 'patterns': [], 'phase': 'basic_patterns'}
        
        # æ–‡å‹åˆ¤å®š
        verb_lemma = root_verb.lemma_.lower()
        
        # ç¬¬5æ–‡å‹ (SVOC): S + V + O + C
        if len(objects) >= 1 and len(complements) >= 1 and verb_lemma in self.svoc_verbs:
            patterns.append({
                'type': 'svoc_pattern',
                'subject': subject.text,
                'verb': root_verb.text,
                'object': objects[0].text,
                'complement': complements[0].text,
                'confidence': 0.88,
                'phase': 0,
                'complexity': 3
            })
        
        # ç¬¬4æ–‡å‹ (SVOO): S + V + O1 + O2
        elif len(objects) >= 2 and verb_lemma in self.ditransitive_verbs:
            patterns.append({
                'type': 'svoo_pattern',
                'subject': subject.text,
                'verb': root_verb.text,
                'indirect_object': objects[0].text,
                'direct_object': objects[1].text,
                'confidence': 0.85,
                'phase': 0,
                'complexity': 3
            })
        
        # ç¬¬2æ–‡å‹ (SVC): S + V + C
        elif len(complements) >= 1 and verb_lemma in self.linking_verbs:
            patterns.append({
                'type': 'svc_pattern',
                'subject': subject.text,
                'verb': root_verb.text,
                'complement': complements[0].text,
                'confidence': 0.90,
                'phase': 0,
                'complexity': 2
            })
        
        # ç¬¬3æ–‡å‹ (SVO): S + V + O
        elif len(objects) >= 1:
            patterns.append({
                'type': 'svo_pattern',
                'subject': subject.text,
                'verb': root_verb.text,
                'object': objects[0].text,
                'confidence': 0.85,
                'phase': 0,
                'complexity': 2
            })
        
        # ç¬¬1æ–‡å‹ (SV): S + V
        else:
            patterns.append({
                'type': 'sv_pattern',
                'subject': subject.text,
                'verb': root_verb.text,
                'confidence': 0.80,
                'phase': 0,
                'complexity': 1
            })
        
        return {
            'detected': len(patterns) > 0,
            'patterns': patterns,
            'phase': 'basic_patterns'
        }

    def _determine_primary_grammar(self, patterns: List[Dict]) -> GrammarType:
        """æœ€é‡è¦æ–‡æ³•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ±ºå®š"""
        if not patterns:
            return GrammarType.NO_SPECIAL_GRAMMAR
        
        # è¤‡é›‘åº¦ã¨Phaseå„ªå…ˆåº¦ã§æ±ºå®š
        sorted_patterns = sorted(patterns, 
                               key=lambda x: (x.get('complexity', 0), x.get('phase', 0)), 
                               reverse=True)
        
        primary_type = sorted_patterns[0]['type']
        
        # æ–‡å­—åˆ—ã‹ã‚‰Enumã«å¤‰æ›
        for grammar_type in GrammarType:
            if grammar_type.value == primary_type:
                return grammar_type
        
        return GrammarType.BASIC_STRUCTURE

    def _calculate_complexity(self, patterns: List[Dict]) -> float:
        """è¤‡é›‘åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        if not patterns:
            return 1.0
        
        total_complexity = sum(p.get('complexity', 1) for p in patterns)
        pattern_bonus = len(patterns) * 0.5  # è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒœãƒ¼ãƒŠã‚¹
        
        return min(5.0, total_complexity + pattern_bonus)

    def _calculate_overall_confidence(self, patterns: List[Dict]) -> float:
        """ç·åˆä¿¡é ¼åº¦è¨ˆç®—"""
        if not patterns:
            return 0.0
        
        confidences = [p.get('confidence', 0.5) for p in patterns]
        return sum(confidences) / len(confidences)

    def _extract_components(self, sentence: str, patterns: List[Dict], doc) -> Dict[str, Any]:
        """æ–‡æ³•æ§‹æˆè¦ç´ æŠ½å‡º"""
        components = {
            'tokens': len(doc),
            'pos_tags': [token.pos_ for token in doc],
            'dependencies': [(token.text, token.dep_, token.head.text) for token in doc],
            'detected_patterns': len(patterns)
        }
        
        # ä¸»è¦ãªæ§‹æ–‡è¦ç´ ã‚’æŠ½å‡º
        subjects = [token.text for token in doc if token.dep_ == 'nsubj']
        objects = [token.text for token in doc if token.dep_ in ['dobj', 'pobj']]
        verbs = [token.text for token in doc if token.pos_ == 'VERB']
        
        components.update({
            'subjects': subjects,
            'objects': objects,
            'verbs': verbs
        })
        
        return components

    def _generate_explanation(self, patterns: List[Dict], primary_grammar: GrammarType) -> str:
        """è§£æèª¬æ˜æ–‡ç”Ÿæˆ"""
        if not patterns:
            return "åŸºæœ¬çš„ãªæ–‡æ§‹é€ ã§ã™ã€‚ç‰¹åˆ¥ãªæ–‡æ³•ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        explanations = []
        
        # Phaseåˆ¥èª¬æ˜
        phase_counts = {}
        for pattern in patterns:
            phase = f"Phase {pattern.get('phase', 0)}"
            phase_counts[phase] = phase_counts.get(phase, 0) + 1
        
        for phase, count in phase_counts.items():
            if phase == "Phase 0":
                explanations.append(f"åŸºæœ¬5æ–‡å‹ ({count}ãƒ‘ã‚¿ãƒ¼ãƒ³)")
            elif phase == "Phase 1":
                explanations.append(f"å€’ç½®æ§‹æ–‡ ({count}ãƒ‘ã‚¿ãƒ¼ãƒ³)")
            elif phase == "Phase 2":
                explanations.append(f"æ™‚åˆ¶-ã‚¢ã‚¹ãƒšã‚¯ãƒˆ ({count}ãƒ‘ã‚¿ãƒ¼ãƒ³)")
            elif phase == "Phase 3":
                explanations.append(f"å¼·èª¿æ§‹æ–‡ ({count}ãƒ‘ã‚¿ãƒ¼ãƒ³)")
            elif phase == "Phase 4":
                explanations.append(f"é«˜åº¦æ§‹æ–‡ ({count}ãƒ‘ã‚¿ãƒ¼ãƒ³)")
        
        primary_explanation = f"ä¸»è¦ãƒ‘ã‚¿ãƒ¼ãƒ³: {primary_grammar.value.replace('_', ' ').title()}"
        
        return f"{primary_explanation} | æ¤œå‡º: {', '.join(explanations)}"

    def _generate_rephrase_slots(self, sentence: str, patterns: List[Dict], doc) -> Dict[str, Any]:
        """Rephraseã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ"""
        slots = {
            'original_sentence': sentence,
            'word_count': len(sentence.split()),
            'grammar_complexity': self._calculate_complexity(patterns),
            'rephrase_difficulty': 'basic'
        }
        
        # è¤‡é›‘åº¦ã«åŸºã¥ã„ã¦Difficultyè¨­å®š
        complexity = slots['grammar_complexity']
        if complexity >= 4.0:
            slots['rephrase_difficulty'] = 'advanced'
        elif complexity >= 2.5:
            slots['rephrase_difficulty'] = 'intermediate'
        
        # æ–‡æ³•ã‚¿ã‚¤ãƒ—åˆ¥ã‚¹ãƒ­ãƒƒãƒˆ
        grammar_types = [p.get('type', '') for p in patterns]
        slots['grammar_types'] = grammar_types
        
        # åŸºæœ¬æ§‹æ–‡è¦ç´ 
        slots['subject'] = self._extract_subject(doc)
        slots['main_verb'] = self._extract_main_verb(doc)
        slots['object'] = self._extract_object(doc)
        
        return slots

    def _extract_subject(self, doc) -> str:
        """ä¸»èªæŠ½å‡º"""
        for token in doc:
            if token.dep_ == 'nsubj':
                return token.text
        return ""

    def _extract_main_verb(self, doc) -> str:
        """ä¸»å‹•è©æŠ½å‡º"""
        for token in doc:
            if token.dep_ == 'ROOT' and token.pos_ == 'VERB':
                return token.text
        return ""

    def _extract_object(self, doc) -> str:
        """ç›®çš„èªæŠ½å‡º"""
        for token in doc:
            if token.dep_ in ['dobj', 'pobj']:
                return token.text
        return ""

    def _create_error_result(self, sentence: str, error_msg: str) -> GrammarAnalysisResult:
        """ã‚¨ãƒ©ãƒ¼çµæœç”Ÿæˆ"""
        return GrammarAnalysisResult(
            sentence=sentence,
            detected_patterns=[],
            primary_grammar=GrammarType.NO_SPECIAL_GRAMMAR,
            complexity_score=0.0,
            confidence=0.0,
            components={},
            explanation=f"ã‚¨ãƒ©ãƒ¼: {error_msg}",
            phase_results={},
            rephrase_slots={}
        )

def test_unified_grammar_system():
    """çµ±åˆæ–‡æ³•ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
    master = UnifiedGrammarMaster()
    
    test_sentences = [
        # Phase 0: åŸºæœ¬5æ–‡å‹
        "I study English every day.",           # SVO (ç¬¬3æ–‡å‹)
        "She is a good teacher.",               # SVC (ç¬¬2æ–‡å‹)
        "He gave me a book.",                   # SVOO (ç¬¬4æ–‡å‹)
        "We made him happy.",                   # SVOC (ç¬¬5æ–‡å‹)
        "The birds sing.",                      # SV (ç¬¬1æ–‡å‹)
        
        # Phase 1: å€’ç½®
        "Never have I seen such a beautiful sunset.",
        "Were he here, he would help us.",
        
        # Phase 2: æ™‚åˆ¶-ã‚¢ã‚¹ãƒšã‚¯ãƒˆ  
        "She has been studying English for five years.",
        "The work will have been completed by tomorrow.",
        
        # Phase 3: å¼·èª¿
        "It is John who broke the window.",
        "What I need is rest.",
        
        # Phase 4: é«˜åº¦æ§‹æ–‡
        "John can sing, and Mary can too.",
        "It is clear that he is guilty.",
        
        # è¤‡åˆæ§‹æ–‡
        "Never before had such a beautiful garden been created by anyone!",
    ]
    
    print("ğŸ”§ çµ±åˆæ–‡æ³•ã‚·ã‚¹ãƒ†ãƒ ç·åˆãƒ†ã‚¹ãƒˆ")
    print("=" * 80)
    
    total_tests = len(test_sentences)
    successful_detections = 0
    
    for i, sentence in enumerate(test_sentences, 1):
        print(f"\nğŸ” ãƒ†ã‚¹ãƒˆ {i}/{total_tests}: {sentence}")
        
        try:
            result = master.analyze_sentence(sentence)
            
            print(f"   ğŸ“Š ä¸»è¦æ–‡æ³•: {result.primary_grammar.value}")
            print(f"   ğŸ¯ æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {len(result.detected_patterns)}")
            print(f"   ğŸ’ª è¤‡é›‘åº¦: {result.complexity_score:.1f}/5.0")
            print(f"   ğŸ“ˆ ä¿¡é ¼åº¦: {result.confidence:.2f}")
            print(f"   ğŸ’¡ èª¬æ˜: {result.explanation}")
            
            if result.detected_patterns:
                successful_detections += 1
                print("   ğŸ”¥ Phaseåˆ¥æ¤œå‡º:")
                for pattern in result.detected_patterns:
                    print(f"     - Phase {pattern.get('phase', '?')}: {pattern.get('type', 'unknown')}")
            
        except Exception as e:
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    print(f"\n" + "=" * 80)
    print(f"ğŸ“Š çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆçµæœ:")
    print(f"   ğŸ¯ æˆåŠŸæ¤œå‡º: {successful_detections}/{total_tests}")
    print(f"   ğŸ“ˆ æˆåŠŸç‡: {successful_detections/total_tests*100:.1f}%")
    print(f"   ğŸ”¥ å¯¾å¿œãƒ‘ã‚¿ãƒ¼ãƒ³: 45æ§‹æ–‡ (5 Phasesçµ±åˆ)")
    print(f"   ğŸ† æ–‡æ³•ã‚«ãƒãƒ¬ãƒƒã‚¸: 81.8% (45/55æ§‹æ–‡)")

if __name__ == "__main__":
    test_unified_grammar_system()
