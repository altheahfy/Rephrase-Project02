#!/usr/bin/env python3
"""åè©å¥ç½®æ›ãƒ­ã‚¸ãƒƒã‚¯ã®å•é¡Œã‚’è©³ç´°åˆ†æ"""

import spacy

def analyze_noun_phrase_problem():
    """åè©å¥ã®ç¯„å›²ç‰¹å®šå•é¡Œã‚’åˆ†æ"""
    
    nlp_spacy = spacy.load("en_core_web_sm")
    
    test_sentence = "The book that I read yesterday was interesting."
    
    print(f"ğŸ” åˆ†ææ–‡: {test_sentence}")
    print("=" * 50)
    
    doc = nlp_spacy(test_sentence)
    
    print("ğŸ” åè©å¥ (noun_chunks):")
    for chunk in doc.noun_chunks:
        print(f"  ğŸ“ '{chunk.text}' (root: {chunk.root.text}, start: {chunk.start}, end: {chunk.end})")
    
    print("\nğŸ” é–¢ä¿‚ç¯€ (relcl):")
    for token in doc:
        if token.dep_ == 'relcl':
            clause_tokens = list(token.subtree)
            clause_text = ' '.join([t.text for t in clause_tokens])
            print(f"  ğŸ“ relcl: '{clause_text}' (head: {token.head.text})")
            
            # ä¿®é£¾ã•ã‚Œã‚‹åè©ã®ç¯„å›²
            head_token = token.head
            print(f"  ğŸ“ ä¿®é£¾ã•ã‚Œã‚‹èª: '{head_token.text}' (idx: {head_token.i})")
            
            # åè©å¥å…¨ä½“ã®ç¯„å›²ã‚’ç‰¹å®š
            noun_phrase_start = None
            noun_phrase_end = None
            
            for chunk in doc.noun_chunks:
                if head_token.i >= chunk.start and head_token.i < chunk.end:
                    noun_phrase_start = chunk.start
                    noun_phrase_end = chunk.end
                    print(f"  ğŸ“ åè©å¥ç¯„å›²: tokens {noun_phrase_start}-{noun_phrase_end}")
                    print(f"  ğŸ“ åè©å¥å…¨ä½“: '{chunk.text}'")
                    break
    
    print("\nğŸ” æ­£ã—ã„ç½®æ›:")
    print("  âŒ é–“é•ã„: 'The book something was interesting.'")
    print("  âœ… æ­£è§£: 'Something was interesting.'")
    
    # æ­£ã—ã„ç½®æ›ã®å®Ÿè£…ä¾‹
    correct_replacement = implement_correct_replacement(doc)
    print(f"  ğŸ¯ å®Ÿè£…çµæœ: '{correct_replacement}'")

def implement_correct_replacement(doc):
    """æ­£ã—ã„åè©å¥å…¨ä½“ç½®æ›ã®å®Ÿè£…"""
    
    # é–¢ä¿‚ç¯€ã‚’å«ã‚€åè©å¥ã‚’ç‰¹å®š
    noun_phrases_to_replace = []
    
    for token in doc:
        if token.dep_ == 'relcl':
            # ä¿®é£¾ã•ã‚Œã‚‹åè©ã‚’ç‰¹å®š
            head_token = token.head
            
            # ãã®åè©ã‚’å«ã‚€åè©å¥å…¨ä½“ã‚’ç‰¹å®š
            for chunk in doc.noun_chunks:
                if head_token.i >= chunk.start and head_token.i < chunk.end:
                    noun_phrases_to_replace.append({
                        'start': chunk.start,
                        'end': chunk.end,
                        'text': chunk.text,
                        'replacement': 'Something'
                    })
                    break
    
    # ç½®æ›å®Ÿè¡Œ
    if noun_phrases_to_replace:
        # æœ€åˆã®åè©å¥ã®ã¿å‡¦ç†ï¼ˆã“ã®ä¾‹ã§ã¯1ã¤ã ã‘ï¼‰
        replacement = noun_phrases_to_replace[0]
        
        # ãƒˆãƒ¼ã‚¯ãƒ³å†æ§‹æˆ
        result_tokens = []
        i = 0
        while i < len(doc):
            if i == replacement['start']:
                result_tokens.append(replacement['replacement'])
                i = replacement['end']  # åè©å¥å…¨ä½“ã‚’ã‚¹ã‚­ãƒƒãƒ—
            else:
                result_tokens.append(doc[i].text)
                i += 1
        
        return ' '.join(result_tokens)
    
    return str(doc)

if __name__ == "__main__":
    analyze_noun_phrase_problem()
