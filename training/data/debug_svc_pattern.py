#!/usr/bin/env python3
"""
SVCæ–‡å‹ï¼ˆbeå‹•è©ï¼‹å½¢å®¹è©è£œèªï¼‰å•é¡Œã®è©³ç´°èª¿æŸ»
ã€ŒThe car is red.ã€ã§C1è£œèªãŒèªè­˜ã•ã‚Œãªã„åŸå› åˆ†æ
"""
from dynamic_grammar_mapper import DynamicGrammarMapper

def debug_svc_pattern():
    """SVCæ–‡å‹ã®è©³ç´°ãƒ‡ãƒãƒƒã‚°"""
    
    mapper = DynamicGrammarMapper()
    sentence = "The car is red."
    
    print("=== SVCæ–‡å‹ãƒ‡ãƒãƒƒã‚°åˆ†æ ===")
    print(f"å¯¾è±¡æ–‡: {sentence}\n")
    
    # è©³ç´°è§£æ
    result = mapper.analyze_sentence(sentence)
    
    print("ğŸ“Š ç¾åœ¨ã®èªè­˜çµæœ:")
    print(f"   ãƒ‘ã‚¿ãƒ¼ãƒ³: {result.get('pattern_detected', 'unknown')}")
    print(f"   æ–‡å‹: {result.get('sentence_type', 'unknown')}")
    print(f"   ã‚¹ãƒ­ãƒƒãƒˆ: {result.get('Slot', [])}")
    
    # å†…éƒ¨å‡¦ç†ã®è©³ç´°ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã€ä½ãƒ¬ãƒ™ãƒ«è§£æ
    doc = mapper.nlp(sentence)
    tokens = mapper._extract_tokens(doc)
    
    print(f"\nğŸ” Tokenåˆ†æ:")
    for i, token in enumerate(tokens):
        print(f"   {i}: '{token['text']}' - POS:{token['pos']}, TAG:{token['tag']}, DEP:{token['dep']}")
    
    # ã‚³ã‚¢è¦ç´ åˆ†æ
    core = mapper._identify_core_elements(tokens)
    print(f"\nğŸ¯ ã‚³ã‚¢è¦ç´ èªè­˜:")
    print(f"   ä¸»èª: {core.get('subject', 'ãªã—')}")
    print(f"   å‹•è©: {core.get('verb', {}).get('text', 'ãªã—') if core.get('verb') else 'ãªã—'}")
    print(f"   åŠ©å‹•è©: {core.get('auxiliary', {}).get('text', 'ãªã—') if core.get('auxiliary') else 'ãªã—'}")
    
    # æ–‡å‹åˆ¤å®š
    pattern = mapper._determine_sentence_pattern(core, tokens)
    print(f"\nğŸ“‹ æ–‡å‹åˆ¤å®š:")
    print(f"   åˆ¤å®šçµæœ: {pattern}")
    
    # æ–‡æ³•è¦ç´ å‰²ã‚Šå½“ã¦
    elements = mapper._assign_grammar_roles(tokens, pattern, core)
    print(f"\nâš™ï¸ æ–‡æ³•è¦ç´ å‰²ã‚Šå½“ã¦:")
    for element in elements:
        print(f"   {element.role}: '{element.text}' (ä¿¡é ¼åº¦: {element.confidence:.2f})")
    
    print(f"\nâŒ å•é¡Œåˆ†æ:")
    print("   æœŸå¾…: C1è£œèª 'red' ãŒèªè­˜ã•ã‚Œã‚‹")
    print("   å®Ÿéš›: C1è£œèªãŒèªè­˜ã•ã‚Œã¦ã„ãªã„")
    
    # 'red'ãƒˆãƒ¼ã‚¯ãƒ³ã®è©³ç´°åˆ†æ
    red_token = None
    for token in tokens:
        if token['text'].lower() == 'red':
            red_token = token
            break
    
    if red_token:
        print(f"\nğŸ” 'red'ãƒˆãƒ¼ã‚¯ãƒ³è©³ç´°:")
        print(f"   POS: {red_token['pos']}")
        print(f"   TAG: {red_token['tag']}")
        print(f"   DEP: {red_token['dep']}")
        print(f"   HEAD: {red_token.get('head', 'unknown')}")
    
    # beå‹•è©ã®è©³ç´°åˆ†æ
    is_token = None
    for token in tokens:
        if token['text'].lower() == 'is':
            is_token = token
            break
    
    if is_token:
        print(f"\nğŸ” 'is'ãƒˆãƒ¼ã‚¯ãƒ³è©³ç´°:")
        print(f"   POS: {is_token['pos']}")
        print(f"   TAG: {is_token['tag']}")
        print(f"   DEP: {is_token['dep']}")

if __name__ == "__main__":
    debug_svc_pattern()
