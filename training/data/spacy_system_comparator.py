#!/usr/bin/env python3
"""
spaCy vs CompleteRephraseParsingEngine æ¯”è¼ƒæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
å®Œå…¨ãªä¾å­˜é–¢ä¿‚æŠ½å‡ºã¨ç¾åœ¨ã‚·ã‚¹ãƒ†ãƒ ã®å·®åˆ†åˆ†æ
"""

import spacy
from CompleteRephraseParsingEngine import CompleteRephraseParsingEngine
import json
from typing import Dict, List, Set, Tuple, Any
from collections import defaultdict

class SpacySystemComparator:
    def __init__(self):
        print("âœ… spaCyèªå½™èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
        self.nlp = spacy.load("en_core_web_sm")
        self.engine = CompleteRephraseParsingEngine()
        self.dependency_stats = defaultdict(int)
        self.pos_stats = defaultdict(int)
        
    def extract_complete_spacy_analysis(self, sentence: str) -> Dict[str, Any]:
        """spaCyã«ã‚ˆã‚‹å®Œå…¨ãªä¾å­˜é–¢ä¿‚ãƒ»æ§‹é€ è§£æ"""
        doc = self.nlp(sentence)
        
        analysis = {
            'sentence': sentence,
            'tokens': [],
            'dependencies': {},
            'noun_phrases': [],
            'verb_phrases': [],
            'prepositional_phrases': [],
            'subjects': [],
            'objects': [],
            'modifiers': [],
            'auxiliaries': [],
            'complements': []
        }
        
        # å…¨ãƒˆãƒ¼ã‚¯ãƒ³è§£æ
        for token in doc:
            token_info = {
                'text': token.text,
                'lemma': token.lemma_,
                'pos': token.pos_,
                'tag': token.tag_,
                'dep': token.dep_,
                'head_text': token.head.text,
                'head_pos': token.head.pos_,
                'children': [child.text for child in token.children],
                'is_alpha': token.is_alpha,
                'is_stop': token.is_stop,
                'shape': token.shape_
            }
            analysis['tokens'].append(token_info)
            
            # çµ±è¨ˆæ›´æ–°
            self.dependency_stats[token.dep_] += 1
            self.pos_stats[token.pos_] += 1
            
        # ä¾å­˜é–¢ä¿‚åˆ¥åˆ†é¡
        for token in doc:
            dep_key = f"{token.dep_}_{token.pos_}"
            if dep_key not in analysis['dependencies']:
                analysis['dependencies'][dep_key] = []
            
            analysis['dependencies'][dep_key].append({
                'text': token.text,
                'lemma': token.lemma_,
                'head': token.head.text,
                'full_phrase': self._extract_spacy_phrase(token)
            })
            
            # æ–‡æ³•å½¹å‰²åˆ¥åˆ†é¡
            if token.dep_ in ['nsubj', 'nsubjpass', 'csubj', 'csubjpass']:
                analysis['subjects'].append({
                    'text': token.text,
                    'dep': token.dep_,
                    'full_phrase': self._extract_spacy_phrase(token)
                })
            elif token.dep_ in ['dobj', 'iobj', 'pobj']:
                analysis['objects'].append({
                    'text': token.text,
                    'dep': token.dep_,
                    'full_phrase': self._extract_spacy_phrase(token)
                })
            elif token.dep_ in ['amod', 'advmod', 'prep', 'poss', 'det']:
                analysis['modifiers'].append({
                    'text': token.text,
                    'dep': token.dep_,
                    'modifies': token.head.text,
                    'full_phrase': self._extract_spacy_phrase(token)
                })
            elif token.dep_ in ['aux', 'auxpass']:
                analysis['auxiliaries'].append({
                    'text': token.text,
                    'dep': token.dep_,
                    'assists': token.head.text
                })
            elif token.dep_ in ['acomp', 'xcomp', 'ccomp']:
                analysis['complements'].append({
                    'text': token.text,
                    'dep': token.dep_,
                    'full_phrase': self._extract_spacy_phrase(token)
                })
        
        return analysis
    
    def _extract_spacy_phrase(self, token) -> str:
        """spaCyãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰å®Œå…¨ãªãƒ•ãƒ¬ãƒ¼ã‚ºã‚’æŠ½å‡º"""
        if token.pos_ == 'NOUN':
            # åè©å¥æŠ½å‡º
            phrase_tokens = []
            
            # å·¦å´ä¿®é£¾èªï¼ˆæ‰€æœ‰æ ¼ã€å½¢å®¹è©ã€é™å®šè©ãªã©ï¼‰
            for child in token.children:
                if child.i < token.i and child.dep_ in ['det', 'amod', 'compound', 'nummod', 'poss']:
                    phrase_tokens.append(child)
            
            phrase_tokens.append(token)
            
            # å³å´ä¿®é£¾èª
            for child in token.children:
                if child.i > token.i and child.dep_ in ['amod', 'compound']:
                    phrase_tokens.append(child)
            
            phrase_tokens.sort(key=lambda x: x.i)
            return ' '.join([t.text for t in phrase_tokens])
            
        elif token.pos_ == 'VERB':
            # å‹•è©å¥æŠ½å‡ºï¼ˆåŠ©å‹•è©å«ã‚€ï¼‰
            phrase_tokens = []
            
            # åŠ©å‹•è©
            for child in token.children:
                if child.dep_ in ['aux', 'auxpass'] and child.i < token.i:
                    phrase_tokens.append(child)
            
            phrase_tokens.append(token)
            
            # å¥å‹•è©ç²’å­
            for child in token.children:
                if child.dep_ == 'prt':
                    phrase_tokens.append(child)
            
            phrase_tokens.sort(key=lambda x: x.i)
            return ' '.join([t.text for t in phrase_tokens])
        
        elif token.pos_ == 'ADP':  # å‰ç½®è©
            # å‰ç½®è©å¥æŠ½å‡º
            phrase_tokens = [token]
            for child in token.children:
                if child.dep_ == 'pobj':
                    phrase_tokens.append(child)
                    # å‰ç½®è©ã®ç›®çš„èªã®ä¿®é£¾èªã‚‚å«ã‚ã‚‹
                    phrase_tokens.extend([gc for gc in child.children 
                                        if gc.dep_ in ['det', 'amod', 'poss']])
            
            phrase_tokens.sort(key=lambda x: x.i)
            return ' '.join([t.text for t in phrase_tokens])
        
        else:
            return token.text
    
    def get_current_system_analysis(self, sentence: str) -> Dict[str, Any]:
        """ç¾åœ¨ã®CompleteRephraseParsingEngineã«ã‚ˆã‚‹è§£æ"""
        try:
            result = self.engine.analyze_sentence(sentence)
            return {
                'sentence': sentence,
                'parsing_successful': True,
                'slots': result.get('slots', {}),
                'rules_applied': result.get('rules_applied', []),
                'sentence_pattern': result.get('sentence_pattern', ''),
                'debug_info': result.get('debug_info', {})
            }
        except Exception as e:
            return {
                'sentence': sentence,
                'parsing_successful': False,
                'error': str(e),
                'slots': {},
                'rules_applied': [],
                'sentence_pattern': '',
                'debug_info': {}
            }
    
    def compare_systems(self, sentences: List[str]) -> Dict[str, Any]:
        """ä¸¡ã‚·ã‚¹ãƒ†ãƒ ã®åŒ…æ‹¬çš„æ¯”è¼ƒåˆ†æ"""
        comparison_results = {
            'sentences_analyzed': len(sentences),
            'spacy_coverage': {},
            'current_system_coverage': {},
            'gaps_identified': [],
            'dependency_distribution': dict(self.dependency_stats),
            'pos_distribution': dict(self.pos_stats),
            'detailed_comparisons': []
        }
        
        for sentence in sentences:
            print(f"\nğŸ” åˆ†æä¸­: '{sentence}'")
            
            # spaCyå®Œå…¨è§£æ
            spacy_analysis = self.extract_complete_spacy_analysis(sentence)
            
            # ç¾åœ¨ã‚·ã‚¹ãƒ†ãƒ è§£æ
            current_analysis = self.get_current_system_analysis(sentence)
            
            # æ¯”è¼ƒçµæœ
            comparison = self._compare_single_sentence(spacy_analysis, current_analysis)
            comparison_results['detailed_comparisons'].append(comparison)
            
            # ã‚®ãƒ£ãƒƒãƒ—ç‰¹å®š
            gaps = self._identify_gaps(spacy_analysis, current_analysis)
            comparison_results['gaps_identified'].extend(gaps)
        
        # å…¨ä½“çµ±è¨ˆ
        comparison_results['spacy_coverage'] = self._calculate_spacy_coverage()
        comparison_results['current_system_coverage'] = self._calculate_current_system_coverage(comparison_results['detailed_comparisons'])
        
        return comparison_results
    
    def _compare_single_sentence(self, spacy_analysis: Dict, current_analysis: Dict) -> Dict:
        """å˜ä¸€æ–‡ã®è©³ç´°æ¯”è¼ƒ"""
        return {
            'sentence': spacy_analysis['sentence'],
            'spacy_elements_count': len(spacy_analysis['tokens']),
            'current_slots_count': len(current_analysis['slots']),
            'spacy_dependencies': list(spacy_analysis['dependencies'].keys()),
            'current_rules_applied': current_analysis['rules_applied'],
            'spacy_subjects': spacy_analysis['subjects'],
            'spacy_objects': spacy_analysis['objects'],
            'spacy_modifiers': spacy_analysis['modifiers'],
            'current_slots': current_analysis['slots'],
            'parsing_successful': current_analysis['parsing_successful']
        }
    
    def _identify_gaps(self, spacy_analysis: Dict, current_analysis: Dict) -> List[Dict]:
        """ã‚·ã‚¹ãƒ†ãƒ é–“ã®ã‚®ãƒ£ãƒƒãƒ—ç‰¹å®š"""
        gaps = []
        
        # spaCyãŒæ¤œå‡ºã—ãŸãŒç¾åœ¨ã‚·ã‚¹ãƒ†ãƒ ãŒè¦‹é€ƒã—ãŸè¦ç´ 
        spacy_elements = set()
        for dep_key, elements in spacy_analysis['dependencies'].items():
            for element in elements:
                spacy_elements.add(f"{element['text']}_{dep_key}")
        
        current_elements = set()
        for slot, content in current_analysis['slots'].items():
            if content:  # ç©ºã§ãªã„å ´åˆã®ã¿
                current_elements.add(f"{content}_{slot}")
        
        # ç¾åœ¨ã‚·ã‚¹ãƒ†ãƒ ã§æœªæ¤œå‡ºã®ä¾å­˜é–¢ä¿‚
        for dep_key in spacy_analysis['dependencies']:
            if dep_key not in [f"{slot}" for slot in current_analysis['slots']]:
                gaps.append({
                    'type': 'missing_dependency',
                    'sentence': spacy_analysis['sentence'],
                    'dependency': dep_key,
                    'elements': spacy_analysis['dependencies'][dep_key]
                })
        
        return gaps
    
    def _calculate_spacy_coverage(self) -> Dict:
        """spaCyè§£æç¶²ç¾…æ€§çµ±è¨ˆ"""
        total_deps = sum(self.dependency_stats.values())
        total_pos = sum(self.pos_stats.values())
        
        return {
            'total_dependencies_found': total_deps,
            'unique_dependency_types': len(self.dependency_stats),
            'dependency_distribution': dict(self.dependency_stats),
            'total_pos_tags': total_pos,
            'unique_pos_types': len(self.pos_stats),
            'pos_distribution': dict(self.pos_stats)
        }
    
    def _calculate_current_system_coverage(self, comparisons: List[Dict]) -> Dict:
        """ç¾åœ¨ã‚·ã‚¹ãƒ†ãƒ ç¶²ç¾…æ€§çµ±è¨ˆ"""
        total_sentences = len(comparisons)
        successful_parses = sum(1 for c in comparisons if c['parsing_successful'])
        
        all_slots = set()
        all_rules = set()
        
        for comparison in comparisons:
            all_slots.update(comparison['current_slots'].keys())
            all_rules.update(comparison['current_rules_applied'])
        
        return {
            'total_sentences_analyzed': total_sentences,
            'successful_parses': successful_parses,
            'success_rate': (successful_parses / total_sentences * 100) if total_sentences > 0 else 0,
            'unique_slots_used': list(all_slots),
            'unique_rules_applied': list(all_rules),
            'slots_count': len(all_slots),
            'rules_count': len(all_rules)
        }
    
    def generate_comprehensive_report(self, comparison_results: Dict) -> str:
        """åŒ…æ‹¬çš„åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = []
        report.append("=" * 80)
        report.append("ğŸ”¬ spaCy vs CompleteRephraseParsingEngine åŒ…æ‹¬çš„æ¯”è¼ƒåˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
        report.append("=" * 80)
        
        # å…¨ä½“ã‚µãƒãƒªãƒ¼
        report.append(f"\nğŸ“Š åˆ†ææ¦‚è¦:")
        report.append(f"  åˆ†ææ–‡æ•°: {comparison_results['sentences_analyzed']}")
        report.append(f"  spaCyæ¤œå‡ºä¾å­˜é–¢ä¿‚ç¨®é¡: {comparison_results['spacy_coverage']['unique_dependency_types']}")
        report.append(f"  ç¾åœ¨ã‚·ã‚¹ãƒ†ãƒ æˆåŠŸç‡: {comparison_results['current_system_coverage']['success_rate']:.1f}%")
        report.append(f"  ç‰¹å®šã•ã‚ŒãŸã‚®ãƒ£ãƒƒãƒ—: {len(comparison_results['gaps_identified'])}")
        
        # spaCyç¶²ç¾…æ€§è©³ç´°
        report.append(f"\nğŸ¯ spaCyä¾å­˜é–¢ä¿‚åˆ†å¸ƒ:")
        for dep, count in sorted(comparison_results['dependency_distribution'].items(), key=lambda x: x[1], reverse=True):
            report.append(f"  {dep}: {count}å›")
        
        # ç¾åœ¨ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ
        report.append(f"\nğŸ”§ ç¾åœ¨ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ:")
        report.append(f"  ä½¿ç”¨ã‚¹ãƒ­ãƒƒãƒˆ: {comparison_results['current_system_coverage']['slots_count']}ç¨®é¡")
        report.append(f"  é©ç”¨ãƒ«ãƒ¼ãƒ«: {comparison_results['current_system_coverage']['rules_count']}ç¨®é¡")
        report.append(f"  ã‚¹ãƒ­ãƒƒãƒˆä¸€è¦§: {', '.join(comparison_results['current_system_coverage']['unique_slots_used'])}")
        
        # ã‚®ãƒ£ãƒƒãƒ—è©³ç´°
        if comparison_results['gaps_identified']:
            report.append(f"\nğŸš¨ ç‰¹å®šã•ã‚ŒãŸã‚®ãƒ£ãƒƒãƒ—:")
            gap_types = defaultdict(int)
            for gap in comparison_results['gaps_identified']:
                gap_types[gap['type']] += 1
                
            for gap_type, count in gap_types.items():
                report.append(f"  {gap_type}: {count}ä»¶")
        
        # è©³ç´°æ¯”è¼ƒï¼ˆæœ€åˆã®3æ–‡ã®ã¿è¡¨ç¤ºï¼‰
        report.append(f"\nğŸ“ è©³ç´°æ¯”è¼ƒã‚µãƒ³ãƒ—ãƒ« (æœ€åˆã®3æ–‡):")
        for i, comparison in enumerate(comparison_results['detailed_comparisons'][:3]):
            report.append(f"\n  ä¾‹æ–‡ {i+1}: '{comparison['sentence']}'")
            report.append(f"    spaCyè¦ç´ æ•°: {comparison['spacy_elements_count']}")
            report.append(f"    ç¾åœ¨ã‚¹ãƒ­ãƒƒãƒˆæ•°: {comparison['current_slots_count']}")
            report.append(f"    spaCyä¾å­˜é–¢ä¿‚: {', '.join(comparison['spacy_dependencies'][:5])}{'...' if len(comparison['spacy_dependencies']) > 5 else ''}")
            report.append(f"    ç¾åœ¨ãƒ«ãƒ¼ãƒ«: {', '.join(comparison['current_rules_applied'])}")
        
        report.append("\n" + "=" * 80)
        return '\n'.join(report)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ spaCy vs CompleteRephraseParsingEngine æ¯”è¼ƒæ¤œè¨¼é–‹å§‹")
    
    # ãƒ†ã‚¹ãƒˆæ–‡ã‚»ãƒƒãƒˆï¼ˆå¤šæ§˜ãªæ–‡æ³•æ§‹é€ ï¼‰
    test_sentences = [
        "He resembles his mother.",
        "She gave him a beautiful present.",
        "The big red car parked outside quickly.",
        "I will go to the store tomorrow.",
        "They have been working hard all day.",
        "The book on the table is very interesting.",
        "Turn the music down, please.",
        "If you study hard, you will succeed.",
        "Walking in the park is relaxing.",
        "The man who called yesterday is here."
    ]
    
    # æ¯”è¼ƒå™¨åˆæœŸåŒ–ãƒ»å®Ÿè¡Œ
    comparator = SpacySystemComparator()
    results = comparator.compare_systems(test_sentences)
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ»å‡ºåŠ›
    report = comparator.generate_comprehensive_report(results)
    print(report)
    
    # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    import datetime
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"spacy_system_comparison_{timestamp}.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è©³ç´°çµæœã‚’ä¿å­˜: {output_file}")
    print("ğŸ‰ æ¯”è¼ƒæ¤œè¨¼å®Œäº†ï¼")

if __name__ == "__main__":
    main()
