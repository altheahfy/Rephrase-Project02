"""
Confidence Calculator
çµ±ä¸€ä¿¡é ¼åº¦è¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ 

å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ä¸€è²«ã—ãŸconfidenceè¨ˆç®—ã‚’æä¾›
"""

from typing import Dict, List, Any
import logging
import re


class ConfidenceCalculator:
    """
    çµ±ä¸€ä¿¡é ¼åº¦è¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ 
    
    å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä¿¡é ¼åº¦ã‚’çµ±ä¸€çš„ã«è¨ˆç®—ã—ã€
    ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®å“è³ªã‚’ä¿è¨¼
    """
    
    def __init__(self):
        self.logger = logging.getLogger("ConfidenceCalculator")
        
        # åŸºæœ¬ä¿¡é ¼åº¦è¨­å®š
        self.BASE_CONFIDENCE = {
            'whose_ambiguous_verb': 0.95,
            'passive_voice': 0.90,
            'complex_relative': 0.85,
            'default': 0.80
        }
        
        # ä¿¡é ¼åº¦èª¿æ•´è¦å› 
        self.CONFIDENCE_FACTORS = {
            'pattern_match_strength': 0.3,    # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒå¼·åº¦
            'context_coherence': 0.25,        # æ–‡è„ˆä¸€è²«æ€§
            'grammar_rules': 0.25,            # æ–‡æ³•è¦å‰‡é©åˆæ€§
            'frequency_analysis': 0.20        # é »åº¦åˆ†æ
        }
        
    def calculate_pattern_confidence(self, pattern_type: str, detection_data: Dict) -> float:
        """
        ãƒ‘ã‚¿ãƒ¼ãƒ³ä¿¡é ¼åº¦è¨ˆç®—
        
        Args:
            pattern_type: ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—
            detection_data: æ¤œå‡ºãƒ‡ãƒ¼ã‚¿
            
        Returns:
            è¨ˆç®—ã•ã‚ŒãŸä¿¡é ¼åº¦ (0.0-1.0)
        """
        base_confidence = self.BASE_CONFIDENCE.get(pattern_type, self.BASE_CONFIDENCE['default'])
        
        # å„è¦å› ã®è¨ˆç®—
        pattern_strength = self._calculate_pattern_match_strength(detection_data)
        context_score = self._calculate_context_coherence(detection_data)
        grammar_score = self._calculate_grammar_rules_score(detection_data)
        frequency_score = self._calculate_frequency_score(detection_data)
        
        # é‡ã¿ä»˜ãå¹³å‡ã§æœ€çµ‚ä¿¡é ¼åº¦è¨ˆç®—
        weighted_score = (
            pattern_strength * self.CONFIDENCE_FACTORS['pattern_match_strength'] +
            context_score * self.CONFIDENCE_FACTORS['context_coherence'] +
            grammar_score * self.CONFIDENCE_FACTORS['grammar_rules'] +
            frequency_score * self.CONFIDENCE_FACTORS['frequency_analysis']
        )
        
        # ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦ã¨èª¿æ•´ã‚¹ã‚³ã‚¢ã®çµåˆ
        final_confidence = min(1.0, base_confidence + weighted_score * 0.2)
        
        self.logger.debug(
            f"ğŸ“Š ä¿¡é ¼åº¦è¨ˆç®— [{pattern_type}]: "
            f"base={base_confidence:.3f}, "
            f"pattern={pattern_strength:.3f}, "
            f"context={context_score:.3f}, "
            f"grammar={grammar_score:.3f}, "
            f"frequency={frequency_score:.3f}, "
            f"final={final_confidence:.3f}"
        )
        
        return final_confidence
        
    def _calculate_pattern_match_strength(self, detection_data: Dict) -> float:
        """
        ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒå¼·åº¦è¨ˆç®—
        
        æ­£è¦è¡¨ç¾ãƒãƒƒãƒæ•°ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´åº¦ãªã©
        """
        strength = 0.0
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒæ•°
        keywords_matched = detection_data.get('keywords_matched', 0)
        total_keywords = detection_data.get('total_keywords', 1)
        keyword_ratio = keywords_matched / total_keywords
        strength += keyword_ratio * 0.4
        
        # æ­£è¦è¡¨ç¾ãƒãƒƒãƒç²¾åº¦
        regex_matches = detection_data.get('regex_matches', [])
        if regex_matches:
            match_quality = sum(len(match) for match in regex_matches) / len(regex_matches)
            normalized_quality = min(1.0, match_quality / 10)  # æ­£è¦åŒ–
            strength += normalized_quality * 0.3
            
        # æ–‡æ³•æ§‹é€ ä¸€è‡´åº¦
        structure_match = detection_data.get('structure_match_score', 0.0)
        strength += structure_match * 0.3
        
        return min(1.0, strength)
        
    def _calculate_context_coherence(self, detection_data: Dict) -> float:
        """
        æ–‡è„ˆä¸€è²«æ€§è¨ˆç®—
        
        å‘¨è¾ºèªã¨ã®é–¢ä¿‚ã€æ„å‘³çš„æ•´åˆæ€§ãªã©
        """
        coherence = 0.0
        
        # å‘¨è¾ºèªã¨ã®å“è©ä¸€è²«æ€§
        pos_consistency = detection_data.get('pos_consistency_score', 0.0)
        coherence += pos_consistency * 0.4
        
        # æ„å‘³çš„é–¢é€£æ€§
        semantic_relatedness = detection_data.get('semantic_score', 0.0)
        coherence += semantic_relatedness * 0.3
        
        # èªé †è‡ªç„¶æ€§
        word_order_naturalness = detection_data.get('word_order_score', 0.0)
        coherence += word_order_naturalness * 0.3
        
        return min(1.0, coherence)
        
    def _calculate_grammar_rules_score(self, detection_data: Dict) -> float:
        """
        æ–‡æ³•è¦å‰‡é©åˆæ€§è¨ˆç®—
        
        è‹±èªæ–‡æ³•è¦å‰‡ã¸ã®é©åˆåº¦
        """
        grammar_score = 0.0
        
        # åŸºæœ¬æ–‡æ³•è¦å‰‡ãƒã‚§ãƒƒã‚¯
        basic_rules_passed = detection_data.get('basic_grammar_checks', 0)
        total_basic_rules = detection_data.get('total_basic_rules', 1)
        grammar_score += (basic_rules_passed / total_basic_rules) * 0.5
        
        # é«˜åº¦æ–‡æ³•è¦å‰‡ãƒã‚§ãƒƒã‚¯
        advanced_rules_passed = detection_data.get('advanced_grammar_checks', 0)
        total_advanced_rules = detection_data.get('total_advanced_rules', 1)
        grammar_score += (advanced_rules_passed / total_advanced_rules) * 0.3
        
        # ä¾‹å¤–ã‚±ãƒ¼ã‚¹å‡¦ç†
        exception_handling = detection_data.get('exception_handling_score', 0.0)
        grammar_score += exception_handling * 0.2
        
        return min(1.0, grammar_score)
        
    def _calculate_frequency_score(self, detection_data: Dict) -> float:
        """
        é »åº¦åˆ†æã‚¹ã‚³ã‚¢è¨ˆç®—
        
        é¡ä¼¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®éå»æˆåŠŸç‡ãªã©
        """
        frequency_score = 0.0
        
        # éå»ã®æˆåŠŸç‡
        historical_success_rate = detection_data.get('historical_success_rate', 0.0)
        frequency_score += historical_success_rate * 0.6
        
        # é¡ä¼¼ãƒ‘ã‚¿ãƒ¼ãƒ³é »åº¦
        similar_pattern_frequency = detection_data.get('similar_pattern_freq', 0.0)
        frequency_score += similar_pattern_frequency * 0.4
        
        return min(1.0, frequency_score)
        
    def combine_pattern_confidences(self, pattern_confidences: List[float]) -> float:
        """
        è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä¿¡é ¼åº¦çµåˆ
        
        Args:
            pattern_confidences: å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä¿¡é ¼åº¦ãƒªã‚¹ãƒˆ
            
        Returns:
            çµåˆã•ã‚ŒãŸä¿¡é ¼åº¦
        """
        if not pattern_confidences:
            return 0.0
            
        if len(pattern_confidences) == 1:
            return pattern_confidences[0]
            
        # é‡ã¿ä»˜ãå¹³å‡ï¼ˆé«˜ã„ä¿¡é ¼åº¦ã«ã‚ˆã‚Šå¤§ããªé‡ã¿ï¼‰
        weights = [conf ** 2 for conf in pattern_confidences]  # äºŒä¹—ã§é‡ã¿ä»˜ã‘
        weighted_sum = sum(conf * weight for conf, weight in zip(pattern_confidences, weights))
        total_weight = sum(weights)
        
        combined_confidence = weighted_sum / total_weight if total_weight > 0 else 0.0
        
        self.logger.debug(
            f"ğŸ”— ä¿¡é ¼åº¦çµåˆ: "
            f"å€‹åˆ¥={[f'{c:.3f}' for c in pattern_confidences]}, "
            f"çµåˆå¾Œ={combined_confidence:.3f}"
        )
        
        return combined_confidence
        
    def validate_confidence_threshold(self, confidence: float, threshold: float) -> bool:
        """
        ä¿¡é ¼åº¦é–¾å€¤æ¤œè¨¼
        
        Args:
            confidence: è¨ˆç®—ã•ã‚ŒãŸä¿¡é ¼åº¦
            threshold: æœ€å°é–¾å€¤
            
        Returns:
            é–¾å€¤ã‚’æº€ãŸã™ã‹ã©ã†ã‹
        """
        is_valid = confidence >= threshold
        
        if is_valid:
            self.logger.debug(f"âœ… ä¿¡é ¼åº¦OK: {confidence:.3f} >= {threshold:.3f}")
        else:
            self.logger.debug(f"âŒ ä¿¡é ¼åº¦ä¸è¶³: {confidence:.3f} < {threshold:.3f}")
            
        return is_valid
