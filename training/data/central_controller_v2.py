"""
æ–°ä¸­å¤®ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  (Phase 6a)
çœŸã®ä¸­å¤®ç®¡ç†ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚‹æ–‡æ³•è§£æã‚·ã‚¹ãƒ†ãƒ 

è¨­è¨ˆåŸå‰‡:
1. ç›£ç£çš„ç«‹å ´: CentralControllerãŒå…¨ã¦ã‚’æŠŠæ¡ãƒ»çµ±åˆ¶
2. æƒ…å ±åé›†: å…¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‹ã‚‰ä¸¦è¡Œã—ã¦æƒ…å ±åé›†
3. çµ±åˆåˆ¤æ–­: ä¸­å¤®ã§ã®æœ€çµ‚åˆ¤æ–­ã«ã‚ˆã‚‹å‡¦ç†æ±ºå®š
4. å”åŠ›èª¿æ•´: å¿…è¦æ™‚ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼é–“å”åŠ›ã®èª¿æ•´
5. å“è³ªä¿è¨¼: ãƒãƒƒãƒ†ã‚£ãƒ³ã‚°ãƒ»æ¬ è½ã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯
"""

import spacy
import json
import os
import time
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

# æ—¢å­˜ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆæ®µéšçš„ç§»è¡Œã®ãŸã‚ï¼‰
from basic_five_pattern_handler import BasicFivePatternHandler
from modal_handler import ModalHandler
from omitted_relative_pronoun_handler import OmittedRelativePronounHandler


class AnalysisConfidence(Enum):
    """åˆ†æä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ«"""
    HIGH = "high"
    MEDIUM = "medium" 
    LOW = "low"
    UNKNOWN = "unknown"


@dataclass
class HandlerReport:
    """ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‹ã‚‰ã®æƒ…å ±ãƒ¬ãƒãƒ¼ãƒˆ"""
    handler_name: str
    confidence: float  # 0.0-1.0
    detected_patterns: List[str]
    boundary_info: Optional[Dict[str, Any]] = None
    cooperation_needs: List[str] = None
    metadata: Dict[str, Any] = None
    processing_notes: List[str] = None

    def __post_init__(self):
        if self.cooperation_needs is None:
            self.cooperation_needs = []
        if self.metadata is None:
            self.metadata = {}
        if self.processing_notes is None:
            self.processing_notes = []


@dataclass 
class IntegratedAnalysis:
    """çµ±åˆåˆ†æçµæœ"""
    primary_grammar: str
    secondary_grammars: List[str]
    confidence_score: float
    handler_reports: Dict[str, HandlerReport]
    cooperation_plan: Dict[str, Any]
    quality_checks: Dict[str, bool]


class CentralControllerV2:
    """
    æ–°ä¸­å¤®ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  - çœŸã®ä¸­å¤®ç®¡ç†ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
    
    æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨ã®ä¸¦è¡Œé‹ç”¨ã‚’å‰æã¨ã—ãŸæ®µéšçš„ç§»è¡Œã‚·ã‚¹ãƒ†ãƒ 
    """
    
    def __init__(self, config_file: str = 'central_controller_config.json'):
        """åˆæœŸåŒ–: æ–°ã‚·ã‚¹ãƒ†ãƒ ç”¨ã®è¨­å®š"""
        self.nlp = spacy.load('en_core_web_sm')
        self.config = self._load_config(config_file)
        
        # Phase 6a: æœ€å°ã‚»ãƒƒãƒˆã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã§é–‹å§‹ï¼ˆæ¦‚å¿µå®Ÿè¨¼ï¼‰
        self.active_handlers = self._initialize_poc_handlers()
        
        # çµ±åˆãƒ«ãƒ¼ãƒ«è¨­å®š
        self.integration_rules = self.config.get('central_management', {})
        
        print("ğŸ¯ Central Controller V2 åˆæœŸåŒ–å®Œäº† - æ–°ä¸­å¤®ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ")
        print(f"ğŸ“Š ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼æ•°: {len(self.active_handlers)}")
        
    def _load_config(self, config_file: str) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆæ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨å…±é€šï¼‰"""
        if os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
                return {}
        return {}
    
    def _initialize_poc_handlers(self) -> Dict[str, Any]:
        """Phase 6a: æ¦‚å¿µå®Ÿè¨¼ç”¨ã®æœ€å°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚»ãƒƒãƒˆ"""
        handlers = {}
        
        # åŸºæœ¬çš„ãªãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‹ã‚‰é–‹å§‹ï¼ˆæ—¢å­˜ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã«åˆã‚ã›ã¦ï¼‰
        try:
            # BasicFivePatternHandlerã¯å¼•æ•°ãªã—ã§åˆæœŸåŒ–
            handlers['basic_five_pattern'] = BasicFivePatternHandler()
            print("âœ… BasicFivePatternHandler åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"âš ï¸ BasicFivePatternHandler åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            
        try:
            # ModalHandlerã¯nlpã‚’å¼•æ•°ã«å–ã‚‹
            handlers['modal'] = ModalHandler(self.nlp)
            print("âœ… ModalHandler åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"âš ï¸ ModalHandler åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            
        try:
            # RelativeClauseHandlerã¯ç©ºè¾æ›¸ã‚’å¼•æ•°ã«å–ã‚‹
            handlers['relative_clause'] = OmittedRelativePronounHandler({})
            print("âœ… RelativeClauseHandler åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"âš ï¸ RelativeClauseHandler åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            
        print(f"âœ… POC ãƒãƒ³ãƒ‰ãƒ©ãƒ¼åˆæœŸåŒ–å®Œäº†: {len(handlers)}å€‹")
        return handlers

    def process_sentence(self, sentence):
        """æ–‡ã®ç·åˆåˆ†æã¨åˆ†è§£ã‚’å®Ÿè¡Œï¼ˆunified_test.pyäº’æ›ï¼‰"""
        try:
            start_time = time.time()
            
            # ã‚¹ãƒ†ãƒƒãƒ—1: ä¸¦è¡Œæƒ…å ±åé›†
            handler_reports = self._collect_handler_reports(sentence)
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: çµ±åˆåˆ¤æ–­
            primary_handler, cooperation_plan = self._make_integration_decision(handler_reports)
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£å®Ÿè¡Œ
            slots_result = self._execute_slot_decomposition(sentence, primary_handler, handler_reports)
            
            # ã‚¹ãƒ†ãƒƒãƒ—4: å“è³ªä¿è¨¼
            quality_result = self._quality_assurance(handler_reports, primary_handler)
            
            processing_time = time.time() - start_time
            
            return {
                'main_slots': slots_result.get('main_slots', {}),
                'sub_slots': slots_result.get('sub_slots', {}),
                'detected_grammar': [primary_handler] if primary_handler else [],
                'confidence': handler_reports.get(primary_handler, {}).get('confidence', 0.0),
                'v2_metadata': {
                    'handler_reports': len(handler_reports),
                    'cooperation_plan': cooperation_plan,
                    'quality_checks': quality_result,
                    'processing_time': processing_time
                }
            }
            
        except Exception as e:
            print(f"âŒ CentralControllerV2ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {
                'main_slots': {},
                'sub_slots': {},
                'detected_grammar': [],
                'confidence': 0.0,
                'v2_metadata': {
                    'error': str(e)
                }
            }
    
    def _collect_handler_reports(self, sentence):
        """ä¸¦è¡Œæƒ…å ±åé›†ï¼ˆæ–°ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰"""
        reports = {}
        
        for handler_name, handler in self.active_handlers.items():
            try:
                report = self._get_handler_report(handler_name, handler, sentence)
                reports[handler_name] = {
                    'confidence': report.confidence,
                    'patterns': report.detected_patterns,
                    'metadata': report.metadata
                }
            except Exception as e:
                print(f"âš ï¸ {handler_name} ãƒ¬ãƒãƒ¼ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                reports[handler_name] = {
                    'confidence': 0.0,
                    'patterns': [],
                    'metadata': {'error': str(e)}
                }
        
        return reports
    
    def _make_integration_decision(self, handler_reports):
        """çµ±åˆåˆ¤æ–­ï¼ˆæ–°ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰"""
        if not handler_reports:
            return None, {}
        
        # æœ€ã‚‚ä¿¡é ¼åº¦ã®é«˜ã„ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’é¸æŠ
        best_handler = max(handler_reports.keys(), 
                          key=lambda h: handler_reports[h]['confidence'])
        
        cooperation_plan = {
            'primary': best_handler,
            'strategy': 'single_handler'
        }
        
        return best_handler, cooperation_plan
    
    def _execute_slot_decomposition(self, sentence, primary_handler, handler_reports):
        """ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£å®Ÿè¡Œï¼ˆRephraseãƒ«ãƒ¼ãƒ«æº–æ‹ ï¼‰"""
        if not primary_handler or primary_handler not in self.active_handlers:
            return {'main_slots': {}, 'sub_slots': {}}
        
        handler = self.active_handlers[primary_handler]
        
        try:
            if primary_handler == 'basic_five_pattern':
                # BasicFivePatternHandlerã‚’ä½¿ç”¨ã—ã¦Rephraseã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£å®Ÿè¡Œ
                # æ­£ã—ã„ãƒ¡ã‚½ãƒƒãƒ‰åã¯'process'
                result = handler.process(sentence)
                if result and result.get('success', False):
                    # æ—¢å­˜ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®çµæœã‚’V2å½¢å¼ã«å¤‰æ›
                    # BasicFivePatternHandlerã¯'slots'ã‚­ãƒ¼ã«çµæœã‚’æ ¼ç´
                    all_slots = result.get('slots', {})
                    
                    # main_slotsã¨sub_slotsã‚’åˆ†é›¢
                    main_slots = {}
                    sub_slots = {}
                    
                    for key, value in all_slots.items():
                        if key.startswith('sub-'):
                            sub_slots[key] = value
                        else:
                            main_slots[key] = value
                    
                    print(f"ğŸ” BasicFivePatternçµæœå¤‰æ›: main_slots={main_slots}, sub_slots={sub_slots}")
                    
                    return {
                        'main_slots': main_slots,
                        'sub_slots': sub_slots
                    }
                else:
                    print(f"âš ï¸ BasicFivePatternHandlerçµæœ: {result}")
                    # å¤±æ•—æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
                    return self._basic_slot_decomposition(sentence)
            
            # ä»–ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®å ´åˆï¼ˆå¾Œã§å®Ÿè£…ï¼‰
            elif primary_handler == 'modal':
                # ModalHandlerã‚‚åŸºæœ¬çš„ã«ã¯BasicFivePatternã¨åŒæ§˜ã®å‡¦ç†
                # ç¾åœ¨ã¯POCãªã®ã§ç°¡æ˜“å®Ÿè£…
                return self._basic_slot_decomposition(sentence)
            
            elif primary_handler == 'relative_clause':
                # é–¢ä¿‚ç¯€ã®å ´åˆã¯ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå‡¦ç†ãŒé‡è¦
                # ç¾åœ¨ã¯POCãªã®ã§ç°¡æ˜“å®Ÿè£…
                return self._basic_slot_decomposition(sentence)
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£
            return self._basic_slot_decomposition(sentence)
            
        except Exception as e:
            print(f"âš ï¸ ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ã‚¨ãƒ©ãƒ¼ ({primary_handler}): {e}")
            return {'main_slots': {}, 'sub_slots': {}}
    
    def _extract_sub_slots_from_legacy_result(self, legacy_result):
        """æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®çµæœã‹ã‚‰ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã‚’æŠ½å‡º"""
        sub_slots = {}
        
        # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®slotsã‹ã‚‰ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆï¼ˆsub-ã§å§‹ã¾ã‚‹ï¼‰ã‚’æŠ½å‡º
        all_slots = legacy_result.get('slots', {})
        for key, value in all_slots.items():
            if key.startswith('sub-') and value:  # sub-ã§å§‹ã¾ã‚‹éç©ºã‚¹ãƒ­ãƒƒãƒˆ
                sub_slots[key] = value
        
        return sub_slots
    
    def _basic_slot_decomposition(self, sentence):
        """åŸºæœ¬çš„ãªã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ï¼ˆRephraseãƒ«ãƒ¼ãƒ«ç°¡æ˜“ç‰ˆï¼‰"""
        # spaCyã§åŸºæœ¬çš„ãªè§£æ
        doc = self.nlp(sentence)
        
        main_slots = {}
        sub_slots = {}
        
        # ä¸»èªæ¤œå‡º
        subject = None
        verb = None
        objects = []
        modifiers = []
        
        for token in doc:
            if token.dep_ == 'nsubj':  # ä¸»èª
                subject = token.text
            elif token.dep_ == 'ROOT' and token.pos_ in ['VERB', 'AUX']:  # å‹•è©
                verb = token.text
            elif token.dep_ in ['dobj', 'iobj']:  # ç›®çš„èª
                objects.append(token.text)
            elif token.dep_ in ['acomp', 'attr']:  # è£œèª
                if 'C1' not in main_slots:
                    main_slots['C1'] = token.text
            elif token.dep_ == 'advmod':  # å‰¯è©
                modifiers.append(token.text)
        
        # Rephraseãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦é…ç½®
        if subject:
            main_slots['S'] = subject
        if verb:
            main_slots['V'] = verb
        
        # ç›®çš„èªé…ç½®
        if objects:
            if len(objects) >= 1:
                main_slots['O1'] = objects[0]
            if len(objects) >= 2:
                main_slots['O2'] = objects[1]
        
        # ä¿®é£¾èªé…ç½®ï¼ˆå€‹æ•°ãƒ™ãƒ¼ã‚¹ãƒ«ãƒ¼ãƒ«ï¼‰
        if modifiers:
            if len(modifiers) == 1:
                main_slots['M2'] = modifiers[0]
            elif len(modifiers) == 2:
                main_slots['M1'] = modifiers[0]
                main_slots['M3'] = modifiers[1]
            elif len(modifiers) >= 3:
                main_slots['M1'] = modifiers[0]
                main_slots['M2'] = modifiers[1]
                main_slots['M3'] = modifiers[2]
        
        return {
            'main_slots': main_slots,
            'sub_slots': sub_slots
        }
    
    def _quality_assurance(self, handler_reports, primary_handler):
        """å“è³ªä¿è¨¼ãƒã‚§ãƒƒã‚¯"""
        return {
            'has_primary_grammar': bool(primary_handler),
            'confidence_acceptable': handler_reports.get(primary_handler, {}).get('confidence', 0) > 0.3,
            'no_critical_conflicts': True,
            'text_coverage_adequate': True
        }

    def analyze_grammar_structure_v2(self, text: str) -> Dict[str, Any]:
        """
        æ–°ä¸­å¤®ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹æ–‡æ³•è§£æï¼ˆunified_test.pyäº’æ›ï¼‰
        
        Returns:
            process_sentenceã¨åŒã˜å½¢å¼ã®çµæœ
        """
        print(f"\nğŸ”¬ æ–°ã‚·ã‚¹ãƒ†ãƒ åˆ†æé–‹å§‹: '{text}'")
        
        # process_sentenceãƒ¡ã‚½ãƒƒãƒ‰ã‚’ãã®ã¾ã¾å‘¼ã³å‡ºã—
        return self.process_sentence(text)
    
    def _collect_all_handler_reports(self, text: str) -> Dict[str, HandlerReport]:
        """å…¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‹ã‚‰ä¸¦è¡Œæƒ…å ±åé›†"""
        reports = {}
        
        for handler_name, handler in self.active_handlers.items():
            try:
                # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‹ã‚‰æƒ…å ±ãƒ¬ãƒãƒ¼ãƒˆã‚’å–å¾—ï¼ˆåˆ¤æ–­ã¯ã•ã›ãªã„ï¼‰
                report = self._get_handler_report(handler_name, handler, text)
                reports[handler_name] = report
                
                print(f"ğŸ“‹ {handler_name}: ä¿¡é ¼åº¦={report.confidence:.2f}, ãƒ‘ã‚¿ãƒ¼ãƒ³={report.detected_patterns}")
                
            except Exception as e:
                print(f"âš ï¸ {handler_name} ãƒ¬ãƒãƒ¼ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                reports[handler_name] = HandlerReport(
                    handler_name=handler_name,
                    confidence=0.0,
                    detected_patterns=[],
                    processing_notes=[f"Error: {str(e)}"]
                )
        
        return reports
    
    def _get_handler_report(self, handler_name: str, handler: Any, text: str) -> HandlerReport:
        """å€‹åˆ¥ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‹ã‚‰æƒ…å ±ãƒ¬ãƒãƒ¼ãƒˆå–å¾—"""
        
        # æ—¢å­˜ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®æƒ…å ±ã‚’æ–°å½¢å¼ã«å¤‰æ›
        if handler_name == 'basic_five_pattern':
            # BasicFivePatternHandlerã®å ´åˆ
            confidence = 0.8 if self._has_basic_structure(text) else 0.3
            patterns = ['five_pattern'] if confidence > 0.5 else []
            
        elif handler_name == 'modal':
            # ModalHandlerã®å ´åˆ
            modal_info = handler.detect_modal_structure(text)
            confidence = 0.9 if modal_info.get('has_modal', False) else 0.1
            patterns = ['modal'] if modal_info.get('has_modal', False) else []
            
        elif handler_name == 'relative_clause':
            # RelativeClauseHandlerã®å ´åˆ
            doc = self.nlp(text)
            has_relative = any(token.text.lower() in ['who', 'which', 'that', 'whose', 'whom'] 
                             for token in doc)
            confidence = 0.7 if has_relative else 0.2
            patterns = ['relative_clause'] if has_relative else []
            
        else:
            # æœªçŸ¥ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
            confidence = 0.0
            patterns = []
        
        return HandlerReport(
            handler_name=handler_name,
            confidence=confidence,
            detected_patterns=patterns,
            metadata={'text_length': len(text)}
        )
    
    def _has_basic_structure(self, text: str) -> bool:
        """åŸºæœ¬æ§‹é€ ã®å­˜åœ¨ç¢ºèªï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        doc = self.nlp(text)
        has_verb = any(token.pos_ in ['VERB', 'AUX'] for token in doc)
        has_noun = any(token.pos_ in ['NOUN', 'PRON', 'PROPN'] for token in doc)
        return has_verb and has_noun
    
    def _integrate_handler_reports(self, reports: Dict[str, HandlerReport], text: str) -> IntegratedAnalysis:
        """ä¸­å¤®ã§ã®çµ±åˆåˆ¤æ–­"""
        
        # ä¿¡é ¼åº¦ã«åŸºã¥ã„ã¦ä¸»è¦æ–‡æ³•é …ç›®ã‚’æ±ºå®š
        valid_reports = {name: report for name, report in reports.items() 
                        if report.confidence > 0.5}
        
        if not valid_reports:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€ã‚‚ä¿¡é ¼åº¦ã®é«˜ã„ã‚‚ã®ã‚’é¸æŠ
            if reports:
                best_report = max(reports.values(), key=lambda r: r.confidence)
                primary_grammar = best_report.handler_name
                confidence_score = best_report.confidence
            else:
                primary_grammar = 'unknown'
                confidence_score = 0.0
            secondary_grammars = []
        else:
            # æœ€ã‚‚ä¿¡é ¼åº¦ã®é«˜ã„ã‚‚ã®ã‚’ä¸»è¦æ–‡æ³•ã«
            best_report = max(valid_reports.values(), key=lambda r: r.confidence)
            primary_grammar = best_report.handler_name
            confidence_score = best_report.confidence
            
            # æ®‹ã‚Šã‚’å‰¯æ¬¡æ–‡æ³•ã«
            secondary_grammars = [name for name, report in valid_reports.items() 
                                if name != primary_grammar]
        
        print(f"ğŸ¯ çµ±åˆåˆ¤æ–­: ä¸»è¦={primary_grammar} (ä¿¡é ¼åº¦={confidence_score:.2f}), å‰¯æ¬¡={secondary_grammars}")
        
        return IntegratedAnalysis(
            primary_grammar=primary_grammar,
            secondary_grammars=secondary_grammars,
            confidence_score=confidence_score,
            handler_reports=reports,
            cooperation_plan={},
            quality_checks={'basic_validation': True}
        )
    
    def _requires_collaboration(self, analysis: IntegratedAnalysis) -> bool:
        """å”åŠ›ãŒå¿…è¦ã‹ã©ã†ã‹ã®åˆ¤æ–­"""
        # Phase 6a: ç°¡æ˜“ç‰ˆï¼ˆå¾Œã§æ‹¡å¼µï¼‰
        return len(analysis.secondary_grammars) > 1
    
    def _coordinate_handlers(self, analysis: IntegratedAnalysis, text: str) -> IntegratedAnalysis:
        """ãƒãƒ³ãƒ‰ãƒ©ãƒ¼é–“å”åŠ›ã®èª¿æ•´"""
        # Phase 6a: åŸºæœ¬å®Ÿè£…ï¼ˆå¾Œã§æ‹¡å¼µï¼‰
        print(f"ğŸ¤ å”åŠ›èª¿æ•´å®Ÿè¡Œ: {analysis.primary_grammar} + {analysis.secondary_grammars}")
        
        # å”åŠ›è¨ˆç”»ã‚’æ›´æ–°
        analysis.cooperation_plan = {
            'strategy': 'parallel_processing',
            'involved_handlers': [analysis.primary_grammar] + analysis.secondary_grammars,
            'coordination_notes': ['Phase 6a basic coordination']
        }
        
        return analysis
    
    def _validate_final_result(self, analysis: IntegratedAnalysis, text: str) -> IntegratedAnalysis:
        """æœ€çµ‚å“è³ªä¿è¨¼"""
        
        # åŸºæœ¬çš„ãªå“è³ªãƒã‚§ãƒƒã‚¯
        quality_checks = {
            'has_primary_grammar': bool(analysis.primary_grammar),
            'confidence_acceptable': analysis.confidence_score > 0.3,
            'no_critical_conflicts': True,  # å¾Œã§å®Ÿè£…
            'text_coverage_adequate': True   # å¾Œã§å®Ÿè£…
        }
        
        analysis.quality_checks = quality_checks
        
        all_passed = all(quality_checks.values())
        print(f"âœ… å“è³ªä¿è¨¼: {'åˆæ ¼' if all_passed else 'è¦æ³¨æ„'} - {quality_checks}")
        
        return analysis
    
    def _convert_to_legacy_format(self, analysis: IntegratedAnalysis) -> List[str]:
        """æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ äº’æ›å½¢å¼ã«å¤‰æ›"""
        result = []
        
        if analysis.primary_grammar and analysis.primary_grammar != 'unknown':
            result.append(analysis.primary_grammar)
        
        result.extend(analysis.secondary_grammars)
        
        return result
    
    def compare_with_legacy_system(self, text: str, legacy_controller) -> Dict[str, Any]:
        """æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨ã®æ¯”è¼ƒåˆ†æ"""
        
        # æ–°ã‚·ã‚¹ãƒ†ãƒ ã§åˆ†æ
        v2_result = self.analyze_grammar_structure_v2(text)
        
        # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã§åˆ†æ
        try:
            legacy_result = legacy_controller.analyze_grammar_structure(text)
        except Exception as e:
            legacy_result = []
            print(f"âš ï¸ æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æ¯”è¼ƒåˆ†æ
        comparison = {
            'text': text,
            'v2_system': {
                'result': v2_result['legacy_format'],
                'confidence': v2_result['v2_result'].confidence_score,
                'handlers_used': len(v2_result['v2_result'].handler_reports)
            },
            'legacy_system': {
                'result': legacy_result,
                'confidence': None,  # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã«ã¯ä¿¡é ¼åº¦ãªã—
                'handlers_used': None
            },
            'differences': {
                'result_match': v2_result['legacy_format'] == legacy_result,
                'v2_extra': set(v2_result['legacy_format']) - set(legacy_result),
                'legacy_extra': set(legacy_result) - set(v2_result['legacy_format'])
            }
        }
        
        print(f"\nğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æ¯”è¼ƒçµæœ:")
        print(f"   æ–°ã‚·ã‚¹ãƒ†ãƒ : {v2_result['legacy_format']}")
        print(f"   æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ : {legacy_result}")
        print(f"   ä¸€è‡´: {comparison['differences']['result_match']}")
        
        return comparison


def test_new_system():
    """æ–°ã‚·ã‚¹ãƒ†ãƒ ã®åŸºæœ¬å‹•ä½œãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª æ–°ä¸­å¤®ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    # æ–°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    controller_v2 = CentralControllerV2()
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
    test_cases = [
        "I can speak English.",
        "The book that I read was interesting.",
        "What did you see?",
        "I wish I knew the answer."
    ]
    
    for text in test_cases:
        print(f"\n--- ãƒ†ã‚¹ãƒˆ: '{text}' ---")
        result = controller_v2.analyze_grammar_structure_v2(text)
        print(f"çµæœ: {result['legacy_format']}")
        print(f"ä¿¡é ¼åº¦: {result['v2_result'].confidence_score:.2f}")


if __name__ == "__main__":
    test_new_system()
