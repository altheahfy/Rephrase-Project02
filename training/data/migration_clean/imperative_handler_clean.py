"""
ImperativeHandler - å®Œå…¨ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ç‰ˆ
Clean Version with Zero Hardcoding for New Workspace Migration

æ—¢å­˜ImperativeHandlerã®å…¨æ©Ÿèƒ½ã‚’ç¶­æŒã—ãªãŒã‚‰ã€
ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Œå…¨ã«é™¤å»ã—ãŸæ±ç”¨ç‰ˆ

ä¸»ãªæ”¹å–„ç‚¹:
- å›ºå®šå‘½ä»¤å‹•è© â†’ å‹•çš„ãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ
- å›ºå®šå‘½ä»¤å½¢å¼ â†’ è¨­å®šå¯èƒ½å‘½ä»¤åˆ†é¡
- å›ºå®šèªé †ãƒ‘ã‚¿ãƒ¼ãƒ³ â†’ æ±ç”¨çµ±èªè§£æ
- æ¨™æº–åŒ–ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹æº–æ‹ 
"""

import spacy
import json
import os
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from abc import ABC, abstractmethod


@dataclass
class ImperativePattern:
    """å‘½ä»¤æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©"""
    pattern_type: str
    verb_forms: List[str] = field(default_factory=list)
    subject_patterns: List[str] = field(default_factory=list)
    sentence_patterns: List[str] = field(default_factory=list)
    pos_patterns: List[str] = field(default_factory=list)
    dependency_patterns: List[str] = field(default_factory=list)
    confidence_weight: float = 1.0


@dataclass
class ImperativeConfiguration:
    """å‘½ä»¤æ–‡ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š"""
    imperative_patterns: Dict[str, ImperativePattern] = field(default_factory=dict)
    politeness_analysis: Dict[str, Any] = field(default_factory=dict)
    confidence_settings: Dict[str, float] = field(default_factory=dict)
    command_rules: Dict[str, List[str]] = field(default_factory=dict)


class GenericImperativeAnalyzer:
    """æ±ç”¨å‘½ä»¤æ–‡è§£æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, config: ImperativeConfiguration):
        self.config = config
        self.nlp = spacy.load('en_core_web_sm')
    
    def analyze_imperative_structure(self, doc) -> Dict[str, Any]:
        """æ±ç”¨å‘½ä»¤æ–‡æ§‹é€ è§£æ"""
        # å‘½ä»¤æ–‡å€™è£œã®æ¤œå‡º
        imperative_candidates = self._detect_imperative_candidates(doc)
        
        if not imperative_candidates:
            return {'imperatives': [], 'confidence': 0.0}
        
        # å‘½ä»¤æ–‡ã®è©³ç´°è§£æ
        analyzed_imperatives = []
        for candidate in imperative_candidates:
            imperative_analysis = self._analyze_imperative_details(candidate, doc)
            if imperative_analysis:
                analyzed_imperatives.append(imperative_analysis)
        
        # ä¿¡é ¼åº¦è¨ˆç®—
        confidence = self._calculate_imperative_confidence(analyzed_imperatives)
        
        return {
            'imperatives': analyzed_imperatives,
            'confidence': confidence,
            'analysis_method': 'pattern_based_generic'
        }
    
    def _detect_imperative_candidates(self, doc) -> List[Dict[str, Any]]:
        """å‘½ä»¤æ–‡å€™è£œã®æ¤œå‡º"""
        candidates = []
        
        for pattern_name, pattern in self.config.imperative_patterns.items():
            # æ–‡ãƒ¬ãƒ™ãƒ«ã§ã®å‘½ä»¤æ–‡æ¤œå‡º
            imperative_indicators = self._find_imperative_indicators(doc, pattern)
            
            for indicator in imperative_indicators:
                candidate = self._create_imperative_candidate(indicator, pattern_name, pattern, doc)
                if candidate:
                    candidates.append(candidate)
        
        return candidates
    
    def _find_imperative_indicators(self, doc, pattern: ImperativePattern) -> List[Dict[str, Any]]:
        """å‘½ä»¤æ–‡æŒ‡æ¨™ã®æ¤œå‡º"""
        indicators = []
        
        # æ–‡ã®é–‹å§‹å‹•è©ã‚’ãƒã‚§ãƒƒã‚¯
        root_verb = self._find_root_verb(doc)
        
        if root_verb and self._matches_imperative_pattern(root_verb, pattern, doc):
            indicators.append({
                'type': 'root_verb',
                'token': root_verb,
                'pattern_match': True
            })
        
        # å‘½ä»¤æ–‡ç‰¹æœ‰ã®æ§‹é€ ã‚’ãƒã‚§ãƒƒã‚¯
        structural_indicators = self._find_structural_indicators(doc, pattern)
        indicators.extend(structural_indicators)
        
        return indicators
    
    def _find_root_verb(self, doc):
        """ãƒ«ãƒ¼ãƒˆå‹•è©ã®æ¤œå‡º"""
        for token in doc:
            if token.dep_ == 'ROOT' and token.pos_ == 'VERB':
                return token
        return None
    
    def _matches_imperative_pattern(self, token, pattern: ImperativePattern, doc) -> bool:
        """å‘½ä»¤æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°"""
        # å‹•è©å½¢æ…‹ã®ãƒã‚§ãƒƒã‚¯
        verb_match = self._check_verb_form(token, pattern)
        
        # ä¸»èªã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        subject_match = self._check_subject_pattern(token, pattern, doc)
        
        # å“è©ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        pos_match = not pattern.pos_patterns or token.pos_ in pattern.pos_patterns
        
        # ä¾å­˜é–¢ä¿‚ãƒãƒƒãƒãƒ³ã‚°
        dep_match = not pattern.dependency_patterns or token.dep_ in pattern.dependency_patterns
        
        return verb_match and subject_match and pos_match and dep_match
    
    def _check_verb_form(self, token, pattern: ImperativePattern) -> bool:
        """å‹•è©å½¢æ…‹ã®ãƒã‚§ãƒƒã‚¯"""
        # å‹•è©ã®æ´»ç”¨å½¢ã‚’ãƒã‚§ãƒƒã‚¯
        verb_form = self._analyze_verb_form(token)
        
        if not pattern.verb_forms:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã€åŸºæœ¬å½¢ã‚’ãƒã‚§ãƒƒã‚¯
            return verb_form['is_base_form']
        
        return any(form in pattern.verb_forms for form in verb_form['forms'])
    
    def _analyze_verb_form(self, token) -> Dict[str, Any]:
        """å‹•è©å½¢æ…‹ã®åˆ†æ"""
        return {
            'text': token.text,
            'lemma': token.lemma_,
            'tag': token.tag_,
            'is_base_form': token.tag_ in ['VB', 'VBP'],  # åŸºæœ¬å½¢ã¾ãŸã¯ç¾åœ¨å½¢
            'is_imperative_form': token.tag_ == 'VB' and token.i == 0,  # æ–‡é ­ã®åŸºæœ¬å½¢
            'forms': [token.text.lower(), token.lemma_.lower(), token.tag_]
        }
    
    def _check_subject_pattern(self, verb_token, pattern: ImperativePattern, doc) -> bool:
        """ä¸»èªãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒã‚§ãƒƒã‚¯"""
        # æ˜ç¤ºçš„ä¸»èªã®æ¤œå‡º
        explicit_subject = self._find_explicit_subject(verb_token, doc)
        
        if not pattern.subject_patterns:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³æœªæŒ‡å®šã®å ´åˆã€ä¸»èªãªã—ã¾ãŸã¯"you"ã‚’è¨±å¯
            return not explicit_subject or explicit_subject.lemma_.lower() == 'you'
        
        if explicit_subject:
            return explicit_subject.lemma_.lower() in pattern.subject_patterns
        else:
            return 'implicit' in pattern.subject_patterns
    
    def _find_explicit_subject(self, verb_token, doc):
        """æ˜ç¤ºçš„ä¸»èªã®æ¤œå‡º"""
        for child in verb_token.children:
            if child.dep_ == 'nsubj':
                return child
        return None
    
    def _find_structural_indicators(self, doc, pattern: ImperativePattern) -> List[Dict[str, Any]]:
        """æ§‹é€ çš„æŒ‡æ¨™ã®æ¤œå‡º"""
        indicators = []
        
        # å¥èª­ç‚¹ã«ã‚ˆã‚‹å‘½ä»¤æ–‡åˆ¤å®š
        if self._has_imperative_punctuation(doc):
            indicators.append({
                'type': 'punctuation',
                'token': doc[-1] if doc else None,
                'pattern_match': True
            })
        
        # èªé †ã«ã‚ˆã‚‹åˆ¤å®š
        if self._has_imperative_word_order(doc):
            indicators.append({
                'type': 'word_order',
                'token': doc[0] if doc else None,
                'pattern_match': True
            })
        
        return indicators
    
    def _has_imperative_punctuation(self, doc) -> bool:
        """å‘½ä»¤æ–‡ã®å¥èª­ç‚¹ãƒã‚§ãƒƒã‚¯"""
        if not doc:
            return False
        
        last_token = doc[-1]
        return last_token.text in ['!', '.'] and last_token.pos_ == 'PUNCT'
    
    def _has_imperative_word_order(self, doc) -> bool:
        """å‘½ä»¤æ–‡ã®èªé †ãƒã‚§ãƒƒã‚¯"""
        if not doc:
            return False
        
        # æ–‡é ­ãŒå‹•è©ã®å ´åˆ
        first_token = doc[0]
        if first_token.pos_ == 'VERB' and first_token.dep_ == 'ROOT':
            # ä¸»èªãŒçœç•¥ã•ã‚Œã¦ã„ã‚‹ã‹ã€å¾Œç¶šã™ã‚‹å ´åˆ
            has_pre_subject = any(token.dep_ == 'nsubj' and token.i < first_token.i for token in doc)
            return not has_pre_subject
        
        return False
    
    def _create_imperative_candidate(self, indicator: Dict[str, Any], pattern_name: str, pattern: ImperativePattern, doc) -> Optional[Dict[str, Any]]:
        """å‘½ä»¤æ–‡å€™è£œã®ä½œæˆ"""
        # å‘½ä»¤å‹•è©ã®ç‰¹å®š
        command_verb = self._identify_command_verb(indicator, doc)
        
        if not command_verb:
            return None
        
        # å‘½ä»¤æ–‡ã®æ§‹é€ åˆ†æ
        structure_analysis = self._analyze_imperative_structure(command_verb, doc)
        
        # ä¸å¯§åº¦åˆ†æ
        politeness_analysis = self._analyze_politeness(doc)
        
        # å‘½ä»¤ã‚¿ã‚¤ãƒ—ã®åˆ†æ
        command_type_analysis = self._analyze_command_type(command_verb, doc)
        
        return {
            'indicator': indicator,
            'command_verb': command_verb,
            'pattern_type': pattern_name,
            'structure_analysis': structure_analysis,
            'politeness_analysis': politeness_analysis,
            'command_type_analysis': command_type_analysis,
            'confidence_weight': pattern.confidence_weight
        }
    
    def _identify_command_verb(self, indicator: Dict[str, Any], doc):
        """å‘½ä»¤å‹•è©ã®ç‰¹å®š"""
        if indicator['type'] == 'root_verb':
            return indicator['token']
        
        # ãã®ä»–ã®æŒ‡æ¨™ã‹ã‚‰å‹•è©ã‚’ç‰¹å®š
        for token in doc:
            if token.pos_ == 'VERB' and token.dep_ == 'ROOT':
                return token
        
        return None
    
    def _analyze_imperative_structure(self, command_verb, doc) -> Dict[str, Any]:
        """å‘½ä»¤æ–‡æ§‹é€ ã®åˆ†æ"""
        structure_info = {
            'verb': {
                'text': command_verb.text,
                'lemma': command_verb.lemma_,
                'index': command_verb.i
            },
            'subject': None,
            'objects': [],
            'complements': [],
            'adverbials': [],
            'negation': None
        }
        
        # å„è¦ç´ ã®åˆ†æ
        for child in command_verb.children:
            if child.dep_ == 'nsubj':
                structure_info['subject'] = {
                    'text': child.text,
                    'lemma': child.lemma_,
                    'index': child.i,
                    'explicit': True
                }
            elif child.dep_ in ['dobj', 'iobj']:
                structure_info['objects'].append({
                    'text': child.text,
                    'lemma': child.lemma_,
                    'dependency': child.dep_,
                    'index': child.i
                })
            elif child.dep_ in ['xcomp', 'ccomp', 'attr']:
                structure_info['complements'].append({
                    'text': child.text,
                    'lemma': child.lemma_,
                    'dependency': child.dep_,
                    'index': child.i
                })
            elif child.dep_ in ['advmod', 'npadvmod']:
                structure_info['adverbials'].append({
                    'text': child.text,
                    'lemma': child.lemma_,
                    'dependency': child.dep_,
                    'index': child.i
                })
            elif child.dep_ == 'neg':
                structure_info['negation'] = {
                    'text': child.text,
                    'lemma': child.lemma_,
                    'index': child.i
                }
        
        # æš—é»™ã®ä¸»èªï¼ˆyouï¼‰ã®è£œå®Œ
        if not structure_info['subject']:
            structure_info['subject'] = {
                'text': '(you)',
                'lemma': 'you',
                'index': -1,
                'explicit': False
            }
        
        return structure_info
    
    def _analyze_politeness(self, doc) -> Dict[str, Any]:
        """ä¸å¯§åº¦ã®åˆ†æ"""
        politeness_info = {
            'level': 'neutral',
            'markers': [],
            'score': 0.5
        }
        
        # ä¸å¯§åº¦ãƒãƒ¼ã‚«ãƒ¼ã®æ¤œå‡º
        politeness_markers = self.config.politeness_analysis.get('markers', {})
        
        for token in doc:
            lemma = token.lemma_.lower()
            
            # ä¸å¯§è¡¨ç¾ã®ãƒã‚§ãƒƒã‚¯
            for level, markers in politeness_markers.items():
                if lemma in markers:
                    politeness_info['markers'].append({
                        'text': token.text,
                        'level': level,
                        'index': token.i
                    })
        
        # ä¸å¯§åº¦ãƒ¬ãƒ™ãƒ«ã®æ±ºå®š
        politeness_info['level'], politeness_info['score'] = self._determine_politeness_level(politeness_info['markers'], doc)
        
        return politeness_info
    
    def _determine_politeness_level(self, markers: List[Dict[str, Any]], doc) -> Tuple[str, float]:
        """ä¸å¯§åº¦ãƒ¬ãƒ™ãƒ«ã®æ±ºå®š"""
        if not markers:
            # ãƒãƒ¼ã‚«ãƒ¼ãªã—ã®å ´åˆã€å¥èª­ç‚¹ã§åˆ¤å®š
            if doc and doc[-1].text == '!':
                return 'direct', 0.2
            else:
                return 'neutral', 0.5
        
        # ãƒãƒ¼ã‚«ãƒ¼ãƒ™ãƒ¼ã‚¹ã®åˆ¤å®š
        level_scores = {'polite': 0, 'neutral': 0, 'direct': 0}
        
        for marker in markers:
            level = marker['level']
            if level in level_scores:
                level_scores[level] += 1
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ãƒ¬ãƒ™ãƒ«ã‚’é¸æŠ
        max_level = max(level_scores.items(), key=lambda x: x[1])[0]
        
        # ã‚¹ã‚³ã‚¢è¨ˆç®—
        total_markers = sum(level_scores.values())
        score_map = {'polite': 0.8, 'neutral': 0.5, 'direct': 0.2}
        
        if total_markers > 0:
            score = score_map.get(max_level, 0.5)
        else:
            score = 0.5
        
        return max_level, score
    
    def _analyze_command_type(self, command_verb, doc) -> Dict[str, Any]:
        """å‘½ä»¤ã‚¿ã‚¤ãƒ—ã®åˆ†æ"""
        command_type_info = {
            'type': 'unknown',
            'semantic_category': 'unknown',
            'urgency': 'normal'
        }
        
        # å‹•è©ã®æ„å‘³åˆ†æ
        verb_lemma = command_verb.lemma_.lower()
        command_type_info['semantic_category'] = self._classify_command_semantics(verb_lemma)
        
        # å‘½ä»¤ã‚¿ã‚¤ãƒ—ã®æ±ºå®š
        command_type_info['type'] = self._determine_command_type(command_verb, doc)
        
        # ç·Šæ€¥åº¦ã®åˆ†æ
        command_type_info['urgency'] = self._analyze_urgency(doc)
        
        return command_type_info
    
    def _classify_command_semantics(self, verb_lemma: str) -> str:
        """å‘½ä»¤å‹•è©ã®æ„å‘³åˆ†é¡"""
        # è¨­å®šãƒ™ãƒ¼ã‚¹ã®åˆ†é¡
        semantic_groups = self.config.politeness_analysis.get('semantic_groups', {})
        
        for category, verbs in semantic_groups.items():
            if verb_lemma in verbs:
                return category
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ†é¡
        return self._default_semantic_classification(verb_lemma)
    
    def _default_semantic_classification(self, verb_lemma: str) -> str:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ„å‘³åˆ†é¡"""
        if verb_lemma in ['go', 'come', 'move', 'run', 'walk']:
            return 'motion'
        elif verb_lemma in ['take', 'bring', 'give', 'put']:
            return 'transfer'
        elif verb_lemma in ['do', 'make', 'create', 'build']:
            return 'action'
        elif verb_lemma in ['say', 'tell', 'speak', 'talk']:
            return 'communication'
        elif verb_lemma in ['stop', 'start', 'continue', 'finish']:
            return 'control'
        else:
            return 'general'
    
    def _determine_command_type(self, command_verb, doc) -> str:
        """å‘½ä»¤ã‚¿ã‚¤ãƒ—ã®æ±ºå®š"""
        # å¦å®šå‘½ä»¤ã®ãƒã‚§ãƒƒã‚¯
        if any(child.dep_ == 'neg' for child in command_verb.children):
            return 'prohibition'
        
        # Let'sæ§‹æ–‡ã®ãƒã‚§ãƒƒã‚¯
        if doc and doc[0].lemma_.lower() == 'let':
            return 'suggestion'
        
        # è³ªå•å½¢å‘½ä»¤ã®ãƒã‚§ãƒƒã‚¯
        if doc and doc[-1].text == '?':
            return 'request_question'
        
        return 'direct_command'
    
    def _analyze_urgency(self, doc) -> str:
        """ç·Šæ€¥åº¦ã®åˆ†æ"""
        # æ„Ÿå˜†ç¬¦ã«ã‚ˆã‚‹åˆ¤å®š
        if doc and doc[-1].text == '!':
            return 'urgent'
        
        # ç·Šæ€¥åº¦ã‚’ç¤ºã™èªå½™ã®å­˜åœ¨
        urgency_words = ['now', 'immediately', 'quickly', 'hurry', 'urgent']
        
        for token in doc:
            if token.lemma_.lower() in urgency_words:
                return 'urgent'
        
        return 'normal'
    
    def _analyze_imperative_details(self, candidate: Dict[str, Any], doc) -> Optional[Dict[str, Any]]:
        """å‘½ä»¤æ–‡ã®è©³ç´°è§£æ"""
        # ä¿¡é ¼åº¦è¨ˆç®—
        confidence = self._calculate_individual_confidence(candidate)
        
        if confidence < 0.3:  # ä½ä¿¡é ¼åº¦ã¯é™¤å¤–
            return None
        
        command_verb = candidate['command_verb']
        
        return {
            'imperative': {
                'verb': {
                    'text': command_verb.text,
                    'lemma': command_verb.lemma_,
                    'index': command_verb.i
                },
                'full_text': doc.text
            },
            'structure': candidate['structure_analysis'],
            'politeness': candidate['politeness_analysis'],
            'command_type': candidate['command_type_analysis'],
            'confidence': confidence,
            'pattern_type': candidate['pattern_type']
        }
    
    def _calculate_individual_confidence(self, candidate: Dict[str, Any]) -> float:
        """å€‹åˆ¥å‘½ä»¤æ–‡ã®ä¿¡é ¼åº¦è¨ˆç®—"""
        base_confidence = 0.4
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒã®ä¿¡é ¼åº¦
        base_confidence += 0.2 * candidate['confidence_weight']
        
        # å‹•è©ã®ä½ç½®ï¼ˆæ–‡é ­ã®å ´åˆé«˜ã„ï¼‰
        command_verb = candidate['command_verb']
        if command_verb.i == 0:
            base_confidence += 0.2
        
        # ä¸»èªã®çœç•¥ï¼ˆå‘½ä»¤æ–‡ã®ç‰¹å¾´ï¼‰
        structure = candidate['structure_analysis']
        if not structure['subject'] or not structure['subject']['explicit']:
            base_confidence += 0.2
        
        # å¥èª­ç‚¹ã«ã‚ˆã‚‹ç¢ºèª
        indicator = candidate['indicator']
        if indicator['type'] == 'punctuation':
            base_confidence += 0.1
        
        return min(1.0, base_confidence)
    
    def _calculate_imperative_confidence(self, analyzed_imperatives: List[Dict[str, Any]]) -> float:
        """å…¨ä½“ã®å‘½ä»¤æ–‡è§£æä¿¡é ¼åº¦"""
        if not analyzed_imperatives:
            return 0.0
        
        total_confidence = sum(imp['confidence'] for imp in analyzed_imperatives)
        return min(1.0, total_confidence / len(analyzed_imperatives))


class ImperativeHandlerClean:
    """
    å‘½ä»¤æ–‡å‡¦ç†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œå…¨é™¤å»ç‰ˆ
    
    ç‰¹å¾´:
    - è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©
    - æ±ç”¨çš„å‘½ä»¤æ–‡æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    - å‹•çš„ä¸å¯§åº¦åˆ†æ
    - å®Œå…¨ãªæ‹¡å¼µæ€§
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆæœŸåŒ–"""
        self.nlp = spacy.load('en_core_web_sm')
        self.config = self._load_configuration(config_path)
        self.analyzer = GenericImperativeAnalyzer(self.config)
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼æƒ…å ±
        self.handler_info = {
            'name': 'ImperativeHandlerClean',
            'version': 'clean_v1.0',
            'hardcoding_level': 'zero'
        }
    
    def _load_configuration(self, config_path: Optional[str]) -> ImperativeConfiguration:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
                return self._parse_config_data(config_data)
        else:
            return self._create_default_configuration()
    
    def _create_default_configuration(self) -> ImperativeConfiguration:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®ä½œæˆ"""
        return ImperativeConfiguration(
            imperative_patterns={
                'direct_imperative': ImperativePattern(
                    pattern_type='direct_imperative',
                    verb_forms=['base_form', 'VB'],
                    subject_patterns=['implicit', 'you'],
                    sentence_patterns=['verb_initial'],
                    pos_patterns=['VERB'],
                    dependency_patterns=['ROOT'],
                    confidence_weight=1.2
                ),
                'polite_imperative': ImperativePattern(
                    pattern_type='polite_imperative',
                    verb_forms=['base_form', 'VB'],
                    subject_patterns=['implicit', 'you'],
                    sentence_patterns=['please_pattern', 'modal_pattern'],
                    pos_patterns=['VERB'],
                    dependency_patterns=['ROOT'],
                    confidence_weight=1.1
                )
            },
            politeness_analysis={
                'markers': {
                    'polite': ['please', 'kindly', 'would', 'could', 'may'],
                    'neutral': ['just', 'simply'],
                    'direct': ['now', 'immediately', 'quick']
                },
                'semantic_groups': {
                    'motion': ['go', 'come', 'move', 'run', 'walk'],
                    'transfer': ['take', 'bring', 'give', 'put', 'send'],
                    'action': ['do', 'make', 'create', 'build', 'work'],
                    'communication': ['say', 'tell', 'speak', 'talk', 'call'],
                    'control': ['stop', 'start', 'continue', 'finish', 'wait']
                }
            },
            confidence_settings={
                'minimum_confidence': 0.3,
                'high_confidence': 0.8,
                'imperative_bonus': 0.2
            }
        )
    
    def _parse_config_data(self, config_data: Dict) -> ImperativeConfiguration:
        """è¨­å®šãƒ‡ãƒ¼ã‚¿ã®è§£æ"""
        imperative_patterns = {}
        for name, data in config_data.get('imperative_patterns', {}).items():
            imperative_patterns[name] = ImperativePattern(
                pattern_type=data.get('pattern_type', name),
                verb_forms=data.get('verb_forms', []),
                subject_patterns=data.get('subject_patterns', []),
                sentence_patterns=data.get('sentence_patterns', []),
                pos_patterns=data.get('pos_patterns', []),
                dependency_patterns=data.get('dependency_patterns', []),
                confidence_weight=data.get('confidence_weight', 1.0)
            )
        
        return ImperativeConfiguration(
            imperative_patterns=imperative_patterns,
            politeness_analysis=config_data.get('politeness_analysis', {}),
            confidence_settings=config_data.get('confidence_settings', {})
        )
    
    def process(self, text: str) -> Dict[str, Any]:
        """
        å‘½ä»¤æ–‡å‡¦ç†ãƒ¡ã‚¤ãƒ³ - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãªã—ç‰ˆ
        
        Args:
            text: å‡¦ç†å¯¾è±¡ã®è‹±èªæ–‡
            
        Returns:
            Dict: å‡¦ç†çµæœï¼ˆsuccess, imperatives, politeness, confidenceï¼‰
        """
        try:
            # spaCyè§£æ
            doc = self.nlp(text)
            
            # å‘½ä»¤æ–‡è§£æ
            analysis_result = self.analyzer.analyze_imperative_structure(doc)
            
            if not analysis_result['imperatives']:
                return self._create_no_imperatives_result(text)
            
            # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®æ§‹ç¯‰
            sub_slots = self._build_sub_slots(analysis_result['imperatives'])
            
            # ä¸å¯§åº¦åˆ†æã®å®Ÿè¡Œ
            politeness_summary = self._summarize_politeness(analysis_result['imperatives'])
            
            return {
                'success': True,
                'original_text': text,
                'imperatives': analysis_result['imperatives'],
                'sub_slots': sub_slots,
                'politeness_summary': politeness_summary,
                'confidence': analysis_result['confidence'],
                'metadata': {
                    'handler': self.handler_info,
                    'analysis_method': analysis_result['analysis_method'],
                    'imperative_count': len(analysis_result['imperatives'])
                }
            }
            
        except Exception as e:
            return self._create_failure_result(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def _create_no_imperatives_result(self, text: str) -> Dict[str, Any]:
        """å‘½ä»¤æ–‡ãªã—ã®çµæœä½œæˆ"""
        return {
            'success': True,
            'original_text': text,
            'imperatives': [],
            'sub_slots': {},
            'politeness_summary': {},
            'confidence': self.config.confidence_settings.get('minimum_confidence', 0.3),
            'metadata': {
                'handler': self.handler_info,
                'analysis_method': 'no_imperatives_detected'
            }
        }
    
    def _build_sub_slots(self, imperatives: List[Dict[str, Any]]) -> Dict[str, str]:
        """ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®æ§‹ç¯‰"""
        sub_slots = {}
        
        for i, imperative in enumerate(imperatives):
            slot_key = f"sub-imp{i+1}"
            sub_slots[slot_key] = imperative['imperative']['full_text']
        
        return sub_slots
    
    def _summarize_politeness(self, imperatives: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ä¸å¯§åº¦åˆ†æã®è¦ç´„"""
        summary = {
            'politeness_levels': [],
            'command_types': [],
            'urgency_levels': [],
            'dominant_politeness': None
        }
        
        politeness_counts = {}
        
        for imperative in imperatives:
            politeness = imperative['politeness']['level']
            command_type = imperative['command_type']['type']
            urgency = imperative['command_type']['urgency']
            
            summary['politeness_levels'].append(politeness)
            summary['command_types'].append(command_type)
            summary['urgency_levels'].append(urgency)
            
            politeness_counts[politeness] = politeness_counts.get(politeness, 0) + 1
        
        # ä¸»è¦ãªä¸å¯§åº¦ã®æ±ºå®š
        if politeness_counts:
            summary['dominant_politeness'] = max(politeness_counts.items(), key=lambda x: x[1])[0]
        
        summary['politeness_distribution'] = politeness_counts
        
        return summary
    
    def _create_failure_result(self, error_message: str) -> Dict[str, Any]:
        """å¤±æ•—çµæœã®ä½œæˆ"""
        return {
            'success': False,
            'original_text': '',
            'imperatives': [],
            'sub_slots': {},
            'politeness_summary': {},
            'confidence': 0.0,
            'error': error_message,
            'metadata': {
                'handler': self.handler_info,
                'analysis_method': 'error_handling'
            }
        }


if __name__ == "__main__":
    # ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ç‰ˆã®ãƒ†ã‚¹ãƒˆ
    handler = ImperativeHandlerClean()
    
    test_sentences = [
        "Close the door!",
        "Please sit down.",
        "Don't touch that.",
        "Let's go home."
    ]
    
    print("ğŸ§ª ImperativeHandler - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œå…¨é™¤å»ç‰ˆãƒ†ã‚¹ãƒˆ")
    print("=" * 65)
    
    for sentence in test_sentences:
        result = handler.process(sentence)
        print(f"\nå…¥åŠ›: \"{sentence}\"")
        print(f"æˆåŠŸ: {result['success']}")
        print(f"å‘½ä»¤æ–‡æ•°: {len(result.get('imperatives', []))}")
        print(f"ä¿¡é ¼åº¦: {result.get('confidence', 0):.3f}")
        print(f"ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ: {result.get('sub_slots', {})}")
        if result.get('politeness_summary', {}).get('dominant_politeness'):
            print(f"ä¸»è¦ä¸å¯§åº¦: {result['politeness_summary']['dominant_politeness']}")
        print(f"ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä½¿ç”¨: 0ä»¶ âœ…")
