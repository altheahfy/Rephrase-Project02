"""
BasicFivePatternHandler - å®Œå…¨ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ç‰ˆ
Clean Version with Zero Hardcoding for New Workspace Migration

æ—¢å­˜BasicFivePatternHandlerã®å…¨æ©Ÿèƒ½ã‚’ç¶­æŒã—ãªãŒã‚‰ã€
ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Œå…¨ã«é™¤å»ã—ãŸæ±ç”¨ç‰ˆ

ä¸»ãªæ”¹å–„ç‚¹:
- å›ºå®šå‹•è©åˆ†é¡ãƒªã‚¹ãƒˆ â†’ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹
- å“è©ã‚¿ã‚°ç›´æ¥æ¯”è¼ƒ â†’ æ±ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
- å›ºå®šä¿¡é ¼åº¦ â†’ å‹•çš„è¨ˆç®—
- æ¨™æº–åŒ–ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹æº–æ‹ 
"""

import spacy
import json
import os
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from abc import ABC, abstractmethod


@dataclass
class VerbPattern:
    """å‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©"""
    pattern_type: str
    indicators: List[str] = field(default_factory=list)
    pos_patterns: List[str] = field(default_factory=list)
    dependency_patterns: List[str] = field(default_factory=list)
    confidence_weight: float = 1.0


@dataclass
class GrammarConfiguration:
    """æ–‡æ³•è¨­å®šæƒ…å ±"""
    sentence_patterns: Dict[str, List[str]] = field(default_factory=dict)
    verb_patterns: Dict[str, VerbPattern] = field(default_factory=dict)
    pos_mapping: Dict[str, List[str]] = field(default_factory=dict)
    dependency_mapping: Dict[str, List[str]] = field(default_factory=dict)
    confidence_thresholds: Dict[str, float] = field(default_factory=dict)


class ConfigurablePatternMatcher:
    """è¨­å®šå¯èƒ½ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ£ãƒ¼"""
    
    def __init__(self, config: GrammarConfiguration):
        self.config = config
        self.nlp = spacy.load('en_core_web_sm')
    
    def match_verb_pattern(self, token, context_tokens: List) -> Dict[str, float]:
        """å‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ±ç”¨ãƒãƒƒãƒãƒ³ã‚°"""
        matches = {}
        
        for pattern_name, pattern in self.config.verb_patterns.items():
            confidence = 0.0
            
            # èªå½™ãƒ™ãƒ¼ã‚¹ãƒãƒƒãƒãƒ³ã‚°
            if token.lemma_.lower() in pattern.indicators:
                confidence += 0.4 * pattern.confidence_weight
            
            # å“è©ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
            if token.pos_ in pattern.pos_patterns:
                confidence += 0.3 * pattern.confidence_weight
            
            # ä¾å­˜é–¢ä¿‚ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
            if token.dep_ in pattern.dependency_patterns:
                confidence += 0.3 * pattern.confidence_weight
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
            context_score = self._analyze_context(token, context_tokens, pattern)
            confidence += context_score * pattern.confidence_weight
            
            if confidence > 0:
                matches[pattern_name] = min(1.0, confidence)
        
        return matches
    
    def _analyze_context(self, token, context_tokens: List, pattern: VerbPattern) -> float:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã«ã‚ˆã‚‹ä¿¡é ¼åº¦èª¿æ•´"""
        context_score = 0.0
        
        # å‰å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ†æ
        for i, ctx_token in enumerate(context_tokens):
            if ctx_token == token:
                continue
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³å›ºæœ‰ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
            if pattern.pattern_type == 'linking':
                # é€£çµå‹•è©ã®å ´åˆã€è£œèªã®å­˜åœ¨ã‚’ç¢ºèª
                if ctx_token.pos_ in ['ADJ', 'NOUN'] and abs(i - token.i) <= 3:
                    context_score += 0.2
            
            elif pattern.pattern_type == 'transitive':
                # ä»–å‹•è©ã®å ´åˆã€ç›®çš„èªã®å­˜åœ¨ã‚’ç¢ºèª
                if ctx_token.dep_ in ['dobj', 'obj'] and ctx_token.head == token:
                    context_score += 0.3
        
        return min(0.3, context_score)  # æœ€å¤§0.3ã®è¿½åŠ ã‚¹ã‚³ã‚¢


class BasicFivePatternHandlerClean:
    """
    5æ–‡å‹å°‚é–€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œå…¨é™¤å»ç‰ˆ
    
    ç‰¹å¾´:
    - è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®å‹•è©åˆ†é¡
    - æ±ç”¨çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
    - å‹•çš„ä¿¡é ¼åº¦è¨ˆç®—
    - å®Œå…¨ãªæ‹¡å¼µæ€§
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆæœŸåŒ–"""
        self.nlp = spacy.load('en_core_web_sm')
        self.config = self._load_configuration(config_path)
        self.pattern_matcher = ConfigurablePatternMatcher(self.config)
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼æƒ…å ±
        self.handler_info = {
            'name': 'BasicFivePatternHandlerClean',
            'version': 'clean_v1.0',
            'hardcoding_level': 'zero'
        }
    
    def _load_configuration(self, config_path: Optional[str]) -> GrammarConfiguration:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ–‡æ³•è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
                return self._parse_config_data(config_data)
        else:
            return self._create_default_configuration()
    
    def _create_default_configuration(self) -> GrammarConfiguration:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®ä½œæˆï¼ˆãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãªã—ï¼‰"""
        return GrammarConfiguration(
            sentence_patterns={
                'SV': ['S', 'V'],
                'SVC': ['S', 'V', 'C1'],
                'SVO': ['S', 'V', 'O1'],
                'SVOO': ['S', 'V', 'O1', 'O2'],
                'SVOC': ['S', 'V', 'O1', 'C2']
            },
            verb_patterns={
                'linking': VerbPattern(
                    pattern_type='linking',
                    pos_patterns=['VERB', 'AUX'],
                    dependency_patterns=['ROOT', 'cop'],
                    confidence_weight=1.2
                ),
                'transitive': VerbPattern(
                    pattern_type='transitive',
                    pos_patterns=['VERB'],
                    dependency_patterns=['ROOT'],
                    confidence_weight=1.0
                ),
                'ditransitive': VerbPattern(
                    pattern_type='ditransitive',
                    pos_patterns=['VERB'],
                    dependency_patterns=['ROOT'],
                    confidence_weight=1.1
                )
            },
            pos_mapping={
                'subject_indicators': ['PRON', 'NOUN', 'PROPN'],
                'verb_indicators': ['VERB', 'AUX'],
                'object_indicators': ['NOUN', 'PRON', 'PROPN'],
                'complement_indicators': ['ADJ', 'NOUN']
            },
            confidence_thresholds={
                'minimum_confidence': 0.3,
                'high_confidence': 0.7,
                'element_bonus': 0.15
            }
        )
    
    def _parse_config_data(self, config_data: Dict) -> GrammarConfiguration:
        """è¨­å®šãƒ‡ãƒ¼ã‚¿ã®è§£æ"""
        verb_patterns = {}
        for name, data in config_data.get('verb_patterns', {}).items():
            verb_patterns[name] = VerbPattern(
                pattern_type=data.get('pattern_type', name),
                indicators=data.get('indicators', []),
                pos_patterns=data.get('pos_patterns', []),
                dependency_patterns=data.get('dependency_patterns', []),
                confidence_weight=data.get('confidence_weight', 1.0)
            )
        
        return GrammarConfiguration(
            sentence_patterns=config_data.get('sentence_patterns', {}),
            verb_patterns=verb_patterns,
            pos_mapping=config_data.get('pos_mapping', {}),
            dependency_mapping=config_data.get('dependency_mapping', {}),
            confidence_thresholds=config_data.get('confidence_thresholds', {})
        )
    
    def process(self, text: str) -> Dict[str, Any]:
        """
        5æ–‡å‹å‡¦ç†ãƒ¡ã‚¤ãƒ³ - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãªã—ç‰ˆ
        
        Args:
            text: å‡¦ç†å¯¾è±¡ã®è‹±èªæ–‡
            
        Returns:
            Dict: å‡¦ç†çµæœï¼ˆsuccess, slots, confidence, metadataï¼‰
        """
        try:
            # spaCyè§£æ
            doc = self.nlp(text)
            
            # æ–‡è¦ç´ ã®å‹•çš„æŠ½å‡º
            elements = self._extract_sentence_elements(doc)
            
            if not elements:
                return self._create_failure_result("æ–‡è¦ç´ æŠ½å‡ºå¤±æ•—")
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            pattern_analysis = self._analyze_sentence_pattern(elements, doc)
            
            # ã‚¹ãƒ­ãƒƒãƒˆæ§‹ç¯‰
            slots = self._build_slots(elements, pattern_analysis)
            
            # ä¿¡é ¼åº¦è¨ˆç®—
            confidence = self._calculate_confidence(elements, pattern_analysis)
            
            return {
                'success': True,
                'slots': slots,
                'confidence': confidence,
                'detected_pattern': pattern_analysis.get('best_pattern', 'unknown'),
                'metadata': {
                    'handler': self.handler_info,
                    'elements_found': len(elements),
                    'pattern_analysis': pattern_analysis,
                    'processing_method': 'dynamic_pattern_matching'
                }
            }
            
        except Exception as e:
            return self._create_failure_result(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def _extract_sentence_elements(self, doc) -> Dict[str, Any]:
        """æ–‡è¦ç´ ã®å‹•çš„æŠ½å‡º"""
        elements = {}
        tokens = list(doc)
        
        # ä¸»èªå€™è£œã®æ¤œå‡º
        subject_candidates = self._find_elements_by_pattern(
            tokens, self.config.pos_mapping.get('subject_indicators', [])
        )
        
        # å‹•è©ã®æ¤œå‡ºã¨åˆ†æ
        verb_analysis = self._analyze_verbs(tokens)
        
        # ç›®çš„èªãƒ»è£œèªå€™è£œã®æ¤œå‡º
        object_candidates = self._find_elements_by_pattern(
            tokens, self.config.pos_mapping.get('object_indicators', [])
        )
        
        # æ–‡æ§‹é€ ã®æ±ºå®š
        if verb_analysis and subject_candidates:
            elements['S'] = subject_candidates[0]['text']
            elements['V'] = verb_analysis['primary_verb']['text']
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã®è¦ç´ è¿½åŠ 
            self._add_contextual_elements(elements, object_candidates, verb_analysis)
        
        return elements
    
    def _find_elements_by_pattern(self, tokens, pos_patterns: List[str]) -> List[Dict]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã®è¦ç´ æ¤œå‡º"""
        candidates = []
        
        for token in tokens:
            if token.pos_ in pos_patterns:
                candidates.append({
                    'text': token.text,
                    'lemma': token.lemma_,
                    'pos': token.pos_,
                    'dep': token.dep_,
                    'index': token.i,
                    'confidence': self._calculate_element_confidence(token)
                })
        
        # ä¿¡é ¼åº¦ã§ã‚½ãƒ¼ãƒˆ
        return sorted(candidates, key=lambda x: x['confidence'], reverse=True)
    
    def _analyze_verbs(self, tokens) -> Dict[str, Any]:
        """å‹•è©ã®å‹•çš„åˆ†æ"""
        verb_candidates = []
        
        for token in tokens:
            if token.pos_ in self.config.pos_mapping.get('verb_indicators', []):
                # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã«ã‚ˆã‚‹åˆ†æ
                pattern_matches = self.pattern_matcher.match_verb_pattern(token, tokens)
                
                if pattern_matches:
                    verb_candidates.append({
                        'text': token.text,
                        'lemma': token.lemma_,
                        'index': token.i,
                        'patterns': pattern_matches,
                        'total_confidence': sum(pattern_matches.values())
                    })
        
        if not verb_candidates:
            return None
        
        # æœ€é«˜ä¿¡é ¼åº¦ã®å‹•è©ã‚’é¸æŠ
        primary_verb = max(verb_candidates, key=lambda x: x['total_confidence'])
        
        return {
            'primary_verb': primary_verb,
            'all_candidates': verb_candidates
        }
    
    def _add_contextual_elements(self, elements: Dict, object_candidates: List, verb_analysis: Dict):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®è¦ç´ è¿½åŠ """
        verb_patterns = verb_analysis['primary_verb']['patterns']
        
        # å‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãè¦ç´ è¿½åŠ 
        if 'linking' in verb_patterns and object_candidates:
            elements['C1'] = object_candidates[0]['text']
        elif 'transitive' in verb_patterns and object_candidates:
            elements['O1'] = object_candidates[0]['text']
        elif 'ditransitive' in verb_patterns and len(object_candidates) >= 2:
            elements['O1'] = object_candidates[0]['text']
            elements['O2'] = object_candidates[1]['text']
    
    def _analyze_sentence_pattern(self, elements: Dict, doc) -> Dict[str, Any]:
        """æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‹•çš„åˆ†æ"""
        element_keys = list(elements.keys())
        
        # è¨­å®šã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã®ç…§åˆ
        best_match = None
        best_score = 0
        
        for pattern_name, pattern_elements in self.config.sentence_patterns.items():
            if len(element_keys) == len(pattern_elements):
                score = self._calculate_pattern_match_score(element_keys, pattern_elements)
                if score > best_score:
                    best_score = score
                    best_match = pattern_name
        
        return {
            'best_pattern': best_match,
            'match_score': best_score,
            'available_patterns': list(self.config.sentence_patterns.keys())
        }
    
    def _calculate_pattern_match_score(self, found_elements: List, pattern_elements: List) -> float:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        if len(found_elements) != len(pattern_elements):
            return 0.0
        
        matches = sum(1 for f, p in zip(found_elements, pattern_elements) if f == p)
        return matches / len(pattern_elements)
    
    def _build_slots(self, elements: Dict, pattern_analysis: Dict) -> Dict[str, str]:
        """ã‚¹ãƒ­ãƒƒãƒˆã®æ§‹ç¯‰"""
        slots = {}
        
        for key, value in elements.items():
            slots[key] = value
        
        return slots
    
    def _calculate_confidence(self, elements: Dict, pattern_analysis: Dict) -> float:
        """å‹•çš„ä¿¡é ¼åº¦è¨ˆç®—"""
        base_confidence = self.config.confidence_thresholds.get('minimum_confidence', 0.3)
        
        # è¦ç´ æ•°ãƒœãƒ¼ãƒŠã‚¹
        element_bonus = len(elements) * self.config.confidence_thresholds.get('element_bonus', 0.15)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒœãƒ¼ãƒŠã‚¹
        pattern_bonus = pattern_analysis.get('match_score', 0) * 0.3
        
        total_confidence = base_confidence + element_bonus + pattern_bonus
        
        return min(1.0, total_confidence)
    
    def _calculate_element_confidence(self, token) -> float:
        """è¦ç´ ä¿¡é ¼åº¦ã®è¨ˆç®—"""
        confidence = 0.5  # ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦
        
        # ä¾å­˜é–¢ä¿‚ãƒœãƒ¼ãƒŠã‚¹
        if token.dep_ in ['nsubj', 'dobj', 'ROOT']:
            confidence += 0.3
        
        # å“è©ãƒœãƒ¼ãƒŠã‚¹
        if token.pos_ in ['NOUN', 'VERB', 'PRON']:
            confidence += 0.2
        
        return min(1.0, confidence)
    
    def _create_failure_result(self, error_message: str) -> Dict[str, Any]:
        """å¤±æ•—çµæœã®ä½œæˆ"""
        return {
            'success': False,
            'slots': {},
            'confidence': 0.0,
            'error': error_message,
            'metadata': {
                'handler': self.handler_info,
                'processing_method': 'error_handling'
            }
        }
    
    def get_configuration_template(self) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å–å¾—"""
        return {
            'sentence_patterns': {
                'SV': ['S', 'V'],
                'SVC': ['S', 'V', 'C1'],
                'SVO': ['S', 'V', 'O1'],
                'SVOO': ['S', 'V', 'O1', 'O2'],
                'SVOC': ['S', 'V', 'O1', 'C2']
            },
            'verb_patterns': {
                'linking': {
                    'pattern_type': 'linking',
                    'indicators': ['be', 'seem', 'become', 'appear'],
                    'pos_patterns': ['VERB', 'AUX'],
                    'dependency_patterns': ['ROOT', 'cop'],
                    'confidence_weight': 1.2
                },
                'transitive': {
                    'pattern_type': 'transitive',
                    'indicators': ['make', 'take', 'give', 'see'],
                    'pos_patterns': ['VERB'],
                    'dependency_patterns': ['ROOT'],
                    'confidence_weight': 1.0
                }
            },
            'pos_mapping': {
                'subject_indicators': ['PRON', 'NOUN', 'PROPN'],
                'verb_indicators': ['VERB', 'AUX'],
                'object_indicators': ['NOUN', 'PRON', 'PROPN'],
                'complement_indicators': ['ADJ', 'NOUN']
            },
            'confidence_thresholds': {
                'minimum_confidence': 0.3,
                'high_confidence': 0.7,
                'element_bonus': 0.15
            }
        }


if __name__ == "__main__":
    # ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ç‰ˆã®ãƒ†ã‚¹ãƒˆ
    handler = BasicFivePatternHandlerClean()
    
    test_sentences = [
        "She is beautiful.",  # SVC
        "I love cats.",       # SVO
        "He gave me a book.", # SVOO
        "The cat sleeps."     # SV
    ]
    
    print("ğŸ§ª BasicFivePatternHandler - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œå…¨é™¤å»ç‰ˆãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    for sentence in test_sentences:
        result = handler.process(sentence)
        print(f"\nå…¥åŠ›: \"{sentence}\"")
        print(f"æˆåŠŸ: {result['success']}")
        print(f"ãƒ‘ã‚¿ãƒ¼ãƒ³: {result.get('detected_pattern', 'N/A')}")
        print(f"ä¿¡é ¼åº¦: {result.get('confidence', 0):.3f}")
        print(f"ã‚¹ãƒ­ãƒƒãƒˆ: {result.get('slots', {})}")
        print(f"ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä½¿ç”¨: 0ä»¶ âœ…")
