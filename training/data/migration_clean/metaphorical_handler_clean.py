"""
MetaphoricalHandler - å®Œå…¨ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ç‰ˆ
Clean Version with Zero Hardcoding for New Workspace Migration

æ—¢å­˜MetaphoricalHandlerã®å…¨æ©Ÿèƒ½ã‚’ç¶­æŒã—ãªãŒã‚‰ã€
ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Œå…¨ã«é™¤å»ã—ãŸæ±ç”¨ç‰ˆ

ä¸»ãªæ”¹å–„ç‚¹:
- å›ºå®šãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼è¡¨ç¾ â†’ å‹•çš„ãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ
- å›ºå®šæ¯”å–©åˆ†é¡ â†’ è¨­å®šå¯èƒ½è¡¨ç¾ã‚¿ã‚¤ãƒ—
- å›ºå®šèªå½™ãƒªã‚¹ãƒˆ â†’ æ±ç”¨æ„å‘³è§£æ
- æ¨™æº–åŒ–ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹æº–æ‹ 
"""

import spacy
import json
import os
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from abc import ABC, abstractmethod


@dataclass
class MetaphoricalPattern:
    """æ¯”å–©è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©"""
    pattern_type: str
    metaphor_markers: List[str] = field(default_factory=list)
    comparison_patterns: List[str] = field(default_factory=list)
    semantic_fields: List[str] = field(default_factory=list)
    syntactic_patterns: List[str] = field(default_factory=list)
    pos_patterns: List[str] = field(default_factory=list)
    confidence_weight: float = 1.0


@dataclass
class MetaphoricalConfiguration:
    """æ¯”å–©è¡¨ç¾ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š"""
    metaphorical_patterns: Dict[str, MetaphoricalPattern] = field(default_factory=dict)
    semantic_analysis: Dict[str, Any] = field(default_factory=dict)
    confidence_settings: Dict[str, float] = field(default_factory=dict)
    figurative_rules: Dict[str, List[str]] = field(default_factory=dict)


class GenericMetaphoricalAnalyzer:
    """æ±ç”¨æ¯”å–©è¡¨ç¾è§£æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, config: MetaphoricalConfiguration):
        self.config = config
        self.nlp = spacy.load('en_core_web_sm')
    
    def analyze_metaphorical_structure(self, doc) -> Dict[str, Any]:
        """æ±ç”¨æ¯”å–©è¡¨ç¾æ§‹é€ è§£æ"""
        # æ¯”å–©è¡¨ç¾å€™è£œã®æ¤œå‡º
        metaphor_candidates = self._detect_metaphor_candidates(doc)
        
        if not metaphor_candidates:
            return {'metaphors': [], 'confidence': 0.0}
        
        # æ¯”å–©è¡¨ç¾ã®è©³ç´°è§£æ
        analyzed_metaphors = []
        for candidate in metaphor_candidates:
            metaphor_analysis = self._analyze_metaphor_details(candidate, doc)
            if metaphor_analysis:
                analyzed_metaphors.append(metaphor_analysis)
        
        # ä¿¡é ¼åº¦è¨ˆç®—
        confidence = self._calculate_metaphor_confidence(analyzed_metaphors)
        
        return {
            'metaphors': analyzed_metaphors,
            'confidence': confidence,
            'analysis_method': 'pattern_based_generic'
        }
    
    def _detect_metaphor_candidates(self, doc) -> List[Dict[str, Any]]:
        """æ¯”å–©è¡¨ç¾å€™è£œã®æ¤œå‡º"""
        candidates = []
        
        for pattern_name, pattern in self.config.metaphorical_patterns.items():
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã®æ¤œå‡º
            pattern_matches = self._find_pattern_matches(doc, pattern)
            
            for match in pattern_matches:
                candidate = self._create_metaphor_candidate(match, pattern_name, pattern, doc)
                if candidate:
                    candidates.append(candidate)
        
        return candidates
    
    def _find_pattern_matches(self, doc, pattern: MetaphoricalPattern) -> List[Dict[str, Any]]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒã®æ¤œå‡º"""
        matches = []
        
        # èªå½™ãƒ™ãƒ¼ã‚¹ã®æ¤œå‡º
        lexical_matches = self._find_lexical_matches(doc, pattern)
        matches.extend(lexical_matches)
        
        # çµ±èªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã®æ¤œå‡º
        syntactic_matches = self._find_syntactic_matches(doc, pattern)
        matches.extend(syntactic_matches)
        
        # æ„å‘³ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒ™ãƒ¼ã‚¹ã®æ¤œå‡º
        semantic_matches = self._find_semantic_matches(doc, pattern)
        matches.extend(semantic_matches)
        
        return matches
    
    def _find_lexical_matches(self, doc, pattern: MetaphoricalPattern) -> List[Dict[str, Any]]:
        """èªå½™ãƒ™ãƒ¼ã‚¹ã®ãƒãƒƒãƒæ¤œå‡º"""
        matches = []
        
        for token in doc:
            if self._matches_lexical_pattern(token, pattern):
                matches.append({
                    'type': 'lexical',
                    'token': token,
                    'pattern_elements': pattern.metaphor_markers
                })
        
        return matches
    
    def _matches_lexical_pattern(self, token, pattern: MetaphoricalPattern) -> bool:
        """èªå½™ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°"""
        lemma = token.lemma_.lower()
        text = token.text.lower()
        
        # ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ãƒãƒ¼ã‚«ãƒ¼ã®ãƒã‚§ãƒƒã‚¯
        marker_match = not pattern.metaphor_markers or any(
            marker in lemma or marker in text 
            for marker in pattern.metaphor_markers
        )
        
        # æ¯”è¼ƒãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒã‚§ãƒƒã‚¯
        comparison_match = not pattern.comparison_patterns or any(
            comp in lemma or comp in text 
            for comp in pattern.comparison_patterns
        )
        
        return marker_match or comparison_match
    
    def _find_syntactic_matches(self, doc, pattern: MetaphoricalPattern) -> List[Dict[str, Any]]:
        """çµ±èªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒã®æ¤œå‡º"""
        matches = []
        
        for token in doc:
            if self._matches_syntactic_pattern(token, pattern, doc):
                matches.append({
                    'type': 'syntactic',
                    'token': token,
                    'pattern_elements': pattern.syntactic_patterns
                })
        
        return matches
    
    def _matches_syntactic_pattern(self, token, pattern: MetaphoricalPattern, doc) -> bool:
        """çµ±èªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°"""
        if not pattern.syntactic_patterns:
            return False
        
        # æ¯”è¼ƒæ§‹æ–‡ã®æ¤œå‡º
        if 'comparison' in pattern.syntactic_patterns:
            if self._is_comparison_structure(token, doc):
                return True
        
        # 'be like' æ§‹æ–‡ã®æ¤œå‡º
        if 'be_like' in pattern.syntactic_patterns:
            if self._is_be_like_structure(token, doc):
                return True
        
        # ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼å‹•è©æ§‹æ–‡ã®æ¤œå‡º
        if 'metaphorical_verb' in pattern.syntactic_patterns:
            if self._is_metaphorical_verb_structure(token, doc):
                return True
        
        return False
    
    def _is_comparison_structure(self, token, doc) -> bool:
        """æ¯”è¼ƒæ§‹æ–‡ã®æ¤œå‡º"""
        # 'as...as' æ§‹æ–‡
        if token.lemma_.lower() == 'as':
            # å‰å¾Œã«å½¢å®¹è©ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            for i in range(max(0, token.i - 2), min(len(doc), token.i + 3)):
                if i != token.i and doc[i].pos_ == 'ADJ':
                    return True
        
        # 'like' ã«ã‚ˆã‚‹æ¯”è¼ƒ
        if token.lemma_.lower() == 'like':
            # å‹•è©ã®å¾Œã«ç¶šãå ´åˆ
            if token.i > 0 and doc[token.i - 1].pos_ == 'VERB':
                return True
        
        return False
    
    def _is_be_like_structure(self, token, doc) -> bool:
        """'be like' æ§‹æ–‡ã®æ¤œå‡º"""
        if token.lemma_.lower() == 'like':
            # å‰ã«beå‹•è©ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            for i in range(max(0, token.i - 3), token.i):
                prev_token = doc[i]
                if prev_token.lemma_.lower() in ['be', 'am', 'is', 'are', 'was', 'were']:
                    return True
        
        return False
    
    def _is_metaphorical_verb_structure(self, token, doc) -> bool:
        """ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼å‹•è©æ§‹æ–‡ã®æ¤œå‡º"""
        if token.pos_ == 'VERB':
            # æ„å‘³çš„ä¸æ•´åˆã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            verb_lemma = token.lemma_.lower()
            
            # æ“¬äººåŒ–å‹•è©ã®æ¤œå‡º
            if verb_lemma in ['dance', 'sing', 'cry', 'laugh', 'whisper', 'scream']:
                # ä¸»èªãŒç„¡ç”Ÿç‰©ã®å ´åˆ
                for child in token.children:
                    if child.dep_ == 'nsubj':
                        # ç°¡æ˜“çš„ãªç„¡ç”Ÿç‰©åˆ¤å®š
                        if not self._is_animate_noun(child):
                            return True
        
        return False
    
    def _is_animate_noun(self, token) -> bool:
        """ç”Ÿç‰©åè©ã®ç°¡æ˜“åˆ¤å®š"""
        animate_markers = ['person', 'people', 'man', 'woman', 'child', 'animal', 'dog', 'cat']
        lemma = token.lemma_.lower()
        
        # ä»£åè©ã®ãƒã‚§ãƒƒã‚¯
        if token.pos_ == 'PRON' and lemma in ['he', 'she', 'they', 'who']:
            return True
        
        # ç”Ÿç‰©èªå½™ã®ãƒã‚§ãƒƒã‚¯
        return any(marker in lemma for marker in animate_markers)
    
    def _find_semantic_matches(self, doc, pattern: MetaphoricalPattern) -> List[Dict[str, Any]]:
        """æ„å‘³ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒã®æ¤œå‡º"""
        matches = []
        
        if not pattern.semantic_fields:
            return matches
        
        # èªå½™ã®æ„å‘³ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åˆ†æ
        for token in doc:
            semantic_field = self._analyze_semantic_field(token)
            
            if semantic_field in pattern.semantic_fields:
                matches.append({
                    'type': 'semantic',
                    'token': token,
                    'semantic_field': semantic_field
                })
        
        return matches
    
    def _analyze_semantic_field(self, token) -> str:
        """èªå½™ã®æ„å‘³ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åˆ†æ"""
        lemma = token.lemma_.lower()
        
        # è¨­å®šãƒ™ãƒ¼ã‚¹ã®åˆ†é¡
        semantic_groups = self.config.semantic_analysis.get('semantic_fields', {})
        
        for field, words in semantic_groups.items():
            if lemma in words:
                return field
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ†é¡
        return self._default_semantic_classification(lemma, token)
    
    def _default_semantic_classification(self, lemma: str, token) -> str:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ„å‘³åˆ†é¡"""
        # å“è©ãƒ™ãƒ¼ã‚¹ã®åŸºæœ¬åˆ†é¡
        if token.pos_ == 'NOUN':
            if lemma in ['light', 'fire', 'sun', 'star', 'moon']:
                return 'light_source'
            elif lemma in ['water', 'sea', 'ocean', 'river', 'rain']:
                return 'water'
            elif lemma in ['mountain', 'hill', 'rock', 'stone']:
                return 'earth'
            elif lemma in ['wind', 'air', 'breath', 'breeze']:
                return 'air'
        elif token.pos_ == 'VERB':
            if lemma in ['flow', 'run', 'move', 'go']:
                return 'motion'
            elif lemma in ['shine', 'glow', 'burn', 'light']:
                return 'light'
            elif lemma in ['grow', 'bloom', 'flower']:
                return 'growth'
        
        return 'general'
    
    def _create_metaphor_candidate(self, match: Dict[str, Any], pattern_name: str, pattern: MetaphoricalPattern, doc) -> Optional[Dict[str, Any]]:
        """æ¯”å–©è¡¨ç¾å€™è£œã®ä½œæˆ"""
        # ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ã®ç¯„å›²ç‰¹å®š
        metaphor_span = self._identify_metaphor_span(match, doc)
        
        if not metaphor_span:
            return None
        
        # ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ã®æ§‹é€ åˆ†æ
        structure_analysis = self._analyze_metaphor_structure(metaphor_span, doc)
        
        # æ„å‘³åˆ†æ
        semantic_analysis = self._analyze_metaphor_semantics(metaphor_span, doc)
        
        # æ¯”å–©ã‚¿ã‚¤ãƒ—ã®åˆ†æ
        figurative_type_analysis = self._analyze_figurative_type(match, metaphor_span, doc)
        
        return {
            'match': match,
            'metaphor_span': metaphor_span,
            'pattern_type': pattern_name,
            'structure_analysis': structure_analysis,
            'semantic_analysis': semantic_analysis,
            'figurative_type_analysis': figurative_type_analysis,
            'confidence_weight': pattern.confidence_weight
        }
    
    def _identify_metaphor_span(self, match: Dict[str, Any], doc) -> Optional[Dict[str, Any]]:
        """ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ã®ç¯„å›²ç‰¹å®š"""
        central_token = match['token']
        
        # åŸºæœ¬ç¯„å›²ã®è¨­å®š
        start_index = central_token.i
        end_index = central_token.i
        
        # å¥ã‚„ç¯€ãƒ¬ãƒ™ãƒ«ã§ã®æ‹¡å¼µ
        if match['type'] == 'syntactic':
            span = self._expand_syntactic_span(central_token, doc)
            if span:
                start_index, end_index = span
        elif match['type'] == 'semantic':
            span = self._expand_semantic_span(central_token, doc)
            if span:
                start_index, end_index = span
        else:
            span = self._expand_lexical_span(central_token, doc)
            if span:
                start_index, end_index = span
        
        if start_index <= end_index:
            return {
                'start': start_index,
                'end': end_index,
                'text': ' '.join([token.text for token in doc[start_index:end_index+1]]),
                'tokens': list(doc[start_index:end_index+1])
            }
        
        return None
    
    def _expand_syntactic_span(self, central_token, doc) -> Optional[Tuple[int, int]]:
        """çµ±èªçš„ç¯„å›²ã®æ‹¡å¼µ"""
        # å¥ãƒ¬ãƒ™ãƒ«ã§ã®æ‹¡å¼µ
        start = central_token.i
        end = central_token.i
        
        # å·¦æ–¹å‘ã¸ã®æ‹¡å¼µ
        for i in range(central_token.i - 1, -1, -1):
            token = doc[i]
            if token.dep_ in ['det', 'amod', 'compound', 'nmod']:
                start = i
            else:
                break
        
        # å³æ–¹å‘ã¸ã®æ‹¡å¼µ
        for i in range(central_token.i + 1, len(doc)):
            token = doc[i]
            if token.dep_ in ['amod', 'compound', 'prep', 'pobj']:
                end = i
            else:
                break
        
        return (start, end)
    
    def _expand_semantic_span(self, central_token, doc) -> Optional[Tuple[int, int]]:
        """æ„å‘³çš„ç¯„å›²ã®æ‹¡å¼µ"""
        # æ„å‘³çš„ã«é–¢é€£ã™ã‚‹èªã®ç¯„å›²ã‚’æ‹¡å¼µ
        start = central_token.i
        end = central_token.i
        
        central_field = self._analyze_semantic_field(central_token)
        
        # å‰å¾Œã®é–¢é€£èªã‚’ãƒã‚§ãƒƒã‚¯
        for i in range(max(0, central_token.i - 3), min(len(doc), central_token.i + 4)):
            if i != central_token.i:
                token = doc[i]
                token_field = self._analyze_semantic_field(token)
                
                if token_field == central_field:
                    start = min(start, i)
                    end = max(end, i)
        
        return (start, end)
    
    def _expand_lexical_span(self, central_token, doc) -> Optional[Tuple[int, int]]:
        """èªå½™çš„ç¯„å›²ã®æ‹¡å¼µ"""
        # åŸºæœ¬çš„ãªå¥ãƒ¬ãƒ™ãƒ«ã®æ‹¡å¼µ
        start = central_token.i
        end = central_token.i
        
        # ä¿®é£¾èªã®æ‹¡å¼µ
        for child in central_token.children:
            if child.dep_ in ['amod', 'det', 'compound']:
                start = min(start, child.i)
                end = max(end, child.i)
        
        return (start, end)
    
    def _analyze_metaphor_structure(self, metaphor_span: Dict[str, Any], doc) -> Dict[str, Any]:
        """ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼æ§‹é€ ã®åˆ†æ"""
        structure_info = {
            'span_type': 'unknown',
            'head_token': None,
            'modifiers': [],
            'dependencies': [],
            'syntactic_role': 'unknown'
        }
        
        span_tokens = metaphor_span['tokens']
        
        # ä¸»è¦ãƒˆãƒ¼ã‚¯ãƒ³ã®ç‰¹å®š
        head_token = self._find_head_token(span_tokens)
        if head_token:
            structure_info['head_token'] = {
                'text': head_token.text,
                'lemma': head_token.lemma_,
                'pos': head_token.pos_,
                'index': head_token.i
            }
            
            # çµ±èªçš„å½¹å‰²ã®åˆ†æ
            structure_info['syntactic_role'] = head_token.dep_
        
        # ä¿®é£¾é–¢ä¿‚ã®åˆ†æ
        structure_info['modifiers'] = self._analyze_modifiers(span_tokens)
        
        # ä¾å­˜é–¢ä¿‚ã®åˆ†æ
        structure_info['dependencies'] = self._analyze_dependencies(span_tokens)
        
        # ã‚¹ãƒ‘ãƒ³ã‚¿ã‚¤ãƒ—ã®æ±ºå®š
        structure_info['span_type'] = self._determine_span_type(span_tokens)
        
        return structure_info
    
    def _find_head_token(self, tokens):
        """ä¸»è¦ãƒˆãƒ¼ã‚¯ãƒ³ã®ç‰¹å®š"""
        # ãƒ«ãƒ¼ãƒˆã¾ãŸã¯ä¸»è¦ãªä¾å­˜é–¢ä¿‚ã‚’æŒã¤ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç‰¹å®š
        for token in tokens:
            if token.dep_ in ['ROOT', 'nsubj', 'dobj', 'attr']:
                return token
        
        # åè©ã¾ãŸã¯å‹•è©ã‚’å„ªå…ˆ
        for token in tokens:
            if token.pos_ in ['NOUN', 'VERB']:
                return token
        
        # æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        return tokens[0] if tokens else None
    
    def _analyze_modifiers(self, tokens) -> List[Dict[str, Any]]:
        """ä¿®é£¾é–¢ä¿‚ã®åˆ†æ"""
        modifiers = []
        
        for token in tokens:
            if token.dep_ in ['amod', 'det', 'compound', 'nmod']:
                modifiers.append({
                    'text': token.text,
                    'lemma': token.lemma_,
                    'dependency': token.dep_,
                    'index': token.i
                })
        
        return modifiers
    
    def _analyze_dependencies(self, tokens) -> List[Dict[str, Any]]:
        """ä¾å­˜é–¢ä¿‚ã®åˆ†æ"""
        dependencies = []
        
        for token in tokens:
            dependencies.append({
                'text': token.text,
                'dependency': token.dep_,
                'head_text': token.head.text if token.head != token else 'ROOT',
                'index': token.i
            })
        
        return dependencies
    
    def _determine_span_type(self, tokens) -> str:
        """ã‚¹ãƒ‘ãƒ³ã‚¿ã‚¤ãƒ—ã®æ±ºå®š"""
        if len(tokens) == 1:
            return 'single_word'
        elif any(token.pos_ == 'VERB' for token in tokens):
            return 'verbal_phrase'
        elif any(token.pos_ == 'NOUN' for token in tokens):
            return 'nominal_phrase'
        elif any(token.pos_ == 'ADJ' for token in tokens):
            return 'adjectival_phrase'
        else:
            return 'other_phrase'
    
    def _analyze_metaphor_semantics(self, metaphor_span: Dict[str, Any], doc) -> Dict[str, Any]:
        """ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ã®æ„å‘³åˆ†æ"""
        semantic_info = {
            'source_domain': 'unknown',
            'target_domain': 'unknown',
            'conceptual_mapping': {},
            'semantic_fields': []
        }
        
        span_tokens = metaphor_span['tokens']
        
        # æ„å‘³ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®åˆ†æ
        for token in span_tokens:
            field = self._analyze_semantic_field(token)
            if field not in semantic_info['semantic_fields']:
                semantic_info['semantic_fields'].append(field)
        
        # ã‚½ãƒ¼ã‚¹ãƒ»ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé ˜åŸŸã®æ¨å®š
        semantic_info['source_domain'], semantic_info['target_domain'] = self._identify_conceptual_domains(span_tokens, doc)
        
        # æ¦‚å¿µå†™åƒã®åˆ†æ
        semantic_info['conceptual_mapping'] = self._analyze_conceptual_mapping(span_tokens, doc)
        
        return semantic_info
    
    def _identify_conceptual_domains(self, span_tokens, doc) -> Tuple[str, str]:
        """æ¦‚å¿µé ˜åŸŸã®ç‰¹å®š"""
        # ç°¡æ˜“çš„ãªé ˜åŸŸç‰¹å®š
        semantic_fields = [self._analyze_semantic_field(token) for token in span_tokens]
        
        # æœ€ã‚‚é »ç¹ãªæ„å‘³ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ã‚½ãƒ¼ã‚¹é ˜åŸŸã¨ã™ã‚‹
        if semantic_fields:
            source_domain = max(set(semantic_fields), key=semantic_fields.count)
        else:
            source_domain = 'unknown'
        
        # æ–‡è„ˆã‹ã‚‰ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé ˜åŸŸã‚’æ¨å®š
        target_domain = self._infer_target_domain(span_tokens, doc)
        
        return source_domain, target_domain
    
    def _infer_target_domain(self, span_tokens, doc) -> str:
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé ˜åŸŸã®æ¨å®š"""
        # æ–‡è„ˆä¸­ã®ä»–ã®èªå½™ã‹ã‚‰æ¨å®š
        context_fields = []
        
        for token in doc:
            if token not in span_tokens:
                field = self._analyze_semantic_field(token)
                if field != 'general':
                    context_fields.append(field)
        
        if context_fields:
            return max(set(context_fields), key=context_fields.count)
        else:
            return 'unknown'
    
    def _analyze_conceptual_mapping(self, span_tokens, doc) -> Dict[str, Any]:
        """æ¦‚å¿µå†™åƒã®åˆ†æ"""
        mapping = {
            'structural_correspondences': [],
            'property_mappings': [],
            'relational_mappings': []
        }
        
        # æ§‹é€ çš„å¯¾å¿œã®åˆ†æ
        for token in span_tokens:
            if token.pos_ == 'VERB':
                mapping['structural_correspondences'].append({
                    'element': token.text,
                    'type': 'action',
                    'mapping': 'process_correspondence'
                })
            elif token.pos_ == 'NOUN':
                mapping['structural_correspondences'].append({
                    'element': token.text,
                    'type': 'entity',
                    'mapping': 'entity_correspondence'
                })
        
        return mapping
    
    def _analyze_figurative_type(self, match: Dict[str, Any], metaphor_span: Dict[str, Any], doc) -> Dict[str, Any]:
        """æ¯”å–©ã‚¿ã‚¤ãƒ—ã®åˆ†æ"""
        type_info = {
            'primary_type': 'unknown',
            'sub_types': [],
            'rhetorical_function': 'unknown',
            'conceptual_complexity': 'simple'
        }
        
        # åŸºæœ¬ã‚¿ã‚¤ãƒ—ã®æ±ºå®š
        type_info['primary_type'] = self._determine_primary_figurative_type(match, metaphor_span)
        
        # ã‚µãƒ–ã‚¿ã‚¤ãƒ—ã®åˆ†æ
        type_info['sub_types'] = self._identify_sub_types(match, metaphor_span)
        
        # ä¿®è¾æ©Ÿèƒ½ã®åˆ†æ
        type_info['rhetorical_function'] = self._analyze_rhetorical_function(metaphor_span, doc)
        
        # æ¦‚å¿µçš„è¤‡é›‘ã•ã®è©•ä¾¡
        type_info['conceptual_complexity'] = self._evaluate_conceptual_complexity(metaphor_span)
        
        return type_info
    
    def _determine_primary_figurative_type(self, match: Dict[str, Any], metaphor_span: Dict[str, Any]) -> str:
        """åŸºæœ¬æ¯”å–©ã‚¿ã‚¤ãƒ—ã®æ±ºå®š"""
        if match['type'] == 'syntactic':
            # çµ±èªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã®åˆ†é¡
            if 'comparison' in match.get('pattern_elements', []):
                return 'simile'
            elif 'be_like' in match.get('pattern_elements', []):
                return 'simile'
            elif 'metaphorical_verb' in match.get('pattern_elements', []):
                return 'metaphor'
        elif match['type'] == 'semantic':
            return 'metaphor'
        elif match['type'] == 'lexical':
            return 'figurative_expression'
        
        return 'unknown'
    
    def _identify_sub_types(self, match: Dict[str, Any], metaphor_span: Dict[str, Any]) -> List[str]:
        """ã‚µãƒ–ã‚¿ã‚¤ãƒ—ã®ç‰¹å®š"""
        sub_types = []
        
        span_text = metaphor_span['text'].lower()
        
        # æ“¬äººåŒ–ã®æ¤œå‡º
        if self._is_personification(metaphor_span):
            sub_types.append('personification')
        
        # æ›å–©ã®æ¤œå‡º
        if self._is_metonymy(metaphor_span):
            sub_types.append('metonymy')
        
        # èª‡å¼µæ³•ã®æ¤œå‡º
        if self._is_hyperbole(metaphor_span):
            sub_types.append('hyperbole')
        
        return sub_types
    
    def _is_personification(self, metaphor_span: Dict[str, Any]) -> bool:
        """æ“¬äººåŒ–ã®æ¤œå‡º"""
        span_tokens = metaphor_span['tokens']
        
        # äººé–“ã®è¡Œç‚ºå‹•è©ã¨ç„¡ç”Ÿç‰©ä¸»èªã®çµ„ã¿åˆã‚ã›
        for token in span_tokens:
            if token.pos_ == 'VERB':
                verb_lemma = token.lemma_.lower()
                if verb_lemma in ['dance', 'sing', 'cry', 'laugh', 'whisper', 'scream', 'smile']:
                    return True
        
        return False
    
    def _is_metonymy(self, metaphor_span: Dict[str, Any]) -> bool:
        """æ›å–©ã®æ¤œå‡º"""
        span_tokens = metaphor_span['tokens']
        
        # éƒ¨åˆ†ã¨å…¨ä½“ã®é–¢ä¿‚ã‚’ç¤ºã™èªå½™
        metonymy_markers = ['crown', 'throne', 'hand', 'head', 'heart', 'brain']
        
        for token in span_tokens:
            if token.lemma_.lower() in metonymy_markers:
                return True
        
        return False
    
    def _is_hyperbole(self, metaphor_span: Dict[str, Any]) -> bool:
        """èª‡å¼µæ³•ã®æ¤œå‡º"""
        span_tokens = metaphor_span['tokens']
        
        # èª‡å¼µã‚’ç¤ºã™èªå½™
        hyperbole_markers = ['million', 'thousand', 'forever', 'never', 'always', 'impossible']
        
        for token in span_tokens:
            if token.lemma_.lower() in hyperbole_markers:
                return True
        
        return False
    
    def _analyze_rhetorical_function(self, metaphor_span: Dict[str, Any], doc) -> str:
        """ä¿®è¾æ©Ÿèƒ½ã®åˆ†æ"""
        # æ–‡è„ˆã‹ã‚‰ä¿®è¾æ©Ÿèƒ½ã‚’æ¨å®š
        span_tokens = metaphor_span['tokens']
        
        # è©•ä¾¡çš„æ©Ÿèƒ½ã®æ¤œå‡º
        if any(token.pos_ == 'ADJ' for token in span_tokens):
            return 'evaluative'
        
        # èª¬æ˜çš„æ©Ÿèƒ½ã®æ¤œå‡º
        if any(token.dep_ == 'attr' for token in span_tokens):
            return 'descriptive'
        
        # å¼·èª¿çš„æ©Ÿèƒ½ã®æ¤œå‡º
        if any(token.lemma_.lower() in ['very', 'really', 'extremely'] for token in doc):
            return 'emphatic'
        
        return 'illustrative'
    
    def _evaluate_conceptual_complexity(self, metaphor_span: Dict[str, Any]) -> str:
        """æ¦‚å¿µçš„è¤‡é›‘ã•ã®è©•ä¾¡"""
        span_tokens = metaphor_span['tokens']
        
        # é•·ã•ãƒ™ãƒ¼ã‚¹ã®è©•ä¾¡
        if len(span_tokens) > 5:
            return 'complex'
        elif len(span_tokens) > 2:
            return 'moderate'
        else:
            return 'simple'
    
    def _analyze_metaphor_details(self, candidate: Dict[str, Any], doc) -> Optional[Dict[str, Any]]:
        """æ¯”å–©è¡¨ç¾ã®è©³ç´°è§£æ"""
        # ä¿¡é ¼åº¦è¨ˆç®—
        confidence = self._calculate_individual_confidence(candidate)
        
        if confidence < 0.3:  # ä½ä¿¡é ¼åº¦ã¯é™¤å¤–
            return None
        
        metaphor_span = candidate['metaphor_span']
        
        return {
            'metaphor': {
                'text': metaphor_span['text'],
                'span': {
                    'start': metaphor_span['start'],
                    'end': metaphor_span['end'],
                    'length': metaphor_span['end'] - metaphor_span['start'] + 1
                }
            },
            'structure': candidate['structure_analysis'],
            'semantics': candidate['semantic_analysis'],
            'figurative_type': candidate['figurative_type_analysis'],
            'confidence': confidence,
            'pattern_type': candidate['pattern_type']
        }
    
    def _calculate_individual_confidence(self, candidate: Dict[str, Any]) -> float:
        """å€‹åˆ¥æ¯”å–©è¡¨ç¾ã®ä¿¡é ¼åº¦è¨ˆç®—"""
        base_confidence = 0.3
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒã®ä¿¡é ¼åº¦
        base_confidence += 0.2 * candidate['confidence_weight']
        
        # ãƒãƒƒãƒã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹èª¿æ•´
        match_type = candidate['match']['type']
        if match_type == 'syntactic':
            base_confidence += 0.3
        elif match_type == 'semantic':
            base_confidence += 0.2
        elif match_type == 'lexical':
            base_confidence += 0.1
        
        # æ§‹é€ ã®æ˜ç¢ºã•
        structure = candidate['structure_analysis']
        if structure['head_token']:
            base_confidence += 0.1
        
        # æ„å‘³ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å¤šæ§˜æ€§
        semantics = candidate['semantic_analysis']
        if len(semantics['semantic_fields']) > 1:
            base_confidence += 0.1
        
        return min(1.0, base_confidence)
    
    def _calculate_metaphor_confidence(self, analyzed_metaphors: List[Dict[str, Any]]) -> float:
        """å…¨ä½“ã®æ¯”å–©è¡¨ç¾è§£æä¿¡é ¼åº¦"""
        if not analyzed_metaphors:
            return 0.0
        
        total_confidence = sum(metaphor['confidence'] for metaphor in analyzed_metaphors)
        return min(1.0, total_confidence / len(analyzed_metaphors))


class MetaphoricalHandlerClean:
    """
    æ¯”å–©è¡¨ç¾å‡¦ç†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œå…¨é™¤å»ç‰ˆ
    
    ç‰¹å¾´:
    - è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©
    - æ±ç”¨çš„æ¯”å–©è¡¨ç¾æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    - å‹•çš„æ„å‘³åˆ†æ
    - å®Œå…¨ãªæ‹¡å¼µæ€§
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆæœŸåŒ–"""
        self.nlp = spacy.load('en_core_web_sm')
        self.config = self._load_configuration(config_path)
        self.analyzer = GenericMetaphoricalAnalyzer(self.config)
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼æƒ…å ±
        self.handler_info = {
            'name': 'MetaphoricalHandlerClean',
            'version': 'clean_v1.0',
            'hardcoding_level': 'zero'
        }
    
    def _load_configuration(self, config_path: Optional[str]) -> MetaphoricalConfiguration:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
                return self._parse_config_data(config_data)
        else:
            return self._create_default_configuration()
    
    def _create_default_configuration(self) -> MetaphoricalConfiguration:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®ä½œæˆ"""
        return MetaphoricalConfiguration(
            metaphorical_patterns={
                'simile_pattern': MetaphoricalPattern(
                    pattern_type='simile',
                    metaphor_markers=['like', 'as'],
                    comparison_patterns=['as...as', 'like a'],
                    semantic_fields=['comparison', 'similarity'],
                    syntactic_patterns=['comparison', 'be_like'],
                    pos_patterns=['ADP', 'SCONJ'],
                    confidence_weight=1.3
                ),
                'metaphor_pattern': MetaphoricalPattern(
                    pattern_type='metaphor',
                    metaphor_markers=[],
                    comparison_patterns=[],
                    semantic_fields=['abstract', 'concrete'],
                    syntactic_patterns=['metaphorical_verb', 'predicate_metaphor'],
                    pos_patterns=['VERB', 'NOUN', 'ADJ'],
                    confidence_weight=1.1
                ),
                'personification_pattern': MetaphoricalPattern(
                    pattern_type='personification',
                    metaphor_markers=['dance', 'sing', 'cry', 'laugh', 'whisper'],
                    comparison_patterns=[],
                    semantic_fields=['human_action', 'inanimate'],
                    syntactic_patterns=['metaphorical_verb'],
                    pos_patterns=['VERB'],
                    confidence_weight=1.2
                )
            },
            semantic_analysis={
                'semantic_fields': {
                    'light_source': ['light', 'fire', 'sun', 'star', 'moon', 'shine', 'glow'],
                    'water': ['water', 'sea', 'ocean', 'river', 'rain', 'flow', 'wave'],
                    'earth': ['mountain', 'hill', 'rock', 'stone', 'ground', 'solid'],
                    'air': ['wind', 'air', 'breath', 'breeze', 'fly', 'float'],
                    'motion': ['flow', 'run', 'move', 'go', 'dance', 'walk'],
                    'growth': ['grow', 'bloom', 'flower', 'tree', 'plant', 'seed'],
                    'human_action': ['dance', 'sing', 'cry', 'laugh', 'whisper', 'scream']
                }
            },
            confidence_settings={
                'minimum_confidence': 0.3,
                'high_confidence': 0.8,
                'metaphor_bonus': 0.2
            }
        )
    
    def _parse_config_data(self, config_data: Dict) -> MetaphoricalConfiguration:
        """è¨­å®šãƒ‡ãƒ¼ã‚¿ã®è§£æ"""
        metaphorical_patterns = {}
        for name, data in config_data.get('metaphorical_patterns', {}).items():
            metaphorical_patterns[name] = MetaphoricalPattern(
                pattern_type=data.get('pattern_type', name),
                metaphor_markers=data.get('metaphor_markers', []),
                comparison_patterns=data.get('comparison_patterns', []),
                semantic_fields=data.get('semantic_fields', []),
                syntactic_patterns=data.get('syntactic_patterns', []),
                pos_patterns=data.get('pos_patterns', []),
                confidence_weight=data.get('confidence_weight', 1.0)
            )
        
        return MetaphoricalConfiguration(
            metaphorical_patterns=metaphorical_patterns,
            semantic_analysis=config_data.get('semantic_analysis', {}),
            confidence_settings=config_data.get('confidence_settings', {})
        )
    
    def process(self, text: str) -> Dict[str, Any]:
        """
        æ¯”å–©è¡¨ç¾å‡¦ç†ãƒ¡ã‚¤ãƒ³ - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãªã—ç‰ˆ
        
        Args:
            text: å‡¦ç†å¯¾è±¡ã®è‹±èªæ–‡
            
        Returns:
            Dict: å‡¦ç†çµæœï¼ˆsuccess, metaphors, figurative_types, confidenceï¼‰
        """
        try:
            # spaCyè§£æ
            doc = self.nlp(text)
            
            # æ¯”å–©è¡¨ç¾è§£æ
            analysis_result = self.analyzer.analyze_metaphorical_structure(doc)
            
            if not analysis_result['metaphors']:
                return self._create_no_metaphors_result(text)
            
            # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®æ§‹ç¯‰
            sub_slots = self._build_sub_slots(analysis_result['metaphors'])
            
            # æ¯”å–©ã‚¿ã‚¤ãƒ—åˆ†æã®å®Ÿè¡Œ
            figurative_summary = self._summarize_figurative_types(analysis_result['metaphors'])
            
            return {
                'success': True,
                'original_text': text,
                'metaphors': analysis_result['metaphors'],
                'sub_slots': sub_slots,
                'figurative_summary': figurative_summary,
                'confidence': analysis_result['confidence'],
                'metadata': {
                    'handler': self.handler_info,
                    'analysis_method': analysis_result['analysis_method'],
                    'metaphor_count': len(analysis_result['metaphors'])
                }
            }
            
        except Exception as e:
            return self._create_failure_result(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def _create_no_metaphors_result(self, text: str) -> Dict[str, Any]:
        """æ¯”å–»è¡¨ç¾ãªã—ã®çµæœä½œæˆ"""
        return {
            'success': True,
            'original_text': text,
            'metaphors': [],
            'sub_slots': {},
            'figurative_summary': {},
            'confidence': self.config.confidence_settings.get('minimum_confidence', 0.3),
            'metadata': {
                'handler': self.handler_info,
                'analysis_method': 'no_metaphors_detected'
            }
        }
    
    def _build_sub_slots(self, metaphors: List[Dict[str, Any]]) -> Dict[str, str]:
        """ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®æ§‹ç¯‰"""
        sub_slots = {}
        
        for i, metaphor in enumerate(metaphors):
            slot_key = f"sub-metaphor{i+1}"
            sub_slots[slot_key] = metaphor['metaphor']['text']
        
        return sub_slots
    
    def _summarize_figurative_types(self, metaphors: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ¯”å–©ã‚¿ã‚¤ãƒ—åˆ†æã®è¦ç´„"""
        summary = {
            'primary_types': [],
            'sub_types': [],
            'semantic_domains': [],
            'dominant_type': None
        }
        
        type_counts = {}
        
        for metaphor in metaphors:
            primary_type = metaphor['figurative_type']['primary_type']
            sub_types = metaphor['figurative_type']['sub_types']
            source_domain = metaphor['semantics']['source_domain']
            
            summary['primary_types'].append(primary_type)
            summary['sub_types'].extend(sub_types)
            summary['semantic_domains'].append(source_domain)
            
            type_counts[primary_type] = type_counts.get(primary_type, 0) + 1
        
        # ä¸»è¦ãªã‚¿ã‚¤ãƒ—ã®æ±ºå®š
        if type_counts:
            summary['dominant_type'] = max(type_counts.items(), key=lambda x: x[1])[0]
        
        summary['type_distribution'] = type_counts
        
        return summary
    
    def _create_failure_result(self, error_message: str) -> Dict[str, Any]:
        """å¤±æ•—çµæœã®ä½œæˆ"""
        return {
            'success': False,
            'original_text': '',
            'metaphors': [],
            'sub_slots': {},
            'figurative_summary': {},
            'confidence': 0.0,
            'error': error_message,
            'metadata': {
                'handler': self.handler_info,
                'analysis_method': 'error_handling'
            }
        }


if __name__ == "__main__":
    # ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ç‰ˆã®ãƒ†ã‚¹ãƒˆ
    handler = MetaphoricalHandlerClean()
    
    test_sentences = [
        "Time is money.",
        "She dances like a feather in the wind.",
        "The wind whispered through the trees.",
        "His heart is as cold as ice."
    ]
    
    print("ğŸ§ª MetaphoricalHandler - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œå…¨é™¤å»ç‰ˆãƒ†ã‚¹ãƒˆ")
    print("=" * 65)
    
    for sentence in test_sentences:
        result = handler.process(sentence)
        print(f"\nå…¥åŠ›: \"{sentence}\"")
        print(f"æˆåŠŸ: {result['success']}")
        print(f"æ¯”å–©è¡¨ç¾æ•°: {len(result.get('metaphors', []))}")
        print(f"ä¿¡é ¼åº¦: {result.get('confidence', 0):.3f}")
        print(f"ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ: {result.get('sub_slots', {})}")
        if result.get('figurative_summary', {}).get('dominant_type'):
            print(f"ä¸»è¦ã‚¿ã‚¤ãƒ—: {result['figurative_summary']['dominant_type']}")
        print(f"ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä½¿ç”¨: 0ä»¶ âœ…")
