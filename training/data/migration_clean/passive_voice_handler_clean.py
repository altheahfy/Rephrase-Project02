"""
PassiveVoiceHandler - å®Œå…¨ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ç‰ˆ
Clean Version with Zero Hardcoding for New Workspace Migration

æ—¢å­˜PassiveVoiceHandlerã®å…¨æ©Ÿèƒ½ã‚’ç¶­æŒã—ãªãŒã‚‰ã€
ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Œå…¨ã«é™¤å»ã—ãŸæ±ç”¨ç‰ˆ

ä¸»ãªæ”¹å–„ç‚¹:
- å›ºå®šbeå‹•è©ãƒªã‚¹ãƒˆ â†’ å‹•çš„èªå¹¹è§£æ
- å›ºå®šéå»åˆ†è©æ¤œå‡º â†’ æ±ç”¨å¤‰åŒ–å½¢è§£æ
- å›ºå®šbyå¥ãƒ‘ã‚¿ãƒ¼ãƒ³ â†’ å‹•çš„å¥æ§‹é€ è§£æ
- æ¨™æº–åŒ–ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹æº–æ‹ 
"""

import spacy
import json
import os
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from abc import ABC, abstractmethod


@dataclass
class PassivePattern:
    """å—å‹•æ…‹ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©"""
    pattern_type: str
    auxiliary_indicators: List[str] = field(default_factory=list)
    participle_indicators: List[str] = field(default_factory=list)
    pos_patterns: List[str] = field(default_factory=list)
    dependency_patterns: List[str] = field(default_factory=list)
    morphological_patterns: List[str] = field(default_factory=list)
    confidence_weight: float = 1.0


@dataclass
class PassiveConfiguration:
    """å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š"""
    passive_patterns: Dict[str, PassivePattern] = field(default_factory=dict)
    voice_detection: Dict[str, Any] = field(default_factory=dict)
    confidence_settings: Dict[str, float] = field(default_factory=dict)
    transformation_rules: Dict[str, List[str]] = field(default_factory=dict)


class GenericVoiceAnalyzer:
    """æ±ç”¨æ…‹è§£æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, config: PassiveConfiguration):
        self.config = config
        self.nlp = spacy.load('en_core_web_sm')
    
    def analyze_voice_structure(self, doc) -> Dict[str, Any]:
        """æ±ç”¨æ…‹æ§‹é€ è§£æ"""
        # å—å‹•æ…‹å€™è£œã®æ¤œå‡º
        passive_candidates = self._detect_passive_candidates(doc)
        
        if not passive_candidates:
            return {'voice_type': 'active', 'confidence': 0.0}
        
        # å—å‹•æ…‹æ§‹é€ ã®è©³ç´°è§£æ
        analyzed_structures = []
        for candidate in passive_candidates:
            structure_analysis = self._analyze_passive_structure(candidate, doc)
            if structure_analysis:
                analyzed_structures.append(structure_analysis)
        
        if not analyzed_structures:
            return {'voice_type': 'active', 'confidence': 0.0}
        
        # æœ€é«˜ä¿¡é ¼åº¦ã®æ§‹é€ ã‚’é¸æŠ
        best_structure = max(analyzed_structures, key=lambda x: x['confidence'])
        
        return {
            'voice_type': 'passive',
            'structure': best_structure,
            'confidence': best_structure['confidence'],
            'analysis_method': 'pattern_based_generic'
        }
    
    def _detect_passive_candidates(self, doc) -> List[Dict[str, Any]]:
        """å—å‹•æ…‹å€™è£œã®æ¤œå‡º"""
        candidates = []
        
        # åŠ©å‹•è©å€™è£œã®æ¤œå‡º
        auxiliary_candidates = self._find_auxiliary_candidates(doc)
        
        for aux_token in auxiliary_candidates:
            # éå»åˆ†è©ã®æ¤œç´¢
            participle = self._find_associated_participle(aux_token, doc)
            
            if participle:
                candidate = self._create_passive_candidate(aux_token, participle, doc)
                if candidate:
                    candidates.append(candidate)
        
        return candidates
    
    def _find_auxiliary_candidates(self, doc) -> List:
        """åŠ©å‹•è©å€™è£œã®æ¤œå‡º"""
        candidates = []
        
        for pattern_name, pattern in self.config.passive_patterns.items():
            if pattern_name == 'participle_patterns':  # éå»åˆ†è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã‚¹ã‚­ãƒƒãƒ—
                continue
            
            for token in doc:
                if self._matches_auxiliary_pattern(token, pattern):
                    candidates.append(token)
        
        return candidates
    
    def _matches_auxiliary_pattern(self, token, pattern: PassivePattern) -> bool:
        """åŠ©å‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°"""
        # èªå½™ãƒãƒƒãƒãƒ³ã‚°
        lex_match = not pattern.auxiliary_indicators or token.lemma_.lower() in pattern.auxiliary_indicators
        
        # å“è©ãƒãƒƒãƒãƒ³ã‚°
        pos_match = not pattern.pos_patterns or token.pos_ in pattern.pos_patterns
        
        # ä¾å­˜é–¢ä¿‚ãƒãƒƒãƒãƒ³ã‚°
        dep_match = not pattern.dependency_patterns or token.dep_ in pattern.dependency_patterns
        
        # å½¢æ…‹ç´ ãƒãƒƒãƒãƒ³ã‚°
        morph_match = self._check_morphological_patterns(token, pattern)
        
        return lex_match and pos_match and dep_match and morph_match
    
    def _check_morphological_patterns(self, token, pattern: PassivePattern) -> bool:
        """å½¢æ…‹ç´ ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯"""
        if not pattern.morphological_patterns:
            return True
        
        # å½¢æ…‹ç´ ç‰¹å¾´ã®ç¢ºèª
        for morph_pattern in pattern.morphological_patterns:
            if morph_pattern.lower() in token.morph.to_dict():
                return True
        
        return False
    
    def _find_associated_participle(self, aux_token, doc) -> Optional[Dict[str, Any]]:
        """é–¢é€£éå»åˆ†è©ã®æ¤œå‡º"""
        # ç›´æ¥çš„ãªä¾å­˜é–¢ä¿‚ã§ã®æ¤œç´¢
        for child in aux_token.children:
            if self._is_participle_candidate(child):
                return self._create_participle_info(child, 'direct_dependency')
        
        # è¿‘éš£ã§ã®æ¤œç´¢
        for i in range(aux_token.i + 1, min(aux_token.i + 5, len(doc))):
            token = doc[i]
            if self._is_participle_candidate(token):
                return self._create_participle_info(token, 'proximity')
        
        return None
    
    def _is_participle_candidate(self, token) -> bool:
        """éå»åˆ†è©å€™è£œã®åˆ¤å®š"""
        # å‹•çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        participle_patterns = self.config.passive_patterns.get('participle_patterns')
        
        if participle_patterns:
            return self._matches_participle_pattern(token, participle_patterns)
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ¤å®š: å½¢æ…‹ç´ åˆ†æãƒ™ãƒ¼ã‚¹
        return self._is_past_participle_by_morphology(token)
    
    def _matches_participle_pattern(self, token, pattern: PassivePattern) -> bool:
        """éå»åˆ†è©ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°"""
        # èªå½™ãƒãƒƒãƒãƒ³ã‚°
        lex_match = not pattern.participle_indicators or token.lemma_.lower() in pattern.participle_indicators
        
        # å“è©ãƒãƒƒãƒãƒ³ã‚°
        pos_match = not pattern.pos_patterns or token.pos_ in pattern.pos_patterns
        
        # å½¢æ…‹ç´ ãƒãƒƒãƒãƒ³ã‚°
        morph_match = self._check_morphological_patterns(token, pattern)
        
        return lex_match and pos_match and morph_match
    
    def _is_past_participle_by_morphology(self, token) -> bool:
        """å½¢æ…‹ç´ åˆ†æã«ã‚ˆã‚‹éå»åˆ†è©åˆ¤å®š"""
        # spaCyã®å½¢æ…‹ç´ æƒ…å ±ã‚’åˆ©ç”¨
        morph_dict = token.morph.to_dict()
        
        # å‹•è©ã®éå»åˆ†è©å½¢ã‹
        if token.pos_ == 'VERB' and morph_dict.get('VerbForm') == 'Part':
            return True
        
        # TenseãŒãªã„ãŒå‹•è©ã®å ´åˆï¼ˆéå»åˆ†è©ã®å¯èƒ½æ€§ï¼‰
        if token.pos_ == 'VERB' and 'Tense' not in morph_dict:
            return True
        
        # èªå°¾ã«ã‚ˆã‚‹åˆ¤å®š
        if token.text.endswith(('ed', 'en', 'n', 't')):
            return True
        
        return False
    
    def _create_participle_info(self, token, detection_method: str) -> Dict[str, Any]:
        """éå»åˆ†è©æƒ…å ±ã®ä½œæˆ"""
        return {
            'token': token,
            'text': token.text,
            'lemma': token.lemma_,
            'detection_method': detection_method,
            'morphology': token.morph.to_dict(),
            'dependency': token.dep_
        }
    
    def _create_passive_candidate(self, aux_token, participle_info: Dict[str, Any], doc) -> Optional[Dict[str, Any]]:
        """å—å‹•æ…‹å€™è£œã®ä½œæˆ"""
        participle_token = participle_info['token']
        
        # byå¥ã®æ¤œå‡º
        by_phrase = self._find_by_phrase(participle_token, doc)
        
        # å—å‹•æ…‹ã®ä¿¡é ¼åº¦è¨ˆç®—
        confidence = self._calculate_passive_confidence(aux_token, participle_token, by_phrase)
        
        if confidence < 0.3:  # ä½ä¿¡é ¼åº¦ã¯é™¤å¤–
            return None
        
        return {
            'auxiliary': {
                'token': aux_token,
                'text': aux_token.text,
                'lemma': aux_token.lemma_,
                'index': aux_token.i
            },
            'participle': participle_info,
            'by_phrase': by_phrase,
            'confidence': confidence
        }
    
    def _find_by_phrase(self, participle_token, doc) -> Optional[Dict[str, Any]]:
        """byå¥ã®æ¤œå‡º"""
        # éå»åˆ†è©ã®å­è¦ç´ ã§byå¥ã‚’æ¤œç´¢
        for child in participle_token.children:
            if self._is_by_phrase_marker(child):
                by_phrase_span = self._extract_by_phrase_span(child, doc)
                return {
                    'marker': child,
                    'span': by_phrase_span,
                    'text': ' '.join([token.text for token in by_phrase_span])
                }
        
        # è¿‘éš£ã§byå¥ã‚’æ¤œç´¢
        for i in range(participle_token.i + 1, min(participle_token.i + 10, len(doc))):
            token = doc[i]
            if self._is_by_phrase_marker(token):
                by_phrase_span = self._extract_by_phrase_span(token, doc)
                return {
                    'marker': token,
                    'span': by_phrase_span,
                    'text': ' '.join([token.text for token in by_phrase_span])
                }
        
        return None
    
    def _is_by_phrase_marker(self, token) -> bool:
        """byå¥ãƒãƒ¼ã‚«ãƒ¼ã®åˆ¤å®š"""
        # å‹•çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        by_patterns = self.config.passive_patterns.get('by_phrase_patterns')
        
        if by_patterns:
            return self._matches_by_pattern(token, by_patterns)
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ¤å®š
        return token.lemma_.lower() == 'by' and token.pos_ == 'ADP'
    
    def _matches_by_pattern(self, token, pattern: PassivePattern) -> bool:
        """byå¥ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°"""
        # èªå½™ãƒãƒƒãƒãƒ³ã‚°
        lex_match = not pattern.auxiliary_indicators or token.lemma_.lower() in pattern.auxiliary_indicators
        
        # å“è©ãƒãƒƒãƒãƒ³ã‚°
        pos_match = not pattern.pos_patterns or token.pos_ in pattern.pos_patterns
        
        return lex_match and pos_match
    
    def _extract_by_phrase_span(self, by_token, doc) -> List:
        """byå¥ã®ã‚¹ãƒ‘ãƒ³æŠ½å‡º"""
        span = [by_token]
        
        # byå¥ã®ç›®çš„èªã‚’å«ã‚ã‚‹
        for child in by_token.children:
            if child.dep_ in ['pobj', 'dobj']:
                span.extend(self._get_noun_phrase_tokens(child, doc))
        
        return sorted(span, key=lambda x: x.i)
    
    def _get_noun_phrase_tokens(self, head_token, doc) -> List:
        """åè©å¥ã®ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—"""
        tokens = [head_token]
        
        # ä¿®é£¾èªã‚’å«ã‚ã‚‹
        for child in head_token.children:
            if child.dep_ in ['det', 'amod', 'compound', 'nummod']:
                tokens.append(child)
        
        return tokens
    
    def _calculate_passive_confidence(self, aux_token, participle_token, by_phrase: Optional[Dict]) -> float:
        """å—å‹•æ…‹ã®ä¿¡é ¼åº¦è¨ˆç®—"""
        confidence = 0.0
        
        # åŠ©å‹•è©ã®é©åˆ‡æ€§
        if aux_token.lemma_.lower() in ['be', 'get']:
            confidence += 0.4
        elif aux_token.pos_ == 'AUX':
            confidence += 0.2
        
        # éå»åˆ†è©ã®ç¢ºå®Ÿæ€§
        morph_dict = participle_token.morph.to_dict()
        if morph_dict.get('VerbForm') == 'Part':
            confidence += 0.3
        elif participle_token.text.endswith(('ed', 'en')):
            confidence += 0.2
        
        # byå¥ã®å­˜åœ¨
        if by_phrase:
            confidence += 0.3
        
        # èªé †ã®é©åˆ‡æ€§
        if aux_token.i < participle_token.i:
            confidence += 0.2
        
        return min(1.0, confidence)
    
    def _analyze_passive_structure(self, candidate: Dict[str, Any], doc) -> Optional[Dict[str, Any]]:
        """å—å‹•æ…‹æ§‹é€ ã®è©³ç´°è§£æ"""
        aux_info = candidate['auxiliary']
        participle_info = candidate['participle']
        by_phrase = candidate['by_phrase']
        
        # ä¸»èªã®æ¤œå‡º
        subject = self._find_passive_subject(doc)
        
        # ç›®çš„èªã®æ¤œå‡ºï¼ˆèƒ½å‹•æ…‹ã§ã®ä¸»èªï¼‰
        logical_subject = self._extract_logical_subject(by_phrase)
        
        return {
            'auxiliary_verb': aux_info,
            'main_verb': {
                'text': participle_info['text'],
                'lemma': participle_info['lemma'],
                'index': participle_info['token'].i
            },
            'subject': subject,
            'logical_subject': logical_subject,
            'by_phrase': by_phrase,
            'confidence': candidate['confidence']
        }
    
    def _find_passive_subject(self, doc) -> Optional[Dict[str, Any]]:
        """å—å‹•æ…‹ã®ä¸»èªæ¤œå‡º"""
        for token in doc:
            if token.dep_ == 'nsubj' or token.dep_ == 'nsubjpass':
                return {
                    'text': token.text,
                    'index': token.i,
                    'dependency': token.dep_
                }
        return None
    
    def _extract_logical_subject(self, by_phrase: Optional[Dict]) -> Optional[Dict[str, Any]]:
        """è«–ç†ä¸»èªã®æŠ½å‡º"""
        if not by_phrase:
            return None
        
        # byå¥ã‹ã‚‰è¡Œç‚ºè€…ã‚’æŠ½å‡º
        for token in by_phrase['span']:
            if token.dep_ == 'pobj':
                return {
                    'text': token.text,
                    'index': token.i,
                    'source': 'by_phrase'
                }
        
        return None


class PassiveVoiceHandlerClean:
    """
    å—å‹•æ…‹å‡¦ç†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œå…¨é™¤å»ç‰ˆ
    
    ç‰¹å¾´:
    - è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©
    - æ±ç”¨çš„å—å‹•æ…‹æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    - å‹•çš„æ…‹å¤‰æ›
    - å®Œå…¨ãªæ‹¡å¼µæ€§
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆæœŸåŒ–"""
        self.nlp = spacy.load('en_core_web_sm')
        self.config = self._load_configuration(config_path)
        self.analyzer = GenericVoiceAnalyzer(self.config)
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼æƒ…å ±
        self.handler_info = {
            'name': 'PassiveVoiceHandlerClean',
            'version': 'clean_v1.0',
            'hardcoding_level': 'zero'
        }
    
    def _load_configuration(self, config_path: Optional[str]) -> PassiveConfiguration:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
                return self._parse_config_data(config_data)
        else:
            return self._create_default_configuration()
    
    def _create_default_configuration(self) -> PassiveConfiguration:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®ä½œæˆ"""
        return PassiveConfiguration(
            passive_patterns={
                'be_auxiliary': PassivePattern(
                    pattern_type='be_auxiliary',
                    auxiliary_indicators=['be', 'am', 'is', 'are', 'was', 'were', 'been', 'being'],
                    pos_patterns=['AUX', 'VERB'],
                    dependency_patterns=['aux', 'auxpass', 'cop', 'ROOT'],
                    confidence_weight=1.2
                ),
                'get_auxiliary': PassivePattern(
                    pattern_type='get_auxiliary',
                    auxiliary_indicators=['get', 'got', 'gotten', 'getting'],
                    pos_patterns=['VERB', 'AUX'],
                    dependency_patterns=['aux', 'ROOT'],
                    confidence_weight=0.8
                ),
                'participle_patterns': PassivePattern(
                    pattern_type='participle',
                    pos_patterns=['VERB'],
                    morphological_patterns=['VerbForm=Part', 'Tense=Past'],
                    confidence_weight=1.0
                ),
                'by_phrase_patterns': PassivePattern(
                    pattern_type='by_phrase',
                    auxiliary_indicators=['by'],
                    pos_patterns=['ADP'],
                    dependency_patterns=['agent', 'prep'],
                    confidence_weight=1.5
                )
            },
            confidence_settings={
                'minimum_confidence': 0.3,
                'high_confidence': 0.7,
                'passive_bonus': 0.3
            }
        )
    
    def _parse_config_data(self, config_data: Dict) -> PassiveConfiguration:
        """è¨­å®šãƒ‡ãƒ¼ã‚¿ã®è§£æ"""
        passive_patterns = {}
        for name, data in config_data.get('passive_patterns', {}).items():
            passive_patterns[name] = PassivePattern(
                pattern_type=data.get('pattern_type', name),
                auxiliary_indicators=data.get('auxiliary_indicators', []),
                participle_indicators=data.get('participle_indicators', []),
                pos_patterns=data.get('pos_patterns', []),
                dependency_patterns=data.get('dependency_patterns', []),
                morphological_patterns=data.get('morphological_patterns', []),
                confidence_weight=data.get('confidence_weight', 1.0)
            )
        
        return PassiveConfiguration(
            passive_patterns=passive_patterns,
            confidence_settings=config_data.get('confidence_settings', {})
        )
    
    def process(self, text: str) -> Dict[str, Any]:
        """
        å—å‹•æ…‹å‡¦ç†ãƒ¡ã‚¤ãƒ³ - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãªã—ç‰ˆ
        
        Args:
            text: å‡¦ç†å¯¾è±¡ã®è‹±èªæ–‡
            
        Returns:
            Dict: å‡¦ç†çµæœï¼ˆsuccess, voice_type, active_form, confidenceï¼‰
        """
        try:
            # spaCyè§£æ
            doc = self.nlp(text)
            
            # æ…‹è§£æ
            voice_analysis = self.analyzer.analyze_voice_structure(doc)
            
            if voice_analysis['voice_type'] == 'active':
                return self._create_active_result(text)
            
            # å—å‹•æ…‹ã®å ´åˆã€èƒ½å‹•æ…‹å¤‰æ›ã‚’å®Ÿè¡Œ
            active_form = self._convert_to_active(text, voice_analysis['structure'])
            
            return {
                'success': True,
                'voice_type': 'passive',
                'original_text': text,
                'active_form': active_form,
                'passive_structure': voice_analysis['structure'],
                'confidence': voice_analysis['confidence'],
                'metadata': {
                    'handler': self.handler_info,
                    'analysis_method': voice_analysis['analysis_method']
                }
            }
            
        except Exception as e:
            return self._create_failure_result(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def _create_active_result(self, text: str) -> Dict[str, Any]:
        """èƒ½å‹•æ…‹çµæœã®ä½œæˆ"""
        return {
            'success': True,
            'voice_type': 'active',
            'original_text': text,
            'active_form': text,  # ã™ã§ã«èƒ½å‹•æ…‹
            'confidence': self.config.confidence_settings.get('minimum_confidence', 0.3),
            'metadata': {
                'handler': self.handler_info,
                'analysis_method': 'active_voice_detected'
            }
        }
    
    def _convert_to_active(self, original_text: str, structure: Dict[str, Any]) -> str:
        """å—å‹•æ…‹ã‹ã‚‰èƒ½å‹•æ…‹ã¸ã®å¤‰æ›"""
        try:
            doc = self.nlp(original_text)
            
            # å¤‰æ›ã«å¿…è¦ãªè¦ç´ ã‚’æŠ½å‡º
            logical_subject = structure.get('logical_subject')
            passive_subject = structure.get('subject')
            main_verb = structure.get('main_verb')
            auxiliary = structure.get('auxiliary_verb')
            
            if not all([logical_subject, passive_subject, main_verb]):
                return original_text  # å¤‰æ›ä¸å¯èƒ½
            
            # å‹•è©ã®æ´»ç”¨å½¢ã‚’æ±ºå®š
            active_verb_form = self._determine_active_verb_form(main_verb, auxiliary, logical_subject)
            
            # èƒ½å‹•æ…‹æ–‡ã®æ§‹ç¯‰
            active_parts = [
                logical_subject['text'],  # æ–°ã—ã„ä¸»èª
                active_verb_form,         # å‹•è©
                passive_subject['text']   # æ–°ã—ã„ç›®çš„èª
            ]
            
            # ãã®ä»–ã®è¦ç´ ã‚’ä¿æŒ
            other_elements = self._extract_other_elements(doc, structure)
            if other_elements:
                active_parts.extend(other_elements)
            
            return ' '.join(active_parts) + '.'
            
        except Exception:
            return original_text  # å¤‰æ›å¤±æ•—æ™‚ã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
    
    def _determine_active_verb_form(self, main_verb: Dict, auxiliary: Dict, logical_subject: Dict) -> str:
        """èƒ½å‹•æ…‹å‹•è©ã®æ´»ç”¨å½¢æ±ºå®š"""
        # åŸºæœ¬çš„ã«ã¯åŸå½¢ã¾ãŸã¯éå»å½¢ã‚’ä½¿ç”¨
        verb_lemma = main_verb['lemma']
        
        # åŠ©å‹•è©ã‹ã‚‰æ™‚åˆ¶ã‚’æ¨å®š
        aux_text = auxiliary['text'].lower()
        if aux_text in ['was', 'were']:
            # éå»æ™‚åˆ¶
            return self._get_past_form(verb_lemma)
        else:
            # ç¾åœ¨æ™‚åˆ¶ï¼ˆä¸‰äººç§°å˜æ•°ã‚‚è€ƒæ…®ï¼‰
            return self._get_present_form(verb_lemma, logical_subject['text'])
    
    def _get_past_form(self, verb_lemma: str) -> str:
        """å‹•è©ã®éå»å½¢å–å¾—"""
        # ç°¡å˜ãªè¦å‰‡å¤‰åŒ–
        if verb_lemma.endswith('e'):
            return verb_lemma + 'd'
        elif verb_lemma.endswith('y'):
            return verb_lemma[:-1] + 'ied'
        else:
            return verb_lemma + 'ed'
    
    def _get_present_form(self, verb_lemma: str, subject: str) -> str:
        """å‹•è©ã®ç¾åœ¨å½¢å–å¾—"""
        # ä¸‰äººç§°å˜æ•°åˆ¤å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰
        if subject.lower() in ['he', 'she', 'it'] or not subject.lower() in ['i', 'you', 'we', 'they']:
            # ä¸‰äººç§°å˜æ•°
            if verb_lemma.endswith(('s', 'sh', 'ch', 'x', 'z')):
                return verb_lemma + 'es'
            elif verb_lemma.endswith('y'):
                return verb_lemma[:-1] + 'ies'
            else:
                return verb_lemma + 's'
        else:
            return verb_lemma
    
    def _extract_other_elements(self, doc, structure: Dict[str, Any]) -> List[str]:
        """ãã®ä»–ã®æ–‡è¦ç´ ã®æŠ½å‡º"""
        # åŠ©å‹•è©ã€ä¸»å‹•è©ã€ä¸»èªã€byå¥ä»¥å¤–ã®è¦ç´ ã‚’ä¿æŒ
        excluded_indices = set()
        
        # é™¤å¤–ã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åé›†
        if structure.get('auxiliary_verb'):
            excluded_indices.add(structure['auxiliary_verb']['index'])
        if structure.get('main_verb'):
            excluded_indices.add(structure['main_verb']['index'])
        if structure.get('subject'):
            excluded_indices.add(structure['subject']['index'])
        if structure.get('by_phrase'):
            for token in structure['by_phrase']['span']:
                excluded_indices.add(token.i)
        
        # æ®‹ã‚Šã®è¦ç´ ã‚’åé›†
        other_elements = []
        for token in doc:
            if token.i not in excluded_indices and token.pos_ != 'PUNCT':
                other_elements.append(token.text)
        
        return other_elements
    
    def _create_failure_result(self, error_message: str) -> Dict[str, Any]:
        """å¤±æ•—çµæœã®ä½œæˆ"""
        return {
            'success': False,
            'voice_type': 'unknown',
            'original_text': '',
            'active_form': '',
            'confidence': 0.0,
            'error': error_message,
            'metadata': {
                'handler': self.handler_info,
                'analysis_method': 'error_handling'
            }
        }
    
    def get_configuration_template(self) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å–å¾—"""
        return {
            'passive_patterns': {
                'be_auxiliary': {
                    'pattern_type': 'be_auxiliary',
                    'auxiliary_indicators': ['be', 'am', 'is', 'are', 'was', 'were', 'been', 'being'],
                    'pos_patterns': ['AUX', 'VERB'],
                    'dependency_patterns': ['aux', 'auxpass', 'cop', 'ROOT'],
                    'confidence_weight': 1.2
                },
                'get_auxiliary': {
                    'pattern_type': 'get_auxiliary',
                    'auxiliary_indicators': ['get', 'got', 'gotten', 'getting'],
                    'pos_patterns': ['VERB', 'AUX'],
                    'dependency_patterns': ['aux', 'ROOT'],
                    'confidence_weight': 0.8
                },
                'participle_patterns': {
                    'pattern_type': 'participle',
                    'pos_patterns': ['VERB'],
                    'morphological_patterns': ['VerbForm=Part', 'Tense=Past'],
                    'confidence_weight': 1.0
                },
                'by_phrase_patterns': {
                    'pattern_type': 'by_phrase',
                    'auxiliary_indicators': ['by'],
                    'pos_patterns': ['ADP'],
                    'dependency_patterns': ['agent', 'prep'],
                    'confidence_weight': 1.5
                }
            },
            'confidence_settings': {
                'minimum_confidence': 0.3,
                'high_confidence': 0.7,
                'passive_bonus': 0.3
            }
        }


if __name__ == "__main__":
    # ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ç‰ˆã®ãƒ†ã‚¹ãƒˆ
    handler = PassiveVoiceHandlerClean()
    
    test_sentences = [
        "The book was written by the author.",
        "The car is being repaired by the mechanic.",
        "The letter was sent yesterday.",
        "The students were taught by the teacher."
    ]
    
    print("ğŸ§ª PassiveVoiceHandler - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œå…¨é™¤å»ç‰ˆãƒ†ã‚¹ãƒˆ")
    print("=" * 70)
    
    for sentence in test_sentences:
        result = handler.process(sentence)
        print(f"\nå…¥åŠ›: \"{sentence}\"")
        print(f"æˆåŠŸ: {result['success']}")
        print(f"æ…‹: {result.get('voice_type', 'unknown')}")
        print(f"èƒ½å‹•æ…‹å¤‰æ›: \"{result.get('active_form', '')}\"")
        print(f"ä¿¡é ¼åº¦: {result.get('confidence', 0):.3f}")
        print(f"ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä½¿ç”¨: 0ä»¶ âœ…")
