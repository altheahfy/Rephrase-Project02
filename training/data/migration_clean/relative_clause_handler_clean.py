"""
RelativeClauseHandler - å®Œå…¨ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ç‰ˆ
Clean Version with Zero Hardcoding for New Workspace Migration

æ—¢å­˜RelativeClauseHandlerã®å…¨æ©Ÿèƒ½ã‚’ç¶­æŒã—ãªãŒã‚‰ã€
ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Œå…¨ã«é™¤å»ã—ãŸæ±ç”¨ç‰ˆ

ä¸»ãªæ”¹å–„ç‚¹:
- å›ºå®šé–¢ä¿‚ä»£åè©ãƒªã‚¹ãƒˆ â†’ å‹•çš„ãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ
- å›ºå®šå…ˆè¡Œè©ãƒ‘ã‚¿ãƒ¼ãƒ³ â†’ æ±ç”¨åè©å¥æ¤œå‡º
- å›ºå®šç¯€å¢ƒç•Œåˆ¤å®š â†’ å‹•çš„æ§‹é€ è§£æ
- æ¨™æº–åŒ–ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹æº–æ‹ 
"""

import spacy
import json
import os
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from abc import ABC, abstractmethod


@dataclass
class RelativePattern:
    """é–¢ä¿‚ç¯€ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©"""
    pattern_type: str
    relative_indicators: List[str] = field(default_factory=list)
    antecedent_indicators: List[str] = field(default_factory=list)
    pos_patterns: List[str] = field(default_factory=list)
    dependency_patterns: List[str] = field(default_factory=list)
    position_rules: List[str] = field(default_factory=list)
    confidence_weight: float = 1.0


@dataclass
class ClauseConfiguration:
    """é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š"""
    relative_patterns: Dict[str, RelativePattern] = field(default_factory=dict)
    boundary_detection: Dict[str, Any] = field(default_factory=dict)
    confidence_settings: Dict[str, float] = field(default_factory=dict)
    separation_rules: Dict[str, List[str]] = field(default_factory=dict)


class GenericClauseAnalyzer:
    """æ±ç”¨é–¢ä¿‚ç¯€è§£æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, config: ClauseConfiguration):
        self.config = config
        self.nlp = spacy.load('en_core_web_sm')
    
    def analyze_relative_clauses(self, doc) -> Dict[str, Any]:
        """æ±ç”¨é–¢ä¿‚ç¯€è§£æ"""
        # é–¢ä¿‚ä»£åè©å€™è£œã®æ¤œå‡º
        relative_candidates = self._detect_relative_candidates(doc)
        
        if not relative_candidates:
            return {'clauses': [], 'confidence': 0.0}
        
        # é–¢ä¿‚ç¯€ã®æ§‹é€ è§£æ
        analyzed_clauses = []
        for candidate in relative_candidates:
            clause_analysis = self._analyze_clause_structure(candidate, doc)
            if clause_analysis:
                analyzed_clauses.append(clause_analysis)
        
        # ä¿¡é ¼åº¦è¨ˆç®—
        confidence = self._calculate_clause_confidence(analyzed_clauses)
        
        return {
            'clauses': analyzed_clauses,
            'confidence': confidence,
            'analysis_method': 'pattern_based_generic'
        }
    
    def _detect_relative_candidates(self, doc) -> List[Dict[str, Any]]:
        """é–¢ä¿‚ä»£åè©å€™è£œã®æ¤œå‡º"""
        candidates = []
        
        for pattern_name, pattern in self.config.relative_patterns.items():
            if pattern_name == 'antecedent_patterns':  # å…ˆè¡Œè©ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã‚¹ã‚­ãƒƒãƒ—
                continue
            
            for token in doc:
                if self._matches_relative_pattern(token, pattern):
                    candidate = self._create_relative_candidate(token, pattern_name, pattern, doc)
                    if candidate:
                        candidates.append(candidate)
        
        return candidates
    
    def _matches_relative_pattern(self, token, pattern: RelativePattern) -> bool:
        """é–¢ä¿‚ä»£åè©ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°"""
        # èªå½™ãƒãƒƒãƒãƒ³ã‚°
        lex_match = not pattern.relative_indicators or token.lemma_.lower() in pattern.relative_indicators
        
        # å“è©ãƒãƒƒãƒãƒ³ã‚°
        pos_match = not pattern.pos_patterns or token.pos_ in pattern.pos_patterns
        
        # ä¾å­˜é–¢ä¿‚ãƒãƒƒãƒãƒ³ã‚°
        dep_match = not pattern.dependency_patterns or token.dep_ in pattern.dependency_patterns
        
        return lex_match and pos_match and dep_match
    
    def _create_relative_candidate(self, token, pattern_name: str, pattern: RelativePattern, doc) -> Optional[Dict[str, Any]]:
        """é–¢ä¿‚ä»£åè©å€™è£œã®ä½œæˆ"""
        # å…ˆè¡Œè©ã®æ¤œå‡º
        antecedent = self._find_antecedent(token, doc)
        
        if not antecedent:
            return None
        
        # é–¢ä¿‚ç¯€ã®å¢ƒç•Œæ¤œå‡º
        clause_span = self._detect_clause_boundaries(token, doc)
        
        if not clause_span:
            return None
        
        # æ§‹é€ è§£æ
        clause_structure = self._analyze_internal_structure(token, clause_span, doc)
        
        return {
            'relative_pronoun': token,
            'antecedent': antecedent,
            'clause_span': clause_span,
            'clause_structure': clause_structure,
            'pattern_type': pattern_name,
            'confidence_weight': pattern.confidence_weight
        }
    
    def _find_antecedent(self, relative_token, doc) -> Optional[Dict[str, Any]]:
        """å…ˆè¡Œè©ã®å‹•çš„æ¤œå‡º"""
        # ç›´å‰ã®åè©å¥ã‚’æ¤œç´¢
        antecedent_candidates = []
        
        for i in range(relative_token.i - 1, -1, -1):
            token = doc[i]
            
            if self._is_potential_antecedent(token):
                # åè©å¥ã®ç¯„å›²ã‚’æ±ºå®š
                noun_phrase_span = self._get_noun_phrase_span(token, doc)
                
                # é–¢ä¿‚æ€§ã®å¼·ã•ã‚’è¨ˆç®—
                relationship_score = self._calculate_antecedent_score(
                    noun_phrase_span, relative_token, doc
                )
                
                if relationship_score > 0.3:  # é–¾å€¤ä»¥ä¸Šã®é–¢ä¿‚æ€§
                    antecedent_candidates.append({
                        'tokens': noun_phrase_span,
                        'head': token,
                        'text': ' '.join([t.text for t in noun_phrase_span]),
                        'score': relationship_score
                    })
        
        if not antecedent_candidates:
            return None
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®å…ˆè¡Œè©ã‚’é¸æŠ
        return max(antecedent_candidates, key=lambda x: x['score'])
    
    def _is_potential_antecedent(self, token) -> bool:
        """å…ˆè¡Œè©å€™è£œã®åˆ¤å®š"""
        # å‹•çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        antecedent_patterns = self.config.relative_patterns.get('antecedent_patterns')
        
        if antecedent_patterns:
            return self._matches_relative_pattern(token, antecedent_patterns)
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ¤å®š: åè©ç³»ã®å“è©
        return token.pos_ in ['NOUN', 'PROPN', 'PRON']
    
    def _get_noun_phrase_span(self, head_token, doc) -> List:
        """åè©å¥ã®ã‚¹ãƒ‘ãƒ³å–å¾—"""
        # åè©å¥ã®é–‹å§‹ä½ç½®ã‚’æ¤œç´¢
        start_idx = head_token.i
        for token in reversed(list(head_token.lefts)):
            if token.pos_ in ['DET', 'ADJ', 'NUM'] or token.dep_ in ['det', 'amod', 'nummod']:
                start_idx = min(start_idx, token.i)
        
        # åè©å¥ã®çµ‚äº†ä½ç½®ã‚’æ¤œç´¢
        end_idx = head_token.i
        for token in head_token.rights:
            if token.pos_ in ['ADJ', 'NOUN'] or token.dep_ in ['amod', 'compound']:
                end_idx = max(end_idx, token.i)
        
        return [doc[i] for i in range(start_idx, end_idx + 1)]
    
    def _calculate_antecedent_score(self, noun_phrase_span, relative_token, doc) -> float:
        """å…ˆè¡Œè©ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        score = 0.0
        head_token = noun_phrase_span[-1]  # åè©å¥ã®ä¸»è¦éƒ¨
        
        # è·é›¢ã«ã‚ˆã‚‹è©•ä¾¡ï¼ˆè¿‘ã„ã»ã©é«˜ã„ï¼‰
        distance = relative_token.i - head_token.i
        if distance == 1:
            score += 0.8
        elif distance <= 3:
            score += 0.6
        elif distance <= 5:
            score += 0.3
        
        # å¥èª­ç‚¹ã«ã‚ˆã‚‹åŒºåˆ‡ã‚Šã‚’è€ƒæ…®
        has_punctuation = any(token.pos_ == 'PUNCT' 
                            for token in doc[head_token.i:relative_token.i])
        if not has_punctuation:
            score += 0.2
        
        # åè©å¥ã®å®Œå…¨æ€§
        if len(noun_phrase_span) > 1:  # ä¿®é£¾èªä»˜ãã®åè©å¥
            score += 0.3
        
        return score
    
    def _detect_clause_boundaries(self, relative_token, doc) -> Optional[Tuple[int, int]]:
        """é–¢ä¿‚ç¯€ã®å¢ƒç•Œæ¤œå‡º"""
        start_idx = relative_token.i
        
        # é–¢ä¿‚ç¯€ã®çµ‚äº†ä½ç½®ã‚’æ¤œç´¢
        end_idx = self._find_clause_end(relative_token, doc)
        
        if end_idx is None or end_idx <= start_idx:
            return None
        
        return (start_idx, end_idx)
    
    def _find_clause_end(self, relative_token, doc) -> Optional[int]:
        """é–¢ä¿‚ç¯€ã®çµ‚äº†ä½ç½®æ¤œå‡º"""
        # ä¾å­˜é–¢ä¿‚ãƒ™ãƒ¼ã‚¹ã®æ¤œå‡º
        clause_tokens = set()
        self._collect_clause_tokens(relative_token, clause_tokens)
        
        if not clause_tokens:
            return None
        
        # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        return max(token.i for token in clause_tokens)
    
    def _collect_clause_tokens(self, token, clause_tokens: set):
        """é–¢ä¿‚ç¯€ãƒˆãƒ¼ã‚¯ãƒ³ã®åé›†ï¼ˆå†å¸°çš„ï¼‰"""
        if token in clause_tokens:
            return
        
        clause_tokens.add(token)
        
        # å­ãƒˆãƒ¼ã‚¯ãƒ³ã‚‚åé›†
        for child in token.children:
            # ä¸»ç¯€å‹•è©ã«æˆ»ã‚‰ãªã„ã‚ˆã†åˆ¶å¾¡
            if child.dep_ not in ['ROOT', 'conj']:
                self._collect_clause_tokens(child, clause_tokens)
    
    def _analyze_internal_structure(self, relative_token, clause_span: Tuple[int, int], doc) -> Dict[str, Any]:
        """é–¢ä¿‚ç¯€å†…éƒ¨æ§‹é€ ã®è§£æ"""
        start_idx, end_idx = clause_span
        clause_tokens = doc[start_idx:end_idx + 1]
        
        # é–¢ä¿‚ç¯€å†…ã®å‹•è©æ¤œå‡º
        verbs = [token for token in clause_tokens if token.pos_ in ['VERB', 'AUX']]
        
        # é–¢ä¿‚ç¯€å†…ã®ç›®çš„èªãƒ»è£œèªæ¤œå‡º
        objects = [token for token in clause_tokens 
                  if token.dep_ in ['dobj', 'iobj', 'pobj', 'attr']]
        
        # ä¿®é£¾èªæ¤œå‡º
        modifiers = [token for token in clause_tokens 
                    if token.dep_ in ['advmod', 'amod', 'npadvmod']]
        
        return {
            'verbs': [{'text': v.text, 'pos': v.pos_, 'lemma': v.lemma_} for v in verbs],
            'objects': [{'text': o.text, 'dep': o.dep_} for o in objects],
            'modifiers': [{'text': m.text, 'dep': m.dep_} for m in modifiers],
            'clause_length': len(clause_tokens)
        }
    
    def _analyze_clause_structure(self, candidate: Dict[str, Any], doc) -> Optional[Dict[str, Any]]:
        """é–¢ä¿‚ç¯€æ§‹é€ ã®è©³ç´°è§£æ"""
        relative_token = candidate['relative_pronoun']
        antecedent = candidate['antecedent']
        clause_span = candidate['clause_span']
        
        # é–¢ä¿‚ç¯€ã®æ©Ÿèƒ½åˆ†æ
        function = self._determine_clause_function(relative_token, doc)
        
        # ä¿¡é ¼åº¦è¨ˆç®—
        confidence = self._calculate_individual_confidence(candidate)
        
        if confidence < 0.4:  # ä½ä¿¡é ¼åº¦ã¯é™¤å¤–
            return None
        
        return {
            'relative_pronoun': {
                'text': relative_token.text,
                'index': relative_token.i,
                'function': function
            },
            'antecedent': {
                'text': antecedent['text'],
                'head_index': antecedent['head'].i,
                'span_indices': [t.i for t in antecedent['tokens']]
            },
            'clause_span': {
                'start': clause_span[0],
                'end': clause_span[1],
                'text': ' '.join([doc[i].text for i in range(clause_span[0], clause_span[1] + 1)])
            },
            'internal_structure': candidate['clause_structure'],
            'confidence': confidence,
            'pattern_type': candidate['pattern_type']
        }
    
    def _determine_clause_function(self, relative_token, doc) -> str:
        """é–¢ä¿‚ç¯€ã®æ©Ÿèƒ½æ±ºå®š"""
        # ä¾å­˜é–¢ä¿‚ã«åŸºã¥ãæ©Ÿèƒ½åˆ†æ
        if relative_token.dep_ in ['nsubj', 'nsubjpass']:
            return 'subject'
        elif relative_token.dep_ in ['dobj', 'iobj']:
            return 'object'
        elif relative_token.dep_ in ['pobj']:
            return 'prepositional_object'
        else:
            return 'modifier'
    
    def _calculate_individual_confidence(self, candidate: Dict[str, Any]) -> float:
        """å€‹åˆ¥é–¢ä¿‚ç¯€ã®ä¿¡é ¼åº¦è¨ˆç®—"""
        base_confidence = 0.5
        
        # å…ˆè¡Œè©ã®æ˜ç¢ºã•
        base_confidence += candidate['antecedent']['score'] * 0.3
        
        # é–¢ä¿‚ç¯€ã®é•·ã•ï¼ˆé©åº¦ãªé•·ã•ãŒè‰¯ã„ï¼‰
        clause_length = candidate['clause_structure']['clause_length']
        if 3 <= clause_length <= 10:
            base_confidence += 0.2
        elif clause_length > 10:
            base_confidence -= 0.1
        
        # å‹•è©ã®å­˜åœ¨
        if candidate['clause_structure']['verbs']:
            base_confidence += 0.2
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é‡ã¿
        base_confidence *= candidate['confidence_weight']
        
        return min(1.0, base_confidence)
    
    def _calculate_clause_confidence(self, analyzed_clauses: List[Dict[str, Any]]) -> float:
        """å…¨ä½“ã®é–¢ä¿‚ç¯€è§£æä¿¡é ¼åº¦"""
        if not analyzed_clauses:
            return 0.0
        
        total_confidence = sum(clause['confidence'] for clause in analyzed_clauses)
        return min(1.0, total_confidence / len(analyzed_clauses))


class RelativeClauseHandlerClean:
    """
    é–¢ä¿‚ç¯€å‡¦ç†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œå…¨é™¤å»ç‰ˆ
    
    ç‰¹å¾´:
    - è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©
    - æ±ç”¨çš„é–¢ä¿‚ç¯€æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    - å‹•çš„å¢ƒç•Œæ¤œå‡º
    - å®Œå…¨ãªæ‹¡å¼µæ€§
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆæœŸåŒ–"""
        self.nlp = spacy.load('en_core_web_sm')
        self.config = self._load_configuration(config_path)
        self.analyzer = GenericClauseAnalyzer(self.config)
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼æƒ…å ±
        self.handler_info = {
            'name': 'RelativeClauseHandlerClean',
            'version': 'clean_v1.0',
            'hardcoding_level': 'zero'
        }
    
    def _load_configuration(self, config_path: Optional[str]) -> ClauseConfiguration:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
                return self._parse_config_data(config_data)
        else:
            return self._create_default_configuration()
    
    def _create_default_configuration(self) -> ClauseConfiguration:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®ä½œæˆ"""
        return ClauseConfiguration(
            relative_patterns={
                'wh_relatives': RelativePattern(
                    pattern_type='wh_relative',
                    relative_indicators=['who', 'whom', 'whose', 'which', 'that', 'where', 'when', 'why'],
                    pos_patterns=['PRON', 'DET', 'ADV'],
                    dependency_patterns=['nsubj', 'dobj', 'nsubjpass', 'pobj', 'advmod'],
                    confidence_weight=1.2
                ),
                'that_relatives': RelativePattern(
                    pattern_type='that_relative',
                    relative_indicators=['that'],
                    pos_patterns=['PRON', 'SCONJ'],
                    dependency_patterns=['nsubj', 'dobj', 'mark'],
                    confidence_weight=1.0
                ),
                'antecedent_patterns': RelativePattern(
                    pattern_type='antecedent',
                    pos_patterns=['NOUN', 'PROPN', 'PRON'],
                    dependency_patterns=['ROOT', 'nsubj', 'dobj', 'pobj'],
                    confidence_weight=1.0
                )
            },
            boundary_detection={
                'end_markers': ['PUNCT', 'CONJ'],
                'depth_limit': 10
            },
            confidence_settings={
                'minimum_confidence': 0.4,
                'high_confidence': 0.8,
                'clause_bonus': 0.2
            }
        )
    
    def _parse_config_data(self, config_data: Dict) -> ClauseConfiguration:
        """è¨­å®šãƒ‡ãƒ¼ã‚¿ã®è§£æ"""
        relative_patterns = {}
        for name, data in config_data.get('relative_patterns', {}).items():
            relative_patterns[name] = RelativePattern(
                pattern_type=data.get('pattern_type', name),
                relative_indicators=data.get('relative_indicators', []),
                antecedent_indicators=data.get('antecedent_indicators', []),
                pos_patterns=data.get('pos_patterns', []),
                dependency_patterns=data.get('dependency_patterns', []),
                position_rules=data.get('position_rules', []),
                confidence_weight=data.get('confidence_weight', 1.0)
            )
        
        return ClauseConfiguration(
            relative_patterns=relative_patterns,
            boundary_detection=config_data.get('boundary_detection', {}),
            confidence_settings=config_data.get('confidence_settings', {})
        )
    
    def process(self, text: str) -> Dict[str, Any]:
        """
        é–¢ä¿‚ç¯€å‡¦ç†ãƒ¡ã‚¤ãƒ³ - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãªã—ç‰ˆ
        
        Args:
            text: å‡¦ç†å¯¾è±¡ã®è‹±èªæ–‡
            
        Returns:
            Dict: å‡¦ç†çµæœï¼ˆsuccess, clauses, separated_text, confidenceï¼‰
        """
        try:
            # spaCyè§£æ
            doc = self.nlp(text)
            
            # é–¢ä¿‚ç¯€è§£æ
            analysis_result = self.analyzer.analyze_relative_clauses(doc)
            
            if not analysis_result['clauses']:
                return self._create_no_clauses_result(text)
            
            # é–¢ä¿‚ç¯€åˆ†é›¢ãƒ†ã‚­ã‚¹ãƒˆã®ç”Ÿæˆ
            separated_text = self._generate_separated_text(doc, analysis_result['clauses'])
            
            # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®æ§‹ç¯‰
            sub_slots = self._build_sub_slots(analysis_result['clauses'])
            
            return {
                'success': True,
                'separated_text': separated_text,
                'relative_clauses': analysis_result['clauses'],
                'sub_slots': sub_slots,
                'confidence': analysis_result['confidence'],
                'metadata': {
                    'handler': self.handler_info,
                    'analysis_method': analysis_result['analysis_method'],
                    'clause_count': len(analysis_result['clauses'])
                }
            }
            
        except Exception as e:
            return self._create_failure_result(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def _create_no_clauses_result(self, text: str) -> Dict[str, Any]:
        """é–¢ä¿‚ç¯€ãªã—ã®çµæœä½œæˆ"""
        return {
            'success': True,
            'separated_text': text,
            'relative_clauses': [],
            'sub_slots': {},
            'confidence': self.config.confidence_settings.get('minimum_confidence', 0.4),
            'metadata': {
                'handler': self.handler_info,
                'analysis_method': 'no_clauses_detected'
            }
        }
    
    def _generate_separated_text(self, doc, clauses: List[Dict[str, Any]]) -> str:
        """é–¢ä¿‚ç¯€åˆ†é›¢ãƒ†ã‚­ã‚¹ãƒˆã®ç”Ÿæˆ"""
        # é–¢ä¿‚ç¯€ã¨ã—ã¦èªè­˜ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åé›†
        clause_indices = set()
        
        for clause in clauses:
            start = clause['clause_span']['start']
            end = clause['clause_span']['end']
            for i in range(start, end + 1):
                clause_indices.add(i)
        
        # é–¢ä¿‚ç¯€ä»¥å¤–ã®ãƒˆãƒ¼ã‚¯ãƒ³ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’å†æ§‹ç¯‰
        remaining_tokens = [token.text for i, token in enumerate(doc) 
                          if i not in clause_indices]
        
        return ' '.join(remaining_tokens)
    
    def _build_sub_slots(self, clauses: List[Dict[str, Any]]) -> Dict[str, str]:
        """ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®æ§‹ç¯‰"""
        sub_slots = {}
        
        for i, clause in enumerate(clauses):
            slot_key = f"sub-rel{i+1}"
            sub_slots[slot_key] = clause['clause_span']['text']
        
        return sub_slots
    
    def _create_failure_result(self, error_message: str) -> Dict[str, Any]:
        """å¤±æ•—çµæœã®ä½œæˆ"""
        return {
            'success': False,
            'separated_text': '',
            'relative_clauses': [],
            'sub_slots': {},
            'confidence': 0.0,
            'error': error_message,
            'metadata': {
                'handler': self.handler_info,
                'analysis_method': 'error_handling'
            }
        }
    
    def get_configuration_template(self) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å–å¾—"""
        return {
            'relative_patterns': {
                'wh_relatives': {
                    'pattern_type': 'wh_relative',
                    'relative_indicators': ['who', 'whom', 'whose', 'which', 'that', 'where', 'when', 'why'],
                    'pos_patterns': ['PRON', 'DET', 'ADV'],
                    'dependency_patterns': ['nsubj', 'dobj', 'nsubjpass', 'pobj', 'advmod'],
                    'confidence_weight': 1.2
                },
                'that_relatives': {
                    'pattern_type': 'that_relative',
                    'relative_indicators': ['that'],
                    'pos_patterns': ['PRON', 'SCONJ'],
                    'dependency_patterns': ['nsubj', 'dobj', 'mark'],
                    'confidence_weight': 1.0
                },
                'antecedent_patterns': {
                    'pattern_type': 'antecedent',
                    'pos_patterns': ['NOUN', 'PROPN', 'PRON'],
                    'dependency_patterns': ['ROOT', 'nsubj', 'dobj', 'pobj'],
                    'confidence_weight': 1.0
                }
            },
            'boundary_detection': {
                'end_markers': ['PUNCT', 'CONJ'],
                'depth_limit': 10
            },
            'confidence_settings': {
                'minimum_confidence': 0.4,
                'high_confidence': 0.8,
                'clause_bonus': 0.2
            }
        }


if __name__ == "__main__":
    # ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ç‰ˆã®ãƒ†ã‚¹ãƒˆ
    handler = RelativeClauseHandlerClean()
    
    test_sentences = [
        "The book that I read was interesting.",
        "A person who speaks three languages is polyglot.",
        "The house which we visited was beautiful.",
        "The reason why he left remains unclear."
    ]
    
    print("ğŸ§ª RelativeClauseHandler - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œå…¨é™¤å»ç‰ˆãƒ†ã‚¹ãƒˆ")
    print("=" * 70)
    
    for sentence in test_sentences:
        result = handler.process(sentence)
        print(f"\nå…¥åŠ›: \"{sentence}\"")
        print(f"æˆåŠŸ: {result['success']}")
        print(f"åˆ†é›¢ãƒ†ã‚­ã‚¹ãƒˆ: \"{result.get('separated_text', '')}\"")
        print(f"ä¿¡é ¼åº¦: {result.get('confidence', 0):.3f}")
        print(f"é–¢ä¿‚ç¯€æ•°: {len(result.get('relative_clauses', []))}")
        print(f"ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ: {result.get('sub_slots', {})}")
        print(f"ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä½¿ç”¨: 0ä»¶ âœ…")
