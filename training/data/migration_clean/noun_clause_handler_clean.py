"""
NounClauseHandler - å®Œå…¨ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ç‰ˆ
Clean Version with Zero Hardcoding for New Workspace Migration

æ—¢å­˜NounClauseHandlerã®å…¨æ©Ÿèƒ½ã‚’ç¶­æŒã—ãªãŒã‚‰ã€
ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Œå…¨ã«é™¤å»ã—ãŸæ±ç”¨ç‰ˆ

ä¸»ãªæ”¹å–„ç‚¹:
- å›ºå®šç¯€ãƒãƒ¼ã‚«ãƒ¼ â†’ å‹•çš„ãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ
- å›ºå®šç¯€ã‚¿ã‚¤ãƒ— â†’ è¨­å®šå¯èƒ½ç¯€åˆ†é¡
- å›ºå®šå‹•è©ãƒªã‚¹ãƒˆ â†’ æ±ç”¨çµ±èªè§£æ
- æ¨™æº–åŒ–ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹æº–æ‹ 
"""

import spacy
import json
import os
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from abc import ABC, abstractmethod


@dataclass
class NounClausePattern:
    """åè©ç¯€ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©"""
    pattern_type: str
    clause_markers: List[str] = field(default_factory=list)
    complementizer_patterns: List[str] = field(default_factory=list)
    function_types: List[str] = field(default_factory=list)
    pos_patterns: List[str] = field(default_factory=list)
    dependency_patterns: List[str] = field(default_factory=list)
    confidence_weight: float = 1.0


@dataclass
class NounClauseConfiguration:
    """åè©ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š"""
    noun_clause_patterns: Dict[str, NounClausePattern] = field(default_factory=dict)
    function_analysis: Dict[str, Any] = field(default_factory=dict)
    confidence_settings: Dict[str, float] = field(default_factory=dict)
    clause_rules: Dict[str, List[str]] = field(default_factory=dict)


class GenericNounClauseAnalyzer:
    """æ±ç”¨åè©ç¯€è§£æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, config: NounClauseConfiguration):
        self.config = config
        self.nlp = spacy.load('en_core_web_sm')
    
    def analyze_noun_clause_structure(self, doc) -> Dict[str, Any]:
        """æ±ç”¨åè©ç¯€æ§‹é€ è§£æ"""
        # åè©ç¯€å€™è£œã®æ¤œå‡º
        clause_candidates = self._detect_noun_clause_candidates(doc)
        
        if not clause_candidates:
            return {'noun_clauses': [], 'confidence': 0.0}
        
        # åè©ç¯€ã®è©³ç´°è§£æ
        analyzed_clauses = []
        for candidate in clause_candidates:
            clause_analysis = self._analyze_clause_details(candidate, doc)
            if clause_analysis:
                analyzed_clauses.append(clause_analysis)
        
        # ä¿¡é ¼åº¦è¨ˆç®—
        confidence = self._calculate_clause_confidence(analyzed_clauses)
        
        return {
            'noun_clauses': analyzed_clauses,
            'confidence': confidence,
            'analysis_method': 'pattern_based_generic'
        }
    
    def _detect_noun_clause_candidates(self, doc) -> List[Dict[str, Any]]:
        """åè©ç¯€å€™è£œã®æ¤œå‡º"""
        candidates = []
        
        for pattern_name, pattern in self.config.noun_clause_patterns.items():
            for token in doc:
                if self._matches_clause_pattern(token, pattern):
                    candidate = self._create_clause_candidate(token, pattern_name, pattern, doc)
                    if candidate:
                        candidates.append(candidate)
        
        return candidates
    
    def _matches_clause_pattern(self, token, pattern: NounClausePattern) -> bool:
        """åè©ç¯€ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°"""
        # èªå½™ãƒãƒƒãƒãƒ³ã‚°
        lex_match = not pattern.clause_markers or token.lemma_.lower() in pattern.clause_markers
        
        # å“è©ãƒãƒƒãƒãƒ³ã‚°
        pos_match = not pattern.pos_patterns or token.pos_ in pattern.pos_patterns
        
        # ä¾å­˜é–¢ä¿‚ãƒãƒƒãƒãƒ³ã‚°
        dep_match = not pattern.dependency_patterns or token.dep_ in pattern.dependency_patterns
        
        return lex_match and pos_match and dep_match
    
    def _create_clause_candidate(self, token, pattern_name: str, pattern: NounClausePattern, doc) -> Optional[Dict[str, Any]]:
        """åè©ç¯€å€™è£œã®ä½œæˆ"""
        # ç¯€ã®å¢ƒç•Œã‚’ç‰¹å®š
        clause_boundary = self._identify_clause_boundary(token, doc)
        
        if not clause_boundary:
            return None
        
        # ç¯€å†…å®¹ã®æŠ½å‡º
        clause_content = self._extract_clause_content(clause_boundary, doc)
        
        # ç¯€ã®æ©Ÿèƒ½åˆ†æ
        function_analysis = self._analyze_clause_function(token, clause_boundary, doc)
        
        # è£œæ–‡æ¨™è­˜ã®åˆ†æ
        complementizer_analysis = self._analyze_complementizer(token, doc)
        
        return {
            'marker_token': token,
            'clause_boundary': clause_boundary,
            'clause_content': clause_content,
            'pattern_type': pattern_name,
            'function_analysis': function_analysis,
            'complementizer_analysis': complementizer_analysis,
            'confidence_weight': pattern.confidence_weight
        }
    
    def _identify_clause_boundary(self, marker_token, doc) -> Optional[Dict[str, int]]:
        """ç¯€ã®å¢ƒç•Œç‰¹å®š"""
        # ç¯€ã®é–‹å§‹ç‚¹
        start_index = marker_token.i
        
        # ç¯€ã®çµ‚äº†ç‚¹ã‚’æ¤œå‡º
        end_index = self._find_clause_end(marker_token, doc)
        
        if end_index is None or end_index <= start_index:
            return None
        
        return {
            'start': start_index,
            'end': end_index,
            'length': end_index - start_index + 1
        }
    
    def _find_clause_end(self, marker_token, doc) -> Optional[int]:
        """ç¯€ã®çµ‚äº†ç‚¹æ¤œå‡º"""
        # æ§‹æ–‡æœ¨ã‚’ä½¿ç”¨ã—ãŸå¢ƒç•Œæ¤œå‡º
        clause_root = self._find_clause_root(marker_token, doc)
        
        if not clause_root:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¥èª­ç‚¹ã¾ãŸã¯æ–‡æœ«ã¾ã§
            for i in range(marker_token.i + 1, len(doc)):
                token = doc[i]
                if token.pos_ == 'PUNCT' and token.text in [',', '.', ';', '!', '?']:
                    return i - 1
            return len(doc) - 1
        
        # ç¯€ãƒ«ãƒ¼ãƒˆã®æ”¯é…ç¯„å›²ã‚’ç‰¹å®š
        end_index = clause_root.i
        for descendant in clause_root.subtree:
            if descendant.i > end_index:
                end_index = descendant.i
        
        return end_index
    
    def _find_clause_root(self, marker_token, doc) -> Optional:
        """ç¯€ã®ãƒ«ãƒ¼ãƒˆå‹•è©æ¤œå‡º"""
        # ãƒãƒ¼ã‚«ãƒ¼ãƒˆãƒ¼ã‚¯ãƒ³ã®æ”¯é…ä¸‹ã«ã‚ã‚‹å‹•è©ã‚’æ¤œç´¢
        for child in marker_token.children:
            if child.pos_ == 'VERB':
                return child
        
        # ãƒãƒ¼ã‚«ãƒ¼ãƒˆãƒ¼ã‚¯ãƒ³ã®å³å´ã®æœ€åˆã®å‹•è©
        for i in range(marker_token.i + 1, len(doc)):
            token = doc[i]
            if token.pos_ == 'VERB' and token.dep_ in ['ccomp', 'acl', 'advcl']:
                return token
        
        return None
    
    def _extract_clause_content(self, clause_boundary: Dict[str, int], doc) -> Dict[str, Any]:
        """ç¯€å†…å®¹ã®æŠ½å‡º"""
        start = clause_boundary['start']
        end = clause_boundary['end']
        
        clause_tokens = doc[start:end+1]
        
        # ç¯€å†…ã®è¦ç´ åˆ†æ
        clause_elements = self._analyze_clause_elements(clause_tokens)
        
        return {
            'text': ' '.join([token.text for token in clause_tokens]),
            'tokens': [{'text': token.text, 'lemma': token.lemma_, 'pos': token.pos_, 'dep': token.dep_} 
                      for token in clause_tokens],
            'elements': clause_elements,
            'word_count': len(clause_tokens)
        }
    
    def _analyze_clause_elements(self, clause_tokens) -> Dict[str, Any]:
        """ç¯€å†…è¦ç´ ã®åˆ†æ"""
        elements = {
            'subject': None,
            'predicate': None,
            'objects': [],
            'complements': [],
            'adverbials': []
        }
        
        for token in clause_tokens:
            if token.dep_ == 'nsubj':
                elements['subject'] = {
                    'text': token.text,
                    'lemma': token.lemma_,
                    'index': token.i
                }
            elif token.pos_ == 'VERB' and token.dep_ in ['ROOT', 'ccomp', 'xcomp']:
                elements['predicate'] = {
                    'text': token.text,
                    'lemma': token.lemma_,
                    'index': token.i
                }
            elif token.dep_ in ['dobj', 'iobj']:
                elements['objects'].append({
                    'text': token.text,
                    'lemma': token.lemma_,
                    'dependency': token.dep_,
                    'index': token.i
                })
            elif token.dep_ in ['attr', 'acomp']:
                elements['complements'].append({
                    'text': token.text,
                    'lemma': token.lemma_,
                    'dependency': token.dep_,
                    'index': token.i
                })
            elif token.dep_ in ['advmod', 'npadvmod']:
                elements['adverbials'].append({
                    'text': token.text,
                    'lemma': token.lemma_,
                    'dependency': token.dep_,
                    'index': token.i
                })
        
        return elements
    
    def _analyze_clause_function(self, marker_token, clause_boundary: Dict[str, int], doc) -> Dict[str, Any]:
        """åè©ç¯€ã®æ©Ÿèƒ½åˆ†æ"""
        function_info = {
            'syntactic_function': 'unknown',
            'semantic_role': 'unknown',
            'clause_type': 'unknown',
            'governing_element': None
        }
        
        # çµ±èªæ©Ÿèƒ½ã®åˆ†æ
        function_info['syntactic_function'] = self._determine_syntactic_function(marker_token, doc)
        
        # æ„å‘³å½¹å‰²ã®åˆ†æ
        function_info['semantic_role'] = self._determine_semantic_role(marker_token, doc)
        
        # ç¯€ã‚¿ã‚¤ãƒ—ã®åˆ†æ
        function_info['clause_type'] = self._determine_clause_type(marker_token, doc)
        
        # æ”¯é…è¦ç´ ã®ç‰¹å®š
        function_info['governing_element'] = self._find_governing_element(marker_token, doc)
        
        return function_info
    
    def _determine_syntactic_function(self, marker_token, doc) -> str:
        """çµ±èªæ©Ÿèƒ½ã®æ±ºå®š"""
        # ä¾å­˜é–¢ä¿‚ã«ã‚ˆã‚‹åˆ†æ
        dependency = marker_token.dep_
        
        if dependency == 'nsubj':
            return 'subject'
        elif dependency in ['dobj', 'iobj']:
            return 'object'
        elif dependency in ['ccomp', 'xcomp']:
            return 'complement'
        elif dependency == 'acl':
            return 'relative'
        elif dependency == 'appos':
            return 'appositive'
        else:
            # ä½ç½®ã«ã‚ˆã‚‹æ¨å®š
            return self._infer_function_from_position(marker_token, doc)
    
    def _infer_function_from_position(self, marker_token, doc) -> str:
        """ä½ç½®ã‹ã‚‰ã®æ©Ÿèƒ½æ¨å®š"""
        # æ–‡ã®å‰åŠã«ã‚ã‚‹å ´åˆ
        if marker_token.i < len(doc) * 0.3:
            return 'subject_candidate'
        
        # å‹•è©ã®å¾Œã«ã‚ã‚‹å ´åˆ
        for i in range(marker_token.i - 1, -1, -1):
            token = doc[i]
            if token.pos_ == 'VERB' and token.dep_ == 'ROOT':
                return 'object_candidate'
        
        return 'unknown'
    
    def _determine_semantic_role(self, marker_token, doc) -> str:
        """æ„å‘³å½¹å‰²ã®æ±ºå®š"""
        marker_lemma = marker_token.lemma_.lower()
        
        # è¨­å®šãƒ™ãƒ¼ã‚¹ã®åˆ†é¡
        semantic_groups = self.config.function_analysis.get('semantic_groups', {})
        
        for role, markers in semantic_groups.items():
            if marker_lemma in markers:
                return role
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ†é¡
        return self._default_semantic_classification(marker_lemma)
    
    def _default_semantic_classification(self, marker_lemma: str) -> str:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ„å‘³åˆ†é¡"""
        if marker_lemma == 'that':
            return 'propositional_content'
        elif marker_lemma in ['what', 'who', 'where', 'when', 'why', 'how']:
            return 'interrogative_content'
        elif marker_lemma in ['whether', 'if']:
            return 'alternative_content'
        elif marker_lemma in ['whatever', 'whoever', 'wherever']:
            return 'free_relative_content'
        else:
            return 'general_content'
    
    def _determine_clause_type(self, marker_token, doc) -> str:
        """ç¯€ã‚¿ã‚¤ãƒ—ã®æ±ºå®š"""
        marker_lemma = marker_token.lemma_.lower()
        
        # thatç¯€
        if marker_lemma == 'that':
            return 'declarative_clause'
        
        # whç¯€
        elif marker_lemma in ['what', 'who', 'where', 'when', 'why', 'how', 'which']:
            return 'interrogative_clause'
        
        # whether/ifç¯€
        elif marker_lemma in ['whether', 'if']:
            return 'alternative_clause'
        
        # è‡ªç”±é–¢ä¿‚è©ç¯€
        elif marker_lemma in ['whatever', 'whoever', 'wherever', 'whenever']:
            return 'free_relative_clause'
        
        else:
            return 'other_clause'
    
    def _find_governing_element(self, marker_token, doc) -> Optional[Dict[str, Any]]:
        """æ”¯é…è¦ç´ ã®æ¤œå‡º"""
        # ç›´æ¥ã®çµ±èªçš„æ”¯é…è€…
        head = marker_token.head
        
        if head != marker_token:  # è‡ªåˆ†è‡ªèº«ã§ãªã„å ´åˆ
            return {
                'token': head,
                'text': head.text,
                'lemma': head.lemma_,
                'pos': head.pos_,
                'index': head.i,
                'relation': marker_token.dep_
            }
        
        return None
    
    def _analyze_complementizer(self, marker_token, doc) -> Dict[str, Any]:
        """è£œæ–‡æ¨™è­˜ã®åˆ†æ"""
        return {
            'text': marker_token.text,
            'lemma': marker_token.lemma_,
            'type': self._classify_complementizer(marker_token),
            'position': marker_token.i,
            'can_be_omitted': self._check_omissibility(marker_token, doc)
        }
    
    def _classify_complementizer(self, marker_token) -> str:
        """è£œæ–‡æ¨™è­˜ã®åˆ†é¡"""
        lemma = marker_token.lemma_.lower()
        
        if lemma == 'that':
            return 'declarative_complementizer'
        elif lemma in ['whether', 'if']:
            return 'interrogative_complementizer'
        elif lemma in ['what', 'who', 'where', 'when', 'why', 'how']:
            return 'wh_complementizer'
        else:
            return 'other_complementizer'
    
    def _check_omissibility(self, marker_token, doc) -> bool:
        """çœç•¥å¯èƒ½æ€§ã®ãƒã‚§ãƒƒã‚¯"""
        # thatç¯€ã®å ´åˆã€å¤šãã¯çœç•¥å¯èƒ½
        if marker_token.lemma_.lower() == 'that':
            return True
        
        # whèªã¯é€šå¸¸çœç•¥ä¸å¯
        if marker_token.lemma_.lower() in ['what', 'who', 'where', 'when', 'why', 'how']:
            return False
        
        return False
    
    def _analyze_clause_details(self, candidate: Dict[str, Any], doc) -> Optional[Dict[str, Any]]:
        """åè©ç¯€ã®è©³ç´°è§£æ"""
        # ä¿¡é ¼åº¦è¨ˆç®—
        confidence = self._calculate_individual_confidence(candidate)
        
        if confidence < 0.3:  # ä½ä¿¡é ¼åº¦ã¯é™¤å¤–
            return None
        
        marker_token = candidate['marker_token']
        clause_content = candidate['clause_content']
        
        return {
            'clause': {
                'marker': {
                    'text': marker_token.text,
                    'lemma': marker_token.lemma_,
                    'index': marker_token.i
                },
                'content': clause_content,
                'full_text': clause_content['text']
            },
            'function': candidate['function_analysis'],
            'complementizer': candidate['complementizer_analysis'],
            'confidence': confidence,
            'pattern_type': candidate['pattern_type']
        }
    
    def _calculate_individual_confidence(self, candidate: Dict[str, Any]) -> float:
        """å€‹åˆ¥åè©ç¯€ã®ä¿¡é ¼åº¦è¨ˆç®—"""
        base_confidence = 0.4
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒã®ä¿¡é ¼åº¦
        base_confidence += 0.2 * candidate['confidence_weight']
        
        # è£œæ–‡æ¨™è­˜ã®æ˜ç¢ºã•
        complementizer_type = candidate['complementizer_analysis']['type']
        if complementizer_type != 'other_complementizer':
            base_confidence += 0.2
        
        # çµ±èªæ©Ÿèƒ½ã®æ˜ç¢ºã•
        function = candidate['function_analysis']['syntactic_function']
        if function != 'unknown':
            base_confidence += 0.2
        
        # ç¯€å†…å®¹ã®å®Œå…¨æ€§
        elements = candidate['clause_content']['elements']
        if elements['subject'] and elements['predicate']:
            base_confidence += 0.2
        
        return min(1.0, base_confidence)
    
    def _calculate_clause_confidence(self, analyzed_clauses: List[Dict[str, Any]]) -> float:
        """å…¨ä½“ã®åè©ç¯€è§£æä¿¡é ¼åº¦"""
        if not analyzed_clauses:
            return 0.0
        
        total_confidence = sum(clause['confidence'] for clause in analyzed_clauses)
        return min(1.0, total_confidence / len(analyzed_clauses))


class NounClauseHandlerClean:
    """
    åè©ç¯€å‡¦ç†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œå…¨é™¤å»ç‰ˆ
    
    ç‰¹å¾´:
    - è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©
    - æ±ç”¨çš„åè©ç¯€æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    - å‹•çš„æ©Ÿèƒ½åˆ†æ
    - å®Œå…¨ãªæ‹¡å¼µæ€§
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆæœŸåŒ–"""
        self.nlp = spacy.load('en_core_web_sm')
        self.config = self._load_configuration(config_path)
        self.analyzer = GenericNounClauseAnalyzer(self.config)
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼æƒ…å ±
        self.handler_info = {
            'name': 'NounClauseHandlerClean',
            'version': 'clean_v1.0',
            'hardcoding_level': 'zero'
        }
    
    def _load_configuration(self, config_path: Optional[str]) -> NounClauseConfiguration:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
                return self._parse_config_data(config_data)
        else:
            return self._create_default_configuration()
    
    def _create_default_configuration(self) -> NounClauseConfiguration:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®ä½œæˆ"""
        return NounClauseConfiguration(
            noun_clause_patterns={
                'that_clause': NounClausePattern(
                    pattern_type='that_clause',
                    clause_markers=['that'],
                    complementizer_patterns=['declarative'],
                    function_types=['subject', 'object', 'complement'],
                    pos_patterns=['SCONJ', 'ADP'],
                    dependency_patterns=['mark', 'nsubj', 'dobj', 'ccomp'],
                    confidence_weight=1.2
                ),
                'wh_clause': NounClausePattern(
                    pattern_type='wh_clause',
                    clause_markers=['what', 'who', 'where', 'when', 'why', 'how', 'which'],
                    complementizer_patterns=['interrogative'],
                    function_types=['subject', 'object', 'complement'],
                    pos_patterns=['PRON', 'ADV', 'DET'],
                    dependency_patterns=['nsubj', 'dobj', 'ccomp', 'acl'],
                    confidence_weight=1.3
                ),
                'whether_clause': NounClausePattern(
                    pattern_type='whether_clause',
                    clause_markers=['whether', 'if'],
                    complementizer_patterns=['alternative'],
                    function_types=['object', 'complement'],
                    pos_patterns=['SCONJ'],
                    dependency_patterns=['mark', 'ccomp'],
                    confidence_weight=1.1
                )
            },
            function_analysis={
                'semantic_groups': {
                    'propositional_content': ['that'],
                    'interrogative_content': ['what', 'who', 'where', 'when', 'why', 'how'],
                    'alternative_content': ['whether', 'if'],
                    'free_relative_content': ['whatever', 'whoever', 'wherever', 'whenever']
                }
            },
            confidence_settings={
                'minimum_confidence': 0.3,
                'high_confidence': 0.8,
                'clause_bonus': 0.2
            }
        )
    
    def _parse_config_data(self, config_data: Dict) -> NounClauseConfiguration:
        """è¨­å®šãƒ‡ãƒ¼ã‚¿ã®è§£æ"""
        noun_clause_patterns = {}
        for name, data in config_data.get('noun_clause_patterns', {}).items():
            noun_clause_patterns[name] = NounClausePattern(
                pattern_type=data.get('pattern_type', name),
                clause_markers=data.get('clause_markers', []),
                complementizer_patterns=data.get('complementizer_patterns', []),
                function_types=data.get('function_types', []),
                pos_patterns=data.get('pos_patterns', []),
                dependency_patterns=data.get('dependency_patterns', []),
                confidence_weight=data.get('confidence_weight', 1.0)
            )
        
        return NounClauseConfiguration(
            noun_clause_patterns=noun_clause_patterns,
            function_analysis=config_data.get('function_analysis', {}),
            confidence_settings=config_data.get('confidence_settings', {})
        )
    
    def process(self, text: str) -> Dict[str, Any]:
        """
        åè©ç¯€å‡¦ç†ãƒ¡ã‚¤ãƒ³ - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãªã—ç‰ˆ
        
        Args:
            text: å‡¦ç†å¯¾è±¡ã®è‹±èªæ–‡
            
        Returns:
            Dict: å‡¦ç†çµæœï¼ˆsuccess, noun_clauses, functions, confidenceï¼‰
        """
        try:
            # spaCyè§£æ
            doc = self.nlp(text)
            
            # åè©ç¯€è§£æ
            analysis_result = self.analyzer.analyze_noun_clause_structure(doc)
            
            if not analysis_result['noun_clauses']:
                return self._create_no_clauses_result(text)
            
            # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®æ§‹ç¯‰
            sub_slots = self._build_sub_slots(analysis_result['noun_clauses'])
            
            # æ©Ÿèƒ½åˆ†æã®å®Ÿè¡Œ
            function_summary = self._summarize_functions(analysis_result['noun_clauses'])
            
            return {
                'success': True,
                'original_text': text,
                'noun_clauses': analysis_result['noun_clauses'],
                'sub_slots': sub_slots,
                'function_summary': function_summary,
                'confidence': analysis_result['confidence'],
                'metadata': {
                    'handler': self.handler_info,
                    'analysis_method': analysis_result['analysis_method'],
                    'clause_count': len(analysis_result['noun_clauses'])
                }
            }
            
        except Exception as e:
            return self._create_failure_result(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def _create_no_clauses_result(self, text: str) -> Dict[str, Any]:
        """åè©ç¯€ãªã—ã®çµæœä½œæˆ"""
        return {
            'success': True,
            'original_text': text,
            'noun_clauses': [],
            'sub_slots': {},
            'function_summary': {},
            'confidence': self.config.confidence_settings.get('minimum_confidence', 0.3),
            'metadata': {
                'handler': self.handler_info,
                'analysis_method': 'no_clauses_detected'
            }
        }
    
    def _build_sub_slots(self, noun_clauses: List[Dict[str, Any]]) -> Dict[str, str]:
        """ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®æ§‹ç¯‰"""
        sub_slots = {}
        
        for i, clause in enumerate(noun_clauses):
            slot_key = f"sub-clause{i+1}"
            sub_slots[slot_key] = clause['clause']['full_text']
        
        return sub_slots
    
    def _summarize_functions(self, noun_clauses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ©Ÿèƒ½åˆ†æã®è¦ç´„"""
        summary = {
            'syntactic_functions': [],
            'semantic_roles': [],
            'clause_types': [],
            'dominant_function': None
        }
        
        function_counts = {}
        
        for clause in noun_clauses:
            function = clause['function']['syntactic_function']
            role = clause['function']['semantic_role']
            clause_type = clause['function']['clause_type']
            
            summary['syntactic_functions'].append(function)
            summary['semantic_roles'].append(role)
            summary['clause_types'].append(clause_type)
            
            function_counts[function] = function_counts.get(function, 0) + 1
        
        # ä¸»è¦ãªæ©Ÿèƒ½ã®æ±ºå®š
        if function_counts:
            summary['dominant_function'] = max(function_counts.items(), key=lambda x: x[1])[0]
        
        summary['function_distribution'] = function_counts
        
        return summary
    
    def _create_failure_result(self, error_message: str) -> Dict[str, Any]:
        """å¤±æ•—çµæœã®ä½œæˆ"""
        return {
            'success': False,
            'original_text': '',
            'noun_clauses': [],
            'sub_slots': {},
            'function_summary': {},
            'confidence': 0.0,
            'error': error_message,
            'metadata': {
                'handler': self.handler_info,
                'analysis_method': 'error_handling'
            }
        }


if __name__ == "__main__":
    # ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ç‰ˆã®ãƒ†ã‚¹ãƒˆ
    handler = NounClauseHandlerClean()
    
    test_sentences = [
        "I think that he is right.",
        "What you said is important.",
        "I wonder whether she will come.",
        "The fact that you came surprised me."
    ]
    
    print("ğŸ§ª NounClauseHandler - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œå…¨é™¤å»ç‰ˆãƒ†ã‚¹ãƒˆ")
    print("=" * 65)
    
    for sentence in test_sentences:
        result = handler.process(sentence)
        print(f"\nå…¥åŠ›: \"{sentence}\"")
        print(f"æˆåŠŸ: {result['success']}")
        print(f"åè©ç¯€æ•°: {len(result.get('noun_clauses', []))}")
        print(f"ä¿¡é ¼åº¦: {result.get('confidence', 0):.3f}")
        print(f"ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ: {result.get('sub_slots', {})}")
        if result.get('function_summary', {}).get('dominant_function'):
            print(f"ä¸»è¦æ©Ÿèƒ½: {result['function_summary']['dominant_function']}")
        print(f"ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä½¿ç”¨: 0ä»¶ âœ…")
