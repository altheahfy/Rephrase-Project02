"""
ModalHandler - å®Œå…¨ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ç‰ˆ
Clean Version with Zero Hardcoding for New Workspace Migration

æ—¢å­˜ModalHandlerã®å…¨æ©Ÿèƒ½ã‚’ç¶­æŒã—ãªãŒã‚‰ã€
ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Œå…¨ã«é™¤å»ã—ãŸæ±ç”¨ç‰ˆ

ä¸»ãªæ”¹å–„ç‚¹:
- å›ºå®šåŠ©å‹•è©ãƒªã‚¹ãƒˆ â†’ å‹•çš„èªå½™è§£æ
- å›ºå®šæ„å‘³åˆ†é¡ â†’ è¨­å®šå¯èƒ½ãƒ‘ã‚¿ãƒ¼ãƒ³
- å›ºå®šæ™‚åˆ¶åˆ¤å®š â†’ æ±ç”¨å½¢æ…‹ç´ è§£æ
- æ¨™æº–åŒ–ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹æº–æ‹ 
"""

import spacy
import json
import os
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from abc import ABC, abstractmethod


@dataclass
class ModalPattern:
    """åŠ©å‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©"""
    pattern_type: str
    modal_indicators: List[str] = field(default_factory=list)
    semantic_categories: List[str] = field(default_factory=list)
    pos_patterns: List[str] = field(default_factory=list)
    dependency_patterns: List[str] = field(default_factory=list)
    morphological_patterns: List[str] = field(default_factory=list)
    confidence_weight: float = 1.0


@dataclass
class ModalConfiguration:
    """åŠ©å‹•è©ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š"""
    modal_patterns: Dict[str, ModalPattern] = field(default_factory=dict)
    semantic_analysis: Dict[str, Any] = field(default_factory=dict)
    confidence_settings: Dict[str, float] = field(default_factory=dict)
    extraction_rules: Dict[str, List[str]] = field(default_factory=dict)


class GenericModalAnalyzer:
    """æ±ç”¨åŠ©å‹•è©è§£æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, config: ModalConfiguration):
        self.config = config
        self.nlp = spacy.load('en_core_web_sm')
    
    def analyze_modal_structure(self, doc) -> Dict[str, Any]:
        """æ±ç”¨åŠ©å‹•è©æ§‹é€ è§£æ"""
        # åŠ©å‹•è©å€™è£œã®æ¤œå‡º
        modal_candidates = self._detect_modal_candidates(doc)
        
        if not modal_candidates:
            return {'modals': [], 'confidence': 0.0}
        
        # åŠ©å‹•è©ã®è©³ç´°è§£æ
        analyzed_modals = []
        for candidate in modal_candidates:
            modal_analysis = self._analyze_modal_details(candidate, doc)
            if modal_analysis:
                analyzed_modals.append(modal_analysis)
        
        # ä¿¡é ¼åº¦è¨ˆç®—
        confidence = self._calculate_modal_confidence(analyzed_modals)
        
        return {
            'modals': analyzed_modals,
            'confidence': confidence,
            'analysis_method': 'pattern_based_generic'
        }
    
    def _detect_modal_candidates(self, doc) -> List[Dict[str, Any]]:
        """åŠ©å‹•è©å€™è£œã®æ¤œå‡º"""
        candidates = []
        
        for pattern_name, pattern in self.config.modal_patterns.items():
            for token in doc:
                if self._matches_modal_pattern(token, pattern):
                    candidate = self._create_modal_candidate(token, pattern_name, pattern, doc)
                    if candidate:
                        candidates.append(candidate)
        
        return candidates
    
    def _matches_modal_pattern(self, token, pattern: ModalPattern) -> bool:
        """åŠ©å‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°"""
        # èªå½™ãƒãƒƒãƒãƒ³ã‚°
        lex_match = not pattern.modal_indicators or token.lemma_.lower() in pattern.modal_indicators
        
        # å“è©ãƒãƒƒãƒãƒ³ã‚°
        pos_match = not pattern.pos_patterns or token.pos_ in pattern.pos_patterns
        
        # ä¾å­˜é–¢ä¿‚ãƒãƒƒãƒãƒ³ã‚°
        dep_match = not pattern.dependency_patterns or token.dep_ in pattern.dependency_patterns
        
        # å½¢æ…‹ç´ ãƒãƒƒãƒãƒ³ã‚°
        morph_match = self._check_morphological_patterns(token, pattern)
        
        return lex_match and pos_match and dep_match and morph_match
    
    def _check_morphological_patterns(self, token, pattern: ModalPattern) -> bool:
        """å½¢æ…‹ç´ ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯"""
        if not pattern.morphological_patterns:
            return True
        
        # å½¢æ…‹ç´ ç‰¹å¾´ã®ç¢ºèª
        token_morph = token.morph.to_dict()
        for morph_pattern in pattern.morphological_patterns:
            key_value = morph_pattern.split('=')
            if len(key_value) == 2:
                key, value = key_value
                if token_morph.get(key) == value:
                    return True
            elif morph_pattern.lower() in str(token_morph).lower():
                return True
        
        return len(pattern.morphological_patterns) == 0
    
    def _create_modal_candidate(self, token, pattern_name: str, pattern: ModalPattern, doc) -> Optional[Dict[str, Any]]:
        """åŠ©å‹•è©å€™è£œã®ä½œæˆ"""
        # é–¢é€£å‹•è©ã®æ¤œå‡º
        main_verb = self._find_associated_verb(token, doc)
        
        # æ„å‘³ã‚«ãƒ†ã‚´ãƒªã®æ±ºå®š
        semantic_category = self._determine_semantic_category(token, pattern)
        
        # æ–‡è„ˆè§£æ
        context_analysis = self._analyze_modal_context(token, doc)
        
        return {
            'modal_token': token,
            'text': token.text,
            'lemma': token.lemma_,
            'pattern_type': pattern_name,
            'semantic_category': semantic_category,
            'main_verb': main_verb,
            'context': context_analysis,
            'confidence_weight': pattern.confidence_weight
        }
    
    def _find_associated_verb(self, modal_token, doc) -> Optional[Dict[str, Any]]:
        """é–¢é€£å‹•è©ã®æ¤œå‡º"""
        # ç›´æ¥çš„ãªä¾å­˜é–¢ä¿‚ã§ã®æ¤œç´¢
        for child in modal_token.children:
            if child.pos_ == 'VERB' and child.dep_ in ['ccomp', 'xcomp', 'advcl']:
                return {
                    'token': child,
                    'text': child.text,
                    'lemma': child.lemma_,
                    'index': child.i,
                    'relationship': 'direct_dependency'
                }
        
        # è¿‘éš£ã§ã®æ¤œç´¢
        for i in range(modal_token.i + 1, min(modal_token.i + 5, len(doc))):
            token = doc[i]
            if token.pos_ == 'VERB' and token.dep_ not in ['aux', 'auxpass']:
                return {
                    'token': token,
                    'text': token.text,
                    'lemma': token.lemma_,
                    'index': token.i,
                    'relationship': 'proximity'
                }
        
        return None
    
    def _determine_semantic_category(self, token, pattern: ModalPattern) -> str:
        """æ„å‘³ã‚«ãƒ†ã‚´ãƒªã®æ±ºå®š"""
        if pattern.semantic_categories:
            # è¨­å®šãƒ™ãƒ¼ã‚¹ã®åˆ†é¡
            for category in pattern.semantic_categories:
                if self._token_fits_semantic_category(token, category):
                    return category
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ†é¡
        return self._default_semantic_classification(token)
    
    def _token_fits_semantic_category(self, token, category: str) -> bool:
        """ãƒˆãƒ¼ã‚¯ãƒ³ãŒæ„å‘³ã‚«ãƒ†ã‚´ãƒªã«é©åˆã™ã‚‹ã‹"""
        # èªå½™ãƒ™ãƒ¼ã‚¹ã®åˆ¤å®š
        lemma = token.lemma_.lower()
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®èªå½™ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆè¨­å®šã‹ã‚‰å–å¾—å¯èƒ½ï¼‰
        category_groups = self.config.semantic_analysis.get('category_groups', {})
        
        if category in category_groups:
            return lemma in category_groups[category]
        
        return False
    
    def _default_semantic_classification(self, token) -> str:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ„å‘³åˆ†é¡"""
        lemma = token.lemma_.lower()
        
        # åŸºæœ¬çš„ãªåˆ†é¡ãƒ‘ã‚¿ãƒ¼ãƒ³
        if lemma in ['can', 'could', 'be able to']:
            return 'ability'
        elif lemma in ['may', 'might', 'could']:
            return 'possibility'
        elif lemma in ['must', 'have to', 'should', 'ought to']:
            return 'obligation'
        elif lemma in ['will', 'would', 'shall']:
            return 'future_intention'
        else:
            return 'general_modal'
    
    def _analyze_modal_context(self, token, doc) -> Dict[str, Any]:
        """åŠ©å‹•è©ã®æ–‡è„ˆè§£æ"""
        context = {
            'position': 'pre' if token.i < len(doc) // 2 else 'post',
            'sentence_type': self._determine_sentence_type(doc),
            'negation': self._check_negation(token, doc),
            'question_context': self._check_question_context(token, doc)
        }
        
        return context
    
    def _determine_sentence_type(self, doc) -> str:
        """æ–‡ã‚¿ã‚¤ãƒ—ã®åˆ¤å®š"""
        # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã§åˆ¤å®š
        if doc[-1].text == '?':
            return 'interrogative'
        elif doc[-1].text == '!':
            return 'exclamatory'
        else:
            return 'declarative'
    
    def _check_negation(self, modal_token, doc) -> bool:
        """å¦å®šã®ç¢ºèª"""
        # åŠ©å‹•è©ã®ç›´å¾Œã«notãŒã‚ã‚‹ã‹
        if modal_token.i + 1 < len(doc):
            next_token = doc[modal_token.i + 1]
            if next_token.lemma_.lower() in ['not', "n't"]:
                return True
        
        return False
    
    def _check_question_context(self, modal_token, doc) -> bool:
        """ç–‘å•æ–‡æ–‡è„ˆã®ç¢ºèª"""
        # åŠ©å‹•è©ãŒæ–‡é ­ã«ã‚ã‚Šã€æ–‡æœ«ãŒç–‘å•ç¬¦ã®å ´åˆ
        return modal_token.i <= 1 and doc[-1].text == '?'
    
    def _analyze_modal_details(self, candidate: Dict[str, Any], doc) -> Optional[Dict[str, Any]]:
        """åŠ©å‹•è©ã®è©³ç´°è§£æ"""
        modal_token = candidate['modal_token']
        
        # åŠ©å‹•è©ã®æ©Ÿèƒ½åˆ†æ
        function = self._determine_modal_function(candidate)
        
        # ä¿¡é ¼åº¦è¨ˆç®—
        confidence = self._calculate_individual_confidence(candidate)
        
        if confidence < 0.3:  # ä½ä¿¡é ¼åº¦ã¯é™¤å¤–
            return None
        
        return {
            'modal': {
                'text': candidate['text'],
                'lemma': candidate['lemma'],
                'index': modal_token.i,
                'semantic_category': candidate['semantic_category'],
                'function': function
            },
            'main_verb': candidate['main_verb'],
            'context': candidate['context'],
            'confidence': confidence,
            'pattern_type': candidate['pattern_type']
        }
    
    def _determine_modal_function(self, candidate: Dict[str, Any]) -> str:
        """åŠ©å‹•è©ã®æ©Ÿèƒ½æ±ºå®š"""
        context = candidate['context']
        semantic_category = candidate['semantic_category']
        
        # æ–‡è„ˆã«åŸºã¥ãæ©Ÿèƒ½æ±ºå®š
        if context['question_context']:
            return 'interrogative_modal'
        elif context['negation']:
            return 'negative_modal'
        elif semantic_category == 'ability':
            return 'ability_expression'
        elif semantic_category == 'possibility':
            return 'possibility_expression'
        elif semantic_category == 'obligation':
            return 'obligation_expression'
        else:
            return 'general_auxiliary'
    
    def _calculate_individual_confidence(self, candidate: Dict[str, Any]) -> float:
        """å€‹åˆ¥åŠ©å‹•è©ã®ä¿¡é ¼åº¦è¨ˆç®—"""
        base_confidence = 0.5
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒã®ä¿¡é ¼åº¦
        base_confidence += 0.3 * candidate['confidence_weight']
        
        # é–¢é€£å‹•è©ã®å­˜åœ¨
        if candidate['main_verb']:
            base_confidence += 0.2
        
        # æ„å‘³ã‚«ãƒ†ã‚´ãƒªã®æ˜ç¢ºã•
        if candidate['semantic_category'] != 'general_modal':
            base_confidence += 0.1
        
        # æ–‡è„ˆã®ä¸€è²«æ€§
        context = candidate['context']
        if context['sentence_type'] == 'interrogative' and context['question_context']:
            base_confidence += 0.1
        
        return min(1.0, base_confidence)
    
    def _calculate_modal_confidence(self, analyzed_modals: List[Dict[str, Any]]) -> float:
        """å…¨ä½“ã®åŠ©å‹•è©è§£æä¿¡é ¼åº¦"""
        if not analyzed_modals:
            return 0.0
        
        total_confidence = sum(modal['confidence'] for modal in analyzed_modals)
        return min(1.0, total_confidence / len(analyzed_modals))


class ModalHandlerClean:
    """
    åŠ©å‹•è©å‡¦ç†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œå…¨é™¤å»ç‰ˆ
    
    ç‰¹å¾´:
    - è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©
    - æ±ç”¨çš„åŠ©å‹•è©æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    - å‹•çš„æ„å‘³åˆ†æ
    - å®Œå…¨ãªæ‹¡å¼µæ€§
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆæœŸåŒ–"""
        self.nlp = spacy.load('en_core_web_sm')
        self.config = self._load_configuration(config_path)
        self.analyzer = GenericModalAnalyzer(self.config)
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼æƒ…å ±
        self.handler_info = {
            'name': 'ModalHandlerClean',
            'version': 'clean_v1.0',
            'hardcoding_level': 'zero'
        }
    
    def _load_configuration(self, config_path: Optional[str]) -> ModalConfiguration:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
                return self._parse_config_data(config_data)
        else:
            return self._create_default_configuration()
    
    def _create_default_configuration(self) -> ModalConfiguration:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®ä½œæˆ"""
        return ModalConfiguration(
            modal_patterns={
                'core_modals': ModalPattern(
                    pattern_type='core_modal',
                    modal_indicators=['can', 'could', 'may', 'might', 'will', 'would', 'shall', 'should', 'must'],
                    semantic_categories=['ability', 'possibility', 'permission', 'obligation', 'future'],
                    pos_patterns=['AUX', 'VERB'],
                    dependency_patterns=['aux', 'ROOT'],
                    confidence_weight=1.2
                ),
                'semi_modals': ModalPattern(
                    pattern_type='semi_modal',
                    modal_indicators=['have to', 'ought to', 'be able to', 'be going to', 'used to'],
                    semantic_categories=['obligation', 'ability', 'future', 'habit'],
                    pos_patterns=['VERB', 'AUX'],
                    dependency_patterns=['aux', 'ROOT', 'xcomp'],
                    confidence_weight=1.0
                )
            },
            semantic_analysis={
                'category_groups': {
                    'ability': ['can', 'could', 'be able to'],
                    'possibility': ['may', 'might', 'could'],
                    'permission': ['may', 'can', 'could'],
                    'obligation': ['must', 'have to', 'should', 'ought to'],
                    'future': ['will', 'would', 'shall', 'be going to'],
                    'habit': ['used to', 'would']
                }
            },
            confidence_settings={
                'minimum_confidence': 0.3,
                'high_confidence': 0.8,
                'modal_bonus': 0.2
            }
        )
    
    def _parse_config_data(self, config_data: Dict) -> ModalConfiguration:
        """è¨­å®šãƒ‡ãƒ¼ã‚¿ã®è§£æ"""
        modal_patterns = {}
        for name, data in config_data.get('modal_patterns', {}).items():
            modal_patterns[name] = ModalPattern(
                pattern_type=data.get('pattern_type', name),
                modal_indicators=data.get('modal_indicators', []),
                semantic_categories=data.get('semantic_categories', []),
                pos_patterns=data.get('pos_patterns', []),
                dependency_patterns=data.get('dependency_patterns', []),
                morphological_patterns=data.get('morphological_patterns', []),
                confidence_weight=data.get('confidence_weight', 1.0)
            )
        
        return ModalConfiguration(
            modal_patterns=modal_patterns,
            semantic_analysis=config_data.get('semantic_analysis', {}),
            confidence_settings=config_data.get('confidence_settings', {})
        )
    
    def process(self, text: str) -> Dict[str, Any]:
        """
        åŠ©å‹•è©å‡¦ç†ãƒ¡ã‚¤ãƒ³ - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãªã—ç‰ˆ
        
        Args:
            text: å‡¦ç†å¯¾è±¡ã®è‹±èªæ–‡
            
        Returns:
            Dict: å‡¦ç†çµæœï¼ˆsuccess, modals, analysis, confidenceï¼‰
        """
        try:
            # spaCyè§£æ
            doc = self.nlp(text)
            
            # åŠ©å‹•è©è§£æ
            analysis_result = self.analyzer.analyze_modal_structure(doc)
            
            if not analysis_result['modals']:
                return self._create_no_modals_result(text)
            
            # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®æ§‹ç¯‰
            sub_slots = self._build_sub_slots(analysis_result['modals'])
            
            # æ„å‘³è§£æã®å®Ÿè¡Œ
            semantic_analysis = self._perform_semantic_analysis(analysis_result['modals'])
            
            return {
                'success': True,
                'original_text': text,
                'modals': analysis_result['modals'],
                'sub_slots': sub_slots,
                'semantic_analysis': semantic_analysis,
                'confidence': analysis_result['confidence'],
                'metadata': {
                    'handler': self.handler_info,
                    'analysis_method': analysis_result['analysis_method'],
                    'modal_count': len(analysis_result['modals'])
                }
            }
            
        except Exception as e:
            return self._create_failure_result(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def _create_no_modals_result(self, text: str) -> Dict[str, Any]:
        """åŠ©å‹•è©ãªã—ã®çµæœä½œæˆ"""
        return {
            'success': True,
            'original_text': text,
            'modals': [],
            'sub_slots': {},
            'semantic_analysis': {},
            'confidence': self.config.confidence_settings.get('minimum_confidence', 0.3),
            'metadata': {
                'handler': self.handler_info,
                'analysis_method': 'no_modals_detected'
            }
        }
    
    def _build_sub_slots(self, modals: List[Dict[str, Any]]) -> Dict[str, str]:
        """ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®æ§‹ç¯‰"""
        sub_slots = {}
        
        for i, modal in enumerate(modals):
            slot_key = f"sub-modal{i+1}"
            sub_slots[slot_key] = modal['modal']['text']
        
        return sub_slots
    
    def _perform_semantic_analysis(self, modals: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ„å‘³è§£æã®å®Ÿè¡Œ"""
        semantic_summary = {
            'categories': [],
            'functions': [],
            'dominant_meaning': None
        }
        
        category_counts = {}
        function_counts = {}
        
        for modal in modals:
            category = modal['modal']['semantic_category']
            function = modal['modal']['function']
            
            semantic_summary['categories'].append(category)
            semantic_summary['functions'].append(function)
            
            category_counts[category] = category_counts.get(category, 0) + 1
            function_counts[function] = function_counts.get(function, 0) + 1
        
        # ä¸»è¦ãªæ„å‘³ã®æ±ºå®š
        if category_counts:
            semantic_summary['dominant_meaning'] = max(category_counts.items(), key=lambda x: x[1])[0]
        
        semantic_summary['category_distribution'] = category_counts
        semantic_summary['function_distribution'] = function_counts
        
        return semantic_summary
    
    def _create_failure_result(self, error_message: str) -> Dict[str, Any]:
        """å¤±æ•—çµæœã®ä½œæˆ"""
        return {
            'success': False,
            'original_text': '',
            'modals': [],
            'sub_slots': {},
            'semantic_analysis': {},
            'confidence': 0.0,
            'error': error_message,
            'metadata': {
                'handler': self.handler_info,
                'analysis_method': 'error_handling'
            }
        }
    
    def get_configuration_template(self) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å–å¾—"""
        return {
            'modal_patterns': {
                'core_modals': {
                    'pattern_type': 'core_modal',
                    'modal_indicators': ['can', 'could', 'may', 'might', 'will', 'would', 'shall', 'should', 'must'],
                    'semantic_categories': ['ability', 'possibility', 'permission', 'obligation', 'future'],
                    'pos_patterns': ['AUX', 'VERB'],
                    'dependency_patterns': ['aux', 'ROOT'],
                    'confidence_weight': 1.2
                },
                'semi_modals': {
                    'pattern_type': 'semi_modal',
                    'modal_indicators': ['have to', 'ought to', 'be able to', 'be going to', 'used to'],
                    'semantic_categories': ['obligation', 'ability', 'future', 'habit'],
                    'pos_patterns': ['VERB', 'AUX'],
                    'dependency_patterns': ['aux', 'ROOT', 'xcomp'],
                    'confidence_weight': 1.0
                }
            },
            'semantic_analysis': {
                'category_groups': {
                    'ability': ['can', 'could', 'be able to'],
                    'possibility': ['may', 'might', 'could'],
                    'permission': ['may', 'can', 'could'],
                    'obligation': ['must', 'have to', 'should', 'ought to'],
                    'future': ['will', 'would', 'shall', 'be going to'],
                    'habit': ['used to', 'would']
                }
            },
            'confidence_settings': {
                'minimum_confidence': 0.3,
                'high_confidence': 0.8,
                'modal_bonus': 0.2
            }
        }


if __name__ == "__main__":
    # ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ç‰ˆã®ãƒ†ã‚¹ãƒˆ
    handler = ModalHandlerClean()
    
    test_sentences = [
        "She can speak three languages.",
        "You should study harder.",
        "It might rain tomorrow.",
        "We must finish this project."
    ]
    
    print("ğŸ§ª ModalHandler - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œå…¨é™¤å»ç‰ˆãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    for sentence in test_sentences:
        result = handler.process(sentence)
        print(f"\nå…¥åŠ›: \"{sentence}\"")
        print(f"æˆåŠŸ: {result['success']}")
        print(f"åŠ©å‹•è©æ•°: {len(result.get('modals', []))}")
        print(f"ä¿¡é ¼åº¦: {result.get('confidence', 0):.3f}")
        print(f"ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ: {result.get('sub_slots', {})}")
        print(f"æ„å‘³è§£æ: {result.get('semantic_analysis', {}).get('dominant_meaning', 'N/A')}")
        print(f"ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä½¿ç”¨: 0ä»¶ âœ…")
