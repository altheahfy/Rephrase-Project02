#!/usr/bin/env python3
"""
æ–°æ—§ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
æœŸå¾…å€¤ã¨ã®ç…§åˆã«ã‚ˆã‚‹ä¿¡é ¼æ€§ã®é«˜ã„æ¯”è¼ƒæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 

æ©Ÿèƒ½:
1. final_54_test_data_with_absolute_order_corrected.json ã®æœŸå¾…å€¤ã‚’ä½¿ç”¨
2. æ–°æ—§ä¸¡ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè¡Œã¨æœŸå¾…å€¤ç…§åˆ
3. è©³ç´°ãªå·®ç•°åˆ†æã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
4. fast_test.pyäº’æ›ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
"""

import json
import sys
import os
import time
import argparse
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional

# ãƒ‘ã‚¹è¨­å®š
SCRIPT_DIR = Path(__file__).parent
sys.path.insert(0, str(SCRIPT_DIR))

# ã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from central_controller import CentralController
from central_controller_v2 import CentralControllerV2


class ExpectedValueValidator:
    """æœŸå¾…å€¤æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.validation_rules = {
            'main_slots': ['S', 'V', 'O1', 'O2', 'C1', 'C2', 'M1', 'M2', 'M3'],
            'sub_slots': ['sub-s', 'sub-aux', 'sub-v', 'sub-o1', 'sub-o2', 'sub-c1', 'sub-c2', 'sub-m1', 'sub-m2', 'sub-m3']
        }
    
    def validate_against_expected(self, actual_result: Dict[str, Any], expected: Dict[str, Any]) -> Dict[str, Any]:
        """å®Ÿéš›ã®çµæœã¨æœŸå¾…å€¤ã®ç…§åˆ"""
        
        validation_result = {
            'overall_match': False,
            'main_slots_match': False,
            'sub_slots_match': False,
            'differences': {
                'main_slots': {},
                'sub_slots': {}
            },
            'scores': {
                'main_slots_accuracy': 0.0,
                'sub_slots_accuracy': 0.0,
                'overall_accuracy': 0.0
            }
        }
        
        # ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ­ãƒƒãƒˆæ¤œè¨¼
        main_validation = self._validate_slots(
            actual_result.get('main_slots', {}), 
            expected.get('main_slots', {}),
            'main_slots'
        )
        
        # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œè¨¼
        sub_validation = self._validate_slots(
            actual_result.get('sub_slots', {}),
            expected.get('sub_slots', {}), 
            'sub_slots'
        )
        
        validation_result['main_slots_match'] = main_validation['perfect_match']
        validation_result['sub_slots_match'] = sub_validation['perfect_match']
        validation_result['differences']['main_slots'] = main_validation['differences']
        validation_result['differences']['sub_slots'] = sub_validation['differences']
        validation_result['scores']['main_slots_accuracy'] = main_validation['accuracy']
        validation_result['scores']['sub_slots_accuracy'] = sub_validation['accuracy']
        
        # å…¨ä½“è©•ä¾¡
        validation_result['overall_match'] = (
            validation_result['main_slots_match'] and 
            validation_result['sub_slots_match']
        )
        
        validation_result['scores']['overall_accuracy'] = (
            (main_validation['accuracy'] + sub_validation['accuracy']) / 2
        )
        
        return validation_result
    
    def _validate_slots(self, actual_slots: Dict[str, str], expected_slots: Dict[str, str], slot_type: str) -> Dict[str, Any]:
        """ã‚¹ãƒ­ãƒƒãƒˆå˜ä½ã®è©³ç´°æ¤œè¨¼"""
        
        relevant_slots = self.validation_rules[slot_type]
        differences = {
            'missing': [],      # æœŸå¾…å€¤ã«ã‚ã‚‹ãŒå®Ÿéš›ã«ãªã„
            'extra': [],        # å®Ÿéš›ã«ã‚ã‚‹ãŒæœŸå¾…å€¤ã«ãªã„  
            'incorrect': []     # å€¤ãŒç•°ãªã‚‹
        }
        
        total_expected = len(expected_slots)
        correct_count = 0
        
        # æœŸå¾…å€¤ã®å„ã‚¹ãƒ­ãƒƒãƒˆã‚’ãƒã‚§ãƒƒã‚¯
        for slot_key, expected_value in expected_slots.items():
            if slot_key in actual_slots:
                actual_value = actual_slots[slot_key]
                if self._normalize_value(actual_value) == self._normalize_value(expected_value):
                    correct_count += 1
                else:
                    differences['incorrect'].append({
                        'slot': slot_key,
                        'expected': expected_value,
                        'actual': actual_value
                    })
            else:
                differences['missing'].append({
                    'slot': slot_key,
                    'expected': expected_value
                })
        
        # å®Ÿéš›ã®çµæœã§æœŸå¾…å€¤ã«ãªã„ã‚¹ãƒ­ãƒƒãƒˆã‚’ãƒã‚§ãƒƒã‚¯
        for slot_key, actual_value in actual_slots.items():
            if slot_key not in expected_slots and actual_value and actual_value.strip():
                differences['extra'].append({
                    'slot': slot_key,
                    'actual': actual_value
                })
        
        # ç²¾åº¦è¨ˆç®—
        accuracy = correct_count / total_expected if total_expected > 0 else 1.0
        perfect_match = (
            len(differences['missing']) == 0 and 
            len(differences['extra']) == 0 and 
            len(differences['incorrect']) == 0
        )
        
        return {
            'perfect_match': perfect_match,
            'accuracy': accuracy,
            'differences': differences,
            'correct_count': correct_count,
            'total_expected': total_expected
        }
    
    def _normalize_value(self, value: str) -> str:
        """å€¤ã®æ­£è¦åŒ–ï¼ˆæ¯”è¼ƒç”¨ï¼‰"""
        if not value:
            return ""
        return str(value).strip().lower()


class UnifiedTestSystem:
    """æ–°æ—§ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        print("ğŸ”¬ æ–°æ—§ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆ åˆæœŸåŒ–é–‹å§‹")
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.test_data = self._load_test_data()
        print(f"ğŸ“‹ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {self.test_data['meta']['total_count']}ä»¶")
        
        # æœŸå¾…å€¤æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self.validator = ExpectedValueValidator()
        
        # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self.legacy_controller = None
        self.v2_controller = None
        
        self._initialize_systems()
        
        self.test_results = []
    
    def _load_test_data(self) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        try:
            with open('final_54_test_data_with_absolute_order_corrected.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print("âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            sys.exit(1)
    
    def _initialize_systems(self):
        """ä¸¡ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
        
        # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ 
        try:
            self.legacy_controller = CentralController()
            print("âœ… æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"âŒ æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
        
        # æ–°ã‚·ã‚¹ãƒ†ãƒ 
        try:
            self.v2_controller = CentralControllerV2()
            print("âœ… æ–°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"âŒ æ–°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
    
    def parse_range(self, case_range: str) -> List[str]:
        """ã‚±ãƒ¼ã‚¹ç¯„å›²è§£æï¼ˆfast_test.pyäº’æ›ï¼‰"""
        
        # ãƒ—ãƒªã‚»ãƒƒãƒˆç¯„å›²å®šç¾©
        presets = {
            'all': list(self.test_data['data'].keys()),
            'basic': [str(i) for i in range(1, 18)],
            'adverbs': [str(i) for i in range(18, 43)],
            'modal': [str(i) for i in range(87, 111)],
            'relative': ['56', '58', '64'],
            'passive': [str(i) for i in range(66, 70)],
            'v2_test': ['1', '2', '3', '87', '88', '56']  # Phase 6a ãƒ†ã‚¹ãƒˆç”¨
        }
        
        if case_range in presets:
            return presets[case_range]
        
        # æ•°å€¤ç¯„å›²è§£æ
        case_numbers = []
        for part in case_range.split(','):
            if '-' in part:
                start, end = map(int, part.split('-'))
                case_numbers.extend([str(i) for i in range(start, end + 1)])
            else:
                case_numbers.append(part.strip())
        
        return case_numbers
    
    def run_unified_test(self, case_range: str = 'v2_test', verbose: bool = True) -> Dict[str, Any]:
        """çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        
        case_numbers = self.parse_range(case_range)
        print(f"\nğŸš€ çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹: {len(case_numbers)}ä»¶")
        print("=" * 80)
        
        results = {
            'test_summary': {
                'total_cases': len(case_numbers),
                'case_range': case_range,
                'timestamp': time.time()
            },
            'system_results': {
                'legacy': {'success': 0, 'failure': 0, 'total_time': 0},
                'v2': {'success': 0, 'failure': 0, 'total_time': 0}
            },
            'accuracy_results': {
                'legacy': {'perfect_matches': 0, 'total_accuracy': 0},
                'v2': {'perfect_matches': 0, 'total_accuracy': 0}
            },
            'individual_results': []
        }
        
        for i, case_num in enumerate(case_numbers, 1):
            if case_num not in self.test_data['data']:
                print(f"âš ï¸ ã‚±ãƒ¼ã‚¹ {case_num} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                continue
            
            case_data = self.test_data['data'][case_num]
            sentence = case_data['sentence']
            expected = case_data['expected']
            
            if verbose:
                print(f"\nğŸ“ ã‚±ãƒ¼ã‚¹ {i}/{len(case_numbers)}: {case_num}")
                print(f"   æ–‡: {sentence}")
            
            # å€‹åˆ¥ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            test_result = self._run_single_test(case_num, sentence, expected, verbose)
            results['individual_results'].append(test_result)
            
            # çµ±è¨ˆæ›´æ–°
            self._update_statistics(results, test_result)
            
            if verbose and i % 5 == 0:
                print(f"\nâ³ é€²æ—: {i}/{len(case_numbers)} ({i/len(case_numbers)*100:.1f}%)")
        
        # æœ€çµ‚çµ±è¨ˆè¨ˆç®—
        self._finalize_statistics(results)
        
        return results
    
    def _run_single_test(self, case_num: str, sentence: str, expected: Dict[str, Any], verbose: bool) -> Dict[str, Any]:
        """å˜ä¸€ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®å®Ÿè¡Œ"""
        
        result = {
            'case_number': case_num,
            'sentence': sentence,
            'expected': expected,
            'legacy_result': None,
            'v2_result': None,
            'validations': {
                'legacy': None,
                'v2': None
            },
            'performance': {
                'legacy_time': None,
                'v2_time': None
            },
            'errors': {
                'legacy': None,
                'v2': None
            }
        }
        
        # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ
        if self.legacy_controller:
            try:
                start_time = time.time()
                # æ­£ã—ã„ãƒ¡ã‚½ãƒƒãƒ‰åã‚’ä½¿ç”¨
                legacy_output = self.legacy_controller.process_sentence(sentence)
                result['performance']['legacy_time'] = time.time() - start_time
                result['legacy_result'] = legacy_output
                
                # æœŸå¾…å€¤ç…§åˆ
                validation = self.validator.validate_against_expected(legacy_output, expected)
                result['validations']['legacy'] = validation
                
                if verbose:
                    status = "âœ…" if validation['overall_match'] else "âŒ"
                    print(f"   æ—¢å­˜: {status} ç²¾åº¦={validation['scores']['overall_accuracy']:.2f}")
                    
            except Exception as e:
                result['errors']['legacy'] = str(e)
                if verbose:
                    print(f"   æ—¢å­˜: âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æ–°ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ
        if self.v2_controller:
            try:
                start_time = time.time()
                v2_analysis = self.v2_controller.analyze_grammar_structure_v2(sentence)
                result['performance']['v2_time'] = time.time() - start_time
                
                # V2çµæœã‚’æ—¢å­˜å½¢å¼ã«å¤‰æ›ï¼ˆæœŸå¾…å€¤ç…§åˆç”¨ï¼‰
                v2_converted = self._convert_v2_to_legacy_format(v2_analysis)
                result['v2_result'] = v2_converted
                
                # æœŸå¾…å€¤ç…§åˆ
                validation = self.validator.validate_against_expected(v2_converted, expected)
                result['validations']['v2'] = validation
                
                if verbose:
                    status = "âœ…" if validation['overall_match'] else "âŒ"
                    print(f"   æ–°ã‚·ã‚¹ãƒ†ãƒ : {status} ç²¾åº¦={validation['scores']['overall_accuracy']:.2f}")
                    
            except Exception as e:
                result['errors']['v2'] = str(e)
                if verbose:
                    print(f"   æ–°ã‚·ã‚¹ãƒ†ãƒ : âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return result
    
    def _convert_v2_to_legacy_format(self, v2_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """V2çµæœã‚’æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ å½¢å¼ã«å¤‰æ›ï¼ˆæœŸå¾…å€¤ç…§åˆç”¨ï¼‰"""
        
        # V2ã‚·ã‚¹ãƒ†ãƒ ã¯æ—¢ã«main_slotsã€sub_slotsã‚’è¿”ã—ã¦ã„ã‚‹
        # ç›´æ¥ãã‚Œã‚‰ã‚’ä½¿ç”¨ã—ã¦æ—¢å­˜å½¢å¼ã«å¤‰æ›
        converted_result = {
            'main_slots': v2_analysis.get('main_slots', {}),
            'sub_slots': v2_analysis.get('sub_slots', {}),
            'detected_grammar': v2_analysis.get('detected_grammar', []),
            'confidence': v2_analysis.get('confidence', 0.0),
            'v2_metadata': v2_analysis.get('v2_metadata', {})
        }
        
        return converted_result
    
    def _update_statistics(self, results: Dict[str, Any], test_result: Dict[str, Any]):
        """çµ±è¨ˆæƒ…å ±ã®æ›´æ–°"""
        
        # æˆåŠŸ/å¤±æ•—ã‚«ã‚¦ãƒ³ãƒˆ
        if test_result['errors']['legacy'] is None:
            results['system_results']['legacy']['success'] += 1
        else:
            results['system_results']['legacy']['failure'] += 1
            
        if test_result['errors']['v2'] is None:
            results['system_results']['v2']['success'] += 1
        else:
            results['system_results']['v2']['failure'] += 1
        
        # å®Ÿè¡Œæ™‚é–“ç´¯è¨ˆ
        if test_result['performance']['legacy_time']:
            results['system_results']['legacy']['total_time'] += test_result['performance']['legacy_time']
        if test_result['performance']['v2_time']:
            results['system_results']['v2']['total_time'] += test_result['performance']['v2_time']
        
        # ç²¾åº¦çµ±è¨ˆ
        if test_result['validations']['legacy']:
            if test_result['validations']['legacy']['overall_match']:
                results['accuracy_results']['legacy']['perfect_matches'] += 1
            results['accuracy_results']['legacy']['total_accuracy'] += test_result['validations']['legacy']['scores']['overall_accuracy']
            
        if test_result['validations']['v2']:
            if test_result['validations']['v2']['overall_match']:
                results['accuracy_results']['v2']['perfect_matches'] += 1
            results['accuracy_results']['v2']['total_accuracy'] += test_result['validations']['v2']['scores']['overall_accuracy']
    
    def _finalize_statistics(self, results: Dict[str, Any]):
        """æœ€çµ‚çµ±è¨ˆè¨ˆç®—"""
        
        total_cases = results['test_summary']['total_cases']
        
        # æˆåŠŸç‡è¨ˆç®—
        results['system_results']['legacy']['success_rate'] = (
            results['system_results']['legacy']['success'] / total_cases if total_cases > 0 else 0
        )
        results['system_results']['v2']['success_rate'] = (
            results['system_results']['v2']['success'] / total_cases if total_cases > 0 else 0
        )
        
        # å¹³å‡ç²¾åº¦è¨ˆç®—
        results['accuracy_results']['legacy']['average_accuracy'] = (
            results['accuracy_results']['legacy']['total_accuracy'] / total_cases if total_cases > 0 else 0
        )
        results['accuracy_results']['v2']['average_accuracy'] = (
            results['accuracy_results']['v2']['total_accuracy'] / total_cases if total_cases > 0 else 0
        )
        
        # å®Œå…¨ä¸€è‡´ç‡è¨ˆç®—
        results['accuracy_results']['legacy']['perfect_match_rate'] = (
            results['accuracy_results']['legacy']['perfect_matches'] / total_cases if total_cases > 0 else 0
        )
        results['accuracy_results']['v2']['perfect_match_rate'] = (
            results['accuracy_results']['v2']['perfect_matches'] / total_cases if total_cases > 0 else 0
        )
    
    def print_summary(self, results: Dict[str, Any]):
        """ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        
        print(f"\nğŸ“Š çµ±åˆãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
        print("=" * 60)
        
        print(f"ğŸ“‹ ãƒ†ã‚¹ãƒˆæ¦‚è¦:")
        print(f"   ç·ãƒ†ã‚¹ãƒˆæ•°: {results['test_summary']['total_cases']}")
        print(f"   ãƒ†ã‚¹ãƒˆç¯„å›²: {results['test_summary']['case_range']}")
        
        print(f"\nğŸ”§ ã‚·ã‚¹ãƒ†ãƒ æˆåŠŸç‡:")
        print(f"   æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ : {results['system_results']['legacy']['success_rate']:.1%} ({results['system_results']['legacy']['success']}/{results['test_summary']['total_cases']})")
        print(f"   æ–°ã‚·ã‚¹ãƒ†ãƒ : {results['system_results']['v2']['success_rate']:.1%} ({results['system_results']['v2']['success']}/{results['test_summary']['total_cases']})")
        
        print(f"\nğŸ¯ æœŸå¾…å€¤ä¸€è‡´ç²¾åº¦:")
        print(f"   æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ : å¹³å‡={results['accuracy_results']['legacy']['average_accuracy']:.1%}, å®Œå…¨ä¸€è‡´={results['accuracy_results']['legacy']['perfect_match_rate']:.1%}")
        print(f"   æ–°ã‚·ã‚¹ãƒ†ãƒ : å¹³å‡={results['accuracy_results']['v2']['average_accuracy']:.1%}, å®Œå…¨ä¸€è‡´={results['accuracy_results']['v2']['perfect_match_rate']:.1%}")
        
        print(f"\nâš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
        legacy_avg = results['system_results']['legacy']['total_time'] / results['test_summary']['total_cases']
        v2_avg = results['system_results']['v2']['total_time'] / results['test_summary']['total_cases']
        print(f"   æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ : å¹³å‡={legacy_avg:.3f}s")
        print(f"   æ–°ã‚·ã‚¹ãƒ†ãƒ : å¹³å‡={v2_avg:.3f}s")
        if legacy_avg > 0:
            print(f"   æ€§èƒ½æ¯”: æ–°ã‚·ã‚¹ãƒ†ãƒ ãŒ{legacy_avg/v2_avg:.1f}å€é«˜é€Ÿ" if v2_avg < legacy_avg else f"   æ€§èƒ½æ¯”: æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ãŒ{v2_avg/legacy_avg:.1f}å€é«˜é€Ÿ")

        # ä¸ä¸€è‡´æ¡ˆä»¶ã®è©³ç´°è¡¨ç¤º
        self.print_mismatch_details(results)
    
    def print_mismatch_details(self, results: Dict[str, Any]):
        """ä¸ä¸€è‡´æ¡ˆä»¶ã®è©³ç´°è¡¨ç¤º"""
        print(f"\nğŸ“‹ ä¸ä¸€è‡´æ¡ˆä»¶è©³ç´°åˆ†æ:")
        print("=" * 60)
        
        # æ–°ã‚·ã‚¹ãƒ†ãƒ ã®ä¸ä¸€è‡´æ¡ˆä»¶ã‚’æŠ½å‡º
        v2_mismatches = []
        legacy_mismatches = []
        
        for case_result in results.get('individual_results', []):
            case_id = case_result.get('case_number')
            text = case_result.get('sentence', '')
            expected = case_result.get('expected', {})
            
            # æ–°ã‚·ã‚¹ãƒ†ãƒ ã®ä¸ä¸€è‡´ãƒã‚§ãƒƒã‚¯
            v2_validation = case_result.get('validations', {}).get('v2', {})
            if v2_validation and not v2_validation.get('overall_match', False):
                v2_result = case_result.get('v2_result', {})
                v2_mismatches.append({
                    'case_id': case_id,
                    'text': text,
                    'expected': expected.get('main_slots', {}),
                    'actual': v2_result.get('main_slots', {}),
                    'accuracy': v2_validation.get('scores', {}).get('overall_accuracy', 0),
                    'differences': v2_validation.get('differences', {}),
                    'errors': case_result.get('errors', {}).get('v2')
                })
            
            # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®ä¸ä¸€è‡´ãƒã‚§ãƒƒã‚¯  
            legacy_validation = case_result.get('validations', {}).get('legacy', {})
            if legacy_validation and not legacy_validation.get('overall_match', False):
                legacy_result = case_result.get('legacy_result', {})
                legacy_mismatches.append({
                    'case_id': case_id,
                    'text': text,
                    'expected': expected.get('main_slots', {}),
                    'actual': legacy_result.get('main_slots', {}),
                    'accuracy': legacy_validation.get('scores', {}).get('overall_accuracy', 0),
                    'differences': legacy_validation.get('differences', {}),
                    'errors': case_result.get('errors', {}).get('legacy')
                })
        
        # æ–°ã‚·ã‚¹ãƒ†ãƒ ã®ä¸ä¸€è‡´è¡¨ç¤º
        if v2_mismatches:
            print(f"ğŸ”´ æ–°ã‚·ã‚¹ãƒ†ãƒ ä¸ä¸€è‡´æ¡ˆä»¶: {len(v2_mismatches)}ä»¶")
            for i, mismatch in enumerate(v2_mismatches, 1):
                print(f"\n  {i}. ã‚±ãƒ¼ã‚¹{mismatch['case_id']}: \"{mismatch['text']}\"")
                print(f"     ç²¾åº¦: {mismatch['accuracy']:.1%}")
                
                if mismatch.get('errors'):
                    print(f"     ã‚¨ãƒ©ãƒ¼: {mismatch['errors']}")
                    continue
                
                print(f"     æœŸå¾…å€¤: {mismatch['expected']}")
                print(f"     å®Ÿéš›å€¤: {mismatch['actual']}")
                
                # å·®åˆ†è©³ç´°ï¼ˆdifferencesæ§‹é€ ã‚’ä½¿ç”¨ï¼‰
                main_diff = mismatch.get('differences', {}).get('main_slots', {})
                if main_diff.get('missing'):
                    print(f"     ä¸è¶³ã‚¹ãƒ­ãƒƒãƒˆ: {[item['slot'] for item in main_diff['missing']]}")
                if main_diff.get('extra'):
                    print(f"     ä½™åˆ†ã‚¹ãƒ­ãƒƒãƒˆ: {[item['slot'] for item in main_diff['extra']]}")
                if main_diff.get('incorrect'):
                    print(f"     å€¤é•ã„ã‚¹ãƒ­ãƒƒãƒˆ:")
                    for item in main_diff['incorrect']:
                        print(f"       {item['slot']}: æœŸå¾…=\"{item['expected']}\" â†’ å®Ÿéš›=\"{item['actual']}\"")
        else:
            print(f"âœ… æ–°ã‚·ã‚¹ãƒ†ãƒ : å…¨ä»¶å®Œå…¨ä¸€è‡´")
        
        # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®ä¸ä¸€è‡´è¡¨ç¤º
        if legacy_mismatches:
            print(f"\nğŸ”´ æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ä¸ä¸€è‡´æ¡ˆä»¶: {len(legacy_mismatches)}ä»¶")
            for i, mismatch in enumerate(legacy_mismatches, 1):
                print(f"\n  {i}. ã‚±ãƒ¼ã‚¹{mismatch['case_id']}: \"{mismatch['text']}\"")
                print(f"     ç²¾åº¦: {mismatch['accuracy']:.1%}")
                
                if mismatch.get('errors'):
                    print(f"     ã‚¨ãƒ©ãƒ¼: {mismatch['errors']}")
                    continue
                
                print(f"     æœŸå¾…å€¤: {mismatch['expected']}")
                print(f"     å®Ÿéš›å€¤: {mismatch['actual']}")
        else:
            print(f"âœ… æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ : å…¨ä»¶å®Œå…¨ä¸€è‡´")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description='æ–°æ—§ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆ')
    parser.add_argument('case_range', nargs='?', default='v2_test', 
                       help='ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ç¯„å›² (ä¾‹: 1-10, basic, modal, v2_test)')
    parser.add_argument('-v', '--verbose', action='store_true', 
                       help='è©³ç´°ãƒ­ã‚°è¡¨ç¤º')
    parser.add_argument('-q', '--quiet', action='store_true',
                       help='æœ€å°ãƒ­ã‚°è¡¨ç¤º')
    
    args = parser.parse_args()
    
    # ãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    test_system = UnifiedTestSystem()
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    results = test_system.run_unified_test(
        case_range=args.case_range,
        verbose=not args.quiet
    )
    
    # çµæœè¡¨ç¤º
    test_system.print_summary(results)
    
    # çµæœä¿å­˜
    output_file = f"unified_test_results_{args.case_range}_{int(time.time())}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“ è©³ç´°çµæœã‚’ä¿å­˜: {output_file}")


if __name__ == "__main__":
    main()
