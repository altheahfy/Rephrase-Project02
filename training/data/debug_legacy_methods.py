#!/usr/bin/env python3
"""
ãƒ¬ã‚¬ã‚·ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿç®‡æ‰€ã®ç‰¹å®š
å„ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å€‹åˆ¥ãƒ†ã‚¹ãƒˆã—ã¦ã‚¨ãƒ©ãƒ¼ç®‡æ‰€ã‚’ç‰¹å®š
"""

import traceback
from dynamic_grammar_mapper import DynamicGrammarMapper

def test_individual_legacy_methods():
    """ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å€‹åˆ¥ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ” ãƒ¬ã‚¬ã‚·ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ã‚½ãƒƒãƒ‰å€‹åˆ¥ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    mapper = DynamicGrammarMapper()
    
    # ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    test_sentence = "I run."
    doc = mapper.nlp(test_sentence)
    tokens = mapper._extract_tokens(doc)
    
    print(f"ğŸ“ ãƒ†ã‚¹ãƒˆæ–‡: '{test_sentence}'")
    print(f"ğŸ¯ ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(tokens)}")
    print(f"ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³æ§‹é€ : {[type(token) for token in tokens[:3]]}")
    
    # é™¤å¤–ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æº–å‚™
    excluded_indices = set()
    filtered_tokens = [token for i, token in enumerate(tokens) if i not in excluded_indices]
    
    print(f"ğŸ”§ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¾Œãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(filtered_tokens)}")
    
    # å„ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å€‹åˆ¥ãƒ†ã‚¹ãƒˆ
    methods_to_test = [
        ('_identify_core_elements', lambda: mapper._identify_core_elements(filtered_tokens)),
        ('_determine_sentence_pattern', None),  # core_elementsãŒå¿…è¦
        ('_assign_grammar_roles', None),       # multiple paramsãŒå¿…è¦
        ('_convert_to_rephrase_format', None)  # multiple paramsãŒå¿…è¦
    ]
    
    core_elements = None
    sentence_pattern = None
    grammar_elements = None
    
    for method_name, method_func in methods_to_test:
        print(f"\nğŸ“ ãƒ†ã‚¹ãƒˆ: {method_name}")
        print("-" * 40)
        
        try:
            if method_name == '_identify_core_elements':
                result = method_func()
                core_elements = result
                print(f"   âœ… æˆåŠŸ: {type(result)}")
                print(f"   ğŸ“Š ã‚­ãƒ¼: {list(result.keys()) if isinstance(result, dict) else 'Not Dict'}")
                
            elif method_name == '_determine_sentence_pattern':
                if core_elements is not None:
                    result = mapper._determine_sentence_pattern(core_elements, filtered_tokens)
                    sentence_pattern = result
                    print(f"   âœ… æˆåŠŸ: {result}")
                else:
                    print("   â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: core_elementsãŒNone")
                    
            elif method_name == '_assign_grammar_roles':
                if core_elements is not None and sentence_pattern is not None:
                    relative_clause_info = {'found': False, 'type': None}
                    result = mapper._assign_grammar_roles(filtered_tokens, sentence_pattern, core_elements, relative_clause_info)
                    grammar_elements = result
                    print(f"   âœ… æˆåŠŸ: {type(result)}")
                    print(f"   ğŸ“Š ã‚­ãƒ¼: {list(result.keys()) if isinstance(result, dict) else 'Not Dict'}")
                else:
                    print("   â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: å‰ææ¡ä»¶ãŒä¸è¶³")
                    
            elif method_name == '_convert_to_rephrase_format':
                if grammar_elements is not None and sentence_pattern is not None:
                    sub_slots = {}
                    result = mapper._convert_to_rephrase_format(grammar_elements, sentence_pattern, sub_slots)
                    print(f"   âœ… æˆåŠŸ: {type(result)}")
                    print(f"   ğŸ“Š ã‚­ãƒ¼: {list(result.keys()) if isinstance(result, dict) else 'Not Dict'}")
                else:
                    print("   â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: å‰ææ¡ä»¶ãŒä¸è¶³")
                    
        except Exception as e:
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {type(e).__name__}: {e}")
            print("   ğŸ“Š è©³ç´°ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹:")
            
            # ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã®è©³ç´°è¡¨ç¤º
            tb_lines = traceback.format_exc().split('\n')
            for line in tb_lines:
                if line.strip():
                    if 'list indices must be integers' in line:
                        print(f"   ğŸ¯ TARGET ERROR: {line}")
                    elif 'dynamic_grammar_mapper.py' in line and 'line' in line:
                        print(f"   ğŸ“ FILE LOCATION: {line}")
                    else:
                        print(f"   {line}")
            
            # ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã®ã§ã€ä»¥é™ã¯ã‚¹ã‚­ãƒƒãƒ—
            break

def test_data_structure_analysis():
    """ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®è©³ç´°åˆ†æ"""
    print("\nğŸ” ãƒ‡ãƒ¼ã‚¿æ§‹é€ è©³ç´°åˆ†æ")
    print("=" * 60)
    
    mapper = DynamicGrammarMapper()
    doc = mapper.nlp("I run.")
    tokens = mapper._extract_tokens(doc)
    
    print("ğŸ“Š tokensé…åˆ—ã®è©³ç´°:")
    for i, token in enumerate(tokens):
        print(f"   [{i}] Type: {type(token)}")
        if hasattr(token, '__dict__'):
            print(f"       Attributes: {list(token.__dict__.keys())[:5]}...")  # æœ€åˆã®5å€‹ã®ã¿
        if isinstance(token, dict):
            print(f"       Dict Keys: {list(token.keys())}")
        if hasattr(token, 'text'):
            print(f"       Text: '{token.text}'")
        print()

def test_access_patterns():
    """å•é¡Œã®ã‚ã‚‹ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ” ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    mapper = DynamicGrammarMapper()
    doc = mapper.nlp("I run.")
    tokens = mapper._extract_tokens(doc)
    
    # å±é™ºãªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ†ã‚¹ãƒˆ
    test_patterns = [
        ("tokens[0]['text']", lambda: tokens[0]['text']),
        ("tokens[0].text", lambda: tokens[0].text),
        ("str(tokens[0])", lambda: str(tokens[0])),
        ("isinstance(tokens[0], dict)", lambda: isinstance(tokens[0], dict)),
    ]
    
    for pattern_name, pattern_func in test_patterns:
        print(f"ğŸ“ ãƒ†ã‚¹ãƒˆ: {pattern_name}")
        try:
            result = pattern_func()
            print(f"   âœ… æˆåŠŸ: {result}")
        except Exception as e:
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}")

if __name__ == "__main__":
    # ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰å€‹åˆ¥ãƒ†ã‚¹ãƒˆ
    test_individual_legacy_methods()
    
    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ åˆ†æ
    test_data_structure_analysis()
    
    # ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ†ã‚¹ãƒˆ
    test_access_patterns()
    
    print("\nğŸ¯ èª¿æŸ»å®Œäº†")
    print("ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿç®‡æ‰€ã‚’ç‰¹å®šã—ã¾ã—ãŸ")
