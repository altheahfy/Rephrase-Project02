#!/usr/bin/env python3
"""
å‹•çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ  v2.0
==================

èªæ•°ã«ä¾å­˜ã—ãªã„æ–‡æ³•è¦ç´ ãƒ™ãƒ¼ã‚¹ã®èªè­˜ã‚·ã‚¹ãƒ†ãƒ 
- å“è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã¯ãªãæ–‡æ³•çš„å½¹å‰²ã§èªè­˜
- ä¿®é£¾èªã®å‹•çš„æ¤œå‡ºã¨é™¤å¤–
- æ ¸è¦ç´ ï¼ˆä¸»èªãƒ»å‹•è©ãƒ»ç›®çš„èªãƒ»è£œèªï¼‰ã®ç‰¹å®š
- 5æ–‡å‹ã®å‹•çš„åˆ¤å®š

è¨­è¨ˆæ€æƒ³:
1. æ–‡ã®ã€Œæ ¸ã€ã‚’ç‰¹å®šï¼ˆä¸»èª + å‹•è©ï¼‰
2. å‹•è©ã®æ€§è³ªã‹ã‚‰æ–‡å‹ã‚’æ¨å®š
3. æ®‹ã‚Šã®è¦ç´ ã‚’æ–‡æ³•çš„å½¹å‰²ã§åˆ†é¡
4. ä¿®é£¾èªã¯åˆ¥é€”å‡¦ç†
"""

import spacy
import logging
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass

# ğŸ†• Phase 1.2: æ–‡å‹èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³è¿½åŠ 
from sentence_type_detector import SentenceTypeDetector

@dataclass
class GrammarElement:
    """æ–‡æ³•è¦ç´ ã®å®šç¾©"""
    text: str
    tokens: List[Dict]
    role: str  # S, V, O1, O2, C1, C2, M1, M2, M3, Aux
    start_idx: int
    end_idx: int
    confidence: float
    
    # ğŸ†• Orderæ©Ÿèƒ½é–¢é€£ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ (Phase 1.1)
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®šã«ã‚ˆã‚Šæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¸ã®å½±éŸ¿ã‚’æœ€å°åŒ–
    slot_display_order: int = 0      # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆé †åº
    display_order: int = 0           # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå†…é †åº  
    v_group_key: str = ""            # å‹•è©ã‚°ãƒ«ãƒ¼ãƒ—ã‚­ãƒ¼
    sentence_type: str = ""          # æ–‡å‹ (statement/wh_question/yes_no_question)
    is_subslot: bool = False         # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆãƒ•ãƒ©ã‚°
    parent_slot: str = ""            # è¦ªã‚¹ãƒ­ãƒƒãƒˆ (ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”¨)
    subslot_id: str = ""             # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆID (sub-s, sub-vç­‰)

class DynamicGrammarMapper:
    """
    å‹•çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ 
    èªæ•°ã«ä¾å­˜ã—ãªã„æ–‡æ³•èªè­˜ã‚’å®Ÿç¾
    """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        try:
            self.nlp = spacy.load("en_core_web_sm")
            print("âœ… spaCyå‹•çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except OSError:
            print("âŒ spaCyè‹±èªãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            raise
        
        self.logger = logging.getLogger(__name__)
        
        # ğŸ†• Phase 1.2: æ–‡å‹èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        self.sentence_type_detector = SentenceTypeDetector()
        print("âœ… æ–‡å‹èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
        
        # å‹•è©åˆ†é¡è¾æ›¸
        self.linking_verbs = {
            'be', 'am', 'is', 'are', 'was', 'were', 'being', 'been',
            'become', 'became', 'becoming',
            'seem', 'seemed', 'seeming', 'seems',
            'appear', 'appeared', 'appearing', 'appears',
            'look', 'looked', 'looking', 'looks',
            'sound', 'sounded', 'sounding', 'sounds',
            'feel', 'felt', 'feeling', 'feels',
            'taste', 'tasted', 'tasting', 'tastes',
            'smell', 'smelled', 'smelling', 'smells',
            'remain', 'remained', 'remaining', 'remains',
            'stay', 'stayed', 'staying', 'stays'
        }
        
        self.ditransitive_verbs = {
            'give', 'gave', 'given', 'giving', 'gives',
            'tell', 'told', 'telling', 'tells',
            'show', 'showed', 'shown', 'showing', 'shows',
            'send', 'sent', 'sending', 'sends',
            'teach', 'taught', 'teaching', 'teaches',
            'buy', 'bought', 'buying', 'buys',
            'bring', 'brought', 'bringing', 'brings',
            'offer', 'offered', 'offering', 'offers',
            'lend', 'lent', 'lending', 'lends',
            'sell', 'sold', 'selling', 'sells'
        }
        
        self.objective_complement_verbs = {
            'make', 'made', 'making', 'makes',
            'call', 'called', 'calling', 'calls',
            'consider', 'considered', 'considering', 'considers',
            'find', 'found', 'finding', 'finds',
            'keep', 'kept', 'keeping', 'keeps',
            'leave', 'left', 'leaving', 'leaves',
            'elect', 'elected', 'electing', 'elects',
            'name', 'named', 'naming', 'names',
            'choose', 'chose', 'chosen', 'choosing', 'chooses'
        }
    
    def analyze_sentence(self, sentence: str) -> Dict[str, Any]:
        """
        æ–‡ç« ã‚’å‹•çš„ã«è§£æã—ã¦Rephraseã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ ã‚’ç”Ÿæˆ
        
        Args:
            sentence (str): è§£æå¯¾è±¡ã®æ–‡ç« 
            
        Returns:
            Dict[str, Any]: Rephraseã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ 
        """
        try:
            # ğŸ†• Phase 1.2: æ–‡å‹èªè­˜
            sentence_type = self.sentence_type_detector.detect_sentence_type(sentence)
            sentence_type_confidence = self.sentence_type_detector.get_detection_confidence(sentence)
            
            # 1. spaCyåŸºæœ¬è§£æ
            doc = self.nlp(sentence)
            tokens = self._extract_tokens(doc)
            
            # 2. æ–‡ã®æ ¸è¦ç´ ã‚’ç‰¹å®š
            core_elements = self._identify_core_elements(tokens)
            
            # 3. å‹•è©ã®æ€§è³ªã‹ã‚‰æ–‡å‹ã‚’æ¨å®š
            sentence_pattern = self._determine_sentence_pattern(core_elements, tokens)
            
            # 4. æ–‡æ³•è¦ç´ ã‚’å‹•çš„ã«å‰²ã‚Šå½“ã¦
            grammar_elements = self._assign_grammar_roles(tokens, sentence_pattern, core_elements)
            
            # 5. Rephraseã‚¹ãƒ­ãƒƒãƒˆå½¢å¼ã«å¤‰æ›
            rephrase_result = self._convert_to_rephrase_format(grammar_elements, sentence_pattern)
            
            # ğŸ†• Phase 1.2: æ–‡å‹æƒ…å ±ã‚’çµæœã«è¿½åŠ 
            rephrase_result['sentence_type'] = sentence_type
            rephrase_result['sentence_type_confidence'] = sentence_type_confidence
            
            return rephrase_result
            
        except Exception as e:
            self.logger.error(f"å‹•çš„æ–‡æ³•è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_error_result(sentence, str(e))
    
    def _extract_tokens(self, doc) -> List[Dict]:
        """spaCyãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã‚’æŠ½å‡º"""
        tokens = []
        for token in doc:
            token_info = {
                'text': token.text,
                'pos': token.pos_,
                'tag': token.tag_,
                'lemma': token.lemma_,
                'dep': token.dep_,  # ä¾å­˜é–¢ä¿‚ï¼ˆå‚è€ƒã®ã¿ï¼‰
                'head': token.head.text,
                'is_stop': token.is_stop,
                'is_alpha': token.is_alpha,
                'index': token.i
            }
            tokens.append(token_info)
        return tokens
    
    def _identify_core_elements(self, tokens: List[Dict]) -> Dict[str, Any]:
        """
        æ–‡ã®æ ¸è¦ç´ ï¼ˆä¸»èªãƒ»å‹•è©ï¼‰ã‚’ç‰¹å®š
        ã“ã‚ŒãŒå…¨ã¦ã®æ–‡å‹èªè­˜ã®åŸºç›¤ã¨ãªã‚‹
        """
        core = {
            'subject': None,
            'verb': None,
            'subject_indices': [],
            'verb_indices': [],
            'auxiliary': None,
            'auxiliary_indices': []
        }
        
        # å‹•è©ã‚’æ¢ã™ï¼ˆæœ€ã‚‚é‡è¦ï¼‰
        main_verb_idx = self._find_main_verb(tokens)
        if main_verb_idx is not None:
            core['verb'] = tokens[main_verb_idx]
            core['verb_indices'] = [main_verb_idx]
            
            # åŠ©å‹•è©ã‚’æ¢ã™
            aux_idx = self._find_auxiliary(tokens, main_verb_idx)
            if aux_idx is not None:
                core['auxiliary'] = tokens[aux_idx]
                core['auxiliary_indices'] = [aux_idx]
        
        # ä¸»èªã‚’æ¢ã™ï¼ˆå‹•è©ã®å‰ã§æœ€ã‚‚é©åˆ‡ãªåè©å¥ï¼‰
        if main_verb_idx is not None:
            subject_indices = self._find_subject(tokens, main_verb_idx)
            if subject_indices:
                core['subject_indices'] = subject_indices
                core['subject'] = ' '.join([tokens[i]['text'] for i in subject_indices])
        
        return core
    
    def _find_main_verb(self, tokens: List[Dict]) -> Optional[int]:
        """
        ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’ç‰¹å®š
        å„ªå…ˆé †ä½: 1) å‹•è©ã‚¿ã‚°, 2) æ–‡è„ˆã‹ã‚‰åˆ¤æ–­
        """
        verb_candidates = []
        
        for i, token in enumerate(tokens):
            # å‹•è©ã®å“è©ã‚¿ã‚°ï¼ˆã‚ˆã‚Šåºƒç¯„å›²ã«æ¤œå‡ºï¼‰
            if (token['tag'].startswith('VB') and token['pos'] == 'VERB') or token['pos'] == 'AUX':
                # åŠ©å‹•è©ã®å ´åˆã§ã‚‚ã€ãƒ¡ã‚¤ãƒ³å‹•è©å€™è£œã¨ã—ã¦è€ƒæ…®
                verb_candidates.append((i, token))
        
        # åŠ©å‹•è©ã§ãªã„ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’å„ªå…ˆ
        main_verbs = [(i, token) for i, token in verb_candidates if not self._is_auxiliary_verb(token)]
        if main_verbs:
            return main_verbs[-1][0]  # æœ€å¾Œã®éåŠ©å‹•è©ã‚’é¸æŠ
        
        # ãƒ¡ã‚¤ãƒ³å‹•è©ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€åŠ©å‹•è©ã‚‚å«ã‚ã¦æ¤œè¨
        if verb_candidates:
            return verb_candidates[-1][0]
        
        return None
    
    def _find_auxiliary(self, tokens: List[Dict], main_verb_idx: int) -> Optional[int]:
        """åŠ©å‹•è©ã‚’ç‰¹å®š"""
        # ãƒ¡ã‚¤ãƒ³å‹•è©ã®å‰ã‚’æ¢ã™
        for i in range(main_verb_idx):
            token = tokens[i]
            if self._is_auxiliary_verb(token):
                return i
        return None
    
    def _find_subject(self, tokens: List[Dict], verb_idx: int) -> List[int]:
        """
        ä¸»èªã‚’ç‰¹å®šï¼ˆå‹•è©ã®å‰ã®åè©å¥ï¼‰
        è¤‡æ•°èªã®åè©å¥ã«å¯¾å¿œ
        """
        subject_indices = []
        
        # å‹•è©ã®å‰ã‚’å³ã‹ã‚‰å·¦ã«æ¢ã™
        for i in range(verb_idx - 1, -1, -1):
            token = tokens[i]
            
            # åŠ©å‹•è©ã¯é£›ã°ã™
            if self._is_auxiliary_verb(token):
                continue
            
            # åè©ãƒ»ä»£åè©ãƒ»å† è©ã‚’ä¸»èªã®ä¸€éƒ¨ã¨ã—ã¦åé›†
            if (token['pos'] in ['NOUN', 'PROPN', 'PRON'] or 
                token['tag'] in ['DT', 'PRP', 'PRP$', 'WP']):
                subject_indices.insert(0, i)  # é †åºã‚’ä¿ã¤ãŸã‚å‰ã«æŒ¿å…¥
            else:
                # ä¸»èªã®å¢ƒç•Œã«åˆ°é”
                break
        
        return subject_indices
    
    def _determine_sentence_pattern(self, core_elements: Dict, tokens: List[Dict]) -> str:
        """
        å‹•è©ã®æ€§è³ªã¨æ–‡è„ˆã‹ã‚‰æ–‡å‹ã‚’å‹•çš„ã«åˆ¤å®š
        """
        if not core_elements['verb']:
            return 'UNKNOWN'
        
        verb = core_elements['verb']
        verb_lemma = verb['lemma'].lower()
        verb_indices = core_elements['verb_indices'] + core_elements.get('auxiliary_indices', [])
        subject_indices = core_elements['subject_indices']
        
        # ä½¿ç”¨æ¸ˆã¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        used_indices = set(verb_indices + subject_indices)
        
        # æ®‹ã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åˆ†æ
        remaining_tokens = [
            (i, token) for i, token in enumerate(tokens) 
            if i not in used_indices and token['pos'] != 'PUNCT'
        ]
        
        # é€£çµå‹•è©ã®å ´åˆ â†’ SVCå€™è£œ
        if verb_lemma in self.linking_verbs:
            if remaining_tokens:
                # è£œèªãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                for i, token in remaining_tokens:
                    if self._can_be_complement(token):
                        return 'SVC'
            return 'SV'  # è£œèªãŒãªã„å ´åˆ
        
        # æˆä¸å‹•è©ã®å ´åˆ â†’ SVOOå€™è£œ
        if verb_lemma in self.ditransitive_verbs:
            if len(remaining_tokens) >= 2:
                return 'SVOO'
            elif len(remaining_tokens) == 1:
                return 'SVO'
        
        # ç›®çš„æ ¼è£œèªå‹•è©ã®å ´åˆ â†’ SVOCå€™è£œ
        if verb_lemma in self.objective_complement_verbs:
            if len(remaining_tokens) >= 2:
                return 'SVOC'
            elif len(remaining_tokens) == 1:
                return 'SVO'
        
        # ä¸€èˆ¬çš„ãªä»–å‹•è© â†’ SVO
        if remaining_tokens:
            # ç›®çš„èªå€™è£œãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            for i, token in remaining_tokens:
                if self._can_be_object(token):
                    return 'SVO'
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ â†’ SV
        return 'SV'
    
    def _assign_grammar_roles(self, tokens: List[Dict], pattern: str, core_elements: Dict) -> List[GrammarElement]:
        """
        æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ã„ã¦æ–‡æ³•çš„å½¹å‰²ã‚’å‹•çš„ã«å‰²ã‚Šå½“ã¦
        """
        elements = []
        used_indices = set()
        
        # ä¸»èª
        if core_elements['subject_indices']:
            subject_element = GrammarElement(
                text=core_elements['subject'],
                tokens=[tokens[i] for i in core_elements['subject_indices']],
                role='S',
                start_idx=min(core_elements['subject_indices']),
                end_idx=max(core_elements['subject_indices']),
                confidence=0.95
            )
            elements.append(subject_element)
            used_indices.update(core_elements['subject_indices'])
        
        # åŠ©å‹•è©
        if core_elements['auxiliary_indices']:
            aux_element = GrammarElement(
                text=core_elements['auxiliary']['text'],
                tokens=[core_elements['auxiliary']],
                role='Aux',
                start_idx=core_elements['auxiliary_indices'][0],
                end_idx=core_elements['auxiliary_indices'][0],
                confidence=0.95
            )
            elements.append(aux_element)
            used_indices.update(core_elements['auxiliary_indices'])
        
        # å‹•è©
        if core_elements['verb_indices']:
            verb_element = GrammarElement(
                text=core_elements['verb']['text'],
                tokens=[core_elements['verb']],
                role='V',
                start_idx=core_elements['verb_indices'][0],
                end_idx=core_elements['verb_indices'][0],
                confidence=0.95
            )
            elements.append(verb_element)
            used_indices.update(core_elements['verb_indices'])
        
        # æ®‹ã‚Šã®è¦ç´ ã‚’æ–‡å‹ã«å¿œã˜ã¦å‰²ã‚Šå½“ã¦
        remaining_tokens = [
            (i, token) for i, token in enumerate(tokens) 
            if i not in used_indices and token['pos'] != 'PUNCT'
        ]
        
        # æ–‡å‹åˆ¥ã®å‰²ã‚Šå½“ã¦
        if pattern == 'SVC':
            elements.extend(self._assign_svc_elements(remaining_tokens))
        elif pattern == 'SVO':
            elements.extend(self._assign_svo_elements(remaining_tokens))
        elif pattern == 'SVOO':
            elements.extend(self._assign_svoo_elements(remaining_tokens))
        elif pattern == 'SVOC':
            elements.extend(self._assign_svoc_elements(remaining_tokens))
        else:  # SV or other
            elements.extend(self._assign_modifiers(remaining_tokens))
        
        return elements
    
    def _assign_svc_elements(self, remaining_tokens: List[Tuple[int, Dict]]) -> List[GrammarElement]:
        """SVCæ–‡å‹ã®è¦ç´ ã‚’å‰²ã‚Šå½“ã¦"""
        elements = []
        complement_assigned = False
        
        for i, token in remaining_tokens:
            if not complement_assigned and self._can_be_complement(token):
                elements.append(GrammarElement(
                    text=token['text'],
                    tokens=[token],
                    role='C1',
                    start_idx=i,
                    end_idx=i,
                    confidence=0.9
                ))
                complement_assigned = True
            else:
                # ä¿®é£¾èªã¨ã—ã¦å‡¦ç†
                elements.append(self._create_modifier_element(i, token))
        
        return elements
    
    def _assign_svo_elements(self, remaining_tokens: List[Tuple[int, Dict]]) -> List[GrammarElement]:
        """SVOæ–‡å‹ã®è¦ç´ ã‚’å‰²ã‚Šå½“ã¦"""
        elements = []
        object_assigned = False
        
        for i, token in remaining_tokens:
            if not object_assigned and self._can_be_object(token):
                elements.append(GrammarElement(
                    text=token['text'],
                    tokens=[token],
                    role='O1',
                    start_idx=i,
                    end_idx=i,
                    confidence=0.9
                ))
                object_assigned = True
            else:
                # ä¿®é£¾èªã¨ã—ã¦å‡¦ç†
                elements.append(self._create_modifier_element(i, token))
        
        return elements
    
    def _assign_svoo_elements(self, remaining_tokens: List[Tuple[int, Dict]]) -> List[GrammarElement]:
        """SVOOæ–‡å‹ã®è¦ç´ ã‚’å‰²ã‚Šå½“ã¦"""
        elements = []
        o1_assigned = False
        o2_assigned = False
        
        for i, token in remaining_tokens:
            if not o1_assigned and self._can_be_object(token):
                elements.append(GrammarElement(
                    text=token['text'],
                    tokens=[token],
                    role='O1',
                    start_idx=i,
                    end_idx=i,
                    confidence=0.85
                ))
                o1_assigned = True
            elif not o2_assigned and self._can_be_object(token):
                elements.append(GrammarElement(
                    text=token['text'],
                    tokens=[token],
                    role='O2',
                    start_idx=i,
                    end_idx=i,
                    confidence=0.85
                ))
                o2_assigned = True
            else:
                # ä¿®é£¾èªã¨ã—ã¦å‡¦ç†
                elements.append(self._create_modifier_element(i, token))
        
        return elements
    
    def _assign_svoc_elements(self, remaining_tokens: List[Tuple[int, Dict]]) -> List[GrammarElement]:
        """SVOCæ–‡å‹ã®è¦ç´ ã‚’å‰²ã‚Šå½“ã¦"""
        elements = []
        object_assigned = False
        complement_assigned = False
        
        for i, token in remaining_tokens:
            if not object_assigned and self._can_be_object(token):
                elements.append(GrammarElement(
                    text=token['text'],
                    tokens=[token],
                    role='O1',
                    start_idx=i,
                    end_idx=i,
                    confidence=0.85
                ))
                object_assigned = True
            elif not complement_assigned and (self._can_be_complement(token) or object_assigned):
                elements.append(GrammarElement(
                    text=token['text'],
                    tokens=[token],
                    role='C1',
                    start_idx=i,
                    end_idx=i,
                    confidence=0.85
                ))
                complement_assigned = True
            else:
                # ä¿®é£¾èªã¨ã—ã¦å‡¦ç†
                elements.append(self._create_modifier_element(i, token))
        
        return elements
    
    def _assign_modifiers(self, remaining_tokens: List[Tuple[int, Dict]]) -> List[GrammarElement]:
        """ä¿®é£¾èªã‚’å‰²ã‚Šå½“ã¦"""
        elements = []
        for i, token in remaining_tokens:
            elements.append(self._create_modifier_element(i, token))
        return elements
    
    def _create_modifier_element(self, idx: int, token: Dict) -> GrammarElement:
        """ä¿®é£¾èªè¦ç´ ã‚’ä½œæˆ"""
        # ä¿®é£¾èªã®ç¨®é¡ã‚’åˆ¤å®š
        if token['pos'] in ['ADV', 'PART']:
            role = 'M1'  # å‰¯è©çš„ä¿®é£¾
        elif token['pos'] in ['ADP'] or token['tag'] in ['IN', 'TO']:
            role = 'M2'  # å‰ç½®è©å¥
        else:
            role = 'M3'  # ãã®ä»–ã®ä¿®é£¾
        
        return GrammarElement(
            text=token['text'],
            tokens=[token],
            role=role,
            start_idx=idx,
            end_idx=idx,
            confidence=0.7
        )
    
    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    def _is_auxiliary_verb(self, token: Dict) -> bool:
        """åŠ©å‹•è©åˆ¤å®š"""
        aux_words = {'be', 'am', 'is', 'are', 'was', 'were', 'being', 'been',
                     'have', 'has', 'had', 'having',
                     'do', 'does', 'did', 'doing',
                     'will', 'would', 'shall', 'should', 'can', 'could',
                     'may', 'might', 'must', 'ought'}
        return token['lemma'].lower() in aux_words or token['tag'] in ['MD']
    
    def _can_be_object(self, token: Dict) -> bool:
        """ç›®çš„èªã«ãªã‚Œã‚‹ã‹ã®åˆ¤å®š"""
        return token['pos'] in ['NOUN', 'PROPN', 'PRON'] or token['tag'] in ['PRP', 'DT']
    
    def _can_be_complement(self, token: Dict) -> bool:
        """è£œèªã«ãªã‚Œã‚‹ã‹ã®åˆ¤å®š"""
        return token['pos'] in ['ADJ', 'NOUN', 'PROPN'] or token['tag'] in ['JJ', 'NN', 'NNS']
    
    def _convert_to_rephrase_format(self, elements: List[GrammarElement], pattern: str) -> Dict[str, Any]:
        """Rephraseã‚¹ãƒ­ãƒƒãƒˆå½¢å¼ã«å¤‰æ›"""
        slots = []
        slot_phrases = []
        slot_display_order = []
        display_order = []
        phrase_types = []
        subslot_ids = []
        
        # ğŸ”§ main_slotsè¾æ›¸å½¢å¼ã‚‚ç”Ÿæˆ
        main_slots = {}
        
        # è¦ç´ ã‚’ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
        elements.sort(key=lambda x: x.start_idx)
        
        role_order = {'S': 1, 'Aux': 2, 'V': 3, 'O1': 4, 'O2': 5, 'C1': 6, 'C2': 7, 'M1': 8, 'M2': 9, 'M3': 10}
        
        for i, element in enumerate(elements):
            # ã‚¹ãƒ­ãƒƒãƒˆåã®èª¿æ•´
            slot_name = element.role
            if slot_name == 'O1':
                slot_name = 'O'  # Rephraseã‚·ã‚¹ãƒ†ãƒ ã®å½¢å¼ã«åˆã‚ã›ã‚‹
            
            slots.append(slot_name)
            slot_phrases.append(element.text)
            
            # ğŸ”§ main_slotsè¾æ›¸ã«è¿½åŠ 
            main_slots[element.role] = element.text
            
            order = role_order.get(element.role, 99)
            slot_display_order.append(order)
            display_order.append(order)
            
            # å“è©ã‚¿ã‚¤ãƒ—ã®åˆ¤å®š
            if element.role in ['S', 'O1', 'O2']:
                phrase_types.append('åè©å¥')
            elif element.role == 'V':
                phrase_types.append('å‹•è©å¥')
            elif element.role in ['C1', 'C2']:
                phrase_types.append('è£œèªå¥')
            else:
                phrase_types.append('ä¿®é£¾å¥')
            
            subslot_ids.append(i)
        
        return {
            'Slot': slots,
            'SlotPhrase': slot_phrases,
            'Slot_display_order': slot_display_order,
            'display_order': display_order,
            'PhraseType': phrase_types,
            'SubslotID': subslot_ids,
            'main_slots': main_slots,  # ğŸ”§ è¾æ›¸å½¢å¼è¿½åŠ 
            'slots': main_slots,       # ğŸ”§ çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ äº’æ›æ€§
            'pattern_detected': pattern,
            'confidence': 0.9,
            'analysis_method': 'dynamic_grammar',
            'lexical_tokens': len([e for e in elements if e.role != 'PUNCT'])
        }
    
    def _create_error_result(self, sentence: str, error: str) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼çµæœã‚’ä½œæˆ"""
        return {
            'Slot': [],
            'SlotPhrase': [],
            'Slot_display_order': [],
            'display_order': [],
            'PhraseType': [],
            'SubslotID': [],
            'main_slots': {},    # ğŸ”§ è¾æ›¸å½¢å¼è¿½åŠ 
            'slots': {},         # ğŸ”§ çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ äº’æ›æ€§
            'error': error,
            'sentence': sentence,
            'analysis_method': 'dynamic_grammar'
        }

# ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°ã¨ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
def run_full_test_suite(test_data_path: str = None) -> Dict[str, Any]:
    """
    53ä¾‹æ–‡ã®å®Œå…¨ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    
    Args:
        test_data_path: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
    Returns:
        Dict: ãƒ†ã‚¹ãƒˆçµæœ
    """
    import json
    import os
    from datetime import datetime
    
    if test_data_path is None:
        test_data_path = os.path.join(
            os.path.dirname(__file__),
            "final_test_system",
            "final_54_test_data.json"
        )
    
    try:
        with open(test_data_path, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
    except FileNotFoundError:
        print(f"âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {test_data_path}")
        return {}
    
    mapper = DynamicGrammarMapper()
    results = {
        "timestamp": datetime.now().isoformat(),
        "test_system": "dynamic_grammar_mapper",
        "total_tests": len(test_data["data"]),
        "successful_tests": 0,
        "failed_tests": 0,
        "test_results": {}
    }
    
    print("=== å‹•çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ  53ä¾‹æ–‡ãƒ†ã‚¹ãƒˆ ===\n")
    
    for test_id, test_case in test_data["data"].items():
        sentence = test_case["sentence"]
        expected = test_case["expected"]
        
        print(f"ãƒ†ã‚¹ãƒˆ {test_id}: {sentence}")
        
        try:
            result = mapper.analyze_sentence(sentence)
            
            if 'error' in result:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
                results["failed_tests"] += 1
                status = "ERROR"
            else:
                print(f"âœ… æ–‡å‹: {result.get('pattern_detected', 'UNKNOWN')}")
                print(f"ğŸ“Š ã‚¹ãƒ­ãƒƒãƒˆ: {result['Slot']}")
                results["successful_tests"] += 1
                status = "SUCCESS"
            
            results["test_results"][test_id] = {
                "sentence": sentence,
                "expected": expected,
                "actual": result,
                "status": status
            }
            
        except Exception as e:
            print(f"âŒ ä¾‹å¤–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            results["failed_tests"] += 1
            results["test_results"][test_id] = {
                "sentence": sentence,
                "expected": expected,
                "actual": {"error": str(e)},
                "status": "EXCEPTION"
            }
        
        print("-" * 60)
    
    # çµæœã‚µãƒãƒªãƒ¼
    success_rate = results["successful_tests"] / results["total_tests"] * 100
    print(f"\n=== ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼ ===")
    print(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {results['total_tests']}")
    print(f"æˆåŠŸ: {results['successful_tests']}")
    print(f"å¤±æ•—: {results['failed_tests']}")
    print(f"æˆåŠŸç‡: {success_rate:.1f}%")
    
    return results

def save_test_results(results: Dict[str, Any], output_path: str = None) -> str:
    """
    ãƒ†ã‚¹ãƒˆçµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    
    Args:
        results: ãƒ†ã‚¹ãƒˆçµæœ
        output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆNone ã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        
    Returns:
        str: ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    import json
    from datetime import datetime
    
    if output_path is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"dynamic_grammar_test_results_{timestamp}.json"
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“ ãƒ†ã‚¹ãƒˆçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
    return output_path

def main():
    """å‹•çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--full-test":
        # 53ä¾‹æ–‡ã®å®Œå…¨ãƒ†ã‚¹ãƒˆ
        results = run_full_test_suite()
        save_test_results(results)
    else:
        # ç°¡æ˜“ãƒ†ã‚¹ãƒˆ
        mapper = DynamicGrammarMapper()
        
        test_sentences = [
            "The car is red.",
            "I love you.",
            "He has finished his homework.",
            "The students study hard for exams.",
            "The teacher explains grammar clearly to confused students daily.",
            "She made him very happy yesterday.",
            "The man who runs fast is strong."
        ]
        
        print("=== å‹•çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ  ç°¡æ˜“ãƒ†ã‚¹ãƒˆ ===\n")
        
        for sentence in test_sentences:
            print(f"ğŸ“ ãƒ†ã‚¹ãƒˆæ–‡: '{sentence}'")
            result = mapper.analyze_sentence(sentence)
            
            if 'error' in result:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
            else:
                print(f"âœ… æ–‡å‹: {result.get('pattern_detected', 'UNKNOWN')}")
                print(f"ğŸ“Š ã‚¹ãƒ­ãƒƒãƒˆ: {result['Slot']}")
                print(f"ğŸ“„ ãƒ•ãƒ¬ãƒ¼ã‚º: {result['SlotPhrase']}")
                print(f"ğŸ¯ ä¿¡é ¼åº¦: {result.get('confidence', 0.0)}")
            
            print("-" * 50)

if __name__ == "__main__":
    main()
