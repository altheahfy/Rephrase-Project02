#!/usr/bin/env python3
"""
å‹•çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ  v2.0
==================

èªæ•°ã«ä¾å­˜ã—ãªã„æ–‡æ³•è¦ç´ ãƒ™ãƒ¼ã‚¹ã®èªè­˜ã‚·ã‚¹ãƒ†ãƒ 
- å“è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã¯ãªãæ–‡æ³•çš„å½¹å‰²ã§èªè­˜
- ä¿®é£¾èªã®å‹•çš„æ¤œå‡ºã¨é™¤å¤–
- æ ¸è¦ç´ ï¼ˆä¸»èªãƒ»å‹•è©ãƒ»ç›®çš„èªãƒ»è£œèªï¼‰ã®ç‰¹å®š
- 5æ–‡å‹ã®å‹•çš„åˆ¤å®š

è¨­è¨ˆæ€æƒ³:
1. æ–‡ã®ã€Œæ ¸ã€ã‚’ç‰¹å®šï¼ˆä¸»èª + å‹•è©ï¼‰
2. å‹•è©ã®æ€§è³ªã‹ã‚‰æ–‡å‹ã‚’æ¨å®š
3. æ®‹ã‚Šã®è¦ç´ ã‚’æ–‡æ³•çš„å½¹å‰²ã§åˆ†é¡
4. ä¿®é£¾èªã¯åˆ¥é€”å‡¦ç†
"""

import spacy
import logging
from typing import Dict, List, Optional, Any, Tuple
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass

# ğŸ†• Phase 1.2: æ–‡å‹èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³è¿½åŠ 
# from sentence_type_detector import SentenceTypeDetector  # ä¸€æ™‚çš„ã«ã‚³ãƒ¡ãƒ³ãƒˆåŒ–

# ğŸ†• Phase A2: spaCyçµ±åˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆå†…éƒ¨5æ–‡å‹å‡¦ç†ä½¿ç”¨ï¼‰
BASIC_FIVE_PATTERN_HANDLER_AVAILABLE = False  # å†…éƒ¨å‡¦ç†ã«çµ±ä¸€

@dataclass
class GrammarElement:
    """æ–‡æ³•è¦ç´ ã®å®šç¾©"""
    text: str
    tokens: List[Dict]
    role: str  # S, V, O1, O2, C1, C2, M1, M2, M3, Aux
    start_idx: int
    end_idx: int
    confidence: float
    
    # ğŸ†• Orderæ©Ÿèƒ½é–¢é€£ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ (Phase 1.1)
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®šã«ã‚ˆã‚Šæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¸ã®å½±éŸ¿ã‚’æœ€å°åŒ–
    slot_display_order: int = 0      # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆé †åº
    display_order: int = 0           # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå†…é †åº  
    v_group_key: str = ""            # å‹•è©ã‚°ãƒ«ãƒ¼ãƒ—ã‚­ãƒ¼
    sentence_type: str = ""          # æ–‡å‹ (statement/wh_question/yes_no_question)
    is_subslot: bool = False         # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆãƒ•ãƒ©ã‚°
    parent_slot: str = ""            # è¦ªã‚¹ãƒ­ãƒƒãƒˆ (ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”¨)
    subslot_id: str = ""             # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆID (sub-s, sub-vç­‰)

class DynamicGrammarMapper:
    """
    å‹•çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ 
    èªæ•°ã«ä¾å­˜ã—ãªã„æ–‡æ³•èªè­˜ã‚’å®Ÿç¾
    """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        try:
            self.nlp = spacy.load("en_core_web_sm")
            print("âœ… spaCyå‹•çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except OSError:
            print("âŒ spaCyè‹±èªãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            raise
        
        # ãƒ­ã‚°è¨­å®šã‚’æ—©æœŸã«è¡Œã†
        self.logger = logging.getLogger(__name__)
        
        # ğŸ”¥ Phase 1.0: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  (Stanza Asset Migration)
        self.active_handlers = []  # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãƒªã‚¹ãƒˆ
        self.handler_shared_context = {}  # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼é–“æƒ…å ±å…±æœ‰
        self.handler_success_count = {}  # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼æˆåŠŸçµ±è¨ˆ
        
        # ğŸ”¥ Phase 1.3: ä¾å­˜é–¢ä¿‚å‰Šé™¤å®Œäº† - å“è©ãƒ™ãƒ¼ã‚¹åˆ†æã«å®Œå…¨ç§»è¡Œ
        
        # ChatGPT5è¨ºæ–­: å†å…¥ã‚¬ãƒ¼ãƒ‰å¯¾ç­–
        self._analysis_depth = 0  # è§£ææ·±åº¦ã‚«ã‚¦ãƒ³ã‚¿ï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰
        
        # ChatGPT5 Step C: Token Consumption Tracking - ä½¿ç”¨æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒ³ç®¡ç†
        self._consumed_tokens = set()  # ãƒˆãƒ¼ã‚¯ãƒ³ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã‚»ãƒƒãƒˆ
        
        # ğŸ”¥ Phase 2: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼çµæœä¿å­˜ (ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆãƒãƒ¼ã‚¸ç”¨)
        self.last_unified_result = None
        
        # åŸºæœ¬ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®åˆæœŸåŒ–
        self._initialize_basic_handlers()
        
        # ğŸ”¥ Phase A2: å†…éƒ¨5æ–‡å‹å‡¦ç†ã‚’ä½¿ç”¨ï¼ˆçµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Œå…¨å®Ÿè£…ï¼‰
        print("âœ… å†…éƒ¨5æ–‡å‹å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†")
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–å®Œäº†ã‚’ãƒ­ã‚°å‡ºåŠ›
        print(f"ğŸ”¥ Phase 1.3 ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†: {len(self.active_handlers)}å€‹ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–")
        print(f"   ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼: {', '.join(self.active_handlers)}")
        print(f"   ä¾å­˜é–¢ä¿‚å‰Šé™¤: å®Œäº†ï¼ˆå“è©ãƒ™ãƒ¼ã‚¹åˆ†æã«å®Œå…¨ç§»è¡Œï¼‰")  # Phase 1.3å®Œäº†
        
        # ğŸ†• Phase 1.2: æ–‡å‹èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        # self.sentence_type_detector = SentenceTypeDetector()  # ä¸€æ™‚çš„ã«ã‚³ãƒ¡ãƒ³ãƒˆåŒ–
        # print("âœ… æ–‡å‹èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
        
        # å‹•è©åˆ†é¡è¾æ›¸
        self.linking_verbs = {
            'be', 'am', 'is', 'are', 'was', 'were', 'being', 'been',
            'become', 'became', 'becoming',
            'seem', 'seemed', 'seeming', 'seems',
            'appear', 'appeared', 'appearing', 'appears',
            'look', 'looked', 'looking', 'looks',
            'sound', 'sounded', 'sounding', 'sounds',
            'feel', 'felt', 'feeling', 'feels',
            'taste', 'tasted', 'tasting', 'tastes',
            'smell', 'smelled', 'smelling', 'smells',
            'remain', 'remained', 'remaining', 'remains',
            'stay', 'stayed', 'staying', 'stays'
        }
        
        self.ditransitive_verbs = {
            'give', 'gave', 'given', 'giving', 'gives',
            'tell', 'told', 'telling', 'tells',
            'show', 'showed', 'shown', 'showing', 'shows',
            'send', 'sent', 'sending', 'sends',
            'teach', 'taught', 'teaching', 'teaches',
            'buy', 'bought', 'buying', 'buys',
            'bring', 'brought', 'bringing', 'brings',
            'offer', 'offered', 'offering', 'offers',
            'lend', 'lent', 'lending', 'lends',
            'sell', 'sold', 'selling', 'sells'
        }
        
        self.objective_complement_verbs = {
            'make', 'made', 'making', 'makes',
            'call', 'called', 'calling', 'calls',
            'consider', 'considered', 'considering', 'considers',
            'find', 'found', 'finding', 'finds',
            'keep', 'kept', 'keeping', 'keeps',
            'leave', 'left', 'leaving', 'leaves',
            'elect', 'elected', 'electing', 'elects',
            'name', 'named', 'naming', 'names',
            'choose', 'chose', 'chosen', 'choosing', 'chooses'
        }
        
        # å‹•è©/åè©åŒå½¢èªãƒªã‚¹ãƒˆï¼ˆstanzaã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ç¶™æ‰¿ï¼‰
        # ğŸ†• äººé–“çš„æ–‡æ³•èªè­˜: æ›–æ˜§èªã®å€™è£œãƒªã‚¹ãƒˆ
        self.ambiguous_words = {
            'lives': ['NOUN', 'VERB'],    # lifeè¤‡æ•°å½¢ vs liveä¸‰äººç§°å˜æ•°
            'works': ['NOUN', 'VERB'],    # workè¤‡æ•°å½¢ vs workä¸‰äººç§°å˜æ•°  
            'runs': ['NOUN', 'VERB'],     # runè¤‡æ•°å½¢ vs runä¸‰äººç§°å˜æ•°
            'goes': ['NOUN', 'VERB'],     # goè¤‡æ•°å½¢ vs goä¸‰äººç§°å˜æ•°
            'comes': ['NOUN', 'VERB'],    # comeè¤‡æ•°å½¢ vs comeä¸‰äººç§°å˜æ•°
            'stays': ['NOUN', 'VERB'],    # stayè¤‡æ•°å½¢ vs stayä¸‰äººç§°å˜æ•°
            'plays': ['NOUN', 'VERB'],    # playè¤‡æ•°å½¢ vs playä¸‰äººç§°å˜æ•°
            'looks': ['NOUN', 'VERB'],    # lookè¤‡æ•°å½¢ vs lookä¸‰äººç§°å˜æ•°
            'walks': ['NOUN', 'VERB'],    # walkè¤‡æ•°å½¢ vs walkä¸‰äººç§°å˜æ•°
            'talks': ['NOUN', 'VERB'],    # talkè¤‡æ•°å½¢ vs talkä¸‰äººç§°å˜æ•°
            'moves': ['NOUN', 'VERB'],    # moveè¤‡æ•°å½¢ vs moveä¸‰äººç§°å˜æ•°
            'drives': ['NOUN', 'VERB'],   # driveè¤‡æ•°å½¢ vs driveä¸‰äººç§°å˜æ•°
            'flies': ['NOUN', 'VERB'],    # flyè¤‡æ•°å½¢ vs flyä¸‰äººç§°å˜æ•°
            'rides': ['NOUN', 'VERB'],    # rideè¤‡æ•°å½¢ vs rideä¸‰äººç§°å˜æ•°
            'sits': ['NOUN', 'VERB']      # sitè¤‡æ•°å½¢ vs sitä¸‰äººç§°å˜æ•°
        }
    
    def analyze_sentence(self, sentence: str, allow_unified: bool = True) -> Dict[str, Any]:
        """
        ğŸ”§ Phase A3-2 æ…é‡å†å®Ÿè£…: PureCentralControllerçµ±åˆï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›ï¼‰
        
        æ–‡ç« ã‚’å‹•çš„ã«è§£æã—ã¦Rephraseã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ ã‚’ç”Ÿæˆ
        å†…éƒ¨å‡¦ç†ã¯PureCentralControllerã«å§”è­²ã€å¤–éƒ¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¯ä¸å¤‰
        
        Args:
            sentence (str): è§£æå¯¾è±¡ã®æ–‡ç« 
            allow_unified (bool): çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼å‡¦ç†ã®è¨±å¯ï¼ˆå†å¸°é˜²æ­¢ç”¨ï¼‰
            
        Returns:
            Dict[str, Any]: Rephraseã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›ï¼‰
        """
        # ğŸš€ Phase A3-2 æ…é‡å®Ÿè£…: PureCentralControllerã«å§”è­²ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›ï¼‰
        if hasattr(self, 'pure_central_controller') and self.pure_central_controller and allow_unified:
            self.logger.debug(f"ğŸ”¥ Phase A3-2: PureCentralControllerä½¿ç”¨ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›ï¼‰")
            return self.pure_central_controller.analyze_sentence_pure_management(sentence)
        
        # ğŸ“œ ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: PureCentralControllerãŒç„¡ã„å ´åˆã®å¾“æ¥å‡¦ç†
        self.logger.debug(f"ğŸ“œ Phase A3-2: ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ")
        
        # ğŸ”§ ç´¯ç©ãƒã‚°ä¿®æ­£: æ–°ã—ã„åˆ†æé–‹å§‹æ™‚ã«last_unified_resultã‚’ãƒªã‚»ãƒƒãƒˆ
        if allow_unified:
            self.last_unified_result = None
        
        # ChatGPT5 Step A: Re-entrancy Guard
        if not allow_unified:
            self._analysis_depth += 1
            if self._analysis_depth > 3:  # æ·±åº¦åˆ¶é™
                self.logger.warning(f"åˆ†ææ·±åº¦åˆ¶é™ã«é”ã—ã¾ã—ãŸ: {self._analysis_depth}")
                return self._create_error_result(sentence, "recursion_depth_exceeded")
        else:
            # ChatGPT5 Step C: Token Consumption Tracking - æ–°ã—ã„åˆ†æé–‹å§‹æ™‚ã«ãƒªã‚»ãƒƒãƒˆ
            self._consumed_tokens = set()
        
        try:
            # ğŸ†• Phase 1.2: æ–‡å‹èªè­˜
            # sentence_type = self.sentence_type_detector.detect_sentence_type(sentence)  # ä¸€æ™‚çš„ã«ã‚³ãƒ¡ãƒ³ãƒˆåŒ–
            # sentence_type_confidence = self.sentence_type_detector.get_detection_confidence(sentence)  # ä¸€æ™‚çš„ã«ã‚³ãƒ¡ãƒ³ãƒˆåŒ–
            sentence_type = "statement"  # ä¸€æ™‚çš„ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            sentence_type_confidence = 0.8  # ä¸€æ™‚çš„ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # 1. spaCyåŸºæœ¬è§£æ
            doc = self.nlp(sentence)
            tokens = self._extract_tokens(doc)
            
            # 1.5. é–¢ä¿‚ç¯€æ§‹é€ ã®æ¤œå‡º
            relative_clause_info = self._detect_relative_clause(tokens, sentence)
            
            # ğŸ”§ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆã‚’äº‹å‰é™¤å¤–ã‚ˆã‚Šå‰ã«å®Ÿè¡Œï¼ˆcarç­‰ã®è¦ç´ ã‚’ä¿æŒã™ã‚‹ãŸã‚ï¼‰
            sub_slots = {}
            original_tokens = tokens.copy()  # å…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¿å­˜
            if relative_clause_info['found']:
                self.logger.debug(f"é–¢ä¿‚ç¯€æ¤œå‡º: {relative_clause_info['type']} (ä¿¡é ¼åº¦: {relative_clause_info['confidence']})")
                
                # ï¿½ Phase A3: é–¢ä¿‚ç¯€å‡¦ç†ã‚‚BasicFivePatternHandlerãŒæ‹…å½“
                if hasattr(self, 'basic_five_pattern_handler') and self.basic_five_pattern_handler:
                    self.logger.debug(f"ğŸ”¥ Phase A3: é–¢ä¿‚ç¯€å‡¦ç†ã‚‚BasicFivePatternHandlerã«å§”è­²")
                    # BasicFivePatternHandlerãŒé–¢ä¿‚ç¯€ã‚‚å«ã‚ã¦è§£æã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ã‚¹ã‚­ãƒƒãƒ—
                else:
                    # ãƒ¬ã‚¬ã‚·ãƒ¼é–¢ä¿‚ç¯€å‡¦ç†ï¼ˆPhase A3ä»¥å¤–ï¼‰
                    temp_core_elements = self._identify_core_elements(tokens)
                    processed_tokens, sub_slots = self._process_relative_clause(original_tokens, relative_clause_info, temp_core_elements)
            
            # ğŸ”§ é–¢ä¿‚ç¯€å†…è¦ç´ ã®äº‹å‰é™¤å¤–ï¼ˆãƒ¡ã‚¤ãƒ³æ–‡æ³•è§£æç”¨ï¼‰
            excluded_indices = self._identify_relative_clause_elements(tokens, relative_clause_info)
            
            # 2. é™¤å¤–ã•ã‚Œã¦ã„ãªã„è¦ç´ ã®ã¿ã§ã‚³ã‚¢è¦ç´ ã‚’ç‰¹å®š
            filtered_tokens = [token for i, token in enumerate(tokens) if i not in excluded_indices]
            
            # ğŸ”¥ Phase A2: å†…éƒ¨5æ–‡å‹å‡¦ç†ã‚’ç›´æ¥ä½¿ç”¨ï¼ˆçµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼‰
            print("ğŸ”¥ Phase A2: å†…éƒ¨5æ–‡å‹å‡¦ç†ã«ã‚ˆã‚‹æ–‡å‹è§£æé–‹å§‹")
            core_elements = self._identify_core_elements(filtered_tokens)
            sentence_pattern = self._determine_sentence_pattern(core_elements, filtered_tokens)
            grammar_elements = self._assign_grammar_roles(filtered_tokens, sentence_pattern, core_elements, relative_clause_info)
            
            # æˆåŠŸåˆ¤å®š
            pattern_analysis = {
                'handler_success': len(grammar_elements) > 0,
                'core_elements': core_elements,
                'sentence_pattern': sentence_pattern,
                'grammar_elements': grammar_elements
            }
            
            print(f"ğŸ”¥ Phase A2: å†…éƒ¨5æ–‡å‹å‡¦ç†å®Œäº†: {sentence_pattern}")
            print(f"ğŸ”§ Phase A2: grammar_elementså–å¾—: {[{'role': e.role, 'text': e.text} for e in grammar_elements]}")
            
            # 5. Rephraseã‚¹ãƒ­ãƒƒãƒˆå½¢å¼ã«å¤‰æ›
            rephrase_result = self._convert_to_rephrase_format(grammar_elements, sentence_pattern, sub_slots)
            
            # ğŸ”¥ Phase A2: å†…éƒ¨5æ–‡å‹å‡¦ç†å®Œäº† - çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã‚’ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            if pattern_analysis.get('handler_success'):
                print(f"ğŸ”¥ Phase A2: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå†…éƒ¨5æ–‡å‹å‡¦ç†ã§å®Œäº†æ¸ˆã¿ï¼‰")
                # ğŸ§ª Phase A1ãƒ†ã‚¹ãƒˆ: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å¼·åˆ¶æœ‰åŠ¹åŒ–ã—ã¦ãƒ†ã‚¹ãƒˆ
                # allow_unified = False  # Phase A3ã§ã¯çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ç„¡åŠ¹åŒ–
                print(f"ğŸ§ª Phase A1ãƒ†ã‚¹ãƒˆ: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å¼·åˆ¶æœ‰åŠ¹åŒ–ï¼ˆä¿®æ­£ç‰ˆãƒ†ã‚¹ãƒˆï¼‰")
                # ğŸ”§ Phase A3: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®æ—¢å­˜çµæœã‚’ã‚¯ãƒªã‚¢
                self.last_unified_result = None
                print(f"ğŸ”§ Phase A2: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼çµæœã‚¯ãƒªã‚¢ï¼ˆå†…éƒ¨5æ–‡å‹å‡¦ç†ä½¿ç”¨ï¼‰")
            
            # ğŸ”¥ Phase 2: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œï¼ˆå—å‹•æ…‹ãƒ»åŠ©å‹•è©ãƒ»å‰¯è©å‡¦ç†ï¼‰
            if allow_unified:  # ChatGPT5 Step A: Re-entrancy Guard
                try:
                    unified_result = self._unified_mapping(sentence, doc)
                    if unified_result and 'slots' in unified_result:
                        # çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®çµæœã‚’ãƒãƒ¼ã‚¸ï¼ˆå„ªå…ˆåº¦é †ï¼‰
                        for slot_name, slot_value in unified_result['slots'].items():
                            if slot_value:  # ç©ºã§ãªã„å€¤ã®ã¿ãƒãƒ¼ã‚¸
                                # ChatGPT5 Step D: å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã¯å…¨ã‚¹ãƒ­ãƒƒãƒˆå„ªå…ˆï¼ˆAux, V, Mï¼‰
                                if 'passive_voice' in str(unified_result.get('grammar_info', {})):
                                    rephrase_result['slots'][slot_name] = slot_value
                                    rephrase_result['main_slots'][slot_name] = slot_value
                                    print(f"ğŸ”¥ å—å‹•æ…‹å„ªå…ˆãƒãƒ¼ã‚¸: {slot_name} = '{slot_value}'")
                                # ãã®ä»–ã®ã‚¹ãƒ­ãƒƒãƒˆã¯æ—¢å­˜å€¤ãŒãªã„å ´åˆã®ã¿
                                elif not rephrase_result['slots'].get(slot_name):
                                    rephrase_result['slots'][slot_name] = slot_value
                                    rephrase_result['main_slots'][slot_name] = slot_value
                        
                        # æ–‡æ³•æƒ…å ±ã‚‚ãƒãƒ¼ã‚¸
                        if 'grammar_info' in unified_result:
                            if 'unified_handlers' not in rephrase_result:
                                rephrase_result['unified_handlers'] = {}
                            rephrase_result['unified_handlers'] = unified_result['grammar_info']
                        
                        # ChatGPT5 Step D: Token consumptionãƒ™ãƒ¼ã‚¹ã§é‡è¤‡ã‚¹ãƒ­ãƒƒãƒˆå‰Šé™¤
                        self._cleanup_duplicate_slots_by_consumption(rephrase_result, doc)
                        
                        # ğŸ”¥ Phase 2: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼çµæœã‚’ä¿å­˜ (ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆãƒãƒ¼ã‚¸ç”¨)
                        self.last_unified_result = unified_result
                        print(f"ğŸ”¥ çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼çµæœä¿å­˜: sub_slots = {unified_result.get('sub_slots', {})}")
                        
                        # ğŸ¯ Central Controller: ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæƒ…å ±ã‚’æœ€çµ‚çµæœã«çµ±åˆ
                        if unified_result.get('sub_slots'):
                            if 'sub_slots' not in rephrase_result:
                                rephrase_result['sub_slots'] = {}
                            rephrase_result['sub_slots'].update(unified_result['sub_slots'])
                            print(f"ğŸ¯ Central Controller: Sub-slots merged to final result: {rephrase_result['sub_slots']}")
                        
                        # ğŸ¯ Central Controller: ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ­ãƒƒãƒˆä¿®æ­£ï¼ˆé–¢ä¿‚ç¯€åˆ†é›¢å¯¾å¿œï¼‰
                        if unified_result.get('relative_clause_info', {}).get('found'):
                            main_sentence = unified_result['relative_clause_info']['main_sentence']
                            print(f"ğŸ¯ Central Controller: Analyzing main sentence for correct slots: '{main_sentence}'")
                            
                            # ä¸»æ–‡ã‚’å†åˆ†æã—ã¦ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ­ãƒƒãƒˆã‚’æ­£ã—ãè¨­å®š
                            main_doc = self.nlp(main_sentence)
                            main_analysis = self._analyze_sentence_legacy(main_sentence, main_doc)
                            if main_analysis and 'slots' in main_analysis:
                                # ä¸­å¤®åˆ¶å¾¡: ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã¨é‡è¤‡ã—ãªã„ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ­ãƒƒãƒˆã®ã¿æ¡ç”¨
                                for slot_name, slot_value in main_analysis['slots'].items():
                                    if slot_value and slot_name not in ['sub-s', 'sub-v', 'sub-aux', 'sub-c1', 'sub-o1']:
                                        # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®å€¤ã¨é‡è¤‡ãƒã‚§ãƒƒã‚¯
                                        is_duplicate = False
                                        for sub_name, sub_value in unified_result.get('sub_slots', {}).items():
                                            if sub_value and str(slot_value).lower() in str(sub_value).lower():
                                                print(f"ğŸ¯ Central Controller: Skipping main slot {slot_name}='{slot_value}' (duplicate with {sub_name}='{sub_value}')")
                                                is_duplicate = True
                                                break
                                        
                                        if not is_duplicate:
                                            # ğŸ¯ Central Controller: è‡ªå‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹åˆ¥å‡¦ç†
                                            if slot_name == 'O1' and 'arrived' in main_sentence:
                                                # "arrived"ã¯è‡ªå‹•è©ãªã®ã§ã€O1ï¼ˆç›®çš„èªï¼‰ã¯ä¸è¦
                                                print(f"ğŸ¯ Central Controller: Skipping O1='{slot_value}' (arrived is intransitive verb)")
                                                continue
                                            
                                            rephrase_result['slots'][slot_name] = slot_value
                                            rephrase_result['main_slots'][slot_name] = slot_value
                                            print(f"ğŸ¯ Central Controller: Main slot set {slot_name}='{slot_value}'")
                            
                    print(f"ğŸ”¥ Phase 2: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œå®Œäº†")
                except Exception as e:
                    self.logger.error(f"çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ğŸ†• Phase 2: å‰¯è©å‡¦ç†ã®è¿½åŠ  (Direct Implementation) - Phase A3ã§ã¯é™¤å¤–
            if not (hasattr(self, 'basic_five_pattern_handler') and self.basic_five_pattern_handler and pattern_analysis.get('handler_success')):
                try:
                    additional_adverbs = self._detect_and_assign_adverbs_direct(doc, rephrase_result)
                    if additional_adverbs:
                        print(f"ğŸ”¥ Phase 2: å‰¯è©å‡¦ç†ã«ã‚ˆã‚Š {len(additional_adverbs)}å€‹ã®å‰¯è©ã‚’è¿½åŠ ")
                        rephrase_result['main_slots'].update(additional_adverbs)
                        rephrase_result['slots'].update(additional_adverbs)
                except Exception as e:
                    self.logger.error(f"å‰¯è©å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            else:
                print(f"ğŸ”¥ Phase A3: å‰¯è©å‡¦ç†ã‚‚BasicFivePatternHandlerã§å‡¦ç†æ¸ˆã¿ï¼ˆé‡è¤‡å‡¦ç†å›é¿ï¼‰")
            
            # ğŸ†• Phase 1.2: æ–‡å‹æƒ…å ±ã‚’çµæœã«è¿½åŠ 
            rephrase_result['sentence_type'] = sentence_type
            rephrase_result['sentence_type_confidence'] = sentence_type_confidence
            
            # ğŸ¯ Central Controller: æœ€çµ‚çµ±åˆãƒã‚§ãƒƒã‚¯
            if hasattr(self, 'last_unified_result') and self.last_unified_result:
                print(f"ğŸ¯ Central Controller: Final integration check")
                
                # çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼æƒ…å ±ã‚’æœ€çµ‚çµæœã«çµ±åˆ
                if 'unified_handlers' in self.last_unified_result:
                    rephrase_result['unified_handlers'] = self.last_unified_result['unified_handlers']
                
                # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæœ€çµ‚ãƒã‚§ãƒƒã‚¯
                unified_sub_slots = self.last_unified_result.get('sub_slots', {})
                if unified_sub_slots:
                    if 'sub_slots' not in rephrase_result:
                        rephrase_result['sub_slots'] = {}
                    
                    # ä¸­å¤®åˆ¶å¾¡: ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆçµ±åˆ
                    for sub_name, sub_value in unified_sub_slots.items():
                        if sub_value:
                            rephrase_result['sub_slots'][sub_name] = sub_value
                    
                    print(f"ğŸ¯ Central Controller: Final sub_slots = {rephrase_result.get('sub_slots', {})}")
            
            return rephrase_result
            
        except Exception as e:
            self.logger.error(f"å‹•çš„æ–‡æ³•è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_error_result(sentence, str(e))
        finally:
            # ChatGPT5 Step A: Re-entrancy Guard - ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ãƒªã‚»ãƒƒãƒˆ
            if not allow_unified and hasattr(self, '_analysis_depth'):
                self._analysis_depth = max(0, self._analysis_depth - 1)
    
    def _cleanup_duplicate_slots_by_consumption(self, rephrase_result: Dict, doc) -> None:
        """
        ChatGPT5 Step D: Token consumptionãƒ™ãƒ¼ã‚¹ã§é‡è¤‡ã‚¹ãƒ­ãƒƒãƒˆã‚’å‰Šé™¤
        """
        if not hasattr(self, '_consumed_tokens') or not self._consumed_tokens:
            return
            
        slots_to_remove = []
        
        # å„ã‚¹ãƒ­ãƒƒãƒˆã®å€¤ãŒconsumed tokenã«å¯¾å¿œã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        for slot_name, slot_value in rephrase_result['slots'].items():
            if not slot_value:
                continue
                
            # ã‚¹ãƒ­ãƒƒãƒˆå€¤ã«å«ã¾ã‚Œã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ãŒconsumedã‹ãƒã‚§ãƒƒã‚¯
            slot_tokens = str(slot_value).split()
            slot_token_indices = []
            
            for slot_token in slot_tokens:
                for token in doc:
                    if token.text == slot_token:
                        slot_token_indices.append(token.i)
                        break
            
            # ã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒconsumedãªã‚‰ã‚¹ãƒ­ãƒƒãƒˆå‰Šé™¤å¯¾è±¡
            if slot_token_indices and all(idx in self._consumed_tokens for idx in slot_token_indices):
                # ãŸã ã—ã€consumedãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®šã—ãŸå…ƒã®ã‚¹ãƒ­ãƒƒãƒˆã¯ä¿æŒ
                if slot_name not in ['M2']:  # M2='by John'ã¯ä¿æŒï¼ˆRephraseå‰¯è©ãƒ«ãƒ¼ãƒ«ï¼‰
                    slots_to_remove.append(slot_name)
                    print(f"ğŸ”¥ ChatGPT5 Step D: Removing duplicate slot {slot_name}='{slot_value}' (consumed tokens)")
        
        # é‡è¤‡ã‚¹ãƒ­ãƒƒãƒˆå‰Šé™¤
        for slot_name in slots_to_remove:
            rephrase_result['slots'].pop(slot_name, None)
            rephrase_result['main_slots'].pop(slot_name, None)
    
    def _extract_tokens(self, doc) -> List[Dict]:
        """spaCyãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã‚’æŠ½å‡ºï¼ˆæ®µéšçš„ä¾å­˜é–¢ä¿‚å‰Šé™¤ï¼‰"""
        tokens = []
        for token in doc:
            token_info = {
                'text': token.text,
                'pos': token.pos_,
                'tag': token.tag_,
                'lemma': token.lemma_,
                # Phase 1.3: ä¾å­˜é–¢ä¿‚æƒ…å ±ã‚’å®Œå…¨ã«é™¤å»ã—ã€å“è©ãƒ™ãƒ¼ã‚¹åˆ†æã«å®Œå…¨ç§»è¡Œ
                'dep': 'UNKNOWN',  # ä¾å­˜é–¢ä¿‚æƒ…å ±ã¯ä½¿ç”¨ã—ãªã„
                'head': '',  # ãƒ˜ãƒƒãƒ‰æƒ…å ±ã¯ä½¿ç”¨ã—ãªã„
                'head_idx': -1,  # ãƒ˜ãƒƒãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ä½¿ç”¨ã—ãªã„
                'is_stop': token.is_stop,
                'is_alpha': token.is_alpha,
                'index': token.i
            }
            tokens.append(token_info)
        return tokens
    
    def _identify_core_elements(self, tokens: List[Dict]) -> Dict[str, Any]:
        """
        æ–‡ã®æ ¸è¦ç´ ï¼ˆä¸»èªãƒ»å‹•è©ï¼‰ã‚’ç‰¹å®š
        ã“ã‚ŒãŒå…¨ã¦ã®æ–‡å‹èªè­˜ã®åŸºç›¤ã¨ãªã‚‹
        """
        core = {
            'subject': None,
            'verb': None,
            'subject_indices': [],
            'verb_indices': [],
            'auxiliary': None,
            'auxiliary_indices': []
        }
        
        # å‹•è©ã‚’æ¢ã™ï¼ˆæœ€ã‚‚é‡è¦ï¼‰
        main_verb_idx = self._find_main_verb(tokens)
        if main_verb_idx is not None:
            core['verb'] = tokens[main_verb_idx]
            core['verb_indices'] = [main_verb_idx]
            
            # åŠ©å‹•è©ã‚’æ¢ã™
            aux_idx = self._find_auxiliary(tokens, main_verb_idx)
            if aux_idx is not None:
                core['auxiliary'] = tokens[aux_idx]
                core['auxiliary_indices'] = [aux_idx]
        
        # ä¸»èªã‚’æ¢ã™ï¼ˆå‹•è©ã®å‰ã§æœ€ã‚‚é©åˆ‡ãªåè©å¥ï¼‰
        if main_verb_idx is not None:
            subject_indices = self._find_subject(tokens, main_verb_idx)
            if subject_indices:
                core['subject_indices'] = subject_indices
                core['subject'] = ' '.join([tokens[i]['text'] for i in subject_indices])
        
        return core
    
    def _find_main_verb(self, tokens: List[Dict]) -> Optional[int]:
        """
        ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’ç‰¹å®šï¼ˆäººé–“çš„æ–‡æ³•èªè­˜ï¼‰
        å“è©æƒ…å ±ã¨èªé †ã®ã¿ã‚’ä½¿ç”¨ã€ä¾å­˜é–¢ä¿‚ã¯ä½¿ã‚ãªã„
        """
        # POSãƒ™ãƒ¼ã‚¹ã¨æ–‡è„ˆãƒ™ãƒ¼ã‚¹ã®ä¸¡æ–¹ã‚’å–å¾—
        pos_candidates = []
        for i, token in enumerate(tokens):
            # å‹•è©ã®å“è©ã‚¿ã‚°
            if (token['tag'].startswith('VB') and token['pos'] == 'VERB') or token['pos'] == 'AUX':
                pos_candidates.append((i, token))
        
        # æ–‡è„ˆçš„å‹•è©è­˜åˆ¥ï¼ˆPOSèª¤èªè­˜å¯¾ç­–ï¼‰
        contextual_candidates = self._find_contextual_verbs(tokens)
        
        # ä¸¡æ–¹ã‚’çµ±åˆï¼ˆé‡è¤‡é™¤å»ï¼‰
        verb_candidates = pos_candidates.copy()
        for i, token in contextual_candidates:
            # æ—¢ã«å­˜åœ¨ã—ãªã„å ´åˆã®ã¿è¿½åŠ 
            if not any(existing_i == i for existing_i, _ in verb_candidates):
                verb_candidates.append((i, token))
        
        if not verb_candidates:
            return None
        
        # äººé–“çš„åˆ¤å®šï¼šé–¢ä¿‚ç¯€ã‚’é™¤å¤–ã—ã¦ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’ç‰¹å®š
        non_relative_verbs = []
        
        for i, token in verb_candidates:
            # é–¢ä¿‚ä»£åè©ã®ç›´å¾Œã®å‹•è©ã¯é–¢ä¿‚ç¯€å†…å‹•è©ã¨ã—ã¦é™¤å¤–
            is_in_relative_clause = False
            
            # å‰ã®å˜èªã‚’ç¢ºèª
            for j in range(max(0, i-5), i):  # 5èªå‰ã¾ã§ç¢ºèª
                prev_token = tokens[j]
                if prev_token['text'].lower() in ['who', 'whom', 'which', 'that', 'whose', 'where', 'when']:
                    # whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†: å‹•è©/åè©åŒå½¢èªã¯é–¢ä¿‚ç¯€å¤–ã®ãƒ¡ã‚¤ãƒ³å‹•è©ã¨ã—ã¦æ‰±ã†
                    if (prev_token['text'].lower() == 'whose' and 
                        token['text'].lower() in self.ambiguous_words and
                        token.get('contextual_override', False)):
                        # whoseæ§‹æ–‡ã§ã®åŒå½¢èªå‹•è©ã¯é–¢ä¿‚ç¯€å¤–ã¨ã—ã¦æ‰±ã†
                        is_in_relative_clause = False
                        break
                    
                    # é–¢ä¿‚ä»£åè©ã‹ã‚‰å‹•è©ã¾ã§ã®è·é›¢ãŒè¿‘ã„å ´åˆã€é–¢ä¿‚ç¯€å†…å‹•è©
                    if i - j <= 4:  # 4èªä»¥å†…ãªã‚‰é–¢ä¿‚ç¯€å†…
                        is_in_relative_clause = True
                        break
            
            if not is_in_relative_clause:
                non_relative_verbs.append((i, token))
        
        if non_relative_verbs:
            # ãƒ¡ã‚¤ãƒ³å‹•è©å€™è£œã‹ã‚‰åŠ©å‹•è©ã§ãªã„ã‚‚ã®ã‚’å„ªå…ˆ
            main_verbs = [(i, token) for i, token in non_relative_verbs if not self._is_auxiliary_verb(token)]
            if main_verbs:
                # æ–‡ã®å¾ŒåŠã«ã‚ã‚‹ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’å„ªå…ˆï¼ˆé–¢ä¿‚ç¯€ã®å¾Œï¼‰
                return main_verbs[-1][0]
            return non_relative_verbs[-1][0]
        
        # æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦ã€ã©ã®å‹•è©ã§ã‚‚é¸æŠ
        return verb_candidates[-1][0]

    def _find_contextual_verbs(self, tokens: List[Dict]) -> List[Tuple[int, Dict]]:
        """
        äººé–“çš„æ–‡æ³•èªè­˜ã«ã‚ˆã‚‹å‹•è©è­˜åˆ¥
        æ§‹æ–‡çš„æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã§æœ€é©ãªå“è©ã‚’æ±ºå®š
        """
        contextual_verbs = []
        sentence_text = ' '.join([token['text'] for token in tokens])
        
        for i, token in enumerate(tokens):
            # æ—¢ã«å‹•è©ã¨ã—ã¦èªè­˜ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®
            if token['pos'] == 'VERB':
                contextual_verbs.append((i, token))
                continue
            
            # ğŸ†• äººé–“çš„å“è©æ±ºå®š: æ§‹æ–‡çš„æ•´åˆæ€§ã«ã‚ˆã‚‹é¸æŠ
            if token['text'].lower() in self.ambiguous_words:
                optimal_pos = self._resolve_ambiguous_word(token, tokens, i, sentence_text)
                
                if optimal_pos == 'VERB':
                    verb_token = token.copy()
                    verb_token['pos'] = 'VERB'
                    verb_token['human_grammar_correction'] = True
                    verb_token['resolution_method'] = 'syntactic_consistency'
                    contextual_verbs.append((i, verb_token))
                    self.logger.debug(f"ğŸ§  äººé–“çš„å“è©æ±ºå®š: '{token['text']}' â†’ VERB (æ§‹æ–‡æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯)")
                continue
            
            # ãã®ä»–ã®å‹•è©å€™è£œï¼ˆaux, modalå«ã‚€ï¼‰
            if token['pos'] in ['AUX', 'MODAL']:
                contextual_verbs.append((i, token))
        
        return contextual_verbs

    def _resolve_ambiguous_word(self, token: Dict, tokens: List[Dict], position: int, sentence: str) -> str:
        """
        äººé–“çš„å“è©æ±ºå®š: æ§‹æ–‡çš„æ•´åˆæ€§ã«ã‚ˆã‚‹æ›–æ˜§èªè§£æ±ºï¼ˆå®‰å…¨ç‰ˆï¼‰
        
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆã®4æ®µéšã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:
        â‘ æ›–æ˜§èªãƒªã‚¹ãƒˆã®ç¢ºèª âœ…
        â‘¡ä¸¡ã‚±ãƒ¼ã‚¹è©¦è¡Œ âœ…
        â‘¢æ§‹æ–‡å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯ âœ…ï¼ˆå¾ªç’°å‚ç…§å›é¿ç‰ˆï¼‰
        â‘£æœ€é©è§£æ¡ç”¨ âœ…
        """
        word_text = token['text'].lower()
        
        if word_text not in self.ambiguous_words:
            return token['pos']  # é€šå¸¸ã®spaCyåˆ¤å®š
        
        candidates = self.ambiguous_words[word_text]
        best_pos = token['pos']  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯spaCyåˆ¤å®š
        best_score = 0
        
        self.logger.debug(f"ğŸ§  æ›–æ˜§èªè§£æ±ºé–‹å§‹: '{token['text']}' å€™è£œ={candidates}")
        
        # å„å€™è£œã‚’è©¦è¡Œã—ã¦æ§‹æ–‡çš„æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆå¾ªç’°å‚ç…§å›é¿ç‰ˆï¼‰
        for candidate_pos in candidates:
            score = self._evaluate_syntactic_consistency_safe(token, candidate_pos, tokens, position, sentence)
            self.logger.debug(f"  ã‚±ãƒ¼ã‚¹è©¦è¡Œ: {candidate_pos} â†’ ã‚¹ã‚³ã‚¢={score}")
            
            if score > best_score:
                best_pos = candidate_pos
                best_score = score
        
        self.logger.debug(f"ğŸ§  æœ€é©è§£æ¡ç”¨: '{token['text']}' â†’ {best_pos} (ã‚¹ã‚³ã‚¢={best_score})")
        return best_pos

    def _evaluate_syntactic_consistency_safe(self, ambiguous_token: Dict, candidate_pos: str, 
                                           tokens: List[Dict], position: int, sentence: str) -> float:
        """
        æ§‹æ–‡çš„æ•´åˆæ€§ã®è©•ä¾¡ï¼ˆå¾ªç’°å‚ç…§å›é¿ç‰ˆï¼‰
        
        äººé–“çš„æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹:
        - ã‚±ãƒ¼ã‚¹1è©¦è¡Œ: åè©ã¨ã—ã¦è§£é‡ˆ â†’ æ–‡æ§‹é€ ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
        - ã‚±ãƒ¼ã‚¹2è©¦è¡Œ: å‹•è©ã¨ã—ã¦è§£é‡ˆ â†’ æ–‡æ§‹é€ ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
        - ã‚ˆã‚Šå®Œå…¨ãªæ§‹é€ ã‚’æŒã¤ã‚±ãƒ¼ã‚¹ã‚’é¸æŠ
        """
        # ä»®æƒ³çš„ã«ãƒˆãƒ¼ã‚¯ãƒ³ã®å“è©ã‚’å¤‰æ›´
        test_tokens = [t.copy() for t in tokens]
        test_tokens[position]['pos'] = candidate_pos
        
        # å¾ªç’°å‚ç…§ã‚’å›é¿ã—ãŸæ§‹æ–‡æ§‹é€ ã®è©•ä¾¡
        structure_score = self._analyze_sentence_structure_completeness_safe(test_tokens, sentence)
        
        return structure_score

    def _analyze_sentence_structure_completeness_safe(self, tokens: List[Dict], sentence: str) -> float:
        """
        æ–‡æ§‹é€ ã®å®Œå…¨æ€§ã‚’åˆ†æï¼ˆå¾ªç’°å‚ç…§å›é¿ç‰ˆï¼‰
        
        äººé–“çš„æ€è€ƒ:
        - é–¢ä¿‚è©ãŒã‚ã‚‹ãªã‚‰ã€é–¢ä¿‚ç¯€ + ãƒ¡ã‚¤ãƒ³æ–‡ã®ä¸¡æ–¹ãŒå¿…è¦
        - é–¢ä¿‚ç¯€ã®ã¿ã§çµ‚ã‚ã‚‹ â†’ æ§‹é€ çš„ã«ä¸å®Œå…¨
        - é–¢ä¿‚ç¯€ + ãƒ¡ã‚¤ãƒ³æ–‡ â†’ æ§‹é€ çš„ã«å®Œå…¨
        """
        score = 0.0
        
        # é–¢ä¿‚è©ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        has_relative_pronoun = self._has_relative_pronoun(sentence)
        
        if has_relative_pronoun:
            self.logger.debug(f"    ğŸ” é–¢ä¿‚ç¯€æ–‡ã¨ã—ã¦è©•ä¾¡é–‹å§‹")
            
            # é–¢ä¿‚ç¯€ + ãƒ¡ã‚¤ãƒ³æ–‡ã®åˆ†é›¢è©•ä¾¡ï¼ˆå¾ªç’°å‚ç…§å›é¿ç‰ˆï¼‰
            relative_clause_complete = self._check_relative_clause_completeness_safe(tokens)
            main_clause_complete = self._check_main_clause_completeness_safe(tokens)
            
            self.logger.debug(f"    é–¢ä¿‚ç¯€å®Œå…¨æ€§: {relative_clause_complete}")
            self.logger.debug(f"    ãƒ¡ã‚¤ãƒ³æ–‡å®Œå…¨æ€§: {main_clause_complete}")
            
            # é–¢ä¿‚ç¯€æ§‹æ–‡ã§ã¯ä¸¡æ–¹ãŒå¿…è¦
            if relative_clause_complete and main_clause_complete:
                score = 100.0  # å®Œå…¨ãªé–¢ä¿‚ç¯€æ§‹æ–‡
                self.logger.debug(f"    âœ… å®Œå…¨ãªé–¢ä¿‚ç¯€æ§‹æ–‡: +100")
            elif relative_clause_complete and not main_clause_complete:
                score = 20.0   # é–¢ä¿‚ç¯€ã®ã¿ï¼ˆæ§‹é€ çš„ã«ä¸å®Œå…¨ï¼‰
                self.logger.debug(f"    âŒ é–¢ä¿‚ç¯€ã®ã¿ï¼ˆãƒ¡ã‚¤ãƒ³æ–‡æ¬ å¦‚ï¼‰: +20")
            elif not relative_clause_complete and main_clause_complete:
                score = 30.0   # ãƒ¡ã‚¤ãƒ³æ–‡ã®ã¿ï¼ˆé–¢ä¿‚ç¯€ç„¡è¦–ã¯ä¸è‡ªç„¶ï¼‰
                self.logger.debug(f"    âŒ ãƒ¡ã‚¤ãƒ³æ–‡ã®ã¿ï¼ˆé–¢ä¿‚ç¯€ç„¡è¦–ï¼‰: +30")
            else:
                score = 0.0    # ä¸¡æ–¹ä¸å®Œå…¨
                self.logger.debug(f"    âŒ ä¸¡æ–¹ä¸å®Œå…¨: +0")
        else:
            # é€šå¸¸æ–‡ã®è©•ä¾¡
            if self._has_main_verb_simple(tokens) and self._has_subject_structure_simple(tokens):
                score = 100.0
                self.logger.debug(f"    âœ… é€šå¸¸æ–‡å®Œå…¨: +100")
        
        self.logger.debug(f"    ç·åˆã‚¹ã‚³ã‚¢: {score}/100")
        return score

    def _check_main_clause_completeness_safe(self, tokens: List[Dict]) -> bool:
        """ãƒ¡ã‚¤ãƒ³æ–‡ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå¾ªç’°å‚ç…§å›é¿ç‰ˆï¼‰"""
        # å“è©ãƒ™ãƒ¼ã‚¹ã§ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’ç‰¹å®šï¼ˆä¾å­˜é–¢ä¿‚ã«ä¾å­˜ã—ãªã„ï¼‰
        main_verbs = []
        for i, token in enumerate(tokens):
            if token['pos'] in ['VERB', 'AUX']:
                # å“è©ãƒ™ãƒ¼ã‚¹åˆ¤å®š: æ–‡é ­ä»˜è¿‘ã®å‹•è©ã€ã¾ãŸã¯Auxã®å¾Œã®å‹•è©ã‚’ãƒ¡ã‚¤ãƒ³å‹•è©ã¨ã¿ãªã™
                is_main_verb = False
                if i == 0 or token['pos'] == 'AUX':  # æ–‡é ­å‹•è©ã¾ãŸã¯Aux
                    is_main_verb = True
                elif i > 0 and tokens[i-1]['pos'] == 'AUX':  # Auxã®ç›´å¾Œã®å‹•è©
                    is_main_verb = True
                elif i < len(tokens)-1 and any(t['pos'] in ['NOUN', 'PRON'] for t in tokens[:i]):  # ä¸»èªã®å¾Œã®å‹•è©
                    is_main_verb = True
                
                if is_main_verb:
                    main_verbs.append(token)
                    self.logger.debug(f"      ãƒ¡ã‚¤ãƒ³å‹•è©å€™è£œ: '{token['text']}' (pos={token['pos']}) - å“è©ãƒ™ãƒ¼ã‚¹åˆ¤å®š")
        
        return len(main_verbs) > 0

    def _check_relative_clause_completeness_safe(self, tokens: List[Dict]) -> bool:
        """é–¢ä¿‚ç¯€ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå¾ªç’°å‚ç…§å›é¿ç‰ˆï¼‰"""
        has_relative_pronoun = False
        has_relative_verb = False
        
        for i, token in enumerate(tokens):
            if token['text'].lower() in ['who', 'whom', 'which', 'that', 'whose']:
                has_relative_pronoun = True
            elif token['pos'] in ['VERB', 'AUX']:
                # å“è©ãƒ™ãƒ¼ã‚¹é–¢ä¿‚ç¯€å‹•è©åˆ¤å®š: é–¢ä¿‚ä»£åè©ã®å¾Œã«ã‚ã‚‹å‹•è©
                if has_relative_pronoun and i > 0:
                    # é–¢ä¿‚ä»£åè©ãŒæ—¢ã«è¦‹ã¤ã‹ã£ã¦ã„ã¦ã€ãã®å¾Œã®å‹•è©
                    prev_tokens = tokens[:i]
                    if any(t['text'].lower() in ['who', 'whom', 'which', 'that', 'whose'] for t in prev_tokens):
                        has_relative_verb = True
                        self.logger.debug(f"      é–¢ä¿‚ç¯€å‹•è©å€™è£œ: '{token['text']}' (pos={token['pos']}) - å“è©ãƒ™ãƒ¼ã‚¹åˆ¤å®š")
        
        return has_relative_pronoun and has_relative_verb

    def _has_main_verb_simple(self, tokens: List[Dict]) -> bool:
        """ç°¡æ˜“ãƒ¡ã‚¤ãƒ³å‹•è©ãƒã‚§ãƒƒã‚¯"""
        return any(token['pos'] == 'VERB' for token in tokens)

    def _has_subject_structure_simple(self, tokens: List[Dict]) -> bool:
        """ç°¡æ˜“ä¸»èªæ§‹é€ ãƒã‚§ãƒƒã‚¯"""
        return any(token['pos'] in ['NOUN', 'PRON', 'PROPN'] for token in tokens)

    def _evaluate_syntactic_consistency(self, ambiguous_token: Dict, candidate_pos: str, 
                                       tokens: List[Dict], position: int, sentence: str) -> float:
        """
        æ§‹æ–‡çš„æ•´åˆæ€§ã®è©•ä¾¡
        
        äººé–“çš„æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹:
        - ã‚±ãƒ¼ã‚¹1è©¦è¡Œ: åè©ã¨ã—ã¦è§£é‡ˆ â†’ æ–‡æ§‹é€ ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
        - ã‚±ãƒ¼ã‚¹2è©¦è¡Œ: å‹•è©ã¨ã—ã¦è§£é‡ˆ â†’ æ–‡æ§‹é€ ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
        - ã‚ˆã‚Šå®Œå…¨ãªæ§‹é€ ã‚’æŒã¤ã‚±ãƒ¼ã‚¹ã‚’é¸æŠ
        """
        # ä»®æƒ³çš„ã«ãƒˆãƒ¼ã‚¯ãƒ³ã®å“è©ã‚’å¤‰æ›´
        test_tokens = [t.copy() for t in tokens]
        test_tokens[position]['pos'] = candidate_pos
        
        # æ§‹æ–‡æ§‹é€ ã®è©•ä¾¡
        structure_score = self._analyze_sentence_structure_completeness(test_tokens, sentence)
        
        return structure_score

    def _analyze_sentence_structure_completeness(self, tokens: List[Dict], sentence: str) -> float:
        """
        æ–‡æ§‹é€ ã®å®Œå…¨æ€§ã‚’åˆ†æï¼ˆé–¢ä¿‚ç¯€å­˜åœ¨å‰æç‰ˆï¼‰
        
        äººé–“çš„æ€è€ƒ:
        - é–¢ä¿‚è©ãŒã‚ã‚‹ãªã‚‰ã€é–¢ä¿‚ç¯€ + ãƒ¡ã‚¤ãƒ³æ–‡ã®ä¸¡æ–¹ãŒå¿…è¦
        - é–¢ä¿‚ç¯€ã®ã¿ã§çµ‚ã‚ã‚‹ â†’ æ§‹é€ çš„ã«ä¸å®Œå…¨
        - é–¢ä¿‚ç¯€ + ãƒ¡ã‚¤ãƒ³æ–‡ â†’ æ§‹é€ çš„ã«å®Œå…¨
        """
        score = 0.0
        
        # ğŸ†• CRITICAL: é–¢ä¿‚è©ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        has_relative_pronoun = self._has_relative_pronoun(sentence)
        
        if has_relative_pronoun:
            self.logger.debug(f"    ğŸ” é–¢ä¿‚ç¯€æ–‡ã¨ã—ã¦è©•ä¾¡é–‹å§‹")
            
            # é–¢ä¿‚ç¯€ + ãƒ¡ã‚¤ãƒ³æ–‡ã®åˆ†é›¢è©•ä¾¡
            relative_clause_complete = self._check_relative_clause_completeness(tokens, sentence)
            main_clause_complete = self._check_main_clause_completeness(tokens, sentence)
            
            self.logger.debug(f"    é–¢ä¿‚ç¯€å®Œå…¨æ€§: {relative_clause_complete}")
            self.logger.debug(f"    ãƒ¡ã‚¤ãƒ³æ–‡å®Œå…¨æ€§: {main_clause_complete}")
            
            # é–¢ä¿‚ç¯€æ§‹æ–‡ã§ã¯ä¸¡æ–¹ãŒå¿…è¦
            if relative_clause_complete and main_clause_complete:
                score = 100.0  # å®Œå…¨ãªé–¢ä¿‚ç¯€æ§‹æ–‡
                self.logger.debug(f"    âœ… å®Œå…¨ãªé–¢ä¿‚ç¯€æ§‹æ–‡: +100")
            elif relative_clause_complete and not main_clause_complete:
                score = 20.0   # é–¢ä¿‚ç¯€ã®ã¿ï¼ˆæ§‹é€ çš„ã«ä¸å®Œå…¨ï¼‰
                self.logger.debug(f"    âŒ é–¢ä¿‚ç¯€ã®ã¿ï¼ˆãƒ¡ã‚¤ãƒ³æ–‡æ¬ å¦‚ï¼‰: +20")
            elif not relative_clause_complete and main_clause_complete:
                score = 30.0   # ãƒ¡ã‚¤ãƒ³æ–‡ã®ã¿ï¼ˆé–¢ä¿‚ç¯€ç„¡è¦–ã¯ä¸è‡ªç„¶ï¼‰
                self.logger.debug(f"    âŒ ãƒ¡ã‚¤ãƒ³æ–‡ã®ã¿ï¼ˆé–¢ä¿‚ç¯€ç„¡è¦–ï¼‰: +30")
            else:
                score = 0.0    # ä¸¡æ–¹ä¸å®Œå…¨
                self.logger.debug(f"    âŒ ä¸¡æ–¹ä¸å®Œå…¨: +0")
        else:
            # é€šå¸¸æ–‡ã®è©•ä¾¡
            if self._has_main_verb(tokens) and self._has_subject_structure(tokens):
                score = 100.0
                self.logger.debug(f"    âœ… é€šå¸¸æ–‡å®Œå…¨: +100")
        
        self.logger.debug(f"    ç·åˆã‚¹ã‚³ã‚¢: {score}/100")
        return score

    def _has_relative_pronoun(self, sentence: str) -> bool:
        """é–¢ä¿‚ä»£åè©ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯"""
        relative_pronouns = ['who', 'whom', 'which', 'that', 'whose', 'where', 'when']
        sentence_lower = sentence.lower()
        return any(pronoun in sentence_lower for pronoun in relative_pronouns)

    def _check_relative_clause_completeness(self, tokens: List[Dict], sentence: str) -> bool:
        """é–¢ä¿‚ç¯€ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯"""
        # whoseæ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³: whose + åè© + å‹•è© + (è£œèª)
        if 'whose' in sentence.lower():
            return self._check_whose_clause_completeness(tokens)
        # ä»–ã®é–¢ä¿‚ä»£åè©ãƒ‘ã‚¿ãƒ¼ãƒ³
        return self._check_general_relative_clause_completeness(tokens)

    def _check_main_clause_completeness(self, tokens: List[Dict], sentence: str) -> bool:
        """ãƒ¡ã‚¤ãƒ³æ–‡ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆé–¢ä¿‚ç¯€ã‚’é™¤ã„ã¦ï¼‰"""
        # é–¢ä¿‚ç¯€ä»¥å¤–ã®éƒ¨åˆ†ã«ãƒ¡ã‚¤ãƒ³å‹•è©ãŒå­˜åœ¨ã™ã‚‹ã‹
        main_verbs = []
        for i, token in enumerate(tokens):
            # äººé–“çš„POSåˆ¤å®šã‚’ä½¿ç”¨ï¼ˆæ›–æ˜§èªã®å“è©å¤‰æ›´ã‚’åæ˜ ï¼‰
            corrected_pos = self._resolve_ambiguous_word(token, tokens, i, sentence)
            if corrected_pos in ['VERB', 'AUX']:
                if not self._is_likely_in_relative_clause(token, tokens):
                    main_verbs.append(token)
                    self.logger.debug(f"      ãƒ¡ã‚¤ãƒ³å‹•è©å€™è£œ: '{token['text']}' (pos={corrected_pos})")
        
        return len(main_verbs) > 0

    def _check_whose_clause_completeness(self, tokens: List[Dict]) -> bool:
        """whoseæ§‹æ–‡ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯: whose + åè© + å‹•è© + (è£œèª)"""
        whose_idx = None
        possessed_noun = False
        relative_verb = False
        
        for i, token in enumerate(tokens):
            if token['text'].lower() == 'whose':
                whose_idx = i
            elif whose_idx is not None and i == whose_idx + 1:
                if token['pos'] in ['NOUN', 'PROPN']:
                    possessed_noun = True
            elif whose_idx is not None and i > whose_idx + 1 and token['pos'] in ['VERB', 'AUX']:
                relative_verb = True
                break
        
        return whose_idx is not None and possessed_noun and relative_verb

    def _check_general_relative_clause_completeness(self, tokens: List[Dict]) -> bool:
        """ä¸€èˆ¬çš„ãªé–¢ä¿‚ç¯€ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯"""
        has_relative_pronoun = False
        has_relative_verb = False
        
        for i, token in enumerate(tokens):
            if token['text'].lower() in ['who', 'which', 'that', 'whom']:
                has_relative_pronoun = True
            elif has_relative_pronoun and token['pos'] in ['VERB', 'AUX']:
                has_relative_verb = True
                break
        
        return has_relative_pronoun and has_relative_verb

    def _is_likely_main_verb_by_position(self, token: Dict, tokens: List[Dict], position: int) -> bool:
        """ä½ç½®çš„ã«ãƒ¡ã‚¤ãƒ³å‹•è©ã§ã‚ã‚‹å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        # whoseç¯€ã®å¾Œã«æ¥ã‚‹å‹•è©ã¯ãƒ¡ã‚¤ãƒ³å‹•è©ã®å¯èƒ½æ€§ãŒé«˜ã„
        for i in range(position):
            if tokens[i]['text'].lower() in ['whose', 'who', 'which', 'that']:
                # é–¢ä¿‚ä»£åè©ã‚ˆã‚Šå¾Œã«ã‚ã‚‹æ›–æ˜§èª
                # ã‹ã¤ã€é–¢ä¿‚ç¯€å†…å‹•è©ï¼ˆbeå‹•è©ç­‰ï¼‰ã®å¾Œã«ã‚ã‚‹
                relative_verb_found = False
                for j in range(i + 1, position):
                    if tokens[j]['pos'] in ['VERB', 'AUX']:
                        relative_verb_found = True
                        break
                
                if relative_verb_found:
                    return True  # é–¢ä¿‚ç¯€å‹•è©ã®å¾Œ â†’ ãƒ¡ã‚¤ãƒ³å‹•è©ã®å¯èƒ½æ€§
        
        return False

    def _has_main_verb(self, tokens: List[Dict]) -> bool:
        """ãƒ¡ã‚¤ãƒ³å‹•è©ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯"""
        for token in tokens:
            if token['pos'] in ['VERB', 'AUX']:
                return True
        return False

    def _is_likely_in_relative_clause(self, token: Dict, tokens: List[Dict]) -> bool:
        """ãƒˆãƒ¼ã‚¯ãƒ³ãŒé–¢ä¿‚ç¯€å†…ã«ã‚ã‚‹å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        token_idx = None
        for i, t in enumerate(tokens):
            if t['text'] == token['text'] and t.get('index', i) == token.get('index', -1):
                token_idx = i
                break
        
        if token_idx is None:
            return False
        
        # é–¢ä¿‚ä»£åè©ã®å¾Œã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        for i in range(token_idx):
            if tokens[i]['text'].lower() in ['who', 'whom', 'which', 'that', 'whose']:
                return True
        
        return False

    def _has_main_verb_outside_relative_clause(self, tokens: List[Dict]) -> bool:
        """ãƒ¡ã‚¤ãƒ³æ–‡ï¼ˆé–¢ä¿‚ç¯€å¤–ï¼‰ã«å‹•è©ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        for token in tokens:
            # é–¢ä¿‚ç¯€å†…ã§ã¯ãªã„å‹•è©ã‚’æ¢ã™
            if (token['pos'] in ['VERB', 'AUX'] and 
                not token.get('is_in_relative_clause', False)):
                return True
        return False

    def _has_subject_structure(self, tokens: List[Dict]) -> bool:
        """ä¸»èªæ§‹é€ ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯"""
        for token in tokens:
            if token['pos'] in ['NOUN', 'PROPN', 'PRON']:
                return True
        return False

    def _is_relative_clause_structurally_complete(self, tokens: List[Dict]) -> bool:
        """é–¢ä¿‚ç¯€æ§‹é€ ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯"""
        # ç°¡æ˜“å®Ÿè£…: é–¢ä¿‚ä»£åè©ãŒã‚ã‚Œã°é–¢ä¿‚ç¯€ã¨ã—ã¦èªè­˜
        has_relative_pronoun = any(
            token['text'].lower() in ['who', 'whom', 'which', 'that', 'whose'] 
            for token in tokens
        )
        return has_relative_pronoun

    def _is_modifier_placement_valid(self, tokens: List[Dict]) -> bool:
        """ä¿®é£¾èªé…ç½®ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        # ç°¡æ˜“å®Ÿè£…: åŸºæœ¬çš„ã«å¦¥å½“ã¨ã™ã‚‹
        return True

    def _get_human_corrected_pos(self, token: Dict) -> str:
        """
        äººé–“çš„å“è©åˆ¤å®šã®çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
        
        ğŸ”¥ Phase A3: BasicFivePatternHandlerãŒæ›–æ˜§èªè§£æ±ºã‚’æ‹…å½“
        ãƒ¬ã‚¬ã‚·ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦spaCyåˆ¤å®šã‚’ä½¿ç”¨
        """
        # ğŸ”¥ Phase A3: BasicFivePatternHandlerã«ã‚ˆã‚‹æ›–æ˜§èªè§£æ±ºã‚’å„ªå…ˆ
        if hasattr(self, 'basic_five_pattern_handler') and self.basic_five_pattern_handler:
            # Phase A3ã§ã¯ç´”ç²‹ã«spaCyåˆ¤å®šã‚’ä½¿ç”¨ï¼ˆé‡è¤‡å‡¦ç†å›é¿ï¼‰
            return token['pos']
        
        # ãƒ¬ã‚¬ã‚·ãƒ¼å‡¦ç†ï¼ˆPhase A3ä»¥å¤–ã§ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if token['text'].lower() not in self.ambiguous_words:
            return token['pos']  # é€šå¸¸ã®spaCyåˆ¤å®š
        
        # ğŸ§  é©å‘½çš„äºŒé‡è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®é©ç”¨
        # Note: ç°¡æ˜“ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã§äºŒé‡è©•ä¾¡ã‚’å®Ÿè¡Œ
        word_text = token['text'].lower()
        
        # VERBå€™è£œã¨NOUNå€™è£œã§æ§‹æ–‡çš„æ•´åˆæ€§ã‚’æ¯”è¼ƒ
        verb_score = self._evaluate_word_as_verb_simple(token, word_text)
        noun_score = self._evaluate_word_as_noun_simple(token, word_text)
        
        if verb_score > noun_score:
            self.logger.debug(f"ğŸ§  äººé–“çš„åˆ¤å®š: '{token['text']}' â†’ VERB (ã‚¹ã‚³ã‚¢: {verb_score} vs {noun_score})")
            return 'VERB'
        else:
            self.logger.debug(f"ğŸ§  äººé–“çš„åˆ¤å®š: '{token['text']}' â†’ NOUN (ã‚¹ã‚³ã‚¢: {verb_score} vs {noun_score})")
            return 'NOUN'

    def _evaluate_word_as_verb_simple(self, token: Dict, word_text: str) -> float:
        """èªã‚’å‹•è©ã¨ã—ã¦è©•ä¾¡ã™ã‚‹ç°¡æ˜“ã‚¹ã‚³ã‚¢"""
        score = 0.0
        
        # åŸºæœ¬çš„ãªå‹•è©ã‚‰ã—ã•ãƒã‚§ãƒƒã‚¯
        if word_text.endswith('s'):  # ä¸‰äººç§°å˜æ•°å½¢
            score += 30.0
            
        # whoseæ§‹æ–‡ã§ã®å‹•è©åˆ¤å®šï¼ˆlivesç­‰ï¼‰
        # ğŸ”¥ Phase A3: ãƒ¬ã‚¬ã‚·ãƒ¼å‹•è©åˆ¤å®šã‚’ç„¡åŠ¹åŒ–
        if hasattr(self, 'basic_five_pattern_handler') and self.basic_five_pattern_handler:
            pass  # Phase A3ã§ã¯ä½¿ç”¨ã—ãªã„
        elif word_text in ['lives', 'works', 'runs', 'goes', 'comes']:
            score += 50.0
            
        return score
    
    def _evaluate_word_as_noun_simple(self, token: Dict, word_text: str) -> float:
        """èªã‚’åè©ã¨ã—ã¦è©•ä¾¡ã™ã‚‹ç°¡æ˜“ã‚¹ã‚³ã‚¢"""
        score = 0.0
        
        # åŸºæœ¬çš„ãªåè©ã‚‰ã—ã•ãƒã‚§ãƒƒã‚¯
        if word_text.endswith('s'):  # è¤‡æ•°å½¢
            score += 20.0
            
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®spaCyåˆ¤å®šã‚’å°Šé‡
        if token['pos'] == 'NOUN':
            score += 10.0
            
        return score

    def _is_likely_verb_in_context(self, token: Dict, word_text: str) -> bool:
        """æ–‡è„ˆãƒ™ãƒ¼ã‚¹ã®å‹•è©åˆ¤å®š"""
        # ç°¡æ˜“å®Ÿè£…: ambiguous_wordsãƒªã‚¹ãƒˆã«ã‚ã‚‹èªã¯å‹•è©ã¨ã—ã¦æ‰±ã†
        # (ã‚ˆã‚Šé«˜ç²¾åº¦ãªå®Ÿè£…ã¯ä»Šå¾Œã®æ”¹å–„ã§)
        return word_text in self.ambiguous_words
        
        return contextual_verbs
    
    def _is_verb_in_whose_context(self, token: Dict, tokens: List[Dict], 
                                 position: int, sentence: str) -> bool:
        """
        whoseæ§‹æ–‡ã§ã®å‹•è©/åè©åŒå½¢èªåˆ¤å®š
        stanzaã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ã‚’POSãƒ™ãƒ¼ã‚¹ã§å†å®Ÿè£…
        """
        import re
        word = token['text'].lower()
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: whose [åè©] is [å½¢å®¹è©] [å‹•è©] (here|there|å ´æ‰€)
        pattern1 = rf'whose\s+\w+\s+is\s+\w+\s+{word}\s+(here|there|in\s+\w+)'
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: whose [åè©] [ä¿®é£¾èª]* [å‹•è©] (å ´æ‰€è¡¨ç¾)
        pattern2 = rf'whose\s+\w+.*?\s+{word}\s+(here|there|in|at|on)\s+\w+'
        
        if re.search(pattern1, sentence.lower()) or re.search(pattern2, sentence.lower()):
            # æ–‡ä¸­ã«whoseãŒã‚ã‚Šã€è©²å½“ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆ
            return True
        
        # ã‚ˆã‚Šä¸€èˆ¬çš„ãªåˆ¤å®š: whoseå¾Œã§ã€å ´æ‰€è¡¨ç¾ã®å‰ã«ã‚ã‚‹åŒå½¢èª
        if 'whose' in sentence.lower():
            # whoseå¾Œã®ä½ç½®ç¢ºèª
            whose_pos = None
            for i, t in enumerate(tokens):
                if t['text'].lower() == 'whose':
                    whose_pos = i
                    break
            
            if whose_pos is not None and position > whose_pos:
                # whoseå¾Œã§ã€å ´æ‰€è¡¨ç¾ãŒã‚ã‚‹å ´åˆ
                for j in range(position + 1, len(tokens)):
                    next_token = tokens[j]['text'].lower()
                    if next_token in ['here', 'there', 'in', 'at', 'on']:
                        return True
        
        return False

    def _identify_relative_clause_elements(self, tokens: List[Dict], relative_info: Dict) -> set:
        """
        é–¢ä¿‚ç¯€å†…ã®è¦ç´ ã‚’äº‹å‰ã«ç‰¹å®šï¼ˆå…ˆè¡Œè©ã¯ä¿æŒã€é–¢ä¿‚ç¯€éƒ¨åˆ†ã®ã¿é™¤å¤–ï¼‰
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆã®æ–¹æ³•è«–ï¼š
        â‘ é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒé–¢ä¿‚ç¯€ã®éƒ¨åˆ†ã‚’æ­£ã—ãåˆ‡ã‚Šå–ã‚‹
        â‘¡5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®åˆ¤æ–­ç”¨ã«å…ˆè¡Œè©ã ã‘æ®‹ã™ï¼ˆã€Œå¾Œã«""ã«ã™ã¹ãã€æƒ…å ±ä»˜ãï¼‰
        """
        excluded_indices = set()
        
        if not relative_info['found']:
            return excluded_indices
        
        # å…ˆè¡Œè©ã¯ä¿æŒã—ã€é–¢ä¿‚ç¯€éƒ¨åˆ†ã®ã¿ã‚’é™¤å¤–
        rel_start = relative_info.get('clause_start_idx', -1)  # é–¢ä¿‚ä»£åè©ã®ä½ç½®
        rel_end = relative_info.get('clause_end_idx', -1)
        antecedent_idx = relative_info.get('antecedent_idx', -1)  # å…ˆè¡Œè©ã¯ä¿æŒ
        
        if rel_start >= 0 and rel_end >= 0:
            # é–¢ä¿‚ä»£åè©ã‹ã‚‰é–¢ä¿‚ç¯€çµ‚äº†ã¾ã§é™¤å¤–ï¼ˆå…ˆè¡Œè©ã¨ãƒ¡ã‚¤ãƒ³å‹•è©ã¯ä¿è­·ï¼‰
            # rel_endã¯ã‚¯ãƒ©ã‚¦ã‚ºã®æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãªã®ã§ +1 ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
            for i in range(rel_start, rel_end + 1):
                if i < len(tokens):
                    # å…ˆè¡Œè©ã¯ä¿è­·ï¼ˆ5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã§åˆ¤æ–­ã«ä½¿ç”¨ï¼‰
                    if i != antecedent_idx:
                        excluded_indices.add(i)
            
            self.logger.debug(f"é–¢ä¿‚ç¯€è¦ç´ é™¤å¤–: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {rel_start}-{rel_end} (å…ˆè¡Œè©{antecedent_idx}ã¯ä¿æŒ)")
        
        return excluded_indices
        
        # ã‚ˆãèª¤èªè­˜ã•ã‚Œã‚‹å‹•è©ã®ãƒªã‚¹ãƒˆ
        common_verbs = {
            'lives', 'live', 'lived', 'living',
            'works', 'work', 'worked', 'working',
            'runs', 'run', 'ran', 'running',
            'goes', 'go', 'went', 'going',
            'comes', 'come', 'came', 'coming',
            'sits', 'sit', 'sat', 'sitting',
            'stands', 'stand', 'stood', 'standing',
            'plays', 'play', 'played', 'playing'
        }
        
        for i, token in enumerate(tokens):
            word = token['text'].lower()
            
            # è¾æ›¸ã«å«ã¾ã‚Œã‚‹ä¸€èˆ¬çš„ãªå‹•è©
            if word in common_verbs:
                contextual_verbs.append((i, token))
            
            # èªå°¾ã«ã‚ˆã‚‹å‹•è©åˆ¤å®šï¼ˆ-s, -ed, -ingï¼‰
            elif (word.endswith('s') and len(word) > 2 and 
                  not word.endswith('ss') and not word.endswith('us')):
                # ä¸‰äººç§°å˜æ•°å½¢ã‚‰ã—ã„èª
                if self._looks_like_verb_context(tokens, i):
                    contextual_verbs.append((i, token))
        
        return contextual_verbs
    
    def _looks_like_verb_context(self, tokens: List[Dict], index: int) -> bool:
        """
        å‹•è©ã‚‰ã—ã„æ–‡è„ˆã‹ã‚’åˆ¤å®š
        """
        if index == 0:
            return False
        
        # å‰ã®èªãŒåè©ãƒ»ä»£åè©ãªã‚‰å‹•è©ã®å¯èƒ½æ€§ãŒé«˜ã„
        prev_token = tokens[index - 1]
        if prev_token['pos'] in ['NOUN', 'PRON', 'PROPN']:
            return True
        
        # å¾Œã®èªãŒå‰¯è©ãªã‚‰å‹•è©ã®å¯èƒ½æ€§ãŒé«˜ã„
        if index < len(tokens) - 1:
            next_token = tokens[index + 1]
            if next_token['pos'] == 'ADV':
                return True
        
        return False
    
    def _find_auxiliary(self, tokens: List[Dict], main_verb_idx: int) -> Optional[int]:
        """åŠ©å‹•è©ã‚’ç‰¹å®š"""
        # ãƒ¡ã‚¤ãƒ³å‹•è©ã®å‰ã‚’æ¢ã™
        for i in range(main_verb_idx):
            token = tokens[i]
            if self._is_auxiliary_verb(token):
                return i
        return None
    
    def _find_subject(self, tokens: List[Dict], verb_idx: int) -> List[int]:
        """
        ä¸»èªã‚’ç‰¹å®šï¼ˆå‹•è©ã®å‰ã®åè©å¥ï¼‰
        è¤‡æ•°èªã®åè©å¥ã«å¯¾å¿œ
        é–¢ä¿‚ç¯€ã‚’å«ã‚€å ´åˆã¯é–¢ä¿‚ç¯€å…¨ä½“ã‚’ä¸»èªã«å«ã‚ã‚‹
        """
        subject_indices = []
        
        # ğŸ†• é–¢ä¿‚ç¯€ã‚’å«ã‚€ä¸»èªã®ç‰¹å®šï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        # ãƒˆãƒ¼ã‚¯ãƒ³ã«é–¢ä¿‚ç¯€ãƒãƒ¼ã‚«ãƒ¼ãŒã‚ã‚‹å ´åˆã®å‡¦ç†
        antecedent_idx = None
        relative_clause_end_idx = None
        
        for i, token in enumerate(tokens):
            if token.get('is_antecedent', False):
                antecedent_idx = i
            if token.get('is_relative_pronoun', False):
                # é–¢ä¿‚ç¯€ã®å®Ÿéš›ã®çµ‚äº†ä½ç½®ã‚’ä½¿ç”¨
                relative_clause_end_idx = token.get('relative_clause_end', verb_idx - 1)
                break
        
        # é–¢ä¿‚ç¯€ã‚’å«ã‚€ä¸»èªã®å ´åˆ
        if antecedent_idx is not None and relative_clause_end_idx is not None:
            # ğŸ”§ Rephraseã‚·ã‚¹ãƒ†ãƒ ä»•æ§˜: é–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆã§ã‚‚é€šå¸¸ã®ä¸»èªæ¤œå‡ºã‚’è¡Œã†
            # _assign_grammar_rolesã§ã€Œã‹ãŸã¾ã‚Šã€åˆ¤å®šã«ã‚ˆã‚Šç©ºã«ã™ã‚‹ã‹ã‚’æ±ºå®š
            self.logger.debug(f"é–¢ä¿‚ç¯€æ¤œå‡º: é€šå¸¸ã®ä¸»èªæ¤œå‡ºã‚’ç¶™ç¶šï¼ˆã‹ãŸã¾ã‚Šåˆ¤å®šã¯å¾Œã§å®Ÿè¡Œï¼‰")
            # return []  # ã“ã®æ—©æœŸãƒªã‚¿ãƒ¼ãƒ³ã‚’å‰Šé™¤
        
        # é€šå¸¸ã®ä¸»èªæ¤œå‡ºï¼ˆé–¢ä¿‚ç¯€ã‚ã‚Šãƒ»ãªã—ä¸¡å¯¾å¿œï¼‰
        # å‹•è©ã®å‰ã‚’å³ã‹ã‚‰å·¦ã«æ¢ã™
        for i in range(verb_idx - 1, -1, -1):
            token = tokens[i]
            
            # åŠ©å‹•è©ã¯é£›ã°ã™
            if self._is_auxiliary_verb(token):
                continue
            
            # åè©ãƒ»ä»£åè©ãƒ»å† è©ã‚’ä¸»èªã®ä¸€éƒ¨ã¨ã—ã¦åé›†
            if (token['pos'] in ['NOUN', 'PROPN', 'PRON'] or 
                token['tag'] in ['DT', 'PRP', 'PRP$', 'WP']):
                subject_indices.insert(0, i)  # é †åºã‚’ä¿ã¤ãŸã‚å‰ã«æŒ¿å…¥
            else:
                # ä¸»èªã®å¢ƒç•Œã«åˆ°é”
                break
        
        return subject_indices
    
    def _determine_sentence_pattern(self, core_elements: Dict, tokens: List[Dict]) -> str:
        """
        å‹•è©ã®æ€§è³ªã¨æ–‡è„ˆã‹ã‚‰æ–‡å‹ã‚’å‹•çš„ã«åˆ¤å®š
        """
        if not core_elements['verb']:
            return 'UNKNOWN'
        
        verb = core_elements['verb']
        verb_lemma = verb['lemma'].lower()
        verb_indices = core_elements['verb_indices'] + core_elements.get('auxiliary_indices', [])
        subject_indices = core_elements['subject_indices']
        
        # ğŸ¯ Central Controller: è‡ªå‹•è©ãƒªã‚¹ãƒˆ
        intransitive_verbs = {
            'arrive', 'arrived', 'come', 'came', 'go', 'went', 'sleep', 'slept',
            'walk', 'walked', 'run', 'ran', 'happen', 'happened', 'occur', 'occurred',
            'exist', 'existed', 'fall', 'fell', 'rise', 'rose', 'sit', 'sat',
            'stand', 'stood', 'lie', 'lay', 'work', 'worked', 'laugh', 'laughed',
            'cry', 'cried', 'smile', 'smiled', 'die', 'died'
        }
        
        # ä½¿ç”¨æ¸ˆã¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        used_indices = set(verb_indices + subject_indices)
        
        # æ®‹ã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åˆ†æ
        remaining_tokens = [
            (i, token) for i, token in enumerate(tokens) 
            if i not in used_indices and token['pos'] != 'PUNCT'
        ]
        
        # ğŸ¯ Central Controller: è‡ªå‹•è©ã®å ´åˆã¯å¼·åˆ¶çš„ã«SVãƒ‘ã‚¿ãƒ¼ãƒ³
        if verb_lemma in intransitive_verbs or verb['text'].lower() in intransitive_verbs:
            return 'SV'
        
        # é€£çµå‹•è©ã®å ´åˆ â†’ SVCå€™è£œ
        if verb_lemma in self.linking_verbs:
            if remaining_tokens:
                # è£œèªãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                for i, token in remaining_tokens:
                    if self._can_be_complement(token):
                        return 'SVC'
            return 'SV'  # è£œèªãŒãªã„å ´åˆ
        
        # æˆä¸å‹•è©ã®å ´åˆ â†’ SVOOå€™è£œ
        if verb_lemma in self.ditransitive_verbs:
            if len(remaining_tokens) >= 2:
                return 'SVOO'
            elif len(remaining_tokens) == 1:
                return 'SVO'
        
        # ç›®çš„æ ¼è£œèªå‹•è©ã®å ´åˆ â†’ SVOCå€™è£œ
        if verb_lemma in self.objective_complement_verbs:
            if len(remaining_tokens) >= 2:
                return 'SVOC'
            elif len(remaining_tokens) == 1:
                return 'SVO'
        
        # ä¸€èˆ¬çš„ãªä»–å‹•è© â†’ SVO
        if remaining_tokens:
            # ç›®çš„èªå€™è£œãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            for i, token in remaining_tokens:
                if self._can_be_object(token):
                    return 'SVO'
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ â†’ SV
        return 'SV'
    
    def _assign_grammar_roles(self, tokens: List[Dict], pattern: str, core_elements: Dict, relative_info: Dict = None) -> List[GrammarElement]:
        """
        æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ã„ã¦æ–‡æ³•çš„å½¹å‰²ã‚’å‹•çš„ã«å‰²ã‚Šå½“ã¦
        é–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆã¯è©²å½“ã‚¹ãƒ­ãƒƒãƒˆã‚’ç©ºã«ã™ã‚‹
        """
        if relative_info is None:
            relative_info = {'found': False}
            
        elements = []
        used_indices = set()
        
        # ğŸ†• é–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆï¼šã€Œã‹ãŸã¾ã‚Šã€ã®æ–‡æ³•çš„å½¹å‰²ã‚’å‹•è©ã¨ã®é–¢ä¿‚ã‹ã‚‰æ¨å®š
        relative_slot_to_empty = None
        if relative_info['found']:
            relative_slot_to_empty = self._determine_chunk_grammatical_role(tokens, core_elements, relative_info)
        
        # ä¸»èªå‡¦ç†ï¼ˆé–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆã¯å¼·åˆ¶çš„ã«ä¸»èªè¦ç´ ã‚’ä½œæˆï¼‰
        if core_elements['subject_indices'] or (relative_info['found'] and relative_slot_to_empty == 'S'):
            if relative_slot_to_empty == 'S':
                # â‘£ é–¢ä¿‚ç¯€ãŒSä½ç½®ã«ã‚ã‚‹å ´åˆï¼šã€Œå¾Œã«""ã«ã™ã¹ãã€æƒ…å ±ã‚’é©ç”¨
                subject_element = GrammarElement(
                    text="",  # ç©ºæ–‡å­—åˆ—ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆã®â‘£ï¼‰
                    tokens=[],
                    role='S',
                    start_idx=relative_info.get('antecedent_idx', 0),
                    end_idx=relative_info.get('antecedent_idx', 0),
                    confidence=0.95
                )
                self.logger.debug(f"é–¢ä¿‚ç¯€ä¸»èªã‚’ç©ºã‚¹ãƒ­ãƒƒãƒˆã«å¤‰æ›: antecedent_idx={relative_info.get('antecedent_idx')}")
            elif core_elements['subject_indices']:
                # é€šå¸¸ã®ä¸»èªå‡¦ç†
                subject_text = self._clean_relative_clause_from_text(core_elements['subject'], relative_info)
                subject_element = GrammarElement(
                    text=subject_text,
                    tokens=[tokens[i] for i in core_elements['subject_indices']],
                    role='S',
                    start_idx=min(core_elements['subject_indices']),
                    end_idx=max(core_elements['subject_indices']),
                    confidence=0.95
                )
            
            elements.append(subject_element)
            if core_elements['subject_indices']:
                used_indices.update(core_elements['subject_indices'])
        
        # åŠ©å‹•è©
        if core_elements['auxiliary_indices']:
            aux_element = GrammarElement(
                text=core_elements['auxiliary']['text'],
                tokens=[core_elements['auxiliary']],
                role='Aux',
                start_idx=core_elements['auxiliary_indices'][0],
                end_idx=core_elements['auxiliary_indices'][0],
                confidence=0.95
            )
            elements.append(aux_element)
            used_indices.update(core_elements['auxiliary_indices'])
        
        # å‹•è©
        if core_elements['verb_indices']:
            verb_element = GrammarElement(
                text=core_elements['verb']['text'],
                tokens=[core_elements['verb']],
                role='V',
                start_idx=core_elements['verb_indices'][0],
                end_idx=core_elements['verb_indices'][0],
                confidence=0.95
            )
            elements.append(verb_element)
            used_indices.update(core_elements['verb_indices'])
        
        # æ®‹ã‚Šã®è¦ç´ ã‚’æ–‡å‹ã«å¿œã˜ã¦å‰²ã‚Šå½“ã¦
        remaining_tokens = [
            (i, token) for i, token in enumerate(tokens) 
            if i not in used_indices and token['pos'] != 'PUNCT'
        ]
        
        # æ–‡å‹åˆ¥ã®å‰²ã‚Šå½“ã¦ï¼ˆé–¢ä¿‚ç¯€æƒ…å ±ã‚’æ¸¡ã™ï¼‰
        if pattern == 'SVC':
            elements.extend(self._assign_svc_elements(remaining_tokens, relative_slot_to_empty))
        elif pattern == 'SVO':
            elements.extend(self._assign_svo_elements(remaining_tokens, relative_slot_to_empty))
        elif pattern == 'SVOO':
            elements.extend(self._assign_svoo_elements(remaining_tokens, relative_slot_to_empty))
        elif pattern == 'SVOC':
            elements.extend(self._assign_svoc_elements(remaining_tokens, relative_slot_to_empty))
        else:  # SV or other
            elements.extend(self._assign_modifiers(remaining_tokens))
        
        return elements
    
    def _assign_svc_elements(self, remaining_tokens: List[Tuple[int, Dict]], relative_slot_to_empty: str = None) -> List[GrammarElement]:
        """SVCæ–‡å‹ã®è¦ç´ ã‚’å‰²ã‚Šå½“ã¦ - è¤‡åˆå¥å¯¾å¿œ"""
        elements = []
        complement_assigned = False
        used_indices = set()
        
        i = 0
        while i < len(remaining_tokens):
            idx, token = remaining_tokens[i]
            
            # æ—¢ã«ä½¿ç”¨æ¸ˆã¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ã‚¹ã‚­ãƒƒãƒ—
            if idx in used_indices:
                i += 1
                continue
            
            if not complement_assigned and (self._can_be_complement(token) or token['tag'] == 'DT'):
                # C1ã¨ã—ã¦è¤‡åˆå¥ã‚’æ¤œå‡ºï¼ˆå† è©ã‹ã‚‰å§‹ã¾ã‚‹å ´åˆã‚‚å«ã‚€ï¼‰
                phrase_indices, phrase_text = self._find_noun_phrase(remaining_tokens, i)
                if phrase_indices:
                    elements.append(GrammarElement(
                        text=phrase_text,
                        tokens=[remaining_tokens[j][1] for j in range(i, i + len(phrase_indices))],
                        role='C1',
                        start_idx=min(phrase_indices),
                        end_idx=max(phrase_indices),
                        confidence=0.9
                    ))
                    used_indices.update(phrase_indices)
                    complement_assigned = True
                    i += len(phrase_indices)
                else:
                    i += 1
            else:
                # ä¿®é£¾èªã¨ã—ã¦å‡¦ç†
                if idx not in used_indices:
                    elements.append(self._create_modifier_element(idx, token))
                i += 1
        
        return elements
    
    def _assign_svo_elements(self, remaining_tokens: List[Tuple[int, Dict]], relative_slot_to_empty: str = None) -> List[GrammarElement]:
        """SVOæ–‡å‹ã®è¦ç´ ã‚’å‰²ã‚Šå½“ã¦ï¼ˆé–¢ä¿‚ç¯€å¯¾å¿œï¼‰"""
        elements = []
        object_assigned = False
        
        # é–¢ä¿‚ç¯€ã«ã‚ˆã‚Šç›®çš„èªã‚¹ãƒ­ãƒƒãƒˆã‚’æŠ½å‡º
        object_text = ""
        if relative_slot_to_empty == 'O1':
            # O1ã«é–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆã¯ç©ºæ–‡å­—åˆ—
            object_text = ""
        else:
            # é€šå¸¸ã®ç›®çš„èªå‡¦ç† - è¤‡æ•°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã¾ã¨ã‚ã‚‹
            object_tokens = []
            for i, token in remaining_tokens:
                if self._can_be_object(token) or token['pos'] in ['DET', 'ADJ']:
                    object_tokens.append((i, token))
                elif object_tokens:  # ç›®çš„èªå¥ãŒçµ‚äº†
                    break
            
            if object_tokens:
                object_text = ' '.join([token['text'] for _, token in object_tokens])
                used_indices = {i for i, _ in object_tokens}
                
                elements.append(GrammarElement(
                    text=object_text,
                    tokens=[token for _, token in object_tokens],
                    role='O1',
                    start_idx=object_tokens[0][0],
                    end_idx=object_tokens[-1][0],
                    confidence=0.9
                ))
                object_assigned = True
                
                # æ®‹ã‚Šã®è¦ç´ ã‚’ä¿®é£¾èªã¨ã—ã¦å‡¦ç†
                for i, token in remaining_tokens:
                    if i not in used_indices:
                        elements.append(self._create_modifier_element(i, token))
            
        # é–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆã¯ç©ºã®O1è¦ç´ ã‚’ä½œæˆ
        if relative_slot_to_empty == 'O1' and not object_assigned:
            elements.append(GrammarElement(
                text="",  # ç©ºæ–‡å­—åˆ—
                tokens=[],
                role='O1',
                start_idx=0,
                end_idx=0,
                confidence=0.9
            ))
            
            # æ®‹ã‚Šã‚’ä¿®é£¾èªã¨ã—ã¦å‡¦ç†
            for i, token in remaining_tokens:
                elements.append(self._create_modifier_element(i, token))
        
        return elements
    
    def _assign_svoo_elements(self, remaining_tokens: List[Tuple[int, Dict]], relative_slot_to_empty: str = None) -> List[GrammarElement]:
        """SVOOæ–‡å‹ã®è¦ç´ ã‚’å‰²ã‚Šå½“ã¦ - O1/O2åˆ†é›¢å¯¾å¿œ"""
        elements = []
        o1_assigned = False
        o2_assigned = False
        used_indices = set()
        
        i = 0
        while i < len(remaining_tokens):
            idx, token = remaining_tokens[i]
            
            # æ—¢ã«ä½¿ç”¨æ¸ˆã¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ã‚¹ã‚­ãƒƒãƒ—
            if idx in used_indices:
                i += 1
                continue
            
            if not o1_assigned and (self._can_be_object(token) or token['tag'] == 'DT'):
                # SVOOæ–‡å‹ã®O1ã¯é€šå¸¸å˜ä¸€èªï¼ˆä»£åè©ãªã©ï¼‰
                if token['pos'] == 'PRON':
                    # ä»£åè©ã®å ´åˆã¯å˜èªã®ã¿ã§O1
                    elements.append(GrammarElement(
                        text=token['text'],
                        tokens=[token],
                        role='O1',
                        start_idx=idx,
                        end_idx=idx,
                        confidence=0.9
                    ))
                    used_indices.add(idx)
                    o1_assigned = True
                    i += 1
                else:
                    # ä»£åè©ä»¥å¤–ã‚‚å˜èªã®ã¿ã§O1ã¨ã—ã¦æ‰±ã†
                    elements.append(GrammarElement(
                        text=token['text'],
                        tokens=[token],
                        role='O1',
                        start_idx=idx,
                        end_idx=idx,
                        confidence=0.85
                    ))
                    used_indices.add(idx)
                    o1_assigned = True
                    i += 1
            elif not o2_assigned and (self._can_be_object(token) or token['tag'] == 'DT'):
                # O2ã¨ã—ã¦è¤‡åˆå¥ã‚’æ¤œå‡ºï¼ˆå† è©ã‹ã‚‰å§‹ã¾ã‚‹å ´åˆã‚‚å«ã‚€ï¼‰
                phrase_indices, phrase_text = self._find_noun_phrase(remaining_tokens, i)
                if phrase_indices:
                    elements.append(GrammarElement(
                        text=phrase_text,
                        tokens=[remaining_tokens[j][1] for j in range(i, i + len(phrase_indices))],
                        role='O2',
                        start_idx=min(phrase_indices),
                        end_idx=max(phrase_indices),
                        confidence=0.85
                    ))
                    used_indices.update(phrase_indices)
                    o2_assigned = True
                    i += len(phrase_indices)
                else:
                    i += 1
            else:
                # ä¿®é£¾èªã¨ã—ã¦å‡¦ç†
                if idx not in used_indices:
                    elements.append(self._create_modifier_element(idx, token))
                i += 1
        
        return elements
    
    def _assign_svoc_elements(self, remaining_tokens: List[Tuple[int, Dict]], relative_slot_to_empty: str = None) -> List[GrammarElement]:
        """SVOCæ–‡å‹ã®è¦ç´ ã‚’å‰²ã‚Šå½“ã¦"""
        elements = []
        object_assigned = False
        complement_assigned = False
        
        for i, token in remaining_tokens:
            if not object_assigned and self._can_be_object(token):
                elements.append(GrammarElement(
                    text=token['text'],
                    tokens=[token],
                    role='O1',
                    start_idx=i,
                    end_idx=i,
                    confidence=0.85
                ))
                object_assigned = True
            elif not complement_assigned and (self._can_be_complement(token) or object_assigned):
                elements.append(GrammarElement(
                    text=token['text'],
                    tokens=[token],
                    role='C2',  # ğŸ”§ SVOCã®Cã¯C2ã«ä¿®æ­£
                    start_idx=i,
                    end_idx=i,
                    confidence=0.85
                ))
                complement_assigned = True
            else:
                # ä¿®é£¾èªã¨ã—ã¦å‡¦ç†
                elements.append(self._create_modifier_element(i, token))
        
        return elements
    
    def _assign_modifiers(self, remaining_tokens: List[Tuple[int, Dict]]) -> List[GrammarElement]:
        """ä¿®é£¾èªã‚’å‰²ã‚Šå½“ã¦"""
        elements = []
        for i, token in remaining_tokens:
            elements.append(self._create_modifier_element(i, token))
        return elements
    
    def _create_modifier_element(self, idx: int, token: Dict) -> GrammarElement:
        """ä¿®é£¾èªè¦ç´ ã‚’ä½œæˆ"""
        # ä¿®é£¾èªã®ç¨®é¡ã‚’åˆ¤å®š
        if token['pos'] in ['ADV', 'PART']:
            role = 'M1'  # å‰¯è©çš„ä¿®é£¾ï¼ˆå‰¯è©ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒå¾Œã§ä¸Šæ›¸ãï¼‰
        elif token['pos'] in ['ADP'] or token['tag'] in ['IN', 'TO']:
            role = 'M2'  # å‰ç½®è©å¥
        else:
            role = 'M3'  # ãã®ä»–ã®ä¿®é£¾
        
        return GrammarElement(
            text=token['text'],
            tokens=[token],
            role=role,
            start_idx=idx,
            end_idx=idx,
            confidence=0.7
        )
    
    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    def _is_auxiliary_verb(self, token: Dict) -> bool:
        """åŠ©å‹•è©åˆ¤å®š"""
        aux_words = {'be', 'am', 'is', 'are', 'was', 'were', 'being', 'been',
                     'have', 'has', 'had', 'having',
                     'do', 'does', 'did', 'doing',
                     'will', 'would', 'shall', 'should', 'can', 'could',
                     'may', 'might', 'must', 'ought'}
        return token['lemma'].lower() in aux_words or token['tag'] in ['MD']
    
    def _find_noun_phrase(self, tokens: List[Tuple[int, Dict]], start_idx: int) -> Tuple[List[int], str]:
        """
        æŒ‡å®šä½ç½®ã‹ã‚‰è¤‡åˆåè©å¥ã‚’æ¤œå‡º
        
        Args:
            tokens: ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆ [(index, token), ...]
            start_idx: æ¤œç´¢é–‹å§‹ä½ç½®
            
        Returns:
            Tuple[List[int], str]: (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚¹ãƒˆ, çµåˆã—ãŸãƒ•ãƒ¬ãƒ¼ã‚º)
        """
        phrase_indices = []
        phrase_tokens = []
        
        # é–‹å§‹ä½ç½®ã‹ã‚‰é€£ç¶šã™ã‚‹åè©å¥è¦ç´ ã‚’åé›†
        for i in range(start_idx, len(tokens)):
            idx, token = tokens[i]
            
            # åè©å¥ã®æ§‹æˆè¦ç´ ã‹ãƒã‚§ãƒƒã‚¯
            if (token['pos'] in ['NOUN', 'PROPN', 'PRON'] or 
                token['tag'] in ['DT', 'CD', 'JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS']):
                phrase_indices.append(idx)
                phrase_tokens.append(token['text'])
            else:
                # åè©å¥ã®çµ‚äº†
                break
        
        if phrase_indices:
            phrase_text = ' '.join(phrase_tokens)
            return phrase_indices, phrase_text
        else:
            return [], ""

    def _can_be_object(self, token: Dict) -> bool:
        """ç›®çš„èªã«ãªã‚Œã‚‹ã‹ã®åˆ¤å®š"""
        return token['pos'] in ['NOUN', 'PROPN', 'PRON'] or token['tag'] in ['PRP', 'DT']
    
    def _can_be_complement(self, token: Dict) -> bool:
        """è£œèªã«ãªã‚Œã‚‹ã‹ã®åˆ¤å®š"""
        return token['pos'] in ['ADJ', 'NOUN', 'PROPN', 'PRON'] or token['tag'] in ['JJ', 'NN', 'NNS', 'PRP']
    
    def _convert_to_rephrase_format(self, elements: List[GrammarElement], pattern: str, sub_slots: Dict = None) -> Dict[str, Any]:
        """Rephraseã‚¹ãƒ­ãƒƒãƒˆå½¢å¼ã«å¤‰æ›"""
        if sub_slots is None:
            sub_slots = {}
        
        # ï¿½ ãƒ‡ãƒãƒƒã‚°: å…¥åŠ›ã•ã‚ŒãŸè¦ç´ ã‚’ç¢ºèª
        print(f"ğŸ”§ _convert_to_rephrase_formatå…¥åŠ›:")
        print(f"  elements: {[{'role': e.role, 'text': e.text} for e in elements]}")
        print(f"  pattern: {pattern}")
        print(f"  sub_slots: {sub_slots}")
        
        # ï¿½ğŸ”§ é–¢ä¿‚ç¯€ã®æœ‰ç„¡ã‚’ç¢ºèªã—ã¦ã‚¹ãƒ­ãƒƒãƒˆç•ªå·ã‚’èª¿æ•´
        has_relative_clause = bool(sub_slots)
        
        # C. ã€Œç›¸å¯¾ç¯€ã‚ã‚Šã®ã‚·ãƒ•ãƒˆã€ã¯ä¸€åº¦ãã‚Šã«ã™ã‚‹
        result = {}
        if has_relative_clause and not getattr(self, '_shifted_for_relcl', False):
            for element in elements:
                if element.role == 'M1':
                    element.role = 'M2'  # M1 â†’ M2
                elif element.role == 'M2':
                    element.role = 'M3'  # M2 â†’ M3
            self._shifted_for_relcl = True  # ä¸€åº¦ã ã‘å®Ÿè¡Œã™ã‚‹ãƒ•ãƒ©ã‚°
            result['_shifted_for_relcl'] = True  # çµæœã«ã‚‚å°ã‚’æ®‹ã™
            
        slots = []
        slot_phrases = []
        slot_display_order = []
        display_order = []
        phrase_types = []
        subslot_ids = []
        
        # ğŸ”§ main_slotsè¾æ›¸å½¢å¼ã‚‚ç”Ÿæˆ
        main_slots = {}
        
        # ğŸ”¥ Phase 2: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‹ã‚‰ã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæƒ…å ±ãƒãƒ¼ã‚¸
        unified_sub_slots = {}
        if hasattr(self, 'last_unified_result') and self.last_unified_result:
            unified_sub_slots = self.last_unified_result.get('sub_slots', {})
            print(f"ğŸ”¥ çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‹ã‚‰ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå–å¾—: {unified_sub_slots}")
        
        # æ—¢å­˜ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã¨çµ±åˆ
        merged_sub_slots = sub_slots.copy()
        merged_sub_slots.update(unified_sub_slots)
        print(f"ğŸ”¥ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆãƒãƒ¼ã‚¸çµæœ: {merged_sub_slots}")
        
        # è¦ç´ ã‚’ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
        elements.sort(key=lambda x: x.start_idx)
        
        role_order = {'S': 1, 'Aux': 2, 'V': 3, 'O1': 4, 'O2': 5, 'C1': 6, 'C2': 7, 'M1': 8, 'M2': 9, 'M3': 10}
        
        for i, element in enumerate(elements):
            # ã‚¹ãƒ­ãƒƒãƒˆåã®èª¿æ•´
            slot_name = element.role
            # çµ±ä¸€å½¢å¼: å¸¸ã«O1, O2, C1, C2ã‚’ä½¿ç”¨
            
            slots.append(slot_name)
            slot_phrases.append(element.text)
            
            # ğŸ”§ main_slotsè¾æ›¸ã«è¿½åŠ 
            main_slots[element.role] = element.text
            
            order = role_order.get(element.role, 99)
            slot_display_order.append(order)
            display_order.append(order)
            
            # å“è©ã‚¿ã‚¤ãƒ—ã®åˆ¤å®š
            if element.role in ['S', 'O1', 'O2']:
                phrase_types.append('åè©å¥')
            elif element.role == 'V':
                phrase_types.append('å‹•è©å¥')
            elif element.role in ['C1', 'C2']:
                phrase_types.append('è£œèªå¥')
            else:
                phrase_types.append('ä¿®é£¾å¥')
            
            subslot_ids.append(i)
        
        return {
            'Slot': slots,
            'SlotPhrase': slot_phrases,
            'Slot_display_order': slot_display_order,
            'display_order': display_order,
            'PhraseType': phrase_types,
            'SubslotID': subslot_ids,
            'main_slots': main_slots,  # ğŸ”§ è¾æ›¸å½¢å¼è¿½åŠ 
            'sub_slots': merged_sub_slots,    # ğŸ”¥ çµ±åˆã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆä½¿ç”¨
            'slots': main_slots,       # ğŸ”§ çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ äº’æ›æ€§
            'pattern_detected': pattern,
            'confidence': 0.9,
            'analysis_method': 'dynamic_grammar',
            'lexical_tokens': len([e for e in elements if e.role != 'PUNCT'])
        }

    def _detect_and_assign_adverbs_direct(self, doc, current_result: Dict) -> Dict:
        """
        ç›´æ¥çš„ãªå‰¯è©æ¤œå‡ºã¨é…ç½® (Phase 2ç°¡æ˜“å®Ÿè£…)
        
        Rephraseãƒ«ãƒ¼ãƒ«ï¼ˆæ­£ã—ã„ç†è§£ï¼‰:
        - 1å€‹: M2
        - 2å€‹: å‹•è©ã‚ˆã‚Šå‰ â†’ M1,M2 / å‹•è©ã‚ˆã‚Šå¾Œ â†’ M2,M3
        - 3å€‹: M1, M2, M3 (ä½ç½®é †)
        """
        try:
            # spaCyã‹ã‚‰å‰¯è©ã‚’æŠ½å‡º (é–¢ä¿‚ç¯€å‡¦ç†ã¯æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã«ä»»ã›ã‚‹)
            adverbs = []
            
            # B. ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå†…å‰¯è©ã®é™¤å¤–ã¯"èªå¢ƒç•Œ"ã§
            sub_words = set()
            for v in (current_result.get('sub_slots') or {}).values():
                if isinstance(v, str):
                    sub_words.update(v.split())  # æ–‡å­—åˆ—â†’èªãƒªã‚¹ãƒˆåŒ–
            
            # ãƒ¡ã‚¤ãƒ³å‹•è©ã®ä½ç½®ã‚’ç‰¹å®š
            main_verb_pos = None
            main_verb = current_result.get('main_slots', {}).get('V', '')
            if main_verb:
                for token in doc:
                    if token.text == main_verb and token.pos_ in ['VERB', 'AUX']:
                        main_verb_pos = token.i
                        break
            
            for token in doc:
                if token.pos_ == 'ADV':
                    # ChatGPT5 Step C: æ¶ˆè²»æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    if hasattr(self, '_consumed_tokens') and token.i in self._consumed_tokens:
                        print(f"ğŸ”¥ ChatGPT5 Step C: Adverb handler skipping consumed token {token.i}='{token.text}'")
                        continue
                        
                    # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå†…ã®å‰¯è©ã¯é™¤å¤–ï¼ˆèªå˜ä½ã®ä¸€è‡´ï¼‰
                    if token.text not in sub_words:
                        adverbs.append({
                            'text': token.text,
                            'index': token.i,
                            'pos': token.pos_
                        })
                    else:
                        print(f"ğŸ” é–¢ä¿‚ç¯€å†…å‰¯è©ã‚’é™¤å¤–: {token.text} (ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã§å‡¦ç†æ¸ˆã¿)")
            
            # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
            adverbs.sort(key=lambda x: x['index'])
            
            if not adverbs:
                return {}
            
            print(f"ğŸ” æ¤œå‡ºã•ã‚ŒãŸå‰¯è©: {[adv['text'] for adv in adverbs]}")
            print(f"ğŸ” ãƒ¡ã‚¤ãƒ³å‹•è© '{main_verb}' ã®ä½ç½®: {main_verb_pos}")
            
            # Rephraseãƒ«ãƒ¼ãƒ«ã«åŸºã¥ãé…ç½®ï¼ˆå‹•è©ä½ç½®ãƒ™ãƒ¼ã‚¹ï¼‰- æ—¢å­˜ã‚¹ãƒ­ãƒƒãƒˆç„¡è¦–ã§å®Œå…¨å†é…ç½®
            modifier_assignments = {}
            
            # ğŸ”¥ A. ã¾ãšæ˜ç¤ºçš„ã«æ—¢å­˜ M1/M2/M3 ã‚’æ¶ˆã™ï¼ˆç©ºæ–‡å­—ã§ã¯ãªãå‰Šé™¤ï¼‰
            for k in ('M1', 'M2', 'M3'):
                current_result.get('main_slots', {}).pop(k, None)
                current_result.get('slots', {}).pop(k, None)
            
            adverb_count = len(adverbs)
            
            if adverb_count == 1:
                # 1å€‹ã®å ´åˆ: M2ã«é…ç½®
                modifier_assignments['M2'] = adverbs[0]['text']
                    
            elif adverb_count == 2:
                # 2å€‹ã®å ´åˆ: å‹•è©ä½ç½®ã§åˆ¤å®š
                if main_verb_pos is not None:
                    pre_verb_adverbs = [adv for adv in adverbs if adv['index'] < main_verb_pos]
                    post_verb_adverbs = [adv for adv in adverbs if adv['index'] > main_verb_pos]
                    
                    if len(pre_verb_adverbs) == 1 and len(post_verb_adverbs) == 1:
                        # å‰1å€‹ã€å¾Œ1å€‹ â†’ M2(å‰), M3(å¾Œ)
                        modifier_assignments['M2'] = pre_verb_adverbs[0]['text']
                        modifier_assignments['M3'] = post_verb_adverbs[0]['text']
                    elif len(pre_verb_adverbs) == 2:
                        # å‰2å€‹ â†’ M1, M2
                        modifier_assignments['M1'] = pre_verb_adverbs[0]['text']
                        modifier_assignments['M2'] = pre_verb_adverbs[1]['text']
                    elif len(post_verb_adverbs) == 2:
                        # å¾Œ2å€‹ â†’ M2, M3
                        modifier_assignments['M2'] = post_verb_adverbs[0]['text']
                        modifier_assignments['M3'] = post_verb_adverbs[1]['text']
                else:
                    # å‹•è©ä½ç½®ä¸æ˜ã®å ´åˆã¯ä½ç½®é †ã§M2, M3
                    modifier_assignments['M2'] = adverbs[0]['text']
                    modifier_assignments['M3'] = adverbs[1]['text']
                    
            elif adverb_count >= 3:
                # 3å€‹ä»¥ä¸Šã®å ´åˆ: M1, M2, M3ã«é…ç½®ï¼ˆä½ç½®é †ï¼‰
                modifier_assignments['M1'] = adverbs[0]['text']
                modifier_assignments['M2'] = adverbs[1]['text']
                modifier_assignments['M3'] = adverbs[2]['text']
            
            print(f"ğŸ” å‰¯è©é…ç½®çµæœ: {modifier_assignments}")
            
            # ãƒ‡ãƒãƒƒã‚°ï¼šåæŸç¢ºèªç”¨ã®ãƒãƒƒã‚·ãƒ¥
            sig = '|'.join([current_result['main_slots'].get(k,'') for k in ('M1','M2','M3')])
            print(f"ğŸ” ADV_SIGNATURE_BEFORE={sig}")
            
            # ç©ºæ–‡å­—åˆ—ã®ã‚¹ãƒ­ãƒƒãƒˆã¯è¿”ã•ãªã„
            result = {k: v for k, v in modifier_assignments.items() if v}
            
            # é©ç”¨å¾Œã®ãƒãƒƒã‚·ãƒ¥ã‚‚ç¢ºèª
            if result:
                new_sig = '|'.join([result.get(k,'') for k in ('M1','M2','M3')])
                print(f"ğŸ” ADV_SIGNATURE_AFTER={new_sig}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"ç›´æ¥å‰¯è©å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def _create_error_result(self, sentence: str, error: str) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼çµæœã‚’ä½œæˆ"""
        return {
            'Slot': [],
            'SlotPhrase': [],
            'Slot_display_order': [],
            'display_order': [],
            'PhraseType': [],
            'SubslotID': [],
            'main_slots': {},    # ğŸ”§ è¾æ›¸å½¢å¼è¿½åŠ 
            'sub_slots': {},     # ğŸ”§ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆï¼ˆç¾åœ¨ã¯ç©ºï¼‰
            'slots': {},         # ğŸ”§ çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ äº’æ›æ€§
            'error': error,
            'sentence': sentence,
            'analysis_method': 'dynamic_grammar'
        }

    # ============================================
    # é–¢ä¿‚ç¯€å‡¦ç†ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤
    # ============================================
    
    def _detect_relative_clause(self, tokens: List[Dict], sentence: str) -> Dict[str, Any]:
        """é–¢ä¿‚ç¯€æ§‹é€ ã®æ¤œå‡º"""
        result = {
            'found': False,
            'type': None,
            'confidence': 0.0,
            'relative_pronoun_idx': None,
            'antecedent_idx': None,
            'clause_start_idx': None,
            'clause_end_idx': None
        }
        
        sentence_lower = sentence.lower()
        
        # é–¢ä¿‚ä»£åè©ã®æ¤œå‡º
        relative_pronouns = ['who', 'whom', 'which', 'that', 'whose', 'where', 'when', 'why', 'how']
        
        for rel_pronoun in relative_pronouns:
            if rel_pronoun in sentence_lower:
                # ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆã§é–¢ä¿‚ä»£åè©ã‚’æ¢ã™
                for i, token in enumerate(tokens):
                    if token['text'].lower() == rel_pronoun:
                        result.update({
                            'found': True,
                            'type': f'{rel_pronoun}_clause',
                            'confidence': 0.8,
                            'relative_pronoun_idx': i
                        })
                        
                        # å…ˆè¡Œè©ã‚’æ¢ã™ï¼ˆé–¢ä¿‚ä»£åè©ã®ç›´å‰ã®åè©ï¼‰
                        if i > 0 and tokens[i-1]['pos'] in ['NOUN', 'PROPN']:
                            result['antecedent_idx'] = i - 1
                            result['confidence'] = 0.9
                        
                        # é–¢ä¿‚ç¯€ã®ç¯„å›²ã‚’æ±ºå®šï¼ˆæ”¹è‰¯ç‰ˆï¼‰
                        result['clause_start_idx'] = i
                        result['clause_end_idx'] = self._find_relative_clause_end(tokens, i, rel_pronoun)
                        
                        self.logger.debug(f"é–¢ä¿‚ç¯€æ¤œå‡º: {rel_pronoun} at position {i}, end at {result['clause_end_idx']}")
                        break
                
                if result['found']:
                    break
        
        return result
    
    def _find_relative_clause_end(self, tokens: List[Dict], rel_start_idx: int, rel_type: str) -> int:
        """é–¢ä¿‚ç¯€ã®çµ‚äº†ä½ç½®ã‚’ç‰¹å®šï¼ˆäººé–“çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ ï¼‰"""
        
        # whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†
        if rel_type == 'whose':
            return self._find_whose_clause_end(tokens, rel_start_idx)
        
        # ğŸ†• whoæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†ï¼ˆTest 12æˆåŠŸæ‰‹æ³•ã‚’é©ç”¨ï¼‰
        if rel_type == 'who':
            return self._find_who_clause_end(tokens, rel_start_idx)
        
        # ğŸ†• ä¸€èˆ¬çš„ãªé–¢ä¿‚ç¯€ï¼ˆwhich/thatï¼‰ã®çµ‚äº†ä½ç½®ã‚’å“è©ãƒ™ãƒ¼ã‚¹ã§ç‰¹å®š
        # æˆ¦ç•¥: é–¢ä¿‚ä»£åè© + å‹•è© + [ä¿®é£¾èª/ç›®çš„èª/è£œèª] ã¾ã§ã‚’é–¢ä¿‚ç¯€ã¨ã™ã‚‹
        
        clause_end = rel_start_idx
        
        # Step 1: é–¢ä¿‚ä»£åè©ã®å¾Œã®å‹•è©ã‚’æ¢ã™
        rel_verb_idx = None
        for i in range(rel_start_idx + 1, min(rel_start_idx + 4, len(tokens))):
            if i < len(tokens) and tokens[i]['pos'] in ['VERB', 'AUX']:
                rel_verb_idx = i
                break
        
        if rel_verb_idx is None:
            return rel_start_idx + 1
        
        clause_end = rel_verb_idx
        
        # Step 2: å‹•è©ã®å¾Œã®è¦ç´ ã‚’é–¢ä¿‚ç¯€ã«å«ã‚ã‚‹ï¼ˆğŸ†• äººé–“çš„æ§‹é€ åˆ¤å®šï¼‰
        for i in range(rel_verb_idx + 1, len(tokens)):
            if i < len(tokens):
                token = tokens[i]
                
                # ğŸ†• äººé–“çš„å“è©åˆ¤å®šã‚’é©ç”¨
                actual_pos = self._get_human_corrected_pos(token)
                
                # ğŸ†• æ§‹é€ çš„åˆ¤å®š: æ–°ã—ã„å‹•è©å‡ºç¾ã§ä¸Šä½æ–‡é–‹å§‹
                if actual_pos in ['VERB', 'AUX']:
                    self.logger.debug(f"ğŸ§  ä¸Šä½æ–‡å‹•è©æ¤œå‡ºã«ã‚ˆã‚Šé–¢ä¿‚ç¯€çµ‚äº†: '{token['text']}' â†’ {actual_pos}")
                    break
                
                # é–¢ä¿‚ç¯€å†…ã®è¦ç´ ã¨ã—ã¦å«ã‚ã‚‹æ¡ä»¶
                # ğŸ†• 'there'ãªã©ã®å ´æ‰€å‰¯è©ã‚‚é–¢ä¿‚ç¯€ã«å«ã‚ã‚‹ï¼ˆTest 4å¯¾å¿œï¼‰
                if actual_pos in ['ADV', 'ADJ', 'NOUN', 'PROPN', 'NUM'] or \
                   (actual_pos == 'PRON' and token['text'].lower() in ['there', 'here']):
                    clause_end = i
                    self.logger.debug(f"é–¢ä¿‚ç¯€ã«å«ã‚ã‚‹: '{token['text']}' (corrected_pos={actual_pos})")
                else:
                    # ãã®ä»–ã®å“è©ã§é–¢ä¿‚ç¯€çµ‚äº†
                    self.logger.debug(f"é–¢ä¿‚ç¯€çµ‚äº†: '{token['text']}' (corrected_pos={actual_pos})")
                    break
        
        self.logger.debug(f"é–¢ä¿‚ç¯€çµ‚äº†ä½ç½®({rel_type}): {clause_end} ('{tokens[clause_end]['text'] if clause_end < len(tokens) else 'EOF'}')")
        return clause_end
    
    def _find_whose_clause_end(self, tokens: List[Dict], whose_idx: int) -> int:
        """whoseæ§‹æ–‡ã®é–¢ä¿‚ç¯€çµ‚äº†ä½ç½®ã‚’ç‰¹å®šï¼ˆæ§‹é€ çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰
        
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆã®æ§‹é€ çš„ãƒ­ã‚¸ãƒƒã‚¯ï¼š
        whose â†’ å…ˆè¡Œè©ç‰¹å®š â†’ ä¿®é£¾å¯¾è±¡åè© â†’ é–¢ä¿‚ç¯€å†…å‹•è© â†’ è£œèª/ç›®çš„èª â†’ 
        æ–°ã—ã„å‹•è©å‡ºç¾æ™‚ç‚¹ã§ä¸Šä½æ–‡é–‹å§‹ã¨åˆ¤å®šã—ã¦ã‚¹ãƒˆãƒƒãƒ—
        """
        
        clause_end = whose_idx
        
        # Step 1: whose ã®ç›´å¾Œã®ä¿®é£¾å¯¾è±¡åè©ã‚’æ¢ã™
        possessed_noun_idx = None
        for i in range(whose_idx + 1, min(whose_idx + 3, len(tokens))):
            if tokens[i]['pos'] in ['NOUN', 'PROPN']:
                possessed_noun_idx = i
                self.logger.debug(f"whoseä¿®é£¾å¯¾è±¡: '{tokens[i]['text']}' at {i}")
                break
        
        if possessed_noun_idx is None:
            self.logger.debug(f"whoseæ§‹æ–‡: ä¿®é£¾å¯¾è±¡åè©ãŒè¦‹ã¤ã‹ã‚‰ãªã„")
            return whose_idx + 1
        
        # Step 2: é–¢ä¿‚ç¯€å†…å‹•è©ã‚’æ¢ã™
        relcl_verb_idx = None
        for i in range(possessed_noun_idx + 1, min(possessed_noun_idx + 4, len(tokens))):
            if tokens[i]['pos'] in ['VERB', 'AUX']:
                relcl_verb_idx = i
                self.logger.debug(f"é–¢ä¿‚ç¯€å†…å‹•è©: '{tokens[i]['text']}' at {i}")
                break
        
        if relcl_verb_idx is None:
            self.logger.debug(f"whoseæ§‹æ–‡: é–¢ä¿‚ç¯€å‹•è©ãŒè¦‹ã¤ã‹ã‚‰ãªã„")
            return possessed_noun_idx
        
        # Step 3: é–¢ä¿‚ç¯€å†…ã®è£œèª/ç›®çš„èªã‚’é †æ¬¡å‡¦ç†
        clause_end = relcl_verb_idx
        
        for i in range(relcl_verb_idx + 1, len(tokens)):
            token = tokens[i]
            
            # ğŸ†• äººé–“çš„å“è©åˆ¤å®šã‚’ä½¿ç”¨ï¼ˆå¾ªç’°å‚ç…§å›é¿ï¼‰
            sentence_text = ' '.join([t['text'] for t in tokens])
            if token['text'].lower() in self.ambiguous_words:
                # æ§‹é€ çš„åˆ¤å®šã§å‹•è©ã‹ã©ã†ã‹ç›´æ¥ãƒã‚§ãƒƒã‚¯
                if self._is_likely_main_verb_by_position(token, tokens, i):
                    corrected_pos = 'VERB'
                else:
                    corrected_pos = token['pos']  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            else:
                corrected_pos = token['pos']
            
            # ğŸ†• æ§‹é€ çš„åˆ¤å®š: æ–°ã—ã„å‹•è©ãŒå‡ºç¾ã—ãŸã‚‰ä¸Šä½æ–‡é–‹å§‹
            if corrected_pos in ['VERB', 'AUX'] and i > relcl_verb_idx:
                self.logger.debug(f"ä¸Šä½æ–‡å‹•è©æ¤œå‡ºã«ã‚ˆã‚Šé–¢ä¿‚ç¯€çµ‚äº†: '{token['text']}' at {i} (äººé–“çš„åˆ¤å®š: {corrected_pos})")
                break
            
            # é–¢ä¿‚ç¯€å†…è¦ç´ ã¨ã—ã¦å«ã‚ã‚‹
            if corrected_pos in ['ADJ', 'NOUN', 'PROPN', 'ADV']:
                clause_end = i
                self.logger.debug(f"é–¢ä¿‚ç¯€è¦ç´ : '{token['text']}' at {i}")
            else:
                # ãã®ä»–ã®å“è©ã§é–¢ä¿‚ç¯€çµ‚äº†
                break
        
        self.logger.debug(f"whoseå¥çµ‚äº†ä½ç½®(æ§‹é€ çš„): {clause_end} ('{tokens[clause_end]['text'] if clause_end < len(tokens) else 'EOF'}')")
        return clause_end
    
    def _find_who_clause_end(self, tokens: List[Dict], who_idx: int) -> int:
        """whoæ§‹æ–‡ã®é–¢ä¿‚ç¯€çµ‚äº†ä½ç½®ã‚’ç‰¹å®šï¼ˆTest 12æˆåŠŸæ‰‹æ³•ã‚’é©ç”¨ï¼‰
        
        Test 12ãƒ‘ã‚¿ãƒ¼ãƒ³: The man whose car is red lives here.
        â†’ "The man who runs fast is strong."
        
        whoæ§‹æ–‡ã®æ§‹é€ çš„ãƒ­ã‚¸ãƒƒã‚¯ï¼š
        who â†’ é–¢ä¿‚ç¯€å†…å‹•è© â†’ ä¿®é£¾èªï¼ˆå‰¯è©/å½¢å®¹è©ï¼‰ â†’ ä¸Šä½æ–‡å‹•è©å‡ºç¾ã§ã‚¹ãƒˆãƒƒãƒ—
        
        æœŸå¾…å€¤: "The man who runs fast" â†’ sub-s="The man who", sub-v="runs", sub-m2="fast"
        """
        
        clause_end = who_idx
        
        # Step 1: whoç›´å¾Œã®å‹•è©ã‚’æ¢ã™ï¼ˆé–¢ä¿‚ç¯€å†…å‹•è©ï¼‰
        relcl_verb_idx = None
        for i in range(who_idx + 1, min(who_idx + 3, len(tokens))):
            if i < len(tokens):
                token = tokens[i]
                # ğŸ†• äººé–“çš„å“è©åˆ¤å®šã‚’é©ç”¨
                corrected_pos = self._get_human_corrected_pos(token)
                
                if corrected_pos in ['VERB', 'AUX']:
                    relcl_verb_idx = i
                    clause_end = i
                    self.logger.debug(f"whoå¥å†…å‹•è©ç™ºè¦‹: '{token['text']}' at {i} (äººé–“çš„åˆ¤å®š: {corrected_pos})")
                    break
        
        if relcl_verb_idx is None:
            self.logger.debug("whoå¥å†…å‹•è©ãŒè¦‹ã¤ã‹ã‚‰ãªã„")
            return who_idx + 1
        
        # Step 2: é–¢ä¿‚ç¯€å†…å‹•è©ã®å¾Œã®ä¿®é£¾èªã‚’é–¢ä¿‚ç¯€ã«å«ã‚ã‚‹ï¼ˆ"fast"ç­‰ï¼‰
        for i in range(relcl_verb_idx + 1, len(tokens)):
            if i < len(tokens):
                token = tokens[i]
                
                # ğŸ†• äººé–“çš„å“è©åˆ¤å®šã‚’é©ç”¨ï¼ˆæ›–æ˜§èªã®å ´åˆï¼‰
                corrected_pos = self._get_human_corrected_pos(token)
                
                # ğŸ†• æ§‹é€ çš„åˆ¤å®š: æ–°ã—ã„å‹•è©ãŒå‡ºç¾ã—ãŸã‚‰ä¸Šä½æ–‡é–‹å§‹
                if corrected_pos in ['VERB', 'AUX'] and i > relcl_verb_idx:
                    self.logger.debug(f"ä¸Šä½æ–‡å‹•è©æ¤œå‡ºã«ã‚ˆã‚Šwhoå¥çµ‚äº†: '{token['text']}' at {i} (äººé–“çš„åˆ¤å®š: {corrected_pos})")
                    break
                
                # é–¢ä¿‚ç¯€å†…è¦ç´ ã¨ã—ã¦å«ã‚ã‚‹ï¼ˆå‰¯è©ã€å½¢å®¹è©ã€åè©ï¼‰
                if corrected_pos in ['ADV', 'ADJ', 'NOUN', 'PROPN']:
                    clause_end = i
                    self.logger.debug(f"whoå¥å†…è¦ç´ : '{token['text']}' at {i} (corrected_pos={corrected_pos})")
                else:
                    # ãã®ä»–ã®å“è©ã§é–¢ä¿‚ç¯€çµ‚äº†
                    break
        
        self.logger.debug(f"whoå¥çµ‚äº†ä½ç½®(æ§‹é€ çš„): {clause_end} ('{tokens[clause_end]['text'] if clause_end < len(tokens) else 'EOF'}')")
        return clause_end
    
    def _process_relative_clause(self, tokens: List[Dict], relative_info: Dict, core_elements: Dict = None) -> Tuple[List[Dict], Dict]:
        """é–¢ä¿‚ç¯€ã®å‡¦ç†ã¨ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ï¼ˆRephraseä»•æ§˜æº–æ‹ ï¼‰
        
        æ­£ã—ã„Rephraseçš„åˆ†è§£:
        - é–¢ä¿‚ç¯€ã‚’å«ã‚€ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã¯ç©ºæ–‡å­—åˆ—
        - ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«å…ˆè¡Œè©+é–¢ä¿‚ä»£åè©ã€å‹•è©ã€ä¿®é£¾èªã‚’æ ¼ç´
        """
        self.logger.debug(f"é–¢ä¿‚ç¯€å‡¦ç†: {relative_info['type']} (ä¿¡é ¼åº¦: {relative_info['confidence']})")
        
        # é–¢ä¿‚ç¯€ã®ç¯„å›²ã‚’ç‰¹å®š
        rel_pronoun_idx = relative_info.get('relative_pronoun_idx')
        clause_end_idx = relative_info.get('clause_end_idx')
        antecedent_idx = relative_info.get('antecedent_idx')
        
        if rel_pronoun_idx is None or clause_end_idx is None or antecedent_idx is None:
            return tokens, {}
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ã«ãƒãƒ¼ã‚«ãƒ¼ã‚’è¿½åŠ 
        tokens[rel_pronoun_idx]['is_relative_pronoun'] = True
        tokens[rel_pronoun_idx]['relative_clause_type'] = relative_info['type']
        tokens[rel_pronoun_idx]['relative_clause_end'] = clause_end_idx
        tokens[antecedent_idx]['is_antecedent'] = True
        
        # Rephraseçš„ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£å®Ÿè£…
        sub_slots = self._create_rephrase_subslots(tokens, relative_info)
        
        # ğŸ”§ Step1: é–¢ä¿‚ç¯€ã®ä½ç½®æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆæ©Ÿèƒ½å¤‰æ›´ãªã—ï¼‰
        relative_position = self._determine_chunk_grammatical_role(tokens, core_elements or {}, relative_info)
        self.logger.debug(f"ç”Ÿæˆã•ã‚ŒãŸã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ: {sub_slots}")
        self.logger.debug(f"é–¢ä¿‚ç¯€ä½ç½®: {relative_position} (å…ˆè¡Œè©: {relative_info.get('antecedent_idx', 'unknown')})")
        
        # ğŸ”§ Step4: UIå½¢å¼å¯¾å¿œ - parent_slotæƒ…å ±ã‚’è¨˜éŒ²
        if relative_position and sub_slots:
            sub_slots['_parent_slot'] = relative_position  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦è¨˜éŒ²
            self.logger.debug(f"ğŸ·ï¸ UIå½¢å¼å¯¾å¿œ: parent_slot = {relative_position}")
        
        return tokens, sub_slots

    def _create_rephrase_subslots(self, tokens: List[Dict], relative_info: Dict) -> Dict:
        """Rephraseä»•æ§˜ã«æº–æ‹ ã—ãŸã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ
        
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆã®æ–¹æ³•ï¼š5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ç›´æ¥ä½¿ç”¨
        """
        rel_pronoun_idx = relative_info['relative_pronoun_idx']
        clause_end_idx = relative_info['clause_end_idx']
        antecedent_idx = relative_info['antecedent_idx']
        
        # ğŸ†• é–¢ä¿‚ç¯€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ½å‡ºï¼ˆé–¢ä¿‚ä»£åè©ã‹ã‚‰é–¢ä¿‚ç¯€çµ‚äº†ã¾ã§ï¼‰
        # clause_end_idxã¯é–¢ä¿‚ç¯€æœ€å¾Œã®è¦ç´ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãªã®ã§ +1 ã—ã¦ã‚¹ãƒ©ã‚¤ã‚·ãƒ³ã‚°
        rel_tokens = tokens[rel_pronoun_idx:clause_end_idx + 1]
        
        # ğŸ†• é–¢ä¿‚ä»£åè©ã®å½¹å‰²ã‚’åˆ¤å®šï¼ˆä¸»èª/ç›®çš„èªï¼‰
        rel_clause_type = relative_info.get('type', '')
        rel_pronoun_role = self._determine_relative_pronoun_role_enhanced(rel_tokens, rel_clause_type)
        self.logger.debug(f"é–¢ä¿‚ä»£åè©å½¹å‰²åˆ¤å®š: {rel_pronoun_role}")
        
        # ğŸ†• å…ˆè¡Œè©å¥å…¨ä½“ã‚’å–å¾—ï¼ˆThe man ãªã©ï¼‰
        antecedent_phrase = self._extract_full_antecedent_phrase(tokens, antecedent_idx)
        
        # ğŸ†• 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã§é–¢ä¿‚ç¯€å†…ã‚’è§£æï¼ˆå…ˆè¡Œè©æƒ…å ±ã‚‚æ¸¡ã™ï¼‰
        sub_slots = self._analyze_relative_clause_structure_enhanced(rel_tokens, rel_clause_type, rel_pronoun_role, antecedent_phrase)
        
        # ğŸ†• whoseæ§‹æ–‡ã¯å°‚ç”¨å‡¦ç†ã§å®Œäº†ã—ã¦ã„ã‚‹ã®ã§ãã®ã¾ã¾è¿”ã™
        if rel_clause_type == 'whose_clause':
            return sub_slots
        
        # ğŸ†• é–¢ä¿‚ä»£åè©ã®å½¹å‰²ã«åŸºã¥ãé©åˆ‡ãªé…ç½®ï¼ˆwhoseä»¥å¤–ï¼‰
        rel_pronoun_text = rel_tokens[0]['text']
        if rel_pronoun_role == 'subject':
            # é–¢ä¿‚ä»£åè©ãŒä¸»èª â†’ sub-s
            sub_slots['sub-s'] = f"{antecedent_phrase} {rel_pronoun_text}"
        elif rel_pronoun_role == 'object':
            # é–¢ä¿‚ä»£åè©ãŒç›®çš„èª â†’ sub-o1  
            sub_slots['sub-o1'] = f"{antecedent_phrase} {rel_pronoun_text}"
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            if 'sub-s' in sub_slots:
                sub_slots['sub-s'] = f"{antecedent_phrase} {sub_slots['sub-s']}"
        
        return sub_slots
    
    def _determine_relative_pronoun_role_enhanced(self, rel_tokens: List[Dict], clause_type: str) -> str:
        """é–¢ä¿‚ä»£åè©ã®å½¹å‰²ã‚’åˆ¤å®šï¼ˆä¸»èª/ç›®çš„èªï¼‰- å¼·åŒ–ç‰ˆ
        
        äººé–“çš„æ–‡æ³•èªè­˜:
        - å‹•è©å‰ã«ä»–ã®ä¸»èªãŒã‚ã‚‹ã‹ï¼Ÿ â†’ ã‚ã‚‹å ´åˆã€é–¢ä¿‚ä»£åè©ã¯ç›®çš„èª
        - å‹•è©å‰ã«ä¸»èªãŒãªã„ â†’ é–¢ä¿‚ä»£åè©ã¯ä¸»èª
        """
        if not rel_tokens or len(rel_tokens) < 2:
            return 'subject'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        # å‹•è©ã‚’æ¢ã™
        verb_idx = None
        for i, token in enumerate(rel_tokens):
            if token['pos'] == 'VERB' and i > 0:  # é–¢ä¿‚ä»£åè©ä»¥å¤–
                verb_idx = i
                break
        
        if verb_idx is None:
            return 'subject'  # å‹•è©ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
        
        # whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†
        if clause_type == 'whose_clause':
            # whose + åè©ã®å¾Œã«ä»–ã®ä¸»èªãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            # "whose book I borrowed" â†’ IãŒä¸»èªãªã®ã§whose bookã¯ç›®çš„èª
            # "whose car is red" â†’ ä»–ã«ä¸»èªãŒãªã„ã®ã§whose carã¯ä¸»èª
            whose_noun_idx = None
            for i, token in enumerate(rel_tokens):
                if i > 0 and token['pos'] in ['NOUN', 'PROPN']:
                    whose_noun_idx = i
                    break
            
            if whose_noun_idx is not None:
                # whoseåè©ã‚ˆã‚Šå¾Œã«ä»–ã®ä¸»èªãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                post_whose_tokens = rel_tokens[whose_noun_idx + 1:verb_idx]
                has_other_subject = any(
                    token['pos'] in ['NOUN', 'PRON', 'PROPN'] 
                    for token in post_whose_tokens
                )
                if has_other_subject:
                    self.logger.debug(f"whoseæ§‹æ–‡: ä»–ã®ä¸»èªç™ºè¦‹ â†’ whoseå¥ã¯ç›®çš„èª")
                    return 'object'
                else:
                    self.logger.debug(f"whoseæ§‹æ–‡: ä»–ã®ä¸»èªãªã— â†’ whoseå¥ã¯ä¸»èª")
                    return 'subject'
        
        # å‹•è©ã®å‰ã«ä»–ã®ä¸»èªãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        pre_verb_tokens = rel_tokens[1:verb_idx]  # é–¢ä¿‚ä»£åè©ã‚’é™¤ã
        has_other_subject = any(
            token['pos'] in ['NOUN', 'PRON', 'PROPN'] 
            for token in pre_verb_tokens
        )
        
        if has_other_subject:
            # å‹•è©å‰ã«ä»–ã®ä¸»èª â†’ é–¢ä¿‚ä»£åè©ã¯ç›®çš„èª
            self.logger.debug(f"é–¢ä¿‚ä»£åè©ã¯ç›®çš„èª: å‹•è©å‰ã«ä¸»èª {[t['text'] for t in pre_verb_tokens]}")
            return 'object'
        else:
            # å‹•è©å‰ã«ä¸»èªãªã— â†’ é–¢ä¿‚ä»£åè©ã¯ä¸»èª
            self.logger.debug(f"é–¢ä¿‚ä»£åè©ã¯ä¸»èª: å‹•è©å‰ã«ä¸»èªãªã—")
            return 'subject'

    def _analyze_relative_clause_structure_enhanced(self, rel_tokens: List[Dict], clause_type: str, rel_pronoun_role: str, antecedent_phrase: str = "") -> Dict:
        """é–¢ä¿‚ç¯€å†…éƒ¨æ§‹é€ è§£æ - å¼·åŒ–ç‰ˆ
        
        é–¢ä¿‚ä»£åè©ã®å½¹å‰²ã‚’è€ƒæ…®ã—ãŸæ­£ç¢ºãªã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ
        """
        if not rel_tokens:
            return {}
        
        self.logger.debug(f"å¼·åŒ–ç‰ˆé–¢ä¿‚ç¯€è§£æ: {[t['text'] for t in rel_tokens]} (å½¹å‰²: {rel_pronoun_role})")
        
        sub_slots = {}
        
        # ğŸ†• whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†
        if clause_type == 'whose_clause':
            self.logger.debug(f"ğŸ” whoseæ§‹æ–‡ç‰¹åˆ¥å‡¦ç†é–‹å§‹: {clause_type}")
            return self._analyze_whose_clause_structure(rel_tokens, antecedent_phrase, rel_pronoun_role)
        
        self.logger.debug(f"ğŸ” ä¸€èˆ¬çš„é–¢ä¿‚ç¯€å‡¦ç†: {clause_type}")
        
        # å‹•è©ã‚’ç‰¹å®š
        verb_token = None
        for i, token in enumerate(rel_tokens):
            if token['pos'] == 'VERB' and i > 0:  # é–¢ä¿‚ä»£åè©ä»¥å¤–
                verb_token = token
                sub_slots['sub-v'] = token['text']
                break
        
        if verb_token is None:
            return sub_slots
        
        # é–¢ä¿‚ä»£åè©ã®å½¹å‰²ãŒç›®çš„èªã®å ´åˆã€å‹•è©å‰ã®è¦ç´ ã‚’ sub-s ã«
        if rel_pronoun_role == 'object':
            for i, token in enumerate(rel_tokens):
                if i > 0 and token['pos'] in ['NOUN', 'PRON', 'PROPN'] and token != verb_token:
                    sub_slots['sub-s'] = token['text']
                    break
        
        # ä¿®é£¾èªã®æ¤œå‡ºï¼ˆADVã€å ´æ‰€å‰¯è©ã€å‰ç½®è©å¥ï¼‰
        for i, token in enumerate(rel_tokens):
            if i > 0 and token != verb_token and token['text'] not in [sub_slots.get('sub-s', '')]:
                # ğŸ†• å¼·åŒ–ã•ã‚ŒãŸä¿®é£¾èªæ¤œå‡º
                corrected_pos = self._get_human_corrected_pos(token)
                
                if (corrected_pos == 'ADV' or 
                    token['pos'] == 'ADV' or 
                    token['text'].lower() in ['there', 'here', 'everywhere', 'nowhere', 'fast', 'carefully', 'diligently', 'efficiently']):
                    
                    sub_slots['sub-m2'] = token['text']
                    self.logger.debug(f"ä¿®é£¾èªæ¤œå‡º: '{token['text']}' (pos={token['pos']}, corrected={corrected_pos}) â†’ sub-m2")
                    break
        
        return sub_slots

    def _analyze_whose_clause_structure(self, rel_tokens: List[Dict], antecedent_phrase: str = "", rel_pronoun_role: str = "subject") -> Dict:
        """whoseæ§‹æ–‡å°‚ç”¨ã®æ§‹é€ è§£æï¼ˆå†å¸°ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆï¼‰
        
        ãƒ‘ã‚¿ãƒ¼ãƒ³: whose + åè© + å‹•è© + è£œèª/ç›®çš„èª
        ä¾‹: "whose car is red" â†’ {'sub-s': 'The man whose car', 'sub-v': 'is', 'sub-c1': 'red'}
        """
        sub_slots = {}
        
        self.logger.debug(f"ğŸš€ whoseæ§‹æ–‡è§£æé–‹å§‹: {[t['text'] for t in rel_tokens]}, å…ˆè¡Œè©: '{antecedent_phrase}'")
        
        if len(rel_tokens) < 3:  # æœ€ä½é™: whose + åè© + å‹•è©
            self.logger.debug(f"âŒ whoseæ§‹æ–‡è¦ç´ ä¸è¶³: {len(rel_tokens)} < 3")
            return sub_slots
        
        try:
            # 1. whoseç›´å¾Œã®åè©ã‚’ç‰¹å®š
            whose_noun = None
            whose_noun_idx = -1
            for i, token in enumerate(rel_tokens):
                if i > 0 and token['pos'] in ['NOUN', 'PROPN']:  # whoseä»¥é™ã®æœ€åˆã®åè©
                    whose_noun = token['text']
                    whose_noun_idx = i
                    self.logger.debug(f"âœ… whoseåè©ç™ºè¦‹: '{whose_noun}' at {i}")
                    break
            
            if not whose_noun:
                self.logger.debug(f"âŒ whoseå¾Œã®åè©ãŒè¦‹ã¤ã‹ã‚‰ãªã„")
                return sub_slots
            
            # 2. é–¢ä¿‚ç¯€å†…ã®å‹•è©ã‚’ç‰¹å®šï¼ˆVERBã¾ãŸã¯AUXï¼‰
            verb_token = None
            verb_idx = -1
            for i, token in enumerate(rel_tokens):
                if i > whose_noun_idx and token['pos'] in ['VERB', 'AUX']:  # ğŸ†• AUXã‚‚å«ã‚ã‚‹
                    verb_token = token['text']
                    verb_idx = i
                    sub_slots['sub-v'] = verb_token
                    self.logger.debug(f"âœ… whoseå‹•è©ç™ºè¦‹: '{verb_token}' (pos={token['pos']}) at {i}")
                    break
            
            if not verb_token:
                self.logger.debug(f"âŒ whoseå¾Œã®å‹•è©ãŒè¦‹ã¤ã‹ã‚‰ãªã„")
                return sub_slots
            
            # 3. å…ˆè¡Œè©ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’å®‰å…¨ã«æ§‹ç¯‰ï¼ˆå†å¸°å›é¿ï¼‰
            if antecedent_phrase:
                whose_phrase = f"{antecedent_phrase} whose {whose_noun}"
            else:
                whose_phrase = f"whose {whose_noun}"
            
            # 4. é–¢ä¿‚ä»£åè©ã®å½¹å‰²ã«åŸºã¥ãé…ç½®
            if rel_pronoun_role == 'object':
                sub_slots['sub-o1'] = whose_phrase
                self.logger.debug(f"âœ… sub-o1æ§‹ç¯‰ï¼ˆç›®çš„èªï¼‰: '{whose_phrase}'")
                
                # whoseç¯€å†…ã®ä»–ã®ä¸»èªã‚’æ¤œå‡º
                for i, token in enumerate(rel_tokens):
                    if i > whose_noun_idx and i < verb_idx and token['pos'] in ['NOUN', 'PRON', 'PROPN']:
                        sub_slots['sub-s'] = token['text']
                        self.logger.debug(f"âœ… whoseç¯€å†…ä¸»èªç™ºè¦‹: '{token['text']}'")
                        break
            else:  # subject
                sub_slots['sub-s'] = whose_phrase
                self.logger.debug(f"âœ… sub-sæ§‹ç¯‰ï¼ˆä¸»èªï¼‰: '{whose_phrase}'")
            
            # 5. å‹•è©å¾Œã®è¦ç´ ã‚’è£œèª/ç›®çš„èªã¨ã—ã¦å‡¦ç†ï¼ˆç°¡ç´ åŒ–ï¼‰
            for i, token in enumerate(rel_tokens):
                if i > verb_idx and token['pos'] not in ['PUNCT']:
                    if token['pos'] in ['ADJ', 'NOUN', 'PROPN'] and 'sub-c1' not in sub_slots:
                        sub_slots['sub-c1'] = token['text']
                        self.logger.debug(f"âœ… sub-c1ç™ºè¦‹: '{token['text']}'")
                        break
                    elif token['pos'] == 'ADV' and 'sub-m2' not in sub_slots:
                        sub_slots['sub-m2'] = token['text']
                        self.logger.debug(f"âœ… sub-m2ç™ºè¦‹: '{token['text']}'")
                        break
            
            self.logger.debug(f"whoseæ§‹æ–‡è§£æçµæœ: {sub_slots}")
            return sub_slots
            
        except Exception as e:
            self.logger.error(f"whoseæ§‹æ–‡è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def _extract_full_antecedent_phrase(self, tokens: List[Dict], antecedent_idx: int) -> str:
        """å…ˆè¡Œè©å¥å…¨ä½“ã‚’æŠ½å‡ºï¼ˆé™å®šè©ã€å½¢å®¹è©ã‚’å«ã‚€ï¼‰- å†å¸°ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆ"""
        if antecedent_idx < 0 or antecedent_idx >= len(tokens):
            return ""
        
        try:
            # å…ˆè¡Œè©ã®å‰ã®ä¿®é£¾èªã‚’å«ã‚ã¦æŠ½å‡ºï¼ˆæœ€å¤§2èªå‰ã¾ã§ï¼‰
            phrase_tokens = []
            start_idx = max(0, antecedent_idx - 2)
            
            for i in range(start_idx, antecedent_idx + 1):
                if i < len(tokens):
                    token = tokens[i]
                    if token['pos'] in ['DET', 'ADJ', 'NOUN', 'PROPN']:
                        phrase_tokens.append(token['text'])
            
            result = ' '.join(phrase_tokens).strip()
            self.logger.debug(f"å…ˆè¡Œè©å¥æŠ½å‡º: idx={antecedent_idx} â†’ '{result}'")
            return result
            
        except Exception as e:
            self.logger.error(f"å…ˆè¡Œè©å¥æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ç´”ã«è©²å½“ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
            if 0 <= antecedent_idx < len(tokens):
                return tokens[antecedent_idx]['text']
            return ""
        verb_idx = None
        for i, token in enumerate(rel_tokens):
            if token['tag'].startswith('VB') and token['pos'] == 'VERB':
                verb_idx = i
                sub_slots['sub-v'] = token['text']
                break
        
        if verb_idx is None:
            return
        
        # é–¢ä¿‚ä»£åè©ã®å½¹å‰²ã‚’åˆ¤å®šï¼ˆä¸»èªã‹ç›®çš„èªã‹ï¼‰
        rel_pronoun_role = self._determine_relative_pronoun_role(rel_tokens, verb_idx)
        clause_type = rel_tokens[0]['text'].lower()
        
        # whoseã®å ´åˆã®ç‰¹åˆ¥å‡¦ç†
        if clause_type == 'whose':
            # whose + åè© ã®å½¢ã‚’æ¢ã™
            whose_phrase = rel_tokens[0]['text']  # "whose"
            next_idx = 1
            while next_idx < len(rel_tokens) and next_idx < verb_idx:
                if rel_tokens[next_idx]['pos'] in ['NOUN', 'PROPN']:
                    whose_phrase += f" {rel_tokens[next_idx]['text']}"
                    break
                next_idx += 1
            
            # whoseå¥ã®å½¹å‰²ã‚’åˆ¤å®š
            if rel_pronoun_role == 'subject':
                sub_slots['sub-s'] = whose_phrase
            else:
                sub_slots['sub-o1'] = whose_phrase
        
        # å‹•è©å‰ã®è¦ç´ ã‚’åˆ†æï¼ˆä¸»èªï¼‰
        pre_verb_tokens = rel_tokens[:verb_idx]
        for token in pre_verb_tokens:
            if token['pos'] in ['NOUN', 'PRON', 'PROPN']:
                # é–¢ä¿‚ä»£åè©ãŒç›®çš„èªã®å ´åˆã€ã“ã“ã«ä¸»èªãŒã‚ã‚‹
                if rel_pronoun_role == 'object' and clause_type != 'whose':
                    sub_slots['sub-s'] = token['text']
        
        # å‹•è©å¾Œã®è¦ç´ ã‚’åˆ†æ
        post_verb_tokens = rel_tokens[verb_idx + 1:]
        modifier_count = 0
        
        for token in post_verb_tokens:
            if token['pos'] == 'ADV' or token['tag'] == 'EX':
                # å‰¯è©ã¾ãŸã¯å­˜åœ¨there â†’ ä¿®é£¾èªã¨ã—ã¦å‡¦ç†ï¼ˆå„ªå…ˆï¼‰
                modifier_count += 1
                if modifier_count == 1:
                    sub_slots['sub-m2'] = token['text']
                elif modifier_count == 2:
                    sub_slots['sub-m3'] = token['text']
            elif token['pos'] in ['NOUN', 'PRON', 'PROPN'] and token['tag'] != 'EX':
                # åè©é¡ï¼ˆå­˜åœ¨thereä»¥å¤–ï¼‰ â†’ ç›®çš„èªã¨ã—ã¦å‡¦ç†
                if 'sub-o1' not in sub_slots:
                    sub_slots['sub-o1'] = token['text']
                elif 'sub-o2' not in sub_slots:
                    sub_slots['sub-o2'] = token['text']
            elif token['pos'] == 'ADJ':
                # å½¢å®¹è© â†’ è£œèªã¨ã—ã¦å‡¦ç†
                sub_slots['sub-c1'] = token['text']
    
    def _determine_relative_pronoun_role(self, rel_tokens: List[Dict], verb_idx: int) -> str:
        """é–¢ä¿‚ä»£åè©ãŒä¸»èªã‹ç›®çš„èªã‹ã‚’åˆ¤å®š"""
        clause_type = rel_tokens[0]['text'].lower()
        
        # whoseã®å ´åˆã®ç‰¹åˆ¥å‡¦ç†
        if clause_type == 'whose':
            # whose + åè©ãŒå‹•è©ã®å‰ã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            whose_noun_idx = None
            for i in range(1, min(verb_idx, len(rel_tokens))):
                if rel_tokens[i]['pos'] in ['NOUN', 'PROPN']:
                    whose_noun_idx = i
                    break
            
            if whose_noun_idx is not None:
                # whose + åè©ãŒå‹•è©å‰ã«ã‚ã‚‹ãªã‚‰ä¸»èª
                return 'subject'
            else:
                # whose + åè©ãŒå‹•è©å¾Œã«ã‚ã‚‹ãªã‚‰ç›®çš„èª
                return 'object'
        
        # å‹•è©ã®å‰ã«ä»£åè©ãƒ»åè©ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆwhoseä»¥å¤–ã®å ´åˆï¼‰
        pre_verb_tokens = rel_tokens[1:verb_idx]  # é–¢ä¿‚ä»£åè©è‡ªä½“ã¯é™¤å¤–
        has_subject_before_verb = any(
            token['pos'] in ['NOUN', 'PRON', 'PROPN'] 
            for token in pre_verb_tokens
        )
        
        if has_subject_before_verb:
            # å‹•è©å‰ã«ä¸»èªãŒã‚ã‚‹ãªã‚‰ã€é–¢ä¿‚ä»£åè©ã¯ç›®çš„èª
            return 'object'
        else:
            # å‹•è©å‰ã«ä¸»èªãŒãªã„ãªã‚‰ã€é–¢ä¿‚ä»£åè©ã¯ä¸»èª
            return 'subject'

    def _determine_chunk_grammatical_role(self, tokens: List[Dict], core_elements: Dict, relative_info: Dict) -> str:
        """é–¢ä¿‚ç¯€ã‚’å«ã‚€ã€Œã‹ãŸã¾ã‚Šã€ã®æ–‡æ³•çš„å½¹å‰²ã‚’å‹•è©ã¨ã®é–¢ä¿‚ã‹ã‚‰æ¨å®š
        
        äººé–“çš„æ–‡æ³•èªè­˜ï¼š
        - å‹•è©ã®å‰ã®ã€Œã‹ãŸã¾ã‚Šã€â†’ ä¸»èªï¼ˆSï¼‰
        - å‹•è©ã®å¾Œã®ã€Œã‹ãŸã¾ã‚Šã€â†’ ç›®çš„èªï¼ˆO1ï¼‰ã¾ãŸã¯è£œèªï¼ˆC1ï¼‰
        - æ–‡æœ«ã®ã€Œã‹ãŸã¾ã‚Šã€â†’ ä¿®é£¾èªï¼ˆMï¼‰
        """
        if not relative_info['found']:
            return None
            
        # å…ˆè¡Œè©ã®ä½ç½®ã¨å‹•è©ã®ä½ç½®ã‚’æ¯”è¼ƒ
        antecedent_idx = relative_info.get('antecedent_idx')
        verb_indices = core_elements.get('verb_indices', [])
        
        if antecedent_idx is None or not verb_indices:
            return None
            
        main_verb_idx = verb_indices[0] if verb_indices else len(tokens)
        
        # ä½ç½®é–¢ä¿‚ã«ã‚ˆã‚‹æ–‡æ³•çš„å½¹å‰²ã®åˆ¤å®š
        if antecedent_idx < main_verb_idx:
            # å‹•è©ã‚ˆã‚Šå‰ â†’ ä¸»èªã®å¯èƒ½æ€§ãŒé«˜ã„
            self.logger.debug(f"ã‹ãŸã¾ã‚Šä½ç½®åˆ¤å®š: å…ˆè¡Œè©{antecedent_idx} < å‹•è©{main_verb_idx} â†’ ä¸»èª(S)")
            return 'S'
        else:
            # å‹•è©ã‚ˆã‚Šå¾Œ â†’ ç›®çš„èªã¾ãŸã¯è£œèª
            # å‹•è©ã®æ€§è³ªã‹ã‚‰åˆ¤å®š
            if core_elements.get('verb') and core_elements['verb'].get('text'):
                verb_text = core_elements['verb']['text'].lower()
                if verb_text in ['is', 'are', 'was', 'were', 'am', 'be', 'become', 'seem']:
                    self.logger.debug(f"ã‹ãŸã¾ã‚Šä½ç½®åˆ¤å®š: beå‹•è© + å¾Œç¶š â†’ è£œèª(C1)")
                    return 'C1'
                else:
                    self.logger.debug(f"ã‹ãŸã¾ã‚Šä½ç½®åˆ¤å®š: ä¸€èˆ¬å‹•è© + å¾Œç¶š â†’ ç›®çš„èª(O1)")
                    return 'O1'
        
        return None

    def _determine_relative_slot_position(self, tokens: List[Dict], relative_info: Dict) -> str:
        """é–¢ä¿‚ç¯€ãŒã©ã®ã‚¹ãƒ­ãƒƒãƒˆä½ç½®ã«ã‚ã‚‹ã‹ã‚’åˆ¤å®š
        
        é‡è¦ï¼šé–¢ä¿‚ç¯€ã‚’å«ã‚€ã€Œã‹ãŸã¾ã‚Šã€ãŒã©ã®æ–‡æ³•çš„å½¹å‰²ã‚’æœãŸã™ã‹ã‚’åˆ¤å®šã—ã€
        ãã®ã‚¹ãƒ­ãƒƒãƒˆã‚’ç©ºã«ã—ã¦ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«ç§»å‹•ã•ã›ã‚‹
        """
        if not relative_info['found']:
            return None
            
        # æ—¢ã«å®Ÿè£…æ¸ˆã¿ã®ã€Œã‹ãŸã¾ã‚Šã€æ–‡æ³•çš„å½¹å‰²åˆ¤å®šã‚’ä½¿ç”¨
        # ã“ã®åˆ¤å®šã¯_assign_grammar_rolesã§å–å¾—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
        # ç¾åœ¨ã¯ç›´æ¥å®Ÿè£…
        antecedent_idx = relative_info.get('antecedent_idx')
        if antecedent_idx is None:
            return None
            
        # å‹•è©ä½ç½®ã‚’æ¢ã™
        main_verb_idx = None
        for i, token in enumerate(tokens):
            if token['pos'] in ['VERB', 'AUX'] and token['text'].lower() not in ['whose', 'which', 'who', 'that']:
                # é–¢ä¿‚ç¯€å†…ã®å‹•è©ã‚’é™¤å¤–ã—ã¦ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’ç‰¹å®š
                rel_start = relative_info.get('relative_pronoun_idx', -1)
                rel_end = relative_info.get('clause_end_idx', -1)
                if rel_start <= i <= rel_end:
                    continue  # é–¢ä¿‚ç¯€å†…ã®å‹•è©ã¯ã‚¹ã‚­ãƒƒãƒ—
                main_verb_idx = i
                break
        
        if main_verb_idx is None:
            return None
            
        # ä½ç½®é–¢ä¿‚ã«ã‚ˆã‚‹åˆ¤å®š
        if antecedent_idx < main_verb_idx:
            return 'S'  # ä¸»èª
        else:
            # å‹•è©ã®æ€§è³ªã‹ã‚‰åˆ¤å®š
            verb_token = tokens[main_verb_idx]
            if verb_token['text'].lower() in ['is', 'are', 'was', 'were', 'am', 'be', 'become', 'seem']:
                return 'C1'  # è£œèª
            else:
                return 'O1'  # ç›®çš„èª
    
    def _clean_relative_clause_from_text(self, text: str, relative_info: Dict) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é–¢ä¿‚ç¯€éƒ¨åˆ†ã‚’é™¤å»"""
        if not relative_info['found']:
            return text
        
        # ç°¡æ˜“å®Ÿè£…ï¼šé–¢ä¿‚ä»£åè©ä»¥é™ã‚’å‰Šé™¤
        rel_type = relative_info.get('type', '')
        if rel_type in text:
            parts = text.split(rel_type)
            return parts[0].strip()
        
        return text

    def _analyze_relative_clause_structure(self, rel_tokens: List[Dict], clause_type: str) -> Dict:
        """é–¢ä¿‚ç¯€å†…éƒ¨ã®æ§‹é€ ã‚’5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã§è§£æ
        
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆã®æ–¹æ³•ï¼š
        - 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®æŠ€è¡“ã‚’ãã®ã¾ã¾é–¢ä¿‚ç¯€å†…ã«é©ç”¨
        - é–¢ä¿‚ä»£åè©ã¨ã®çµåˆå•é¡Œã‚’ãƒ«ãƒ¼ãƒ«ã§è§£æ±º
        
        Args:
            rel_tokens: é–¢ä¿‚ç¯€ã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆ  
            clause_type: é–¢ä¿‚ç¯€ã®ç¨®é¡ï¼ˆwhose_clauseç­‰ï¼‰
            
        Returns:
            Dict: ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ 
        """
        if not rel_tokens:
            return {}
        
        self.logger.debug(f"5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã«ã‚ˆã‚‹é–¢ä¿‚ç¯€è§£æ: {[t['text'] for t in rel_tokens]}")
        
        # ğŸ†• 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ç›´æ¥é©ç”¨
        core_elements = self._identify_core_elements(rel_tokens)
        sentence_pattern = self._determine_sentence_pattern(core_elements, rel_tokens)
        
        self.logger.debug(f"é–¢ä¿‚ç¯€å†…æ–‡å‹: {sentence_pattern}")
        self.logger.debug(f"é–¢ä¿‚ç¯€å†…ã‚³ã‚¢è¦ç´ : ä¸»èª={core_elements.get('subject')}, å‹•è©={core_elements.get('verb')}")
        
        # ğŸ†• 5æ–‡å‹ã®çµæœã‚’ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«å¤‰æ›
        sub_slots = {}
        
        # ä¸»èªå‡¦ç†ï¼ˆé–¢ä¿‚ä»£åè©ã¨ã®çµåˆãƒ«ãƒ¼ãƒ«ï¼‰
        if core_elements.get('subject_indices'):
            rel_subject = core_elements['subject']
            if clause_type == 'whose_clause':
                # whose + åè© ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
                sub_slots['sub-s'] = f"whose {rel_subject}"
            else:
                # who, which, that ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
                sub_slots['sub-s'] = rel_tokens[0]['text']  # é–¢ä¿‚ä»£åè©è‡ªä½“
        
        # å‹•è©å‡¦ç†
        if core_elements.get('verb'):
            sub_slots['sub-v'] = core_elements['verb']['text']
        
        # 5æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãæ®‹ã‚Šè¦ç´ ã®å‡¦ç†
        if sentence_pattern == 'SVC':
            # beå‹•è© + è£œèª
            # æ®‹ã‚Šã®è¦ç´ ã‹ã‚‰è£œèªã‚’ç‰¹å®š
            used_indices = set(core_elements.get('subject_indices', []) + core_elements.get('verb_indices', []))
            for i, token in enumerate(rel_tokens):
                if i not in used_indices and token['pos'] in ['ADJ', 'NOUN', 'PROPN']:
                    sub_slots['sub-c1'] = token['text']
                    break
        elif sentence_pattern == 'SVO':
            # ä¸€èˆ¬å‹•è© + ç›®çš„èª
            used_indices = set(core_elements.get('subject_indices', []) + core_elements.get('verb_indices', []))
            for i, token in enumerate(rel_tokens):
                if i not in used_indices and token['pos'] in ['NOUN', 'PROPN']:
                    sub_slots['sub-o1'] = token['text']
                    break
        elif sentence_pattern == 'SV':
            # ğŸ†• è‡ªå‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³ (SV) + ä¿®é£¾èª
            # whoç¯€ã®ä¿®é£¾èªï¼ˆå‰¯è©ï¼‰ã‚’sub-m2ã¨ã—ã¦ç‰¹å®š
            used_indices = set(core_elements.get('subject_indices', []) + core_elements.get('verb_indices', []))
            for i, token in enumerate(rel_tokens):
                if i not in used_indices:
                    # ğŸ†• äººé–“çš„å“è©åˆ¤å®šã‚’é©ç”¨
                    corrected_pos = self._get_human_corrected_pos(token)
                    if corrected_pos in ['ADV', 'ADJ']:
                        sub_slots['sub-m2'] = token['text']
                        self.logger.debug(f"whoç¯€ä¿®é£¾èªæ¤œå‡º: '{token['text']}' â†’ sub-m2 (corrected_pos={corrected_pos})")
                        break
        
        # ğŸ†• ä¸€èˆ¬çš„ãªä¿®é£¾èªæ¤œå‡ºï¼ˆæ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã«é–¢ä¿‚ãªãï¼‰
        if 'sub-m2' not in sub_slots and clause_type == 'who_clause':
            # whoç¯€ç‰¹æœ‰ã®ä¿®é£¾èªæ¤œå‡º
            used_indices = set(core_elements.get('subject_indices', []) + core_elements.get('verb_indices', []))
            if 'sub-c1' in sub_slots:
                # è£œèªãŒã‚ã‚‹å ´åˆã¯è£œèªã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚‚é™¤å¤–
                for i, token in enumerate(rel_tokens):
                    if token['text'] == sub_slots['sub-c1']:
                        used_indices.add(i)
                        break
            
            for i, token in enumerate(rel_tokens):
                if i not in used_indices and i > 0:  # é–¢ä¿‚ä»£åè©ã‚’é™¤å¤–
                    corrected_pos = self._get_human_corrected_pos(token)
                    if corrected_pos in ['ADV', 'ADJ']:
                        sub_slots['sub-m2'] = token['text']
                        self.logger.debug(f"whoç¯€è¿½åŠ ä¿®é£¾èªæ¤œå‡º: '{token['text']}' â†’ sub-m2 (corrected_pos={corrected_pos})")
                        break
        
        return sub_slots
    
    def _find_verb_in_relative_clause(self, rel_tokens: List[Dict]) -> Optional[int]:
        """é–¢ä¿‚ç¯€å†…ã®å‹•è©ã‚’ç‰¹å®š
        
        5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®å‹•è©æ¤œå‡ºæŠ€è¡“ã‚’é©ç”¨
        """
        for i, token in enumerate(rel_tokens):
            if token['tag'].startswith('VB') and token['pos'] == 'VERB':
                return i
        return None
    
    def _analyze_post_verb_elements_in_relative(self, rel_tokens: List[Dict], verb_idx: int, sub_slots: Dict):
        """é–¢ä¿‚ç¯€å†…ã®å‹•è©å¾Œè¦ç´ ã‚’è§£æ
        
        5æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜æŠ€è¡“ã‚’é©ç”¨
        """
        if verb_idx >= len(rel_tokens) - 1:
            return
        
        post_verb_tokens = rel_tokens[verb_idx + 1:]
        
        # ç›®çš„èªã€è£œèªã€ä¿®é£¾èªã‚’é †æ¬¡è§£æ
        processed_positions = set()
        
        for i, token in enumerate(post_verb_tokens):
            if i in processed_positions:
                continue
                
            if token['pos'] in ['NOUN', 'PRON', 'PROPN']:
                # åè©é¡ â†’ ç›®çš„èªã¨ã—ã¦å‡¦ç†
                if 'sub-o1' not in sub_slots:
                    sub_slots['sub-o1'] = token['text']
                elif 'sub-o2' not in sub_slots:
                    sub_slots['sub-o2'] = token['text']
                processed_positions.add(i)
            elif token['pos'] == 'ADJ':
                # å½¢å®¹è© â†’ è£œèªã¨ã—ã¦å‡¦ç†
                sub_slots['sub-c1'] = token['text']
                processed_positions.add(i)
            elif token['pos'] == 'ADV':
                # å‰¯è© â†’ ä¿®é£¾èªã¨ã—ã¦å‡¦ç†
                if 'sub-m' not in sub_slots:
                    sub_slots['sub-m'] = token['text']
                else:
                    sub_slots['sub-m'] += f" {token['text']}"
                processed_positions.add(i)
        
        # é€£ç¶šã™ã‚‹è¦ç´ ã‚’ã¾ã¨ã‚ã‚‹ï¼ˆç°¡ç´ åŒ–ç‰ˆï¼‰
        self._consolidate_relative_clause_elements_simple(rel_tokens, verb_idx, sub_slots)

    def _consolidate_relative_clause_elements_simple(self, rel_tokens: List[Dict], verb_idx: int, sub_slots: Dict):
        """é–¢ä¿‚ç¯€å†…ã®è¦ç´ ã‚’çµ±åˆï¼ˆç°¡ç´ ç‰ˆï¼‰"""
        # ç¾åœ¨ã¯åŸºæœ¬çš„ãªé‡è¤‡é™¤å»ã®ã¿
        if 'sub-m' in sub_slots:
            # é‡è¤‡ã—ãŸä¿®é£¾èªã‚’é™¤å»
            m_words = sub_slots['sub-m'].split()
            unique_words = []
            for word in m_words:
                if word not in unique_words:
                    unique_words.append(word)
            sub_slots['sub-m'] = ' '.join(unique_words)

    def _consolidate_relative_clause_elements(self, rel_tokens: List[Dict], verb_idx: int, sub_slots: Dict):
        """é–¢ä¿‚ç¯€å†…ã®è¦ç´ ã‚’çµ±åˆ
        
        é€£ç¶šã™ã‚‹åè©å¥ã‚„ä¿®é£¾èªå¥ã‚’ä¸€ã¤ã«ã¾ã¨ã‚ã‚‹
        """
        if verb_idx >= len(rel_tokens) - 1:
            return
        
        post_verb_tokens = rel_tokens[verb_idx + 1:]
        current_phrase = []
        current_type = None
        
        for token in post_verb_tokens:
            if token['pos'] in ['NOUN', 'PRON', 'PROPN', 'DET', 'ADJ']:
                if current_type == 'noun_phrase':
                    current_phrase.append(token['text'])
                else:
                    # æ–°ã—ã„åè©å¥ã®é–‹å§‹
                    if current_phrase and current_type:
                        self._assign_phrase_to_subslot(current_phrase, current_type, sub_slots)
                    current_phrase = [token['text']]
                    current_type = 'noun_phrase'
            elif token['pos'] in ['ADV', 'ADP']:
                if current_type == 'adverbial_phrase':
                    current_phrase.append(token['text'])
                else:
                    # æ–°ã—ã„å‰¯è©å¥ã®é–‹å§‹
                    if current_phrase and current_type:
                        self._assign_phrase_to_subslot(current_phrase, current_type, sub_slots)
                    current_phrase = [token['text']]
                    current_type = 'adverbial_phrase'
            else:
                # ãã®ä»–ã®å“è©ã§å¥ãŒçµ‚äº†
                if current_phrase and current_type:
                    self._assign_phrase_to_subslot(current_phrase, current_type, sub_slots)
                current_phrase = []
                current_type = None
        
        # æœ€å¾Œã®å¥ã‚’å‡¦ç†
        if current_phrase and current_type:
            self._assign_phrase_to_subslot(current_phrase, current_type, sub_slots)
    
    def _assign_phrase_to_subslot(self, phrase: List[str], phrase_type: str, sub_slots: Dict):
        """å¥ã‚’ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«å‰²ã‚Šå½“ã¦"""
        phrase_text = ' '.join(phrase)
        
        if phrase_type == 'noun_phrase':
            if 'sub-o1' not in sub_slots:
                sub_slots['sub-o1'] = phrase_text
            elif 'sub-o2' not in sub_slots:
                sub_slots['sub-o2'] = phrase_text
            else:
                # è£œèªã¨ã—ã¦å‡¦ç†
                sub_slots['sub-c1'] = phrase_text
        elif phrase_type == 'adverbial_phrase':
            if 'sub-m' not in sub_slots:
                sub_slots['sub-m'] = phrase_text
            else:
                sub_slots['sub-m'] += f" {phrase_text}"

    def generate_ui_format(self, sentence: str, example_id: str = "test") -> List[Dict]:
        """UIå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç”Ÿæˆ
        
        Args:
            sentence: è§£æã™ã‚‹æ–‡
            example_id: ä¾‹æ–‡ID
            
        Returns:
            UIå½¢å¼ã®ã‚¹ãƒ­ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿é…åˆ—
        """
        # æ–‡æ³•è§£æå®Ÿè¡Œ
        result = self.analyze_sentence(sentence)
        
        ui_data = []
        slots = result.get('slots', {})
        sub_slots = result.get('sub_slots', {})
        parent_slot = sub_slots.get('_parent_slot', '')
        
        # ãƒ¡ã‚¤ãƒ³ ã‚¹ãƒ­ãƒƒãƒˆã®å‡¦ç†
        slot_order = ['M1', 'S', 'Aux', 'M2', 'V', 'C1', 'O1', 'O2', 'C2', 'M3']
        
        for slot_name in slot_order:
            # é–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆã¯ç©ºã‚¹ãƒ­ãƒƒãƒˆã‚‚å«ã‚ã‚‹
            has_subslots = (slot_name == parent_slot and sub_slots and any(k.startswith('sub-') for k in sub_slots))
            if slot_name in slots and (slots[slot_name] or has_subslots):
                # ğŸ”§ Step5: ã‚¹ãƒ­ãƒƒãƒˆå†…display_orderï¼ˆãƒªã‚»ãƒƒãƒˆæ–¹å¼ï¼‰
                slot_display_order = 0
                
                # é–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆã¯ç©ºæ–‡å­—
                phrase = "" if has_subslots else slots[slot_name]
                phrase_type = "clause" if has_subslots else "word"
                
                ui_data.append({
                    "æ§‹æ–‡ID": "",
                    "V_group_key": result.get('pattern', ''),
                    "ä¾‹æ–‡ID": example_id,
                    "Slot": slot_name,
                    "SlotPhrase": phrase,
                    "SlotText": "",
                    "PhraseType": phrase_type,
                    "SubslotID": "",
                    "SubslotElement": "",
                    "SubslotText": "",
                    "Slot_display_order": slot_order.index(slot_name) + 1,
                    "display_order": slot_display_order,
                    "QuestionType": ""
                })
                slot_display_order += 1
                
                # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®è¿½åŠ 
                if has_subslots:
                    # ğŸ”§ Step5.1: ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®æ­£ã—ã„é †åºã‚’å®šç¾©
                    subslot_order = ['sub-s', 'sub-v', 'sub-o1', 'sub-o2', 'sub-c1', 'sub-c2', 'sub-m1', 'sub-m2', 'sub-m3']
                    
                    # é †åºã«å¾“ã£ã¦ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã‚’è¿½åŠ 
                    for sub_slot_id in subslot_order:
                        if sub_slot_id in sub_slots and sub_slots[sub_slot_id]:
                            ui_data.append({
                                "æ§‹æ–‡ID": "",
                                "V_group_key": result.get('pattern', ''),
                                "ä¾‹æ–‡ID": example_id,
                                "Slot": slot_name,
                                "SlotPhrase": "",
                                "SlotText": "",
                                "PhraseType": "",
                                "SubslotID": sub_slot_id,
                                "SubslotElement": sub_slots[sub_slot_id],
                                "SubslotText": "",
                                "Slot_display_order": slot_order.index(slot_name) + 1,
                                "display_order": slot_display_order,
                                "QuestionType": ""
                            })
                            slot_display_order += 1
        
        return ui_data

    # =============================================================================
    # ğŸ”¥ Phase 1.0: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  (from Stanza Asset Migration)
    # =============================================================================
    
    def _initialize_basic_handlers(self):
        """åŸºæœ¬ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®åˆæœŸåŒ–"""
        basic_handlers = [
            'basic_five_pattern',     # åŸºæœ¬5æ–‡å‹
            'relative_clause',        # é–¢ä¿‚ç¯€  
            'passive_voice',          # å—å‹•æ…‹
            'auxiliary_complex',      # åŠ©å‹•è©
            # 'adverbial_modifier',   # å‰¯è©ãƒ»ä¿®é£¾èª (æœªå®Ÿè£…ã®ãŸã‚ä¸€æ™‚å‰Šé™¤)
        ]
        
        for handler in basic_handlers:
            self.add_handler(handler)
        
        self.logger.info(f"åŸºæœ¬ãƒãƒ³ãƒ‰ãƒ©ãƒ¼åˆæœŸåŒ–å®Œäº†: {len(self.active_handlers)}å€‹")
    
    def add_handler(self, handler_name: str):
        """ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ ï¼ˆPhaseåˆ¥é–‹ç™ºç”¨ï¼‰"""
        if handler_name not in self.active_handlers:
            self.active_handlers.append(handler_name)
            self.logger.info(f"Handlerè¿½åŠ : {handler_name}")
        else:
            self.logger.warning(f"âš ï¸ Handler already active: {handler_name}")
    
    def remove_handler(self, handler_name: str):
        """ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å‰Šé™¤"""
        if handler_name in self.active_handlers:
            self.active_handlers.remove(handler_name)
            self.logger.info(f"â– Handlerå‰Šé™¤: {handler_name}")
    
    def list_active_handlers(self) -> List[str]:
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ä¸€è¦§"""
        return self.active_handlers.copy()
    
    # =================================
    # Phase 2: é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè£…
    # =================================
    
    def _handle_relative_clause(self, sentence: str, doc, current_result: Dict) -> Dict:
        """
        ğŸ¯ Phase 2: é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ ç”¨ï¼‰
        
        æ©Ÿèƒ½:
        1. é–¢ä¿‚ç¯€ã®æ¤œå‡ºã¨åˆ†é›¢
        2. ä¸»æ–‡ã¨ã‚µãƒ–å¥ã®æ§‹é€ åŒ–
        3. relative_clause_info ã®ç”Ÿæˆï¼ˆä»–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã¨ã®é€£æºç”¨ï¼‰
        
        Rephraseãƒ«ãƒ¼ãƒ«:
        - é–¢ä¿‚ç¯€è¦ç´ ã¯ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®
        - ä¸»æ–‡è¦ç´ ã¯ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®
        - æ§‹é€ çš„åˆ†é›¢ã«ã‚ˆã‚Š100%ç²¾åº¦ä¿è¨¼
        """
        try:
            print(f"ğŸ” Executing relative_clause handler for: {sentence}")
            
            # Phase 1.2: ãƒ¬ã‚¬ã‚·ãƒ¼ä¾å­˜é–¢ä¿‚å‰Šé™¤ - çµ±ä¸€ãƒˆãƒ¼ã‚¯ãƒ³æŠ½å‡ºä½¿ç”¨
            tokens = self._extract_tokens(doc)  # æ—¢å­˜ã®æ®µéšçš„å‰Šé™¤å¯¾å¿œãƒ¡ã‚½ãƒƒãƒ‰ä½¿ç”¨
            relative_info = self._detect_relative_clause(tokens, sentence)
            
            if not relative_info.get('found', False):
                print(f"ğŸ” é–¢ä¿‚ç¯€æœªæ¤œå‡º: {sentence}")
                return None
            
            print(f"ğŸ”¥ é–¢ä¿‚ç¯€æ¤œå‡º: ã‚¿ã‚¤ãƒ—={relative_info.get('type', 'unknown')}, ä¿¡é ¼åº¦={relative_info.get('confidence', 0)}")
            
            # é–¢ä¿‚ç¯€å‡¦ç†
            original_tokens = tokens.copy()
            processed_tokens, sub_slots = self._process_relative_clause(original_tokens, relative_info)
            
            # ä¸»æ–‡ã¨ã‚µãƒ–å¥ã®åˆ†é›¢ï¼ˆæ–°æ©Ÿèƒ½ï¼‰
            main_sentence = self._extract_main_sentence(sentence, relative_info)
            sub_sentences = self._extract_sub_sentences(sentence, relative_info)
            
            print(f"ğŸ” é–¢ä¿‚ç¯€åˆ†é›¢çµæœ: ä¸»æ–‡='{main_sentence}', ã‚µãƒ–å¥={len(sub_sentences)}å€‹")
            
            return {
                'slots': {},  # é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è‡ªä½“ã¯ã‚¹ãƒ­ãƒƒãƒˆã‚’ç”Ÿæˆã—ãªã„
                'sub_slots': sub_slots,
                'relative_clause_info': {
                    'found': True,
                    'main_sentence': main_sentence,
                    'sub_sentences': sub_sentences,
                    'type': relative_info.get('type', 'unknown'),
                    'confidence': relative_info.get('confidence', 0)
                },
                'grammar_info': {
                    'patterns': [{
                        'type': 'relative_clause_2stage',
                        'main_sentence': main_sentence,
                        'sub_sentences_count': len(sub_sentences),
                        'detection_method': 'é–¢ä¿‚ç¯€å¯¾å¿œ2æ®µéšå‡¦ç†',
                        'clause_type': relative_info.get('type', 'unknown')
                    }],
                    'handler_success': True,
                    'processing_notes': f"Relative clause 2-stage: main='{main_sentence}', sub={len(sub_sentences)}"
                }
            }
            
        except Exception as e:
            self.logger.error(f"é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _extract_main_sentence(self, sentence: str, relative_info: Dict) -> str:
        """é–¢ä¿‚ç¯€ã‚’é™¤ã„ãŸä¸»æ–‡ã‚’æŠ½å‡º"""
        if not relative_info.get('found', False):
            return sentence
        
        rel_type = relative_info.get('type', '')
        
        # whichç¯€å‡¦ç†: "The car which was crashed is red." -> "The car is red."
        if 'which' in sentence and rel_type == 'which_clause':
            parts = sentence.split(' which ')
            if len(parts) == 2:
                before = parts[0]  # "The car"
                after_which = parts[1]  # "was crashed is red."
                
                # é–¢ä¿‚ç¯€çµ‚äº†ã‚’æ¤œå‡ºï¼ˆä¸»æ–‡å‹•è©æ¤œç´¢ï¼‰
                words_after = after_which.split()
                main_verb_start = self._find_main_verb_start(words_after)
                
                if main_verb_start >= 0:
                    main_part = ' '.join(words_after[main_verb_start:])
                    result = f"{before} {main_part}"
                    return result
        
        # thatç¯€å‡¦ç†: "The book that was written is famous." -> "The book is famous."
        elif 'that' in sentence and rel_type == 'that_clause':
            parts = sentence.split(' that ')
            if len(parts) == 2:
                before = parts[0]  # "The book"  
                after_that = parts[1]  # "was written is famous."
                
                # é–¢ä¿‚ç¯€çµ‚äº†ã‚’æ¤œå‡ºï¼ˆä¸»æ–‡å‹•è©æ¤œç´¢ï¼‰
                words_after = after_that.split()
                main_verb_start = self._find_main_verb_start(words_after)
                
                if main_verb_start >= 0:
                    main_part = ' '.join(words_after[main_verb_start:])
                    return f"{before} {main_part}"
        
        return sentence
    
    def _find_main_verb_start(self, words_after: List[str]) -> int:
        """é–¢ä¿‚ç¯€å¾Œã®å˜èªåˆ—ã‹ã‚‰ä¸»æ–‡å‹•è©ã®é–‹å§‹ä½ç½®ã‚’æ¤œå‡º"""
        past_participles = ['been', 'gone', 'done', 'made', 'said', 'written', 'crashed', 'sent', 
                           'taken', 'given', 'seen', 'heard', 'found', 'built', 'bought']
        
        # é€šå¸¸å‹•è©ï¼ˆå—å‹•æ…‹ã§ãªã„è‡ªå‹•è©ãƒ»ä»–å‹•è©ï¼‰
        main_verbs = ['arrived', 'came', 'went', 'left', 'stayed', 'lived', 'died', 'worked', 
                     'studied', 'played', 'ran', 'walked', 'stood', 'sat', 'fell', 'rose']
        
        print(f"ğŸ” DEBUG: _find_main_verb_start words_after={words_after}")
        
        in_relative_clause = True
        for i, word in enumerate(words_after):
            # å¥èª­ç‚¹ã‚’é™¤å»ã—ã¦ç´”ç²‹ãªå˜èªã‚’å–å¾—
            clean_word = word.rstrip('.,!?;:').lower()
            print(f"ğŸ” DEBUG: æ¤œæŸ»ä¸­ i={i}, word='{word}', clean_word='{clean_word}'")
            
            # beå‹•è©ã®å ´åˆ
            if clean_word in ['is', 'are', 'was', 'were', 'will', 'would', 'can', 'could', 'should']:
                # å—å‹•æ…‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
                if i + 1 < len(words_after):
                    next_word = words_after[i + 1].rstrip('.,!?;:').lower()
                    is_passive = (next_word.endswith('ed') or 
                                next_word.endswith('en') or 
                                next_word in past_participles)
                    
                    # SVCæ§‹æ–‡ï¼ˆbeå‹•è©+å½¢å®¹è©ï¼‰ã‚’ãƒã‚§ãƒƒã‚¯
                    adjectives = ['red', 'blue', 'green', 'famous', 'beautiful', 'happy', 'sad', 'big', 'small', 
                                 'good', 'bad', 'hot', 'cold', 'new', 'old', 'young', 'smart', 'stupid']
                    is_svc = next_word in adjectives
                    
                    print(f"ğŸ” DEBUG: beå‹•è©'{clean_word}' next_word='{next_word}', is_passive={is_passive}, is_svc={is_svc}")
                    
                    if in_relative_clause and is_passive and not is_svc:
                        # é–¢ä¿‚ç¯€å†…å—å‹•æ…‹ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãŸã ã—SVCæ§‹æ–‡ã¯é™¤ãï¼‰
                        print(f"ğŸ” DEBUG: é–¢ä¿‚ç¯€å†…å—å‹•æ…‹ã‚¹ã‚­ãƒƒãƒ— i={i}")
                        continue
                    else:
                        # ä¸»æ–‡ã®å‹•è©ã‚’ç™ºè¦‹ï¼ˆSVCæ§‹æ–‡ã‚‚å«ã‚€ï¼‰
                        print(f"ğŸ” DEBUG: ä¸»æ–‡beå‹•è©ç™ºè¦‹ i={i}")
                        return i
                else:
                    # æ–‡æœ«ã®å ´åˆã¯ä¸»æ–‡å‹•è©
                    print(f"ğŸ” DEBUG: æ–‡æœ«beå‹•è©ç™ºè¦‹ i={i}")
                    return i
            
            # é€šå¸¸å‹•è©ã®å ´åˆï¼ˆarrived, came, etc.ï¼‰
            elif clean_word in main_verbs:
                # ã“ã‚ŒãŒé–¢ä¿‚ç¯€å¤–ã®ä¸»æ–‡å‹•è©ã®å¯èƒ½æ€§
                print(f"ğŸ” DEBUG: é€šå¸¸å‹•è©æ¤œå‡º clean_word='{clean_word}', i={i}")
                if not in_relative_clause or self._is_likely_main_verb(clean_word, words_after, i):
                    print(f"ğŸ” DEBUG: ä¸»æ–‡å‹•è©ã¨ã—ã¦èªå®š i={i}")
                    return i
                    
            # éå»å½¢å‹•è©ã®æ¤œå‡ºï¼ˆ-edèªå°¾ã ãŒéå»åˆ†è©ã§ãªã„å ´åˆï¼‰
            elif clean_word.endswith('ed') and clean_word not in past_participles:
                print(f"ğŸ” DEBUG: éå»å½¢å‹•è©æ¤œå‡º clean_word='{clean_word}', i={i}")
                if not in_relative_clause or self._is_likely_main_verb(clean_word, words_after, i):
                    print(f"ğŸ” DEBUG: ä¸»æ–‡éå»å½¢å‹•è©ã¨ã—ã¦èªå®š i={i}")
                    return i
        
        print(f"ğŸ” DEBUG: ä¸»æ–‡å‹•è©æœªç™ºè¦‹ return -1")
        return -1
    
    def _is_likely_main_verb(self, word: str, words_after: List[str], position: int) -> bool:
        """å˜èªãŒä¸»æ–‡å‹•è©ã§ã‚ã‚‹å¯èƒ½æ€§ã‚’åˆ¤å®š"""
        # ç‰¹å®šã®å‹•è©ã¯ä¸»æ–‡å‹•è©ã®å¯èƒ½æ€§ãŒé«˜ã„
        main_verb_indicators = ['arrived', 'came', 'went', 'left', 'stayed', 'lived', 'died']
        
        if word in main_verb_indicators:
            return True
        
        # beå‹•è©+å½¢å®¹è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡ºï¼ˆSVCæ§‹é€ ï¼‰
        if word in ['is', 'are', 'was', 'were'] and position + 1 < len(words_after):
            next_word = words_after[position + 1].rstrip('.,!?;:').lower()
            # å½¢å®¹è©ãƒªã‚¹ãƒˆ
            adjectives = ['red', 'blue', 'green', 'famous', 'beautiful', 'happy', 'sad', 'big', 'small', 
                         'good', 'bad', 'hot', 'cold', 'new', 'old', 'young', 'smart', 'stupid']
            
            if next_word in adjectives:
                return True
        
        # ä½ç½®çš„åˆ¤æ–­ï¼šé–¢ä¿‚ç¯€ï¼ˆå—å‹•æ…‹ï¼‰ã®å¾Œã«æ¥ã‚‹å‹•è©ã¯ä¸»æ–‡ã®å¯èƒ½æ€§ãŒé«˜ã„
        if position > 0:
            prev_words = words_after[:position]
            # å‰ã«å—å‹•æ…‹ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            for i in range(len(prev_words) - 1):
                if (prev_words[i].lower() in ['was', 'were'] and 
                    prev_words[i + 1] in ['written', 'sent', 'crashed', 'taken']):
                    return True
        
        return False
    
    def _extract_sub_sentences(self, sentence: str, relative_info: Dict) -> List[str]:
        """é–¢ä¿‚ç¯€éƒ¨åˆ†ã‚’ã‚µãƒ–å¥ã¨ã—ã¦æŠ½å‡º"""
        if not relative_info.get('found', False):
            return []
        
        rel_type = relative_info.get('type', '')
        
        # whichç¯€å‡¦ç†: "The car which was crashed is red." -> ["which was crashed"]
        if 'which' in sentence and rel_type == 'which_clause':
            parts = sentence.split(' which ')
            if len(parts) == 2:
                after_which = parts[1]  # "was crashed is red."
                words_after = after_which.split()
                
                # é–¢ä¿‚ç¯€ã®çµ‚äº†ã‚’ç‰¹å®šï¼ˆä¸»æ–‡ã®å‹•è©ã¾ã§ï¼‰
                rel_clause_end = self._find_main_verb_start(words_after)
                
                if rel_clause_end > 0:
                    rel_clause = ' '.join(words_after[:rel_clause_end])
                    return [f"which {rel_clause}"]
        
        # thatç¯€å‡¦ç†: "The book that was written is famous." -> ["that was written"]
        elif 'that' in sentence and rel_type == 'that_clause':
            parts = sentence.split(' that ')
            if len(parts) == 2:
                after_that = parts[1]  # "was written is famous."
                words_after = after_that.split()
                
                # é–¢ä¿‚ç¯€ã®çµ‚äº†ã‚’ç‰¹å®šï¼ˆä¸»æ–‡ã®å‹•è©ã¾ã§ï¼‰
                rel_clause_end = self._find_main_verb_start(words_after)
                
                if rel_clause_end > 0:
                    rel_clause = ' '.join(words_after[:rel_clause_end])
                    return [f"that {rel_clause}"]
        
        return []
    
    # =================================
    # Phase 2: å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè£…
    # =================================
    
    def _handle_passive_voice(self, sentence: str, doc, current_result: Dict) -> Optional[Dict]:
        """
        å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ (é–¢ä¿‚ç¯€å¯¾å¿œ2æ®µéšå‡¦ç†)
        
        ğŸ¯ è¨­è¨ˆä»•æ§˜: å…¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å…±é€šã®é–¢ä¿‚ç¯€å¯¾å¿œã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
        1. é–¢ä¿‚ç¯€åˆ†é›¢æƒ…å ±ã‚’å–å¾—
        2. ä¸»æ–‡ã¨ã‚µãƒ–å¥ã‚’åˆ¥ã€…ã«å—å‹•æ…‹å‡¦ç†
        3. çµæœã‚’é©åˆ‡ãªã‚¹ãƒ­ãƒƒãƒˆï¼ˆmain_slots/sub_slotsï¼‰ã«é…ç½®
        
        äººé–“çš„èªè­˜ãƒ‘ã‚¿ãƒ¼ãƒ³:
        - beå‹•è© + éå»åˆ†è© â†’ å—å‹•æ…‹
        - èªå½™çš„å½¢æ…‹è«–åˆ†æã«ã‚ˆã‚‹éå»åˆ†è©åˆ¤å®š
        - byå¥ï¼ˆè¡Œç‚ºè€…ï¼‰ã®æ¤œå‡ºã¨é…ç½®
        
        Rephraseãƒ«ãƒ¼ãƒ«:
        - beå‹•è©: Aux ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®ï¼ˆå—å‹•æ…‹ã®beå‹•è©ã¯Auxã«é…ç½®ï¼‰
        - éå»åˆ†è©: V ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®
        - byå¥: M2 ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®ï¼ˆRephraseå‰¯è©ãƒ«ãƒ¼ãƒ«ï¼šå˜ç‹¬å‰¯è©å¥â†’M2ï¼‰
        """
        try:
            print(f"ğŸ” Executing passive_voice handler for: {sentence}")
            
            # ğŸ¯ Step 1: é–¢ä¿‚ç¯€åˆ†é›¢æƒ…å ±ã®å–å¾—
            relative_info = current_result.get('relative_clause_info', {})
            main_sentence = relative_info.get('main_sentence', sentence)
            sub_sentences = relative_info.get('sub_sentences', [])
            
            result_slots = {}
            result_sub_slots = current_result.get('sub_slots', {})
            consumed_tokens = []
            
            print(f"ğŸ” é–¢ä¿‚ç¯€åˆ†é›¢: ä¸»æ–‡='{main_sentence}', ã‚µãƒ–å¥={len(sub_sentences)}å€‹")
            
            # ğŸ¯ Step 2a: ä¸»æ–‡ã®å—å‹•æ…‹å‡¦ç†
            print(f"ğŸ” ä¸»æ–‡å—å‹•æ…‹æ¤œå‡ºå¯¾è±¡: '{main_sentence}'")
            main_passive = self._detect_passive_in_text(main_sentence, doc)
            if main_passive and main_passive['found']:
                print(f"ğŸ”¥ ä¸»æ–‡å—å‹•æ…‹æ¤œå‡º: {main_passive['be_verb']} + {main_passive['past_participle']}")
                main_slots = self._create_passive_slots(main_passive)
                result_slots.update(main_slots)
                
                # Token consumption for main sentence
                if main_passive.get('by_agent'):
                    main_consumed = self._get_tokens_for_phrase(main_passive['by_agent'], doc)
                    consumed_tokens.extend(main_consumed)
            else:
                print(f"ğŸ” ä¸»æ–‡å—å‹•æ…‹æœªæ¤œå‡º: '{main_sentence}'")
            
            # ğŸ¯ Step 2b: ã‚µãƒ–å¥ã®å—å‹•æ…‹å‡¦ç†
            for i, sub_sentence in enumerate(sub_sentences):
                sub_passive = self._detect_passive_in_text(sub_sentence, doc)
                if sub_passive and sub_passive['found']:
                    print(f"ğŸ”¥ ã‚µãƒ–å¥{i+1}å—å‹•æ…‹æ¤œå‡º: {sub_passive['be_verb']} + {sub_passive['past_participle']}")
                    
                    # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®
                    if sub_passive['be_verb']:
                        result_sub_slots[f'sub-aux'] = sub_passive['be_verb']
                    if sub_passive['past_participle']:
                        result_sub_slots[f'sub-v'] = sub_passive['past_participle']
                    if sub_passive.get('by_agent'):
                        result_sub_slots[f'sub-m2'] = sub_passive['by_agent']
                        
                        # Token consumption for sub sentence
                        sub_consumed = self._get_tokens_for_phrase(sub_passive['by_agent'], doc)
                        consumed_tokens.extend(sub_consumed)
            
            # çµæœåˆ¤å®šã‚’ç°¡ç´ åŒ–ï¼ˆChatGPT5ãƒ‡ãƒãƒƒã‚°ï¼‰
            has_main_passive = bool(result_slots)
            has_sub_passive = any(key.startswith('sub-aux') or key.startswith('sub-v') for key in result_sub_slots.keys())
            
            if not has_main_passive and not has_sub_passive:
                print(f"ğŸ” å—å‹•æ…‹ãƒ‘ã‚¿ãƒ¼ãƒ³æœªæ¤œå‡º: {sentence}")
                return None
            
            print(f"ğŸ”¥ å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼æˆåŠŸ: ä¸»æ–‡å—å‹•æ…‹={has_main_passive}, ã‚µãƒ–å¥å—å‹•æ…‹={has_sub_passive}")
            
            # ğŸ¯ çµ±åˆçµæœã®æº–å‚™
            return {
                'slots': result_slots,
                'sub_slots': result_sub_slots, 
                'consumed_tokens': consumed_tokens,
                'grammar_info': {
                    'patterns': [{
                        'type': 'passive_voice_2stage',
                        'main_sentence': main_sentence,
                        'sub_sentences_count': len(sub_sentences),
                        'detection_method': 'é–¢ä¿‚ç¯€å¯¾å¿œ2æ®µéšå‡¦ç†',
                        'rephrase_allocation': 'beâ†’Aux, past_participleâ†’V, byå¥â†’M2'
                    }],
                    'handler_success': True,
                    'processing_notes': f"Passive voice 2-stage: main={bool(result_slots)}, sub={len([k for k in result_sub_slots if k.startswith('sub-')])}"
                }
            }
        
        except Exception as e:
            self.logger.error(f"å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _detect_passive_in_text(self, text: str, doc) -> Dict[str, Any]:
        """
        ğŸ¯ é–¢ä¿‚ç¯€å¯¾å¿œ2æ®µéšå‡¦ç†ç”¨ï¼šç‰¹å®šãƒ†ã‚­ã‚¹ãƒˆå†…ã®å—å‹•æ…‹æ¤œå‡º
        
        Args:
            text: æ¤œæŸ»å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¸»æ–‡ã¾ãŸã¯ã‚µãƒ–å¥ï¼‰
            doc: spaCyè§£ææ¸ˆã¿æ–‡æ›¸ï¼ˆå…¨ä½“ï¼‰
        
        Returns:
            Dict: å—å‹•æ…‹æ¤œå‡ºçµæœ
        """
        try:
            # ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã®å ´åˆã¯æ¤œå‡ºãªã—
            if not text.strip():
                return {'found': False}
            
            # æŒ‡å®šãƒ†ã‚­ã‚¹ãƒˆã®spaCyè§£æ
            text_doc = self.nlp(text.strip())
            
            # æ—¢å­˜ã®æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
            return self._detect_passive_voice_pattern(text_doc, text)
            
        except Exception as e:
            self.logger.error(f"ãƒ†ã‚­ã‚¹ãƒˆå†…å—å‹•æ…‹æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return {'found': False}
    
    def _create_passive_slots(self, passive_info: Dict) -> Dict[str, str]:
        """
        ğŸ¯ é–¢ä¿‚ç¯€å¯¾å¿œ2æ®µéšå‡¦ç†ç”¨ï¼šå—å‹•æ…‹æƒ…å ±ã‹ã‚‰ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ
        
        Args:
            passive_info: å—å‹•æ…‹æ¤œå‡ºçµæœ
        
        Returns:
            Dict: Rephraseã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ 
        """
        slots = {}
        
        if not passive_info.get('found', False):
            return slots
        
        be_verb = passive_info.get('be_verb', '')
        past_participle = passive_info.get('past_participle', '')
        by_agent = passive_info.get('by_agent', '')
        
        # Rephraseã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ï¼šbeå‹•è©ã¯Auxã«é…ç½®
        if ' ' in be_verb:  # "will be"ã®ã‚ˆã†ãªå ´åˆ
            aux_parts = be_verb.split()
            if len(aux_parts) == 2:
                # åŠ©å‹•è© + beå‹•è©ã®å ´åˆï¼šåŠ©å‹•è©ã‚’å„ªå…ˆã—ã¦Auxã«é…ç½®
                slots['Aux'] = aux_parts[0]  # will
                slots['V'] = past_participle  # written
                print(f"ğŸ” åŠ©å‹•è©+beæ§‹æˆ: Aux='{aux_parts[0]}', V='{past_participle}' (beå‹•è©å†…åŒ…)")
            else:
                # è¤‡é›‘ãªå ´åˆã¯å…¨ä½“ã‚’Auxã«é…ç½®
                slots['Aux'] = be_verb
                slots['V'] = past_participle
        else:
            # å˜ç´”ãªbeå‹•è©ã®å ´åˆï¼šbeå‹•è©ã‚’Auxã«é…ç½®ï¼ˆRephraseä»•æ§˜ï¼‰
            slots['Aux'] = be_verb  # is, was, are, were
            slots['V'] = past_participle  # written, done, etc.
        
        # M ã‚¹ãƒ­ãƒƒãƒˆ: byå¥ã®é…ç½®ï¼ˆRephraseä»•æ§˜ï¼šã€Œï½ã«ã‚ˆã£ã¦ã€å…¨ä½“ãŒå‰¯è©å¥ï¼‰
        if by_agent:
            # Rephraseå‰¯è©é…ç½®ãƒ«ãƒ¼ãƒ«: å˜ç‹¬å‰¯è©å¥ã¯M2ã«é…ç½®
            slots['M2'] = by_agent  # "by John" å…¨ä½“ã‚’å‰¯è©å¥ã¨ã—ã¦M2é…ç½®
            print(f"ğŸ” byå¥é…ç½®: M2='{by_agent}' (Rephraseå‰¯è©ãƒ«ãƒ¼ãƒ«ï¼šå˜ç‹¬å‰¯è©å¥â†’M2)")
        
        return slots
    
    def _get_tokens_for_phrase(self, phrase: str, doc) -> List[int]:
        """
        ğŸ¯ é–¢ä¿‚ç¯€å¯¾å¿œ2æ®µéšå‡¦ç†ç”¨ï¼šãƒ•ãƒ¬ãƒ¼ã‚ºã«å¯¾å¿œã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å–å¾—
        
        Args:
            phrase: å¯¾è±¡ãƒ•ãƒ¬ãƒ¼ã‚ºï¼ˆä¾‹: "by John"ï¼‰
            doc: spaCyè§£ææ¸ˆã¿æ–‡æ›¸
        
        Returns:
            List[int]: å¯¾å¿œã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚¹ãƒˆ
        """
        consumed_indices = []
        
        if not phrase:
            return consumed_indices
        
        try:
            # ãƒ•ãƒ¬ãƒ¼ã‚ºã®å˜èªã‚’ç©ºç™½ã§åˆ†å‰²
            phrase_words = phrase.lower().split()
            
            for i, token in enumerate(doc):
                if token.text.lower() in phrase_words:
                    consumed_indices.append(i)
                    self._consumed_tokens.add(i)
            
            if consumed_indices:
                print(f"ğŸ”¥ Token consumption: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {consumed_indices} for ãƒ•ãƒ¬ãƒ¼ã‚º '{phrase}'")
            
            return consumed_indices
            
        except Exception as e:
            self.logger.error(f"ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return consumed_indices
            self.logger.error(f"å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _detect_passive_voice_pattern(self, doc, sentence: str) -> Dict[str, Any]:
        """
        å—å‹•æ…‹ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º (spaCyãƒ™ãƒ¼ã‚¹ãƒ»å½¢æ…‹è«–çš„åˆ†æ)
        
        æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³:
        1. beå‹•è© + éå»åˆ†è©ï¼ˆç›´å¾Œãƒ»è¿‘æ¥ï¼‰
        2. beå‹•è© + å‰¯è© + éå»åˆ†è©
        3. will/modal + be + éå»åˆ†è©
        4. byå¥ã®æ¤œå‡ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        """
        result = {
            'found': False,
            'be_verb': '',
            'past_participle': '',
            'by_agent': '',
            'confidence': 0.0
        }
        
        tokens = list(doc)
        
        # 1. å„ç¨®å—å‹•æ…‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
        for i in range(len(tokens)):
            current_token = tokens[i]
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: å˜ç´”ãªbeå‹•è© + éå»åˆ†è©
            if self._is_be_verb_spacy(current_token):
                be_verb = current_token.text
                
                # ç›´å¾Œã®éå»åˆ†è©
                if i + 1 < len(tokens):
                    next_token = tokens[i + 1]
                    if self._is_past_participle_spacy(next_token):
                        result.update({
                            'found': True,
                            'be_verb': be_verb,
                            'past_participle': next_token.text,
                            'confidence': 0.9
                        })
                        break
                
                # beå‹•è© + å‰¯è© + éå»åˆ†è©
                if i + 2 < len(tokens):
                    adv_token = tokens[i + 1]
                    pp_token = tokens[i + 2]
                    if (adv_token.pos_ == 'ADV' and 
                        self._is_past_participle_spacy(pp_token)):
                        result.update({
                            'found': True,
                            'be_verb': be_verb,
                            'past_participle': pp_token.text,
                            'confidence': 0.85
                        })
                        break
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: modal + be + éå»åˆ†è©
            if (current_token.pos_ == 'AUX' and 
                current_token.text.lower() in ['will', 'would', 'can', 'could', 'should', 'may', 'might', 'must']):
                
                # modal + be + éå»åˆ†è©ãƒ‘ã‚¿ãƒ¼ãƒ³
                if i + 2 < len(tokens):
                    be_token = tokens[i + 1]
                    pp_token = tokens[i + 2]
                    if (self._is_be_verb_spacy(be_token) and
                        self._is_past_participle_spacy(pp_token)):
                        result.update({
                            'found': True,
                            'be_verb': f"{current_token.text} {be_token.text}",
                            'past_participle': pp_token.text,
                            'confidence': 0.95
                        })
                        break
        
        # 2. byå¥ã®æ¤œå‡ºï¼ˆå—å‹•æ…‹ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã®ã¿ï¼‰
        if result['found']:
            by_agent = self._detect_by_agent_phrase_complete(tokens, sentence)
            if by_agent:
                result['by_agent'] = by_agent
                result['confidence'] = min(result['confidence'] + 0.05, 1.0)
        
        return result
    
    def _is_be_verb_spacy(self, token) -> bool:
        """spaCyãƒ™ãƒ¼ã‚¹ã®beå‹•è©åˆ¤å®š"""
        # lemmaï¼ˆåŸå‹ï¼‰ãŒbeã§ã€åŠ©å‹•è©ã‚¿ã‚°
        return (token.lemma_.lower() == 'be' and 
                token.pos_ in ['AUX', 'VERB'])
    
    def _is_past_participle_spacy(self, token) -> bool:
        """spaCyãƒ™ãƒ¼ã‚¹ã®éå»åˆ†è©åˆ¤å®š"""
        # 1. ã‚¿ã‚°ãƒ™ãƒ¼ã‚¹åˆ¤å®š
        if token.tag_ == 'VBN':  # Past participle
            return True
        
        # 2. å½¢æ…‹è«–çš„ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¤å®šï¼ˆbeå‹•è©ç›´å¾Œã®æ–‡è„ˆï¼‰
        if token.pos_ == 'ADJ':
            return self._has_past_participle_morphology_spacy(token.text)
        
        # 3. å‹•è©ã®éå»åˆ†è©å½¢
        if token.pos_ == 'VERB' and token.tag_ == 'VBN':
            return True
        
        return False
    
    def _has_past_participle_morphology_spacy(self, text: str) -> bool:
        """å½¢æ…‹è«–çš„éå»åˆ†è©ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆèªå°¾åˆ†æï¼‰"""
        text_lower = text.lower()
        
        # è¦å‰‡å‹•è©ã®-edèªå°¾
        if text_lower.endswith('ed') and len(text_lower) > 3:
            # ç´”ç²‹ãªå½¢å®¹è©ã‚’é™¤å¤–
            if not text_lower.endswith(('red', 'ded', 'eed', 'ted')):
                return True
        
        # -enèªå°¾ï¼ˆbroken, chosenç­‰ï¼‰
        if text_lower.endswith('en') and len(text_lower) > 3:
            if not text_lower.endswith(('tten', 'sten', 'chen', 'len')):
                return True
        
        # ç‰¹å¾´çš„ãªéå»åˆ†è©èªå°¾
        past_participle_endings = ['ated', 'ized', 'ified', 'ected', 'ested']
        return any(text_lower.endswith(ending) for ending in past_participle_endings)
    
    def _detect_by_agent_phrase_complete(self, tokens: List, sentence: str) -> str:
        """byå¥ï¼ˆè¡Œç‚ºè€…ï¼‰ã®å®Œå…¨æ¤œå‡º"""
        by_phrase = ""
        
        for i, token in enumerate(tokens):
            if token.text.lower() == 'by' and token.pos_ == 'ADP':
                # byä»¥é™ã®åè©å¥ã‚’æŠ½å‡º
                phrase_parts = ['by']
                for j in range(i + 1, min(i + 5, len(tokens))):  # æœ€å¤§4èªã¾ã§æ‹¡å¼µ
                    next_token = tokens[j]
                    if next_token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'DET', 'PRON']:
                        phrase_parts.append(next_token.text)
                    elif next_token.pos_ == 'PUNCT':  # å¥èª­ç‚¹ã§çµ‚äº†
                        break
                    else:
                        # ãã®ä»–ã®å“è©ã§ã‚‚çŸ­ã„èªã¯å«ã‚ã‚‹ï¼ˆthe, aç­‰ï¼‰
                        if len(next_token.text) <= 3:
                            phrase_parts.append(next_token.text)
                        else:
                            break
                
                if len(phrase_parts) > 1:
                    by_phrase = ' '.join(phrase_parts)
                    break
        
        return by_phrase
    
    # Phase 2 å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè£…çµ‚äº†
    
    def _merge_handler_results(self, base_result: Dict, handler_result: Dict, handler_name: str) -> Dict:
        """
        ãƒãƒ³ãƒ‰ãƒ©ãƒ¼çµæœã‚’ãƒ™ãƒ¼ã‚¹çµæœã«ãƒãƒ¼ã‚¸
        
        Args:
            base_result: ãƒ™ãƒ¼ã‚¹çµæœ
            handler_result: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å‡¦ç†çµæœ  
            handler_name: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å
        """
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å„ªå…ˆåº¦å®šç¾©ï¼ˆChatGPT5æ€è€ƒè¨ºæ–­ã«ã‚ˆã‚‹ã€Œå¾Œå‹ã¡ä¸Šæ›¸ãã€å¯¾ç­–ï¼‰
        handler_priority = {
            'passive_voice': 10,      # å—å‹•æ…‹ã¯æœ€é«˜å„ªå…ˆåº¦
            'relative_clause': 9,     # é–¢ä¿‚ç¯€
            'auxiliary_complex': 8,   # åŠ©å‹•è©
            'basic_five_pattern': 1   # åŸºæœ¬5æ–‡å‹ã¯æœ€ä½å„ªå…ˆåº¦
        }
        
        current_priority = handler_priority.get(handler_name, 5)
        
        # ã‚¹ãƒ­ãƒƒãƒˆæƒ…å ±ãƒãƒ¼ã‚¸
        if 'slots' in handler_result:
            for slot_name, slot_data in handler_result['slots'].items():
                if slot_name not in base_result['slots']:
                    base_result['slots'][slot_name] = slot_data
                    # ChatGPT5 Step B: Slot Provenance Tracking
                    if 'slot_provenance' not in base_result:
                        base_result['slot_provenance'] = {}
                    base_result['slot_provenance'][slot_name] = {
                        'handler': handler_name,
                        'priority': current_priority,
                        'value': slot_data
                    }
                else:
                    # ç«¶åˆè§£æ±ºï¼šå„ªå…ˆåº¦ã«ã‚ˆã‚‹ä¿è­·
                    existing_value = base_result['slots'][slot_name]
                    
                    # æ—¢å­˜å€¤ã®å„ªå…ˆåº¦ãƒã‚§ãƒƒã‚¯ï¼ˆChatGPT5 Step B: Provenance-based Priorityï¼‰
                    existing_priority = 1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                    if 'slot_provenance' in base_result and slot_name in base_result['slot_provenance']:
                        existing_priority = base_result['slot_provenance'][slot_name]['priority']
                        existing_handler = base_result['slot_provenance'][slot_name]['handler']
                    else:
                        # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ—ãƒ©ã‚¤ã‚ªãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
                        if 'grammar_info' in base_result and 'handler_contributions' in base_result['grammar_info']:
                            for prev_handler, _ in base_result['grammar_info']['handler_contributions'].items():
                                if prev_handler in handler_priority:
                                    existing_priority = max(existing_priority, handler_priority[prev_handler])
                                    existing_handler = prev_handler
                    
                    # æ—¢å­˜å€¤ãŒç©ºã§æ–°å€¤ãŒæœ‰åŠ¹ãªå ´åˆã¯ä¸Šæ›¸ã
                    if not existing_value and slot_data:
                        base_result['slots'][slot_name] = slot_data
                        # ChatGPT5 Step B: Provenance update
                        if 'slot_provenance' not in base_result:
                            base_result['slot_provenance'] = {}
                        base_result['slot_provenance'][slot_name] = {
                            'handler': handler_name,
                            'priority': current_priority,
                            'value': slot_data
                        }
                    # æ–°ã—ã„ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®å„ªå…ˆåº¦ãŒé«˜ã„å ´åˆã®ã¿ä¸Šæ›¸ã
                    elif current_priority > existing_priority:
                        print(f"ğŸ”¥ ChatGPT5 Step B: Slot override - {slot_name}: '{existing_value}' ({existing_handler if 'existing_handler' in locals() else 'unknown'}) â†’ '{slot_data}' ({handler_name})")
                        base_result['slots'][slot_name] = slot_data
                        # ChatGPT5 Step B: Provenance update
                        if 'slot_provenance' not in base_result:
                            base_result['slot_provenance'] = {}
                        base_result['slot_provenance'][slot_name] = {
                            'handler': handler_name,
                            'priority': current_priority,
                            'value': slot_data
                        }
                    # å„ªå…ˆåº¦ãŒåŒã˜å ´åˆã¯æ—¢å­˜å€¤ãŒæœ‰åŠ¹ãªã‚‰ä¿æŒ
                    elif current_priority == existing_priority and existing_value:
                        pass  # æ—¢å­˜å€¤ã‚’ä¿æŒ
                    # æ—¢å­˜å€¤ãŒæœ‰åŠ¹ã§æ–°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®å„ªå…ˆåº¦ãŒä½ã„å ´åˆã¯ä¿æŒ
                    elif existing_value and current_priority <= existing_priority:
                        pass  # æ—¢å­˜å€¤ã‚’ä¿æŒï¼ˆå—å‹•æ…‹çµæœã‚’ä¿è­·ï¼‰
                    # ä¸¡æ–¹ç©ºã®å ´åˆã¯å¾Œå‹ã¡
                    elif not existing_value and not slot_data:
                        base_result['slots'][slot_name] = slot_data
        
        # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæƒ…å ±ãƒãƒ¼ã‚¸
        if 'sub_slots' in handler_result:
            for sub_slot_name, sub_slot_data in handler_result['sub_slots'].items():
                base_result['sub_slots'][sub_slot_name] = sub_slot_data
        
        # æ–‡æ³•æƒ…å ±è¨˜éŒ²
        if 'grammar_info' in handler_result:
            grammar_info = handler_result['grammar_info']
            if 'handler_contributions' not in base_result['grammar_info']:
                base_result['grammar_info']['handler_contributions'] = {}
            base_result['grammar_info']['handler_contributions'][handler_name] = grammar_info
            
            # æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³è¿½åŠ 
            if 'patterns' in grammar_info:
                if 'detected_patterns' not in base_result['grammar_info']:
                    base_result['grammar_info']['detected_patterns'] = []
                base_result['grammar_info']['detected_patterns'].extend(grammar_info['patterns'])
        
        # é–¢ä¿‚ç¯€æƒ…å ±ãƒãƒ¼ã‚¸ï¼ˆPhase 2: 2æ®µéšå‡¦ç†å¯¾å¿œï¼‰
        if 'relative_clause_info' in handler_result:
            base_result['relative_clause_info'] = handler_result['relative_clause_info']
            print(f"ğŸ”¥ é–¢ä¿‚ç¯€æƒ…å ±ãƒãƒ¼ã‚¸: {handler_result['relative_clause_info']}")
        
        return base_result

    def _unified_mapping(self, sentence: str, doc) -> Dict[str, Any]:
        """
        çµ±åˆãƒãƒƒãƒ”ãƒ³ã‚°å‡¦ç† (from Stanza Asset Migration)
        
        å…¨ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒåŒæ™‚å®Ÿè¡Œ
        å„ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã¯ç‹¬ç«‹ã—ã¦spaCyè§£æçµæœã‚’å‡¦ç†
        """
        result = {
            'sentence': sentence,
            'slots': {},
            'sub_slots': {},
            'grammar_info': {
                'detected_patterns': [],
                'handler_contributions': {},
                'control_flags': {}  # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼åˆ¶å¾¡ãƒ•ãƒ©ã‚°
            }
        }
        
        self.logger.debug(f"Unified mappingé–‹å§‹: {len(self.active_handlers)} handlers active")
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼é–“å…±æœ‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆæœŸåŒ–
        self.handler_shared_context = {
            'predefined_slots': {},        # äº‹å‰ç¢ºå®šã‚¹ãƒ­ãƒƒãƒˆ
            'remaining_elements': {},      # æ®‹ã‚Šè¦ç´ æƒ…å ±
            'handler_metadata': {}         # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼åˆ¥ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        }
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè¡Œé †åºã®åˆ¶å¾¡
        ordered_handlers = self._get_ordered_handlers()
        
        # å…¨ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®åŒæ™‚å®Ÿè¡Œï¼ˆé †åºåˆ¶å¾¡ä»˜ãï¼‰
        for handler_name in ordered_handlers:
            try:
                # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼åˆ¶å¾¡ãƒ•ãƒ©ã‚°ã‚’ãƒã‚§ãƒƒã‚¯
                control_flags = result.get('grammar_info', {}).get('control_flags', {})
                if self._should_skip_handler(handler_name, control_flags):
                    self.logger.debug(f"ğŸš« Handler ã‚¹ã‚­ãƒƒãƒ—: {handler_name} (åˆ¶å¾¡ãƒ•ãƒ©ã‚°)")
                    continue
                
                print(f"ğŸ¯ Handlerå®Ÿè¡Œ: {handler_name}")
                self.logger.debug(f"Handlerå®Ÿè¡Œ: {handler_name}")
                
                # Phase 2: æ–°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ
                handler_method = getattr(self, f'_handle_{handler_name}', None)
                if handler_method:
                    handler_result = handler_method(sentence, doc, result)
                    if handler_result:
                        result = self._merge_handler_results(result, handler_result, handler_name)
                        print(f"ğŸ” Merged result after {handler_name}: {result}")
                    continue  # æ–°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒå®Ÿè¡Œã•ã‚ŒãŸå ´åˆã€ãƒ¬ã‚¬ã‚·ãƒ¼å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
                
                # ãƒ¬ã‚¬ã‚·ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆbasic_five_patternã®ã¿ï¼‰
                if handler_name == 'basic_five_pattern':
                    # ğŸ”¥ é–¢ä¿‚ç¯€åˆ†é›¢å¾Œã®ä¸»æ–‡ã‚’ä½¿ç”¨
                    analysis_sentence = sentence
                    analysis_doc = doc
                    if result.get('relative_clause_info', {}).get('found'):
                        main_sentence = result['relative_clause_info']['main_sentence']
                        print(f"ğŸ”¥ Phase 2: Using main sentence for basic_five_pattern: '{main_sentence}'")
                        analysis_sentence = main_sentence
                        analysis_doc = self.nlp(main_sentence)
                    
                    # ChatGPT5 Step C: Token Consumption - ä½¿ç”¨æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ•ã‚£ãƒ«ã‚¿
                    filtered_doc_tokens = []
                    for i, token in enumerate(analysis_doc):
                        if i not in self._consumed_tokens:
                            filtered_doc_tokens.append(token)
                    
                    if len(filtered_doc_tokens) < len(analysis_doc):
                        print(f"ğŸ”¥ ChatGPT5 Step C: Filtered {len(analysis_doc) - len(filtered_doc_tokens)} consumed tokens for basic_five_pattern")
                    
                    # âœ… Phase A2: å†…éƒ¨5æ–‡å‹å‡¦ç†ã‚’ç›´æ¥ä½¿ç”¨ï¼ˆçµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Œå…¨å®Ÿè£…ï¼‰
                    print(f"ğŸ¯ Phase A2: å†…éƒ¨5æ–‡å‹å‡¦ç†ä½¿ç”¨: '{analysis_sentence}'")
                    
                    # äººé–“çš„æ–‡æ³•èªè­˜: æ›–æ˜§èªå½™è§£æ±ºï¼ˆæ®µéšå®Ÿè£…äºˆå®šï¼‰
                    # ğŸ§ª Phase A2: spaCyãƒˆãƒ¼ã‚¯ãƒ³ã‚’Dictãƒˆãƒ¼ã‚¯ãƒ³å½¢å¼ã«å¤‰æ›
                    enhanced_tokens = self._convert_spacy_to_dict_tokens(filtered_doc_tokens)
                    
                    # å†…éƒ¨5æ–‡å‹å‡¦ç†ã®ç›´æ¥å®Ÿè¡Œ
                    core_elements = self._identify_core_elements(enhanced_tokens)
                    sentence_pattern = self._determine_sentence_pattern(core_elements, enhanced_tokens)
                    grammar_elements = self._assign_grammar_roles(enhanced_tokens, sentence_pattern, core_elements)
                    
                    print(f"ğŸ¯ Phase A2: å†…éƒ¨5æ–‡å‹å‡¦ç†çµæœ: pattern={sentence_pattern}, elements={len(grammar_elements)}")
                    
                    # çµæœã‚’çµ±åˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
                    if grammar_elements:
                        result = self._integrate_internal_pattern_result(result, grammar_elements, sentence_pattern)
                        print(f"ğŸ¯ Phase A2: çµ±åˆçµæœ: {result}")
                    else:
                        print(f"âš ï¸ Phase A2: å†…éƒ¨5æ–‡å‹å‡¦ç†ã§è¦ç´ ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                    
                    # æˆåŠŸã‚«ã‚¦ãƒ³ãƒˆ
                    self.handler_success_count[handler_name] = \
                        self.handler_success_count.get(handler_name, 0) + 1
                
                elif handler_name == 'passive_voice':
                    # å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ (Phase 2å®Ÿè£…)
                    print(f"ğŸ” Executing passive_voice handler for: {sentence}")
                    passive_result = self._handle_passive_voice(sentence, doc, result)
                    print(f"ğŸ” Passive voice handler result: {passive_result}")
                    if passive_result:
                        result = self._merge_handler_results(result, passive_result, handler_name)
                        print(f"ğŸ” Merged result after passive voice: {result}")
                        self.handler_success_count[handler_name] = \
                            self.handler_success_count.get(handler_name, 0) + 1
                    else:
                        print(f"ğŸ” Passive voice handler returned None")
                
                elif handler_name == 'adverbial_modifier':
                    # å‰¯è©ãƒ»ä¿®é£¾èªãƒãƒ³ãƒ‰ãƒ©ãƒ¼ (Phase 2å®Ÿè£…)
                    print(f"ğŸ” Executing adverbial_modifier handler for: {sentence}")
                    adverb_result = self._handle_adverbial_modifier(sentence, doc, result)
                    print(f"ğŸ” Adverb handler result: {adverb_result}")
                    if adverb_result:
                        result = self._merge_handler_results(result, adverb_result, handler_name)
                        print(f"ğŸ” Merged result: {result}")
                        self.handler_success_count[handler_name] = \
                            self.handler_success_count.get(handler_name, 0) + 1
                    else:
                        print(f"ğŸ” Adverb handler returned None")
                        
            except Exception as e:
                self.logger.warning(f"Handler error ({handler_name}): {e}")
                continue
        
        return result
    
    def _should_skip_handler(self, handler_name: str, control_flags: Dict) -> bool:
        """ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã¹ãã‹ãƒã‚§ãƒƒã‚¯"""
        # å°†æ¥ã®åˆ¶å¾¡ãƒ•ãƒ©ã‚°å‡¦ç†ç”¨
        return False
    
    def _get_ordered_handlers(self) -> List[str]:
        """ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®å®Ÿè¡Œé †åºã‚’åˆ¶å¾¡"""
        priority_order = [
            'relative_clause',          # é–¢ä¿‚ç¯€å„ªå…ˆ
            'passive_voice',            # å—å‹•æ…‹
            'auxiliary_complex',        # åŠ©å‹•è©
            'adverbial_modifier',       # å‰¯è©ãƒ»ä¿®é£¾èª
            'basic_five_pattern',       # åŸºæœ¬5æ–‡å‹ï¼ˆæœ€å¾Œï¼‰
        ]
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®ã†ã¡ã€å„ªå…ˆé †ä½ã«å¾“ã£ã¦ä¸¦ã³æ›¿ãˆ
        ordered = []
        for handler in priority_order:
            if handler in self.active_handlers:
                ordered.append(handler)
        
        # å„ªå…ˆé †ä½ã«ãªã„ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ 
        for handler in self.active_handlers:
            if handler not in ordered:
                ordered.append(handler)
        
        return ordered
    
    def _analyze_sentence_legacy(self, sentence: str, doc) -> Dict:
        """æ—¢å­˜ã®analyze_sentenceãƒ­ã‚¸ãƒƒã‚¯ã‚’ãƒ©ãƒƒãƒ—"""
        # æ—¢å­˜ã®analyze_sentenceãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—ã€çµæœã‚’è¿”ã™
        # ç¾åœ¨ã¯æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®æ©‹æ¸¡ã—å½¹
        try:
            # ChatGPT5 Step A: Re-entrancy Guard - å†å¸°é˜²æ­¢ã®ãŸã‚allow_unified=False
            return self.analyze_sentence(sentence, allow_unified=False)
        except Exception as e:
            self.logger.error(f"Legacy analysis error: {e}")
            return {'slots': {}, 'error': str(e)}

# ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°ã¨ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
def run_full_test_suite(test_data_path: str = None) -> Dict[str, Any]:
    """
    53ä¾‹æ–‡ã®å®Œå…¨ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    
    Args:
        test_data_path: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
    Returns:
        Dict: ãƒ†ã‚¹ãƒˆçµæœ
    """
    import json
    import os
    from datetime import datetime
    
    if test_data_path is None:
        test_data_path = os.path.join(
            os.path.dirname(__file__),
            "final_test_system",
            "final_54_test_data.json"
        )
    
    try:
        with open(test_data_path, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
    except FileNotFoundError:
        print(f"âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {test_data_path}")
        return {}
    
    mapper = DynamicGrammarMapper()
    results = {
        "timestamp": datetime.now().isoformat(),
        "test_system": "dynamic_grammar_mapper",
        "total_tests": len(test_data["data"]),
        "successful_tests": 0,
        "failed_tests": 0,
        "test_results": {}
    }
    
    print("=== å‹•çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ  53ä¾‹æ–‡ãƒ†ã‚¹ãƒˆ ===\n")
    
    for test_id, test_case in test_data["data"].items():
        sentence = test_case["sentence"]
        expected = test_case["expected"]
        
        print(f"ãƒ†ã‚¹ãƒˆ {test_id}: {sentence}")
        
        try:
            result = mapper.analyze_sentence(sentence)
            
            if 'error' in result:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
                results["failed_tests"] += 1
                status = "ERROR"
            else:
                print(f"âœ… æ–‡å‹: {result.get('pattern_detected', 'UNKNOWN')}")
                print(f"ğŸ“Š ã‚¹ãƒ­ãƒƒãƒˆ: {result['Slot']}")
                results["successful_tests"] += 1
                status = "SUCCESS"
            
            results["test_results"][test_id] = {
                "sentence": sentence,
                "expected": expected,
                "actual": result,
                "status": status
            }
            
        except Exception as e:
            print(f"âŒ ä¾‹å¤–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            results["failed_tests"] += 1
            results["test_results"][test_id] = {
                "sentence": sentence,
                "expected": expected,
                "actual": {"error": str(e)},
                "status": "EXCEPTION"
            }
        
        print("-" * 60)
    
    # çµæœã‚µãƒãƒªãƒ¼
    success_rate = results["successful_tests"] / results["total_tests"] * 100
    print(f"\n=== ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼ ===")
    print(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {results['total_tests']}")
    print(f"æˆåŠŸ: {results['successful_tests']}")
    print(f"å¤±æ•—: {results['failed_tests']}")
    print(f"æˆåŠŸç‡: {success_rate:.1f}%")
    
    return results

def save_test_results(results: Dict[str, Any], output_path: str = None) -> str:
    """
    ãƒ†ã‚¹ãƒˆçµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    
    Args:
        results: ãƒ†ã‚¹ãƒˆçµæœ
        output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆNone ã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        
    Returns:
        str: ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    import json
    from datetime import datetime
    
    if output_path is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"dynamic_grammar_test_results_{timestamp}.json"
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“ ãƒ†ã‚¹ãƒˆçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
    return output_path

    # ===== Phase A2: äººé–“çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ  =====
    
    def _apply_human_grammar_recognition(self, doc):
        """
        ğŸ§  äººé–“çš„æ–‡æ³•èªè­˜: æ›–æ˜§èªå½™ã®å‹•çš„è§£æ±º
        
        UnifiedStanzaRephraseMapperã‹ã‚‰ç¶™æ‰¿ã—ãŸæ ¸å¿ƒæŠ€è¡“:
        - 2ã‚±ãƒ¼ã‚¹è©¦è¡Œã‚·ã‚¹ãƒ†ãƒ 
        - æ§‹æ–‡å®Œå…¨æ€§è©•ä¾¡
        - spaCyèª¤èªè­˜ã®äººé–“çš„ä¿®æ­£
        """
        enhanced_tokens = []
        for token in doc:
            if self._is_ambiguous_word(token.text):
                print(f"ğŸ§  Ambiguous word detected: '{token.text}' (POS: {token.pos_})")
                # 2ã‚±ãƒ¼ã‚¹è©¦è¡Œã‚·ã‚¹ãƒ†ãƒ 
                corrected_pos = self._resolve_ambiguous_pos(token, doc)
                enhanced_tokens.append(self._create_enhanced_token(token, corrected_pos))
                print(f"ğŸ§  Resolved: '{token.text}' {token.pos_} â†’ {corrected_pos}")
            else:
                enhanced_tokens.append(token)
        return enhanced_tokens
    
    def _is_ambiguous_word(self, word):
        """æ›–æ˜§èªå½™ã®åˆ¤å®šï¼ˆä¾‹: lives, works, savesç­‰ï¼‰"""
        ambiguous_words = [
            'lives', 'works', 'saves', 'love', 'loves', 'help', 'helps',
            'care', 'cares', 'study', 'studies', 'watch', 'watches'
        ]
        return word.lower() in ambiguous_words
    
    def _resolve_ambiguous_pos(self, token, doc):
        """
        ğŸ§  UnifiedStanzaRephraseMapperã®æ ¸å¿ƒæŠ€è¡“: 2ã‚±ãƒ¼ã‚¹è©¦è¡Œ
        
        äººé–“ãŒæ–‡æ³•ã‚’èªè­˜ã™ã‚‹éš›ã®è«–ç†ãƒ—ãƒ­ã‚»ã‚¹:
        1. ç¾åœ¨ã®POSã§æ§‹æ–‡å®Œå…¨æ€§ã‚’è©•ä¾¡
        2. ä»£æ›¿POSã§æ§‹æ–‡å®Œå…¨æ€§ã‚’è©•ä¾¡  
        3. ã‚ˆã‚Šå®Œå…¨ãªæ§‹æ–‡ã‚’é¸æŠ
        """
        # ã‚±ãƒ¼ã‚¹1: å…ƒã®POS
        case1_score = self._evaluate_syntactic_completeness(token, doc, token.pos_)
        print(f"ğŸ§  Case1 ({token.pos_}): score = {case1_score}")
        
        # ã‚±ãƒ¼ã‚¹2: ä»£æ›¿POS
        alt_pos = self._get_alternative_pos(token.pos_)
        if alt_pos != token.pos_:
            case2_score = self._evaluate_syntactic_completeness(token, doc, alt_pos)
            print(f"ğŸ§  Case2 ({alt_pos}): score = {case2_score}")
            
            # æ§‹æ–‡å®Œå…¨æ€§ã®é«˜ã„æ–¹ã‚’é¸æŠ
            return alt_pos if case2_score > case1_score else token.pos_
        
        return token.pos_
    
    def _get_alternative_pos(self, current_pos):
        """ä»£æ›¿POSå€™è£œã‚’å–å¾—"""
        alternatives = {
            'NOUN': 'VERB',    # lives (åè©) â†’ lives (å‹•è©)
            'VERB': 'NOUN',    # saves (å‹•è©) â†’ saves (åè©)
        }
        return alternatives.get(current_pos, current_pos)
    
    def _evaluate_syntactic_completeness(self, token, doc, pos):
        """
        æ§‹æ–‡å®Œå…¨æ€§è©•ä¾¡: äººé–“çš„åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
        
        è©•ä¾¡åŸºæº–:
        1. ä¸»èªãƒ»å‹•è©ãƒ»ç›®çš„èªã®æ§‹é€ çš„æ•´åˆæ€§
        2. æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã®ä¸€è‡´åº¦
        3. å‘¨è¾ºèªå½™ã¨ã®æ„å‘³çš„æ•´åˆæ€§
        """
        score = 0
        
        # åŸºæœ¬ã‚¹ã‚³ã‚¢
        if pos == 'VERB':
            # å‹•è©ã¨ã—ã¦è§£é‡ˆã—ãŸå ´åˆã®è©•ä¾¡
            if self._has_subject_before(token, doc):
                score += 2  # ä¸»èªãŒã‚ã‚‹
            if self._has_object_after(token, doc):
                score += 2  # ç›®çš„èªãŒã‚ã‚‹
            if self._fits_verb_context(token, doc):
                score += 1  # å‹•è©æ–‡è„ˆã«é©åˆ
        elif pos == 'NOUN':
            # åè©ã¨ã—ã¦è§£é‡ˆã—ãŸå ´åˆã®è©•ä¾¡
            if self._has_determiner_before(token, doc):
                score += 1  # é™å®šè©ãŒã‚ã‚‹
            if self._fits_noun_context(token, doc):
                score += 1  # åè©æ–‡è„ˆã«é©åˆ
        
        return score
    
    def _has_subject_before(self, token, doc):
        """å‹•è©ã®å‰ã«ä¸»èªãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        for i, t in enumerate(doc):
            if t == token:
                for j in range(i):
                    if doc[j].pos_ in ['NOUN', 'PRON'] and doc[j].dep_ in ['nsubj']:
                        return True
                break
        return False
    
    def _has_object_after(self, token, doc):
        """å‹•è©ã®å¾Œã«ç›®çš„èªãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        for i, t in enumerate(doc):
            if t == token:
                for j in range(i + 1, len(doc)):
                    if doc[j].pos_ == 'NOUN' and doc[j].dep_ in ['dobj', 'pobj']:
                        return True
                break
        return False
    
    def _fits_verb_context(self, token, doc):
        """å‹•è©æ–‡è„ˆã¸ã®é©åˆåº¦"""
        # å‘¨è¾ºã«å‰¯è©ãŒã‚ã‚Œã°å‹•è©ã®å¯èƒ½æ€§ãŒé«˜ã„
        for t in doc:
            if t.pos_ == 'ADV' and abs(t.i - token.i) <= 2:
                return True
        return False
    
    def _has_determiner_before(self, token, doc):
        """åè©ã®å‰ã«é™å®šè©ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        for i, t in enumerate(doc):
            if t == token and i > 0:
                return doc[i-1].pos_ == 'DET'
        return False
    
    def _fits_noun_context(self, token, doc):
        """åè©æ–‡è„ˆã¸ã®é©åˆåº¦"""
        # å‰å¾Œã«å½¢å®¹è©ãŒã‚ã‚Œã°åè©ã®å¯èƒ½æ€§ãŒé«˜ã„
        for t in doc:
            if t.pos_ == 'ADJ' and abs(t.i - token.i) <= 1:
                return True
        return False
    
    def _create_enhanced_token(self, token, corrected_pos):
        """ä¿®æ­£ã•ã‚ŒãŸPOSã§ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã‚’æ›´æ–°"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³ã®POSæƒ…å ±ã‚’å‹•çš„ã«ä¿®æ­£
        # ç¾åœ¨ã¯å…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãã®ã¾ã¾è¿”ã™ï¼ˆå¾Œã§å¼·åŒ–äºˆå®šï¼‰
        return token
    
    def _integrate_pattern_result_safely(self, result, pattern_result):
        """
        BasicFivePatternHandlerã®çµæœã‚’å®‰å…¨ã«çµ±åˆ
        
        ãƒ¬ã‚¬ã‚·ãƒ¼ä¸Šæ›¸ãã‚’å›é¿ã—ã€æ­£ç¢ºãªçµæœã‚’ä¿è­·
        """
        if not pattern_result or 'slots' not in pattern_result:
            return result
        
        print(f"ğŸ¯ Safely integrating pattern result: {pattern_result['slots']}")
        
        # BasicFivePatternHandlerã®çµæœã‚’å„ªå…ˆçš„ã«çµ±åˆ
        for slot_name, slot_value in pattern_result['slots'].items():
            if slot_value:  # ç©ºã§ãªã„å€¤ã®ã¿
                # æ—¢å­˜ã®å€¤ã‚’ä¸Šæ›¸ãï¼ˆBasicFivePatternHandlerã®çµæœã‚’ä¿¡é ¼ï¼‰
                result['slots'][slot_name] = slot_value
                print(f"ğŸ¯ Integrated slot: {slot_name} = '{slot_value}'")
        
        return result

    def _convert_spacy_to_dict_tokens(self, spacy_tokens):
        """
        spaCyãƒˆãƒ¼ã‚¯ãƒ³ã‚’Dictå½¢å¼ã«å¤‰æ›
        å†…éƒ¨5æ–‡å‹å‡¦ç†ã§ä½¿ç”¨ã™ã‚‹å½¢å¼ã«åˆã‚ã›ã‚‹
        """
        dict_tokens = []
        for i, token in enumerate(spacy_tokens):
            dict_token = {
                'id': i,
                'text': token.text,
                'pos': token.pos_,
                'tag': token.tag_,
                'dep': token.dep_,
                'lemma': token.lemma_,
                'is_alpha': token.is_alpha,
                'is_stop': token.is_stop,
                'is_punct': token.is_punct,
                'idx': token.idx if hasattr(token, 'idx') else i
            }
            dict_tokens.append(dict_token)
        return dict_tokens

    def _integrate_internal_pattern_result(self, result, grammar_elements, sentence_pattern):
        """
        å†…éƒ¨5æ–‡å‹å‡¦ç†ã®çµæœã‚’çµ±åˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«çµ±åˆ
        
        BasicFivePatternHandlerã®çµæœã‚’ä¸Šæ›¸ãã›ãšã€æ­£ç¢ºãªçµæœã‚’ä¿è­·
        """
        print(f"ğŸ¯ çµ±åˆå‡¦ç†é–‹å§‹: pattern={sentence_pattern}, elements={len(grammar_elements)}")
        
        # grammar_elementsã‹ã‚‰ã‚¹ãƒ­ãƒƒãƒˆã‚’æŠ½å‡º
        for element in grammar_elements:
            slot_name = element.role  # 'S', 'V', 'O', 'C' etc.
            slot_value = element.text
            
            if slot_value and slot_value.strip():  # ç©ºã§ãªã„å€¤ã®ã¿
                # å†…éƒ¨å‡¦ç†ã®çµæœã‚’å„ªå…ˆçš„ã«è¨­å®š
                result['slots'][slot_name] = slot_value.strip()
                print(f"ğŸ¯ çµ±åˆã‚¹ãƒ­ãƒƒãƒˆè¨­å®š: {slot_name} = '{slot_value.strip()}'")
        
        return result




# ã‚¯ãƒ©ã‚¹å®šç¾©çµ‚äº†ä½ç½®

class PureCentralController:
    """
    ğŸ¯ Phase A3-5: å®Œå…¨çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè£…
    
    è²¬å‹™: ç®¡ç†ãƒ»èª¿æ•´ã®ã¿ï¼ˆåˆ†è§£ä½œæ¥­ä¸€åˆ‡ãªã—ï¼‰
    â”œâ”€ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè¡Œé †åºåˆ¶å¾¡
    â”œâ”€ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼é–“æƒ…å ±å…±æœ‰ç®¡ç†  
    â”œâ”€ çµæœçµ±åˆãƒ»æœ€çµ‚èª¿æ•´
    â”œâ”€ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»å“è³ªä¿è¨¼
    â”œâ”€ é«˜åº¦åˆ¶å¾¡æ©Ÿæ§‹ï¼ˆPhase A3-5æ–°æ©Ÿèƒ½ï¼‰
    â””â”€ äººé–“çš„åˆ¤å®šã‚·ã‚¹ãƒ†ãƒ ï¼ˆPhase A3-5æ–°æ©Ÿèƒ½ï¼‰
    
    è¨­è¨ˆåŸå‰‡:
    - åˆ†è§£ä½œæ¥­ã¯ä¸€åˆ‡å®Ÿè¡Œã—ãªã„
    - ç´”ç²‹ãªç®¡ç†æ©Ÿèƒ½ã®ã¿å®Ÿè£…
    - å…¨ã¦ã®åˆ†è§£å‡¦ç†ã¯ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã«å§”è­²
    - è¨­è¨ˆä»•æ§˜æ›¸ã®ç†å¿µçš„æ©Ÿèƒ½100%å®Ÿè£…
    
    Phaseé€²åŒ–å±¥æ­´:
    A3-1: åŸºæœ¬å®Ÿè£…
    A3-2: analyze_sentence()å®Œå…¨ç½®æ›
    A3-3: ãƒ¬ã‚¬ã‚·ãƒ¼åˆ†è§£æ©Ÿèƒ½å®Œå…¨é™¤å»  
    A3-4: ã‚¨ãƒ©ãƒ¼ä¿®æ­£ãƒ»å“è³ªä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ 
    A3-5: ç†å¿µçš„æ©Ÿèƒ½100%å®Ÿè£…ï¼ˆä»Šå›ï¼‰
    """
    
    def __init__(self, grammar_mapper: 'DynamicGrammarMapper'):
        """
        ğŸ¯ Phase A3-5: å®Œå…¨çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼åˆæœŸåŒ–
        
        Args:
            grammar_mapper: æ—¢å­˜ã®DynamicGrammarMapperã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        self.grammar_mapper = grammar_mapper
        self.logger = logging.getLogger(__name__)
        
        # âœ… ç´”ç²‹ç®¡ç†æ©Ÿèƒ½: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè¡Œåˆ¶å¾¡è¨­å®š
        self.handler_execution_order = [
            'relative_clause',
            'passive_voice', 
            'basic_five_pattern',
            'auxiliary_complex'
        ]
        
        # ğŸ”¥ Phase A3-5: é«˜åº¦åˆ¶å¾¡æ©Ÿæ§‹å®Ÿè£…
        self.central_handler_controller = self._init_central_handler_controller()
        self.handler_shared_context = {
            'predefined_slots': {},
            'remaining_elements': [],
            'handler_metadata': {},
            'control_flags': {}
        }
        
        # ğŸ§  Phase A3-5: äººé–“çš„åˆ¤å®šã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…
        self.ambiguous_word_resolver = self._init_ambiguous_word_resolver()
        self.syntactic_evaluator = self._init_syntactic_evaluator()
        
        # âœ… ç´”ç²‹ç®¡ç†æ©Ÿèƒ½: å“è³ªä¿è¨¼è¨­å®š
        self.quality_thresholds = {
            'confidence_minimum': 0.7,
            'slot_coverage_minimum': 0.8,
            'error_tolerance': 0.1
        }
        
        self.logger.info("ğŸ¯ Phase A3-5: å®Œå…¨çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼åˆæœŸåŒ–å®Œäº† - ç†å¿µçš„æ©Ÿèƒ½100%å®Ÿè£…")
    
    def _init_central_handler_controller(self):
        """
        ğŸ”¥ Phase A3-5: ä¸­å¤®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼åˆ¶å¾¡æ©Ÿæ§‹åˆæœŸåŒ–
        
        è¨­è¨ˆä»•æ§˜æ›¸Phase 2.5ã®ç†å¿µå®Ÿç¾
        """
        return {
            'structure_manager': {
                'main_sentence': None,
                'sub_sentences': [],
                'hierarchy_map': {}
            },
            'execution_controller': {
                'handler_dependencies': {},
                'execution_scope': {},
                'coordination_rules': {}
            },
            'information_manager': {
                'context_state': {},
                'shared_metadata': {},
                'conflict_resolution': {}
            }
        }
    
    def analyze_sentence_pure_management(self, sentence: str) -> Dict[str, Any]:
        """
        ğŸ¯ Phase A3-2a: å®Œå…¨è¤‡è£½å®Ÿè£…
        
        ç¾åœ¨ã®analyze_sentence()ã®å…¨ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Œå…¨ã‚³ãƒ”ãƒ¼ã—ã¦
        PureCentralControllerã§åŒä¸€ã®86.1%ç²¾åº¦ã‚’å®Ÿç¾
        
        Args:
            sentence (str): è§£æå¯¾è±¡ã®æ–‡ç« 
            
        Returns:
            Dict[str, Any]: Rephraseã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼å®Œå…¨äº’æ›ï¼‰
        """
        self.logger.debug(f"ğŸ¯ Phase A3-2a: PureCentralControllerå®Œå…¨è¤‡è£½å®Ÿè¡Œ: '{sentence}'")
        
        # === å…ƒã®analyze_sentence()ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Œå…¨è¤‡è£½ ===
        allow_unified = False  # PureCentralControllerã¯ç‹¬ç«‹å®Ÿè¡Œï¼ˆå†å¸°é˜²æ­¢ï¼‰
        
        # ç´¯ç©ãƒã‚°ä¿®æ­£: æ–°ã—ã„åˆ†æé–‹å§‹æ™‚ã«last_unified_resultã‚’ãƒªã‚»ãƒƒãƒˆ
        if hasattr(self.grammar_mapper, 'last_unified_result'):
            self.grammar_mapper.last_unified_result = None
        
        # ChatGPT5 Step C: Token Consumption Tracking - æ–°ã—ã„åˆ†æé–‹å§‹æ™‚ã«ãƒªã‚»ãƒƒãƒˆ
        if hasattr(self.grammar_mapper, '_consumed_tokens'):
            self.grammar_mapper._consumed_tokens = set()
        
        try:
            # ğŸ†• Phase 1.2: æ–‡å‹èªè­˜
            sentence_type = "statement"  # ä¸€æ™‚çš„ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            sentence_type_confidence = 0.8  # ä¸€æ™‚çš„ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # 1. spaCyåŸºæœ¬è§£æï¼ˆç´”ç²‹å“è©æƒ…å ±ã®ã¿ï¼‰
            doc = self.grammar_mapper.nlp(sentence)
            tokens = self._pure_extract_tokens(doc)  # ğŸ¯ ç‹¬ç«‹å®Ÿè£…ä½¿ç”¨
            
            # 1.5. é–¢ä¿‚ç¯€æ§‹é€ ã®æ¤œå‡º
            relative_clause_info = self.grammar_mapper._detect_relative_clause(tokens, sentence)
            
            # ğŸ”§ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆã‚’äº‹å‰é™¤å¤–ã‚ˆã‚Šå‰ã«å®Ÿè¡Œï¼ˆcarç­‰ã®è¦ç´ ã‚’ä¿æŒã™ã‚‹ãŸã‚ï¼‰
            sub_slots = {}
            original_tokens = tokens.copy()  # å…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¿å­˜
            if relative_clause_info['found']:
                self.logger.debug(f"é–¢ä¿‚ç¯€æ¤œå‡º: {relative_clause_info['type']} (ä¿¡é ¼åº¦: {relative_clause_info['confidence']})")
                
                # ãƒ¬ã‚¬ã‚·ãƒ¼é–¢ä¿‚ç¯€å‡¦ç†
                temp_core_elements = self.grammar_mapper._identify_core_elements(tokens)
                processed_tokens, sub_slots = self.grammar_mapper._process_relative_clause(original_tokens, relative_clause_info, temp_core_elements)
            
            # ğŸ”§ é–¢ä¿‚ç¯€å†…è¦ç´ ã®äº‹å‰é™¤å¤–ï¼ˆãƒ¡ã‚¤ãƒ³æ–‡æ³•è§£æç”¨ï¼‰
            excluded_indices = self.grammar_mapper._identify_relative_clause_elements(tokens, relative_clause_info)
            
            # 2. é™¤å¤–ã•ã‚Œã¦ã„ãªã„è¦ç´ ã®ã¿ã§ã‚³ã‚¢è¦ç´ ã‚’ç‰¹å®š
            filtered_tokens = [token for i, token in enumerate(tokens) if i not in excluded_indices]
            
            # ğŸ”¥ Phase A2: å†…éƒ¨5æ–‡å‹å‡¦ç†ã‚’ç›´æ¥ä½¿ç”¨ï¼ˆçµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼‰
            print("ğŸ”¥ Phase A2: å†…éƒ¨5æ–‡å‹å‡¦ç†ã«ã‚ˆã‚‹æ–‡å‹è§£æé–‹å§‹")
            core_elements = self.grammar_mapper._identify_core_elements(filtered_tokens)
            sentence_pattern = self.grammar_mapper._determine_sentence_pattern(core_elements, filtered_tokens)
            grammar_elements = self.grammar_mapper._assign_grammar_roles(filtered_tokens, sentence_pattern, core_elements, relative_clause_info)
            
            # æˆåŠŸåˆ¤å®š
            pattern_analysis = {
                'handler_success': len(grammar_elements) > 0,
                'core_elements': core_elements,
                'sentence_pattern': sentence_pattern,
                'grammar_elements': grammar_elements
            }
            
            print(f"ğŸ”¥ Phase A2: å†…éƒ¨5æ–‡å‹å‡¦ç†å®Œäº†: {sentence_pattern}")
            print(f"ğŸ”§ Phase A2: grammar_elementså–å¾—: {[{'role': e.role, 'text': e.text} for e in grammar_elements]}")
            
            # 5. Rephraseã‚¹ãƒ­ãƒƒãƒˆå½¢å¼ã«å¤‰æ›
            rephrase_result = self.grammar_mapper._convert_to_rephrase_format(grammar_elements, sentence_pattern, sub_slots)
            
            # ğŸ”¥ Phase A2: å†…éƒ¨5æ–‡å‹å‡¦ç†å®Œäº† - çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã‚’ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            if pattern_analysis.get('handler_success'):
                print(f"ğŸ”¥ Phase A2: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå†…éƒ¨5æ–‡å‹å‡¦ç†ã§å®Œäº†æ¸ˆã¿ï¼‰")
                print(f"ğŸ§ª Phase A1ãƒ†ã‚¹ãƒˆ: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å¼·åˆ¶æœ‰åŠ¹åŒ–ï¼ˆä¿®æ­£ç‰ˆãƒ†ã‚¹ãƒˆï¼‰")
                # ğŸ”§ Phase A3: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®æ—¢å­˜çµæœã‚’ã‚¯ãƒªã‚¢
                if hasattr(self.grammar_mapper, 'last_unified_result'):
                    self.grammar_mapper.last_unified_result = None
                print(f"ğŸ”§ Phase A2: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼çµæœã‚¯ãƒªã‚¢ï¼ˆå†…éƒ¨5æ–‡å‹å‡¦ç†ä½¿ç”¨ï¼‰")
            
            # ğŸ”¥ Phase 2: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œï¼ˆå—å‹•æ…‹ãƒ»åŠ©å‹•è©ãƒ»å‰¯è©å‡¦ç†ï¼‰
            try:
                unified_result = self.grammar_mapper._unified_mapping(sentence, doc)
                if unified_result and 'slots' in unified_result:
                    # çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®çµæœã‚’ãƒãƒ¼ã‚¸ï¼ˆå„ªå…ˆåº¦é †ï¼‰
                    for slot_name, slot_value in unified_result['slots'].items():
                        if slot_value:  # ç©ºã§ãªã„å€¤ã®ã¿ãƒãƒ¼ã‚¸
                            # ChatGPT5 Step D: å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã¯å…¨ã‚¹ãƒ­ãƒƒãƒˆå„ªå…ˆï¼ˆAux, V, Mï¼‰
                            if 'passive_voice' in str(unified_result.get('grammar_info', {})):
                                rephrase_result['slots'][slot_name] = slot_value
                                rephrase_result['main_slots'][slot_name] = slot_value
                                print(f"ğŸ”¥ å—å‹•æ…‹å„ªå…ˆãƒãƒ¼ã‚¸: {slot_name} = '{slot_value}'")
                            # ãã®ä»–ã®ã‚¹ãƒ­ãƒƒãƒˆã¯æ—¢å­˜å€¤ãŒãªã„å ´åˆã®ã¿
                            elif not rephrase_result['slots'].get(slot_name):
                                rephrase_result['slots'][slot_name] = slot_value
                                rephrase_result['main_slots'][slot_name] = slot_value
                    
                    # æ–‡æ³•æƒ…å ±ã‚‚ãƒãƒ¼ã‚¸
                    if 'grammar_info' in unified_result:
                        if 'unified_handlers' not in rephrase_result:
                            rephrase_result['unified_handlers'] = {}
                        rephrase_result['unified_handlers'] = unified_result['grammar_info']
                    
                    # ChatGPT5 Step D: Token consumptionãƒ™ãƒ¼ã‚¹ã§é‡è¤‡ã‚¹ãƒ­ãƒƒãƒˆå‰Šé™¤
                    self.grammar_mapper._cleanup_duplicate_slots_by_consumption(rephrase_result, doc)
                    
                    # ğŸ”¥ Phase 2: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼çµæœã‚’ä¿å­˜ (ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆãƒãƒ¼ã‚¸ç”¨)
                    self.grammar_mapper.last_unified_result = unified_result
                    print(f"ğŸ”¥ çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼çµæœä¿å­˜: sub_slots = {unified_result.get('sub_slots', {})}")
                    
                    # ğŸ¯ Central Controller: ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæƒ…å ±ã‚’æœ€çµ‚çµæœã«çµ±åˆ
                    if unified_result.get('sub_slots'):
                        if 'sub_slots' not in rephrase_result:
                            rephrase_result['sub_slots'] = {}
                        rephrase_result['sub_slots'].update(unified_result['sub_slots'])
                        print(f"ğŸ¯ Central Controller: Sub-slots merged to final result: {rephrase_result['sub_slots']}")
                    
                    # ğŸ¯ Central Controller: ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ­ãƒƒãƒˆä¿®æ­£ï¼ˆé–¢ä¿‚ç¯€åˆ†é›¢å¯¾å¿œï¼‰
                    if unified_result.get('relative_clause_info', {}).get('found'):
                        main_sentence = unified_result['relative_clause_info']['main_sentence']
                        print(f"ğŸ¯ Central Controller: Analyzing main sentence for correct slots: '{main_sentence}'")
                        
                        # ä¸»æ–‡ã‚’å†åˆ†æã—ã¦ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ­ãƒƒãƒˆã‚’æ­£ã—ãè¨­å®š
                        main_doc = self.grammar_mapper.nlp(main_sentence)
                        main_analysis = self.grammar_mapper._analyze_sentence_legacy(main_sentence, main_doc)
                        if main_analysis and 'slots' in main_analysis:
                            # ä¸­å¤®åˆ¶å¾¡: ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã¨é‡è¤‡ã—ãªã„ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ­ãƒƒãƒˆã®ã¿æ¡ç”¨
                            for slot_name, slot_value in main_analysis['slots'].items():
                                if slot_value and slot_name not in ['sub-s', 'sub-v', 'sub-aux', 'sub-c1', 'sub-o1']:
                                    # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®å€¤ã¨é‡è¤‡ãƒã‚§ãƒƒã‚¯
                                    is_duplicate = False
                                    for sub_name, sub_value in unified_result.get('sub_slots', {}).items():
                                        if sub_value and str(slot_value).lower() in str(sub_value).lower():
                                            print(f"ğŸ¯ Central Controller: Skipping main slot {slot_name}='{slot_value}' (duplicate with {sub_name}='{sub_value}')")
                                            is_duplicate = True
                                            break
                                    
                                    if not is_duplicate:
                                        # ğŸ¯ Central Controller: è‡ªå‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹åˆ¥å‡¦ç†
                                        if slot_name == 'O1' and 'arrived' in main_sentence:
                                            # "arrived"ã¯è‡ªå‹•è©ãªã®ã§ã€O1ï¼ˆç›®çš„èªï¼‰ã¯ä¸è¦
                                            print(f"ğŸ¯ Central Controller: Skipping O1='{slot_value}' (arrived is intransitive verb)")
                                            continue
                                        
                                        rephrase_result['slots'][slot_name] = slot_value
                                        rephrase_result['main_slots'][slot_name] = slot_value
                                        print(f"ğŸ¯ Central Controller: Main slot set {slot_name}='{slot_value}'")
                        
                print(f"ğŸ”¥ Phase 2: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œå®Œäº†")
            except Exception as e:
                self.logger.error(f"çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ğŸ†• Phase 2: å‰¯è©å‡¦ç†ã®è¿½åŠ  (Direct Implementation)
            try:
                additional_adverbs = self.grammar_mapper._detect_and_assign_adverbs_direct(doc, rephrase_result)
                if additional_adverbs:
                    print(f"ğŸ”¥ Phase 2: å‰¯è©å‡¦ç†ã«ã‚ˆã‚Š {len(additional_adverbs)}å€‹ã®å‰¯è©ã‚’è¿½åŠ ")
                    rephrase_result['main_slots'].update(additional_adverbs)
                    rephrase_result['slots'].update(additional_adverbs)
            except Exception as e:
                self.logger.error(f"å‰¯è©å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ğŸ†• Phase 1.2: æ–‡å‹æƒ…å ±ã‚’çµæœã«è¿½åŠ 
            rephrase_result['sentence_type'] = sentence_type
            rephrase_result['sentence_type_confidence'] = sentence_type_confidence
            
            # ğŸ¯ Central Controller: æœ€çµ‚çµ±åˆãƒã‚§ãƒƒã‚¯
            if hasattr(self.grammar_mapper, 'last_unified_result') and self.grammar_mapper.last_unified_result:
                print(f"ğŸ¯ Central Controller: Final integration check")
                
                # çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼æƒ…å ±ã‚’æœ€çµ‚çµæœã«çµ±åˆ
                if 'unified_handlers' in self.grammar_mapper.last_unified_result:
                    rephrase_result['unified_handlers'] = self.grammar_mapper.last_unified_result['unified_handlers']
                
                # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæœ€çµ‚ãƒã‚§ãƒƒã‚¯
                unified_sub_slots = self.grammar_mapper.last_unified_result.get('sub_slots', {})
                if unified_sub_slots:
                    if 'sub_slots' not in rephrase_result:
                        rephrase_result['sub_slots'] = {}
                    
                    # ä¸­å¤®åˆ¶å¾¡: ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆçµ±åˆ
                    for sub_name, sub_value in unified_sub_slots.items():
                        if sub_value:
                            rephrase_result['sub_slots'][sub_name] = sub_value
                    
                    print(f"ğŸ¯ Central Controller: Final sub_slots = {rephrase_result.get('sub_slots', {})}")
            
            return rephrase_result
            
        except Exception as e:
            self.logger.error(f"PureCentralControllerè§£æã‚¨ãƒ©ãƒ¼: {e}")
            return self.grammar_mapper._create_error_result(sentence, str(e))

    def _pure_extract_tokens(self, doc) -> List[Dict]:
        """
        ğŸ¯ Phase A3-2b: ç‹¬ç«‹spaCyãƒˆãƒ¼ã‚¯ãƒ³æŠ½å‡ºï¼ˆæŠ€è¡“çš„è² å‚µé™¤å»ç‰ˆï¼‰
        
        ç´”ç²‹ãªspaCyãƒ™ãƒ¼ã‚¹ã®ãƒˆãƒ¼ã‚¯ãƒ³æŠ½å‡º
        - Stanzaä¾å­˜é–¢ä¿‚ã‚’å®Œå…¨é™¤å»
        - ä¸è¦ãªä¾å­˜é–¢ä¿‚æƒ…å ±ã‚’å‰Šé™¤
        - ã‚¯ãƒªãƒ¼ãƒ³ãªå“è©ãƒ™ãƒ¼ã‚¹åˆ†æã«ç‰¹åŒ–
        
        Args:
            doc: spaCy document
            
        Returns:
            List[Dict]: ã‚¯ãƒªãƒ¼ãƒ³ãªãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ãƒªã‚¹ãƒˆ
        """
        tokens = []
        for token in doc:
            # ğŸ”¥ ã‚¯ãƒªãƒ¼ãƒ³å®Ÿè£…: å¿…è¦æœ€å°é™ã®æƒ…å ±ã®ã¿æŠ½å‡º
            token_info = {
                'text': token.text,
                'pos': token.pos_,      # ä¸»è¦å“è©
                'tag': token.tag_,      # è©³ç´°å“è©ã‚¿ã‚°
                'lemma': token.lemma_,  # åŸºæœ¬å½¢
                'is_stop': token.is_stop,  # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰åˆ¤å®š
                'is_alpha': token.is_alpha,  # ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆåˆ¤å®š
                'index': token.i,       # ãƒˆãƒ¼ã‚¯ãƒ³ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                # ğŸš« æŠ€è¡“çš„è² å‚µé™¤å»: ä¾å­˜é–¢ä¿‚æƒ…å ±ã¯ä¸€åˆ‡ä½¿ç”¨ã—ãªã„
            }
            tokens.append(token_info)
        return tokens

    def _pure_extract_tokens(self, doc) -> List[Dict]:
        """
        ğŸ¯ Phase A3-2b: ç´”ç²‹å“è©+äººé–“çš„èªè­˜ã«ã‚ˆã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æŠ½å‡º
        
        spaCyã®ä¾å­˜é–¢ä¿‚ã‚’ä¸€åˆ‡ä½¿ç”¨ã›ãšã€å“è©æƒ…å ±ã®ã¿ã§
        äººé–“çš„ãªæ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰
        
        Args:
            doc: spaCyãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
            
        Returns:
            List[Dict]: ç´”ç²‹å“è©ãƒ™ãƒ¼ã‚¹ã®ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±
        """
        tokens = []
        for token in doc:
            token_info = {
                'text': token.text,
                'pos': token.pos_,           # âœ… å“è©æƒ…å ±ã®ã¿ä½¿ç”¨
                'tag': token.tag_,           # âœ… è©³ç´°å“è©ã‚¿ã‚°ã®ã¿ä½¿ç”¨
                'lemma': token.lemma_,       # âœ… èªå¹¹æƒ…å ±ã®ã¿ä½¿ç”¨
                # ğŸ¯ Pure Central: ä¾å­˜é–¢ä¿‚å®Œå…¨é™¤å»ï¼ˆäººé–“çš„èªè­˜ã‚·ã‚¹ãƒ†ãƒ ã¸ï¼‰
                'dep': 'PURE_ANALYSIS',      # ä¾å­˜é–¢ä¿‚ã¯äººé–“çš„æ–‡æ³•ã§ä»£æ›¿
                'head': '',                  # ãƒ˜ãƒƒãƒ‰æƒ…å ±ã¯èªé †+å“è©ã§ä»£æ›¿
                'head_idx': -1,              # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ä½ç½®é–¢ä¿‚ã§ä»£æ›¿
                'is_stop': token.is_stop,    # âœ… åŸºæœ¬å±æ€§ã®ã¿ä½¿ç”¨
                'is_alpha': token.is_alpha,  # âœ… åŸºæœ¬å±æ€§ã®ã¿ä½¿ç”¨
                'index': token.i             # âœ… ä½ç½®æƒ…å ±ã®ã¿ä½¿ç”¨
            }
            tokens.append(token_info)
        return tokens

    def _pure_convert_spacy_to_dict_tokens(self, spacy_tokens):
        """
        ğŸ¯ Phase A3-2b: spaCyãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›ï¼ˆç´”ç²‹POS+äººé–“çš„èªè­˜ï¼‰
        
        åˆ¶ç´„:
        - ä¾å­˜é–¢ä¿‚æƒ…å ±ã‚’ä½¿ç”¨ã—ãªã„
        - POSæƒ…å ±ã®ã¿ã§è¾æ›¸å½¢å¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½œæˆ
        - äººé–“çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œ
        
        Args:
            spacy_tokens: spaCyãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            List[Dict]: ç´”ç²‹POSè¾æ›¸å½¢å¼ãƒˆãƒ¼ã‚¯ãƒ³
        """
        dict_tokens = []
        for i, token in enumerate(spacy_tokens):
            dict_token = {
                'id': i,
                'text': token.text,
                'pos': token.pos_,           # âœ… POSåˆ†æã®ã¿ä½¿ç”¨
                'tag': token.tag_,           # âœ… è©³ç´°å“è©ã‚¿ã‚°
                'dep': 'PURE_ANALYSIS',      # ğŸ¯ ä¾å­˜é–¢ä¿‚ã¯ä½¿ã‚ãªã„
                'lemma': token.lemma_,
                'is_alpha': token.is_alpha,
                'is_stop': token.is_stop,
                'is_punct': token.is_punct,
                'idx': getattr(token, 'idx', i)
            }
            dict_tokens.append(dict_token)
        return dict_tokens

    def _pure_is_auxiliary_verb(self, token: Dict) -> bool:
        """
        ğŸ¯ Phase A3-2b: åŠ©å‹•è©åˆ¤å®š - ç‹¬ç«‹å®Ÿè£…ç‰ˆ
        
        åˆ¶ç´„:
        - POSæƒ…å ±ã®ã¿ã‚’ä½¿ç”¨
        - ä¾å­˜é–¢ä¿‚æƒ…å ±ã¯ä½¿ç”¨ã—ãªã„
        - äººé–“çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ æº–æ‹ 
        
        Args:
            token (Dict): è¾æ›¸å½¢å¼ãƒˆãƒ¼ã‚¯ãƒ³
            
        Returns:
            bool: åŠ©å‹•è©ã®å ´åˆTrue
        """
        aux_words = {
            'be', 'am', 'is', 'are', 'was', 'were', 'being', 'been',
            'have', 'has', 'had', 'having',
            'do', 'does', 'did', 'doing',
            'will', 'would', 'shall', 'should', 'can', 'could',
            'may', 'might', 'must', 'ought'
        }
        
        # lemmaåŸºæº–ã§ã®åˆ¤å®š
        if 'lemma' in token and token['lemma'].lower() in aux_words:
            return True
            
        # textåŸºæº–ã§ã®åˆ¤å®šï¼ˆlemmaãŒãªã„å ´åˆï¼‰
        if token['text'].lower() in aux_words:
            return True
            
        # å“è©ã‚¿ã‚°ã§ã®åˆ¤å®š
        if 'tag' in token and token['tag'] == 'MD':  # Modal verbs
            return True
            
        return False

    def _pure_can_be_complement(self, token: Dict) -> bool:
        """
        ğŸ¯ Phase A3-2b: è£œèªåˆ¤å®š - ç‹¬ç«‹å®Ÿè£…ç‰ˆ
        
        åˆ¶ç´„:
        - POSæƒ…å ±ã®ã¿ã‚’ä½¿ç”¨
        - ä¾å­˜é–¢ä¿‚æƒ…å ±ã¯ä½¿ç”¨ã—ãªã„
        - äººé–“çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ æº–æ‹ 
        
        Args:
            token (Dict): è¾æ›¸å½¢å¼ãƒˆãƒ¼ã‚¯ãƒ³
            
        Returns:
            bool: è£œèªã«ãªã‚Šå¾—ã‚‹å ´åˆTrue
        """
        # å½¢å®¹è©ã¯è£œèªã«ãªã‚Œã‚‹
        if token['pos'] == 'ADJ':
            return True
            
        # åè©é¡ã‚‚è£œèªã«ãªã‚Œã‚‹  
        if token['pos'] in ['NOUN', 'PROPN', 'PRON']:
            return True
            
        # è©³ç´°å“è©ã‚¿ã‚°ã§ã®åˆ¤å®š
        if 'tag' in token:
            if token['tag'] in ['JJ', 'NN', 'NNS', 'PRP']:
                return True
                
        return False

    def _init_ambiguous_word_resolver(self):
        """
        ğŸ§  Phase A3-5: äººé–“çš„åˆ¤å®šã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        
        UnifiedStanzaRephraseMapperã®æˆåŠŸæŠ€è¡“ç¶™æ‰¿
        """
        return {
            'two_case_trial_system': True,
            'syntactic_completeness_evaluator': True,
            'context_aware_pos_correction': True,
            'ambiguous_patterns': {
                'lives': ['NOUN', 'VERB'],
                'works': ['NOUN', 'VERB'], 
                'studies': ['NOUN', 'VERB'],
                'processes': ['NOUN', 'VERB']
            }
        }
    
    def _init_syntactic_evaluator(self):
        """
        ğŸ§  Phase A3-5: æ§‹æ–‡å®Œå…¨æ€§è©•ä¾¡å™¨åˆæœŸåŒ–
        """
        return {
            'completeness_metrics': {
                'basic_structure': 0.4,  # S-VåŸºæœ¬æ§‹é€ 
                'modifier_integration': 0.3,  # ä¿®é£¾èªçµ±åˆ
                'semantic_coherence': 0.3  # æ„å‘³çš„ä¸€è²«æ€§
            },
            'evaluation_criteria': {
                'verb_subject_agreement': True,
                'modifier_attachment': True,
                'semantic_plausibility': True
            }
        }
    
    def _pure_find_main_verb(self, tokens: List[Dict]) -> Optional[int]:
        """
        ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’ç‰¹å®šï¼ˆç‹¬ç«‹ç‰ˆãƒ»å“è©ã®ã¿ä½¿ç”¨ï¼‰
        ä¾å­˜é–¢ä¿‚è§£æã‚’ä½¿ã‚ãšã€å“è©æƒ…å ±ã¨èªé †ã®ã¿ã§ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’ç‰¹å®š
        """
        # ğŸ¯ Step 1: å“è©ãƒ™ãƒ¼ã‚¹ã®å‹•è©å€™è£œã‚’åé›†
        pos_candidates = []
        for i, token in enumerate(tokens):
            # å‹•è©ã®å“è©ã‚¿ã‚°
            if (token['tag'].startswith('VB') and token['pos'] == 'VERB') or token['pos'] == 'AUX':
                pos_candidates.append((i, token))
        
        # ğŸ¯ Step 2: æ–‡è„ˆçš„å‹•è©è­˜åˆ¥ï¼ˆPOSèª¤èªè­˜å¯¾ç­–ï¼‰
        contextual_candidates = self._pure_find_contextual_verbs(tokens)
        
        # ğŸ¯ Step 3: ä¸¡æ–¹ã‚’çµ±åˆï¼ˆé‡è¤‡é™¤å»ï¼‰
        verb_candidates = pos_candidates.copy()
        for i, token in contextual_candidates:
            # æ—¢ã«å­˜åœ¨ã—ãªã„å ´åˆã®ã¿è¿½åŠ 
            if not any(existing_i == i for existing_i, _ in verb_candidates):
                verb_candidates.append((i, token))
        
        if not verb_candidates:
            return None
        
        # ğŸ¯ Step 4: äººé–“çš„åˆ¤å®šï¼šé–¢ä¿‚ç¯€ã‚’é™¤å¤–ã—ã¦ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’ç‰¹å®š
        non_relative_verbs = []
        
        for i, token in verb_candidates:
            # é–¢ä¿‚ä»£åè©ã®ç›´å¾Œã®å‹•è©ã¯é–¢ä¿‚ç¯€å†…å‹•è©ã¨ã—ã¦é™¤å¤–
            is_in_relative_clause = False
            
            # å‰ã®å˜èªã‚’ç¢ºèªï¼ˆé–¢ä¿‚ç¯€åˆ¤å®šï¼‰
            for j in range(max(0, i-5), i):  # 5èªå‰ã¾ã§ç¢ºèª
                prev_token = tokens[j]
                if prev_token['text'].lower() in ['who', 'whom', 'which', 'that', 'whose', 'where', 'when']:
                    # whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†: å‹•è©/åè©åŒå½¢èªã¯é–¢ä¿‚ç¯€å¤–ã®ãƒ¡ã‚¤ãƒ³å‹•è©ã¨ã—ã¦æ‰±ã†
                    if (prev_token['text'].lower() == 'whose' and 
                        token['text'].lower() in getattr(self, 'ambiguous_words', []) and
                        token.get('contextual_override', False)):
                        # whoseæ§‹æ–‡ã§ã®åŒå½¢èªå‹•è©ã¯é–¢ä¿‚ç¯€å¤–ã¨ã—ã¦æ‰±ã†
                        is_in_relative_clause = False
                        break
                    
                    # é–¢ä¿‚ä»£åè©ã‹ã‚‰å‹•è©ã¾ã§ã®è·é›¢ãŒè¿‘ã„å ´åˆã€é–¢ä¿‚ç¯€å†…å‹•è©
                    if i - j <= 4:  # 4èªä»¥å†…ãªã‚‰é–¢ä¿‚ç¯€å†…
                        is_in_relative_clause = True
                        break
            
            if not is_in_relative_clause:
                non_relative_verbs.append((i, token))
        
        # ğŸ¯ Step 5: ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’æ±ºå®š
        if non_relative_verbs:
            # ãƒ¡ã‚¤ãƒ³å‹•è©å€™è£œã‹ã‚‰åŠ©å‹•è©ã§ãªã„ã‚‚ã®ã‚’å„ªå…ˆ
            main_verbs = [(i, token) for i, token in non_relative_verbs if not self._pure_is_auxiliary_verb(token)]
            if main_verbs:
                # æ–‡ã®å¾ŒåŠã«ã‚ã‚‹ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’å„ªå…ˆï¼ˆé–¢ä¿‚ç¯€ã®å¾Œï¼‰
                return main_verbs[-1][0]
            return non_relative_verbs[-1][0]
        
        # æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦ã€ã©ã®å‹•è©ã§ã‚‚é¸æŠ
        return verb_candidates[-1][0]

    def _pure_find_contextual_verbs(self, tokens: List[Dict]) -> List[Tuple[int, Dict]]:
        """
        äººé–“çš„æ–‡æ³•èªè­˜ã«ã‚ˆã‚‹å‹•è©è­˜åˆ¥ï¼ˆç‹¬ç«‹ç‰ˆï¼‰
        æ§‹æ–‡çš„æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã§æœ€é©ãªå“è©ã‚’æ±ºå®š
        """
        contextual_verbs = []
        sentence_text = ' '.join([token['text'] for token in tokens])
        
        # ã‚ˆãèª¤èªè­˜ã•ã‚Œã‚‹å‹•è©ã®ãƒªã‚¹ãƒˆ
        common_verbs = {
            'lives', 'live', 'lived', 'living',
            'works', 'work', 'worked', 'working',
            'runs', 'run', 'ran', 'running',
            'goes', 'go', 'went', 'going',
            'comes', 'come', 'came', 'coming',
            'sits', 'sit', 'sat', 'sitting',
            'stands', 'stand', 'stood', 'standing',
            'plays', 'play', 'played', 'playing'
        }
        
        for i, token in enumerate(tokens):
            # æ—¢ã«å‹•è©ã¨ã—ã¦èªè­˜ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®
            if token['pos'] == 'VERB':
                contextual_verbs.append((i, token))
                continue
            
            # ğŸ†• äººé–“çš„å“è©æ±ºå®š: æ§‹æ–‡çš„æ•´åˆæ€§ã«ã‚ˆã‚‹é¸æŠ
            word = token['text'].lower()
            if word in getattr(self, 'ambiguous_words', []):
                optimal_pos = self._pure_resolve_ambiguous_word(token, tokens, i, sentence_text)
                
                if optimal_pos == 'VERB':
                    verb_token = token.copy()
                    verb_token['pos'] = 'VERB'
                    verb_token['human_grammar_correction'] = True
                    verb_token['resolution_method'] = 'syntactic_consistency'
                    contextual_verbs.append((i, verb_token))
                continue
            
            # è¾æ›¸ã«å«ã¾ã‚Œã‚‹ä¸€èˆ¬çš„ãªå‹•è©
            if word in common_verbs:
                contextual_verbs.append((i, token))
            
            # èªå°¾ã«ã‚ˆã‚‹å‹•è©åˆ¤å®šï¼ˆ-s, -ed, -ingï¼‰
            elif (word.endswith('s') and len(word) > 2 and 
                  not word.endswith('ss') and not word.endswith('us')):
                # ä¸‰äººç§°å˜æ•°å½¢ã‚‰ã—ã„èª
                if self._pure_looks_like_verb_context(tokens, i):
                    contextual_verbs.append((i, token))
            
            # ãã®ä»–ã®å‹•è©å€™è£œï¼ˆaux, modalå«ã‚€ï¼‰
            elif token['pos'] in ['AUX', 'MODAL']:
                contextual_verbs.append((i, token))
        
        return contextual_verbs

    def _pure_resolve_ambiguous_word(self, token: Dict, tokens: List[Dict], position: int, sentence: str) -> str:
        """
        äººé–“çš„å“è©æ±ºå®š: æ§‹æ–‡çš„æ•´åˆæ€§ã«ã‚ˆã‚‹æ›–æ˜§èªè§£æ±ºï¼ˆç‹¬ç«‹ç‰ˆï¼‰
        """
        word_text = token['text'].lower()
        
        # åŸºæœ¬çš„ãªæ–‡è„ˆãƒã‚§ãƒƒã‚¯
        # å‰ã®èªãŒåè©ãƒ»ä»£åè©ãªã‚‰å‹•è©ã®å¯èƒ½æ€§ãŒé«˜ã„
        if position > 0:
            prev_token = tokens[position - 1]
            if prev_token['pos'] in ['NOUN', 'PRON', 'PROPN']:
                return 'VERB'
        
        # å¾Œã®èªãŒå‰¯è©ãªã‚‰å‹•è©ã®å¯èƒ½æ€§ãŒé«˜ã„
        if position < len(tokens) - 1:
            next_token = tokens[position + 1]
            if next_token['pos'] == 'ADV':
                return 'VERB'
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å…ƒã®å“è©
        return token['pos']

    def _pure_looks_like_verb_context(self, tokens: List[Dict], index: int) -> bool:
        """
        å‹•è©ã‚‰ã—ã„æ–‡è„ˆã‹ã‚’åˆ¤å®šï¼ˆç‹¬ç«‹ç‰ˆï¼‰
        """
        if index == 0:
            return False
        
        # å‰ã®èªãŒåè©ãƒ»ä»£åè©ãªã‚‰å‹•è©ã®å¯èƒ½æ€§ãŒé«˜ã„
        prev_token = tokens[index - 1]
        if prev_token['pos'] in ['NOUN', 'PRON', 'PROPN']:
            return True
        
        # å¾Œã®èªãŒå‰¯è©ãªã‚‰å‹•è©ã®å¯èƒ½æ€§ãŒé«˜ã„
        if index < len(tokens) - 1:
            next_token = tokens[index + 1]
            if next_token['pos'] == 'ADV':
                return True
        
        return False
    
    def _pure_identify_core_elements(self, tokens: List[Dict]) -> Dict[str, Any]:
        """
        æ–‡ã®æ ¸è¦ç´ ï¼ˆä¸»èªãƒ»å‹•è©ï¼‰ã‚’ç‰¹å®šï¼ˆç‹¬ç«‹ç‰ˆï¼‰
        ã“ã‚ŒãŒå…¨ã¦ã®æ–‡å‹èªè­˜ã®åŸºç›¤ã¨ãªã‚‹
        """
        core = {
            'subject': None,
            'verb': None,
            'subject_indices': [],
            'verb_indices': [],
            'auxiliary': None,
            'auxiliary_indices': []
        }
        
        # ğŸ¯ Step 1: å‹•è©ã‚’æ¢ã™ï¼ˆæœ€ã‚‚é‡è¦ï¼‰
        main_verb_idx = self._pure_find_main_verb(tokens)
        if main_verb_idx is not None:
            core['verb'] = tokens[main_verb_idx]
            core['verb_indices'] = [main_verb_idx]
            
            # ğŸ¯ Step 2: åŠ©å‹•è©ã‚’æ¢ã™
            aux_idx = self._pure_find_auxiliary(tokens, main_verb_idx)
            if aux_idx is not None:
                core['auxiliary'] = tokens[aux_idx]
                core['auxiliary_indices'] = [aux_idx]
        
        # ğŸ¯ Step 3: ä¸»èªã‚’æ¢ã™ï¼ˆå‹•è©ã®å‰ã§æœ€ã‚‚é©åˆ‡ãªåè©å¥ï¼‰
        if main_verb_idx is not None:
            subject_indices = self._pure_find_subject(tokens, main_verb_idx)
            if subject_indices:
                core['subject_indices'] = subject_indices
                core['subject'] = ' '.join([tokens[i]['text'] for i in subject_indices])
        
        return core

    def _pure_find_auxiliary(self, tokens: List[Dict], main_verb_idx: int) -> Optional[int]:
        """åŠ©å‹•è©ã‚’ç‰¹å®šï¼ˆç‹¬ç«‹ç‰ˆï¼‰"""
        # ãƒ¡ã‚¤ãƒ³å‹•è©ã®å‰ã‚’æ¢ã™
        for i in range(main_verb_idx):
            token = tokens[i]
            if self._pure_is_auxiliary_verb(token):
                return i
        return None

    def _pure_find_subject(self, tokens: List[Dict], verb_idx: int) -> List[int]:
        """
        ä¸»èªã‚’ç‰¹å®šï¼ˆç‹¬ç«‹ç‰ˆãƒ»å‹•è©ã®å‰ã®åè©å¥ï¼‰
        è¤‡æ•°èªã®åè©å¥ã«å¯¾å¿œ
        é–¢ä¿‚ç¯€ã‚’å«ã‚€å ´åˆã¯é–¢ä¿‚ç¯€å…¨ä½“ã‚’ä¸»èªã«å«ã‚ã‚‹
        """
        subject_indices = []
        
        # ğŸ†• é–¢ä¿‚ç¯€ã‚’å«ã‚€ä¸»èªã®ç‰¹å®šï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        # ãƒˆãƒ¼ã‚¯ãƒ³ã«é–¢ä¿‚ç¯€ãƒãƒ¼ã‚«ãƒ¼ãŒã‚ã‚‹å ´åˆã®å‡¦ç†
        antecedent_idx = None
        relative_clause_end_idx = None
        
        for i, token in enumerate(tokens):
            if token.get('is_antecedent', False):
                antecedent_idx = i
            if token.get('is_relative_pronoun', False):
                # é–¢ä¿‚ç¯€ã®å®Ÿéš›ã®çµ‚äº†ä½ç½®ã‚’ä½¿ç”¨
                relative_clause_end_idx = token.get('relative_clause_end', verb_idx - 1)
                break
        
        # é–¢ä¿‚ç¯€ã‚’å«ã‚€ä¸»èªã®å ´åˆ
        if antecedent_idx is not None and relative_clause_end_idx is not None:
            # ğŸ”§ Rephraseã‚·ã‚¹ãƒ†ãƒ ä»•æ§˜: é–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆã§ã‚‚é€šå¸¸ã®ä¸»èªæ¤œå‡ºã‚’è¡Œã†
            # _assign_grammar_rolesã§ã€Œã‹ãŸã¾ã‚Šã€åˆ¤å®šã«ã‚ˆã‚Šç©ºã«ã™ã‚‹ã‹ã‚’æ±ºå®š
            pass  # é€šå¸¸ã®ä¸»èªæ¤œå‡ºã‚’ç¶™ç¶š
        
        # é€šå¸¸ã®ä¸»èªæ¤œå‡ºï¼ˆé–¢ä¿‚ç¯€ã‚ã‚Šãƒ»ãªã—ä¸¡å¯¾å¿œï¼‰
        # å‹•è©ã®å‰ã‚’å³ã‹ã‚‰å·¦ã«æ¢ã™
        for i in range(verb_idx - 1, -1, -1):
            token = tokens[i]
            
            # åŠ©å‹•è©ã¯é£›ã°ã™
            if self._pure_is_auxiliary_verb(token):
                continue
            
            # åè©ãƒ»ä»£åè©ãƒ»å† è©ã‚’ä¸»èªã®ä¸€éƒ¨ã¨ã—ã¦åé›†
            if (token['pos'] in ['NOUN', 'PROPN', 'PRON'] or 
                token['tag'] in ['DT', 'PRP', 'PRP$', 'WP']):
                subject_indices.insert(0, i)  # é †åºã‚’ä¿ã¤ãŸã‚å‰ã«æŒ¿å…¥
            else:
                # ä¸»èªã®å¢ƒç•Œã«åˆ°é”
                break
        
        return subject_indices
    
    def _pure_extract_phrase_boundaries(self, tokens):
        """
        ç‹¬ç«‹ãƒ¡ã‚½ãƒƒãƒ‰: å¥å¢ƒç•Œã‚’æŠ½å‡ºï¼ˆPOSåˆ†æã®ã¿ä½¿ç”¨ï¼‰
        ä¾å­˜æ§‹æ–‡è§£æã‚’ä½¿ã‚ãšã«POSãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿ã§å¥å¢ƒç•Œã‚’è­˜åˆ¥
        """
        phrases = []
        current_phrase = None
        
        for i, token in enumerate(tokens):
            pos = token.get('pos_', '')
            tag = token.get('tag_', '')
            text = token.get('text', '')
            
            # åè©å¥ã®é–‹å§‹
            if pos in ['DET', 'ADJ'] or tag in ['DT', 'JJ', 'JJR', 'JJS', 'PRP$']:
                if current_phrase and current_phrase['type'] != 'noun_phrase':
                    phrases.append(current_phrase)
                    current_phrase = None
                
                if not current_phrase:
                    current_phrase = {
                        'type': 'noun_phrase',
                        'start': i,
                        'tokens': [token],
                        'text': text
                    }
                else:
                    current_phrase['tokens'].append(token)
                    current_phrase['text'] += ' ' + text
            
            # åè©å¥ã®ç¶™ç¶š
            elif pos in ['NOUN', 'PROPN', 'PRON'] or tag in ['NN', 'NNS', 'NNP', 'NNPS', 'PRP']:
                if current_phrase and current_phrase['type'] == 'noun_phrase':
                    current_phrase['tokens'].append(token)
                    current_phrase['text'] += ' ' + text
                else:
                    if current_phrase:
                        phrases.append(current_phrase)
                    current_phrase = {
                        'type': 'noun_phrase',
                        'start': i,
                        'tokens': [token],
                        'text': text
                    }
            
            # å‹•è©å¥ã®é–‹å§‹
            elif pos in ['VERB', 'AUX'] or tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'MD']:
                if current_phrase:
                    phrases.append(current_phrase)
                
                current_phrase = {
                    'type': 'verb_phrase',
                    'start': i,
                    'tokens': [token],
                    'text': text
                }
            
            # å‰¯è©å¥ã®é–‹å§‹
            elif pos == 'ADV' or tag in ['RB', 'RBR', 'RBS']:
                if current_phrase and current_phrase['type'] == 'verb_phrase':
                    # å‹•è©å¥ã«å‰¯è©ã‚’è¿½åŠ 
                    current_phrase['tokens'].append(token)
                    current_phrase['text'] += ' ' + text
                else:
                    if current_phrase:
                        phrases.append(current_phrase)
                    current_phrase = {
                        'type': 'adverb_phrase',
                        'start': i,
                        'tokens': [token],
                        'text': text
                    }
            
            # å‰ç½®è©å¥ã®é–‹å§‹
            elif pos == 'ADP' or tag in ['IN', 'TO']:
                if current_phrase:
                    phrases.append(current_phrase)
                
                current_phrase = {
                    'type': 'prepositional_phrase',
                    'start': i,
                    'tokens': [token],
                    'text': text
                }
            
            # å½¢å®¹è©å¥ã®é–‹å§‹
            elif pos == 'ADJ' or tag in ['JJ', 'JJR', 'JJS']:
                if current_phrase and current_phrase['type'] in ['noun_phrase']:
                    # åè©å¥ã«å½¢å®¹è©ã‚’è¿½åŠ 
                    current_phrase['tokens'].append(token)
                    current_phrase['text'] += ' ' + text
                else:
                    if current_phrase:
                        phrases.append(current_phrase)
                    current_phrase = {
                        'type': 'adjective_phrase',
                        'start': i,
                        'tokens': [token],
                        'text': text
                    }
            
            # ãã®ä»–ã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå¥å¢ƒç•Œã®çµ‚äº†ï¼‰
            else:
                if current_phrase:
                    phrases.append(current_phrase)
                    current_phrase = None
        
        # æœ€å¾Œã®å¥ã‚’è¿½åŠ 
        if current_phrase:
            phrases.append(current_phrase)
        
        # å¥å¢ƒç•Œæƒ…å ±ã‚’è¿½åŠ 
        for phrase in phrases:
            phrase['end'] = phrase['start'] + len(phrase['tokens']) - 1
            phrase['length'] = len(phrase['tokens'])
        
        return phrases
    
    def _pure_assign_grammar_roles(self, tokens, phrases):
        """
        ç‹¬ç«‹ãƒ¡ã‚½ãƒƒãƒ‰: æ–‡æ³•çš„å½¹å‰²ã‚’å‰²ã‚Šå½“ã¦ï¼ˆPOSåˆ†æã®ã¿ä½¿ç”¨ï¼‰
        ä¾å­˜æ§‹æ–‡è§£æã‚’ä½¿ã‚ãšã«POSãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿ã§æ–‡æ³•å½¹å‰²ã‚’è­˜åˆ¥
        """
        roles = []
        verb_indices = []
        
        # å‹•è©ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’äº‹å‰ã«åé›†
        for i, token in enumerate(tokens):
            if token.get('pos_') in ['VERB', 'AUX'] or token.get('tag_') in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:
                verb_indices.append(i)
        
        main_verb_idx = None
        if verb_indices:
            # æœ€åˆã®ä¸»è¦å‹•è©ã‚’ç‰¹å®š
            for idx in verb_indices:
                token = tokens[idx]
                if not self._pure_is_auxiliary_verb(token):
                    main_verb_idx = idx
                    break
            
            # åŠ©å‹•è©ã®ã¿ã®å ´åˆã¯æœ€å¾Œã®å‹•è©ã‚’ä½¿ç”¨
            if main_verb_idx is None:
                main_verb_idx = verb_indices[-1]
        
        # å„ãƒˆãƒ¼ã‚¯ãƒ³ã«æ–‡æ³•çš„å½¹å‰²ã‚’å‰²ã‚Šå½“ã¦
        for i, token in enumerate(tokens):
            pos = token.get('pos_', '')
            tag = token.get('tag_', '')
            text = token.get('text', '')
            
            role_info = {
                'index': i,
                'text': text,
                'pos': pos,
                'tag': tag,
                'role': 'OTHER'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå½¹å‰²
            }
            
            # ä¸»èªã®ç‰¹å®š
            if main_verb_idx is not None and i < main_verb_idx:
                if pos in ['NOUN', 'PROPN', 'PRON'] or tag in ['NN', 'NNS', 'NNP', 'NNPS', 'PRP']:
                    # å‹•è©ã®ç›´å‰ã®åè©å¥ã‚’ä¸»èªã¨ã™ã‚‹
                    if i == main_verb_idx - 1 or (i < main_verb_idx and tokens[i+1].get('pos_') not in ['NOUN', 'PROPN']):
                        role_info['role'] = 'SUBJECT'
                    else:
                        role_info['role'] = 'SUBJECT_PART'
                elif pos in ['DET', 'ADJ'] or tag in ['DT', 'JJ', 'JJR', 'JJS']:
                    role_info['role'] = 'SUBJECT_MODIFIER'
            
            # å‹•è©ã®ç‰¹å®š
            elif i == main_verb_idx:
                if self._pure_is_auxiliary_verb(token):
                    role_info['role'] = 'AUXILIARY'
                else:
                    role_info['role'] = 'MAIN_VERB'
            
            # åŠ©å‹•è©ã®ç‰¹å®š
            elif main_verb_idx is not None and i < main_verb_idx:
                if self._pure_is_auxiliary_verb(token):
                    role_info['role'] = 'AUXILIARY'
            
            # ç›®çš„èªã®ç‰¹å®š
            elif main_verb_idx is not None and i > main_verb_idx:
                if pos in ['NOUN', 'PROPN', 'PRON'] or tag in ['NN', 'NNS', 'NNP', 'NNPS', 'PRP']:
                    # å‹•è©ã®ç›´å¾Œã®åè©å¥ã‚’ç›®çš„èªã¨ã™ã‚‹
                    if i == main_verb_idx + 1:
                        role_info['role'] = 'DIRECT_OBJECT'
                    else:
                        # å‰ç½®è©ã®å¾Œã®åè©å¥ã¯é–“æ¥ç›®çš„èª
                        if i > 0 and tokens[i-1].get('pos_') == 'ADP':
                            role_info['role'] = 'INDIRECT_OBJECT'
                        else:
                            role_info['role'] = 'OBJECT_PART'
                elif pos in ['DET', 'ADJ'] or tag in ['DT', 'JJ', 'JJR', 'JJS']:
                    role_info['role'] = 'OBJECT_MODIFIER'
            
            # ä¿®é£¾èªã®ç‰¹å®š
            if pos == 'ADV' or tag in ['RB', 'RBR', 'RBS']:
                if main_verb_idx is not None:
                    if i < main_verb_idx:
                        role_info['role'] = 'PRE_VERBAL_ADVERB'
                    else:
                        role_info['role'] = 'POST_VERBAL_ADVERB'
                else:
                    role_info['role'] = 'ADVERB'
            
            # å‰ç½®è©ã®ç‰¹å®š
            elif pos == 'ADP' or tag in ['IN', 'TO']:
                role_info['role'] = 'PREPOSITION'
            
            # è£œèªã®ç‰¹å®šï¼ˆbeå‹•è©ã®å¾Œã®å½¢å®¹è©ãƒ»åè©ï¼‰
            elif main_verb_idx is not None and i > main_verb_idx:
                main_verb_token = tokens[main_verb_idx]
                if main_verb_token.get('lemma_', '').lower() == 'be':
                    if pos in ['ADJ'] or tag in ['JJ', 'JJR', 'JJS']:
                        role_info['role'] = 'ADJECTIVE_COMPLEMENT'
                    elif pos in ['NOUN', 'PROPN'] or tag in ['NN', 'NNS', 'NNP', 'NNPS']:
                        role_info['role'] = 'NOUN_COMPLEMENT'
            
            # é–¢ä¿‚ä»£åè©ã®ç‰¹å®š
            elif tag in ['WP', 'WDT', 'WRB']:
                role_info['role'] = 'RELATIVE_PRONOUN'
            
            # æ¥ç¶šè©ã®ç‰¹å®š
            elif pos == 'CCONJ' or tag in ['CC']:
                role_info['role'] = 'CONJUNCTION'
            
            # æ„Ÿå˜†è©ã®ç‰¹å®š
            elif pos == 'INTJ' or tag in ['UH']:
                role_info['role'] = 'INTERJECTION'
            
            roles.append(role_info)
        
        # å¥ãƒ¬ãƒ™ãƒ«ã§ã®å½¹å‰²çµ±åˆ
        if phrases:
            self._integrate_phrase_roles(roles, phrases)
        
        return roles
    
    def _integrate_phrase_roles(self, roles, phrases):
        """å¥ãƒ¬ãƒ™ãƒ«ã§ã®å½¹å‰²çµ±åˆ"""
        for phrase in phrases:
            start_idx = phrase['start']
            end_idx = phrase['end']
            phrase_type = phrase['type']
            
            # å¥ã‚¿ã‚¤ãƒ—ã«åŸºã¥ãå½¹å‰²ã®çµ±åˆ
            if phrase_type == 'noun_phrase':
                # åè©å¥å†…ã®æœ€åˆã®è¦ç´ ãŒä¸»è¦å½¹å‰²ã‚’æŒã¤
                primary_role = None
                for i in range(start_idx, end_idx + 1):
                    if i < len(roles):
                        if roles[i]['role'] in ['SUBJECT', 'DIRECT_OBJECT', 'INDIRECT_OBJECT']:
                            primary_role = roles[i]['role']
                            break
                
                # å¥å†…ã®ä»–ã®è¦ç´ ã‚’ä¿®é£¾èªã¨ã—ã¦çµ±åˆ
                if primary_role:
                    for i in range(start_idx, end_idx + 1):
                        if i < len(roles) and roles[i]['role'] != primary_role:
                            if primary_role == 'SUBJECT':
                                roles[i]['role'] = 'SUBJECT_MODIFIER'
                            else:
                                roles[i]['role'] = 'OBJECT_MODIFIER'
            
            elif phrase_type == 'verb_phrase':
                # å‹•è©å¥å†…ã®å½¹å‰²ã‚’çµ±åˆ
                for i in range(start_idx, end_idx + 1):
                    if i < len(roles):
                        if roles[i]['role'] in ['POST_VERBAL_ADVERB', 'ADVERB']:
                            roles[i]['role'] = 'VERBAL_MODIFIER'
    
    def _pure_detect_sentence_patterns(self, roles, tokens):
        """
        ç‹¬ç«‹ãƒ¡ã‚½ãƒƒãƒ‰: æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºï¼ˆPOSåˆ†æã®ã¿ä½¿ç”¨ï¼‰
        ä¾å­˜æ§‹æ–‡è§£æã‚’ä½¿ã‚ãšã«æ–‡æ³•å½¹å‰²ã‹ã‚‰5æ–‡å‹ã‚’è­˜åˆ¥
        """
        pattern_info = {
            'pattern': 'UNKNOWN',
            'confidence': 0.0,
            'elements': [],
            'main_verb_idx': None,
            'subject_found': False,
            'object_count': 0,
            'complement_found': False
        }
        
        # åŸºæœ¬è¦ç´ ã®æ¤œå‡º
        subject_roles = []
        verb_roles = []
        object_roles = []
        complement_roles = []
        auxiliary_roles = []
        
        for role_info in roles:
            role = role_info['role']
            
            if role in ['SUBJECT', 'SUBJECT_PART']:
                subject_roles.append(role_info)
                pattern_info['subject_found'] = True
            elif role == 'MAIN_VERB':
                verb_roles.append(role_info)
                pattern_info['main_verb_idx'] = role_info['index']
            elif role == 'AUXILIARY':
                auxiliary_roles.append(role_info)
            elif role in ['DIRECT_OBJECT', 'OBJECT_PART']:
                object_roles.append(role_info)
                pattern_info['object_count'] += 1
            elif role == 'INDIRECT_OBJECT':
                object_roles.append(role_info)
                pattern_info['object_count'] += 1
            elif role in ['ADJECTIVE_COMPLEMENT', 'NOUN_COMPLEMENT']:
                complement_roles.append(role_info)
                pattern_info['complement_found'] = True
        
        # ãƒ¡ã‚¤ãƒ³å‹•è©ã®ç‰¹å®š
        main_verb = None
        if verb_roles:
            main_verb = verb_roles[0]  # æœ€åˆã®ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’ä½¿ç”¨
        elif auxiliary_roles:
            # åŠ©å‹•è©ã®ã¿ã®å ´åˆã€æœ€å¾Œã®åŠ©å‹•è©ã‚’ãƒ¡ã‚¤ãƒ³å‹•è©ã¨ã—ã¦æ‰±ã†
            main_verb = auxiliary_roles[-1]
            pattern_info['main_verb_idx'] = main_verb['index']
        
        if not main_verb:
            pattern_info['pattern'] = 'NO_VERB'
            pattern_info['confidence'] = 0.1
            return pattern_info
        
        # beå‹•è©ã®ç‰¹å®š
        is_be_verb = False
        if main_verb:
            verb_token = tokens[main_verb['index']]
            if verb_token.get('lemma_', '').lower() == 'be':
                is_be_verb = True
        
        # 5æ–‡å‹ã®åˆ¤å®š
        if pattern_info['subject_found'] and main_verb:
            # ç¬¬1æ–‡å‹: SV (ä¸»èª + å‹•è©)
            if pattern_info['object_count'] == 0 and not pattern_info['complement_found']:
                pattern_info['pattern'] = 'SV'
                pattern_info['confidence'] = 0.9
                pattern_info['elements'] = ['S', 'V']
            
            # ç¬¬2æ–‡å‹: SVC (ä¸»èª + å‹•è© + è£œèª)
            elif pattern_info['complement_found'] and is_be_verb:
                pattern_info['pattern'] = 'SVC'
                pattern_info['confidence'] = 0.95
                pattern_info['elements'] = ['S', 'V', 'C']
            
            # ç¬¬3æ–‡å‹: SVO (ä¸»èª + å‹•è© + ç›®çš„èª)
            elif pattern_info['object_count'] == 1 and not pattern_info['complement_found']:
                pattern_info['pattern'] = 'SVO'
                pattern_info['confidence'] = 0.9
                pattern_info['elements'] = ['S', 'V', 'O']
            
            # ç¬¬4æ–‡å‹: SVOO (ä¸»èª + å‹•è© + é–“æ¥ç›®çš„èª + ç›´æ¥ç›®çš„èª)
            elif pattern_info['object_count'] >= 2:
                pattern_info['pattern'] = 'SVOO'
                pattern_info['confidence'] = 0.85
                pattern_info['elements'] = ['S', 'V', 'O1', 'O2']
            
            # ç¬¬5æ–‡å‹: SVOC (ä¸»èª + å‹•è© + ç›®çš„èª + è£œèª)
            elif pattern_info['object_count'] >= 1 and pattern_info['complement_found']:
                pattern_info['pattern'] = 'SVOC'
                pattern_info['confidence'] = 0.85
                pattern_info['elements'] = ['S', 'V', 'O', 'C']
            
            else:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ¤å®š
                if pattern_info['object_count'] > 0:
                    pattern_info['pattern'] = 'SVO'
                    pattern_info['confidence'] = 0.7
                    pattern_info['elements'] = ['S', 'V', 'O']
                else:
                    pattern_info['pattern'] = 'SV'
                    pattern_info['confidence'] = 0.7
                    pattern_info['elements'] = ['S', 'V']
        
        # å‹•è©ã®ã¿ã®å ´åˆ
        elif main_verb and not pattern_info['subject_found']:
            if pattern_info['object_count'] > 0:
                pattern_info['pattern'] = 'VO'  # å‘½ä»¤æ–‡ãªã©
                pattern_info['confidence'] = 0.6
                pattern_info['elements'] = ['V', 'O']
            else:
                pattern_info['pattern'] = 'V'  # å˜ç´”å‹•è©æ–‡
                pattern_info['confidence'] = 0.5
                pattern_info['elements'] = ['V']
        
        # ç‰¹æ®Šãƒ‘ã‚¿ãƒ¼ãƒ³ã®èª¿æ•´
        self._adjust_pattern_confidence(pattern_info, roles, tokens)
        
        return pattern_info
    
    def _adjust_pattern_confidence(self, pattern_info, roles, tokens):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ä¿¡é ¼åº¦ã®èª¿æ•´"""
        
        # å—å‹•æ…‹ã®æ¤œå‡º
        for i, token in enumerate(tokens):
            if (token.get('tag_') in ['VBN'] and 
                i > 0 and tokens[i-1].get('lemma_', '').lower() in ['be', 'get']):
                # å—å‹•æ…‹ã®å ´åˆã€ä¿¡é ¼åº¦ã‚’ä¸Šã’ã‚‹
                if pattern_info['pattern'] in ['SV', 'SVO']:
                    pattern_info['confidence'] = min(0.95, pattern_info['confidence'] + 0.1)
                break
        
        # åŠ©å‹•è©ã®å­˜åœ¨ç¢ºèª
        has_auxiliary = any(role['role'] == 'AUXILIARY' for role in roles)
        if has_auxiliary:
            pattern_info['confidence'] = min(0.95, pattern_info['confidence'] + 0.05)
        
        # å‰ç½®è©å¥ã®å­˜åœ¨ç¢ºèª
        has_preposition = any(role['role'] == 'PREPOSITION' for role in roles)
        if has_preposition:
            # å‰ç½®è©å¥ãŒã‚ã‚‹å ´åˆã€è¤‡é›‘ãªæ§‹é€ ã¨ã—ã¦ä¿¡é ¼åº¦ã‚’å¾®èª¿æ•´
            if pattern_info['pattern'] in ['SV', 'SVO']:
                pattern_info['confidence'] = min(0.9, pattern_info['confidence'] + 0.02)
        
        # é–¢ä¿‚ä»£åè©ã®å­˜åœ¨ç¢ºèª
        has_relative = any(role['role'] == 'RELATIVE_PRONOUN' for role in roles)
        if has_relative:
            # é–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆã€è¤‡é›‘ãªæ§‹é€ ã¨ã—ã¦ä¿¡é ¼åº¦ã‚’èª¿æ•´
            pattern_info['confidence'] = max(0.6, pattern_info['confidence'] - 0.1)
        
        # è¤‡æ•°ã®å‹•è©ãŒã‚ã‚‹å ´åˆ
        verb_count = sum(1 for role in roles if role['role'] in ['MAIN_VERB', 'AUXILIARY'])
        if verb_count > 2:
            pattern_info['confidence'] = max(0.5, pattern_info['confidence'] - 0.15)
        
        # æœ€çµ‚çš„ãªä¿¡é ¼åº¦ã®åˆ¶é™
        pattern_info['confidence'] = max(0.1, min(0.95, pattern_info['confidence']))
    
    def analyze_sentence_pure_management(self, sentence: str) -> Dict[str, Any]:
        """
        ğŸ¯ Phase A3-5: Pure Managementå®Œå…¨ç‰ˆ
        
        ç†å¿µçš„æ©Ÿèƒ½100%å®Ÿè£…ï¼š
        - é«˜åº¦åˆ¶å¾¡æ©Ÿæ§‹ã«ã‚ˆã‚‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ç®¡ç†
        - äººé–“çš„åˆ¤å®šã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹æ›–æ˜§æ€§è§£æ±º
        - æ§‹æ–‡å®Œå…¨æ€§è©•ä¾¡ã«ã‚ˆã‚‹å“è³ªä¿è¨¼
        
        æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨å®Œå…¨ã«åŒã˜çµæœæ§‹é€ ã§è¿”ã™
        å†…éƒ¨å‡¦ç†ã®ã¿Phase A3-5å®Œå…¨æ©Ÿèƒ½ã‚’ä½¿ç”¨
        
        Args:
            sentence (str): è§£æå¯¾è±¡ã®æ–‡ç« 
            
        Returns:
            Dict[str, Any]: ãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›ã®è§£æçµæœï¼ˆå®Œå…¨å“è³ªä¿è¨¼ä»˜ãï¼‰
        """
        self.logger.info(f"ğŸ¯ Phase A3-5: Pure Managementå®Œå…¨ç‰ˆé–‹å§‹: '{sentence}'")
        
        try:
            # ğŸ§  Phase A3-5: äººé–“çš„åˆ¤å®šã«ã‚ˆã‚‹å‰å‡¦ç†
            enhanced_sentence = self._apply_human_judgment_preprocessing(sentence)
            
            # ï¿½ Phase A3-5: é«˜åº¦åˆ¶å¾¡æ©Ÿæ§‹ã«ã‚ˆã‚‹è§£æ
            enhanced_result = self._execute_enhanced_central_control(enhanced_sentence)
            
            # ï¿½ğŸ”§ Phase A3-5: æ§‹æ–‡å®Œå…¨æ€§è©•ä¾¡ã«ã‚ˆã‚‹å“è³ªå‘ä¸Š
            quality_assured_result = self._apply_syntactic_completeness_evaluation(enhanced_result, enhanced_sentence)
            
            # ãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›æ€§ç¢ºä¿
            legacy_result = self.grammar_mapper.analyze_sentence(sentence)
            
            # ğŸ¯ Phase A3-5: å®Œå…¨å“è³ªç®¡ç†æƒ…å ±ã®ãƒ­ã‚°è¨˜éŒ²ï¼ˆçµæœã«ã¯å½±éŸ¿ã—ãªã„ï¼‰
            quality_info = {
                'enhanced_processing': True,
                'human_judgment_applied': True,
                'central_control_used': True,
                'syntactic_evaluation_score': quality_assured_result.get('quality_score', 0.0),
                'slots_count': len(legacy_result.get('slots', {})),
                'sub_slots_count': len(legacy_result.get('sub_slots', {})),
                'confidence': legacy_result.get('confidence', 0.0),
                'phase': 'A3-5'
            }
            self.logger.info(f"ğŸ¯ Phase A3-5 å®Œå…¨å“è³ªç®¡ç†è¨˜éŒ²: {quality_info}")
            
            self.logger.info("ğŸ¯ Phase A3-5: Pure Managementå®Œå…¨ç‰ˆå®Œäº†ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›ï¼‰")
            return legacy_result
            
        except Exception as e:
            self.logger.error(f"âŒ Phase A3-5: Pure Managementå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´æ¥DynamicGrammarMapperã‚’ä½¿ç”¨
            return self.grammar_mapper.analyze_sentence(sentence)
            
        except Exception as e:
            self.logger.error(f"ğŸ”¥ ç´”ç²‹ç®¡ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return self.grammar_mapper._create_error_result(sentence, str(e))
    
    def _apply_human_judgment_preprocessing(self, sentence: str) -> str:
        """
        ğŸ§  Phase A3-5: äººé–“çš„åˆ¤å®šã«ã‚ˆã‚‹å‰å‡¦ç†
        
        UnifiedStanzaRephraseMapperã®æˆåŠŸæŠ€è¡“ã‚’é©ç”¨
        æ›–æ˜§èªå½™ã®å‹•çš„è§£æ±º
        """
        self.logger.debug(f"ğŸ§  äººé–“çš„åˆ¤å®šå‰å‡¦ç†é–‹å§‹: '{sentence}'")
        
        # æ›–æ˜§èªå½™æ¤œå‡ºã¨2ã‚±ãƒ¼ã‚¹è©¦è¡Œ
        doc = self.grammar_mapper.nlp(sentence)
        enhanced_tokens = []
        
        for token in doc:
            if token.text.lower() in self.ambiguous_word_resolver['ambiguous_patterns']:
                # 2ã‚±ãƒ¼ã‚¹è©¦è¡Œã‚·ã‚¹ãƒ†ãƒ 
                best_pos = self._resolve_ambiguous_word(token, sentence, doc)
                enhanced_tokens.append(f"{token.text}[{best_pos}]")
                self.logger.debug(f"ğŸ§  æ›–æ˜§èªå½™è§£æ±º: '{token.text}' â†’ {best_pos}")
            else:
                enhanced_tokens.append(token.text)
        
        enhanced_sentence = ' '.join(enhanced_tokens).replace('[NOUN]', '').replace('[VERB]', '')
        self.logger.debug(f"ğŸ§  äººé–“çš„åˆ¤å®šå‰å‡¦ç†å®Œäº†: '{enhanced_sentence}'")
        return enhanced_sentence
    
    def _resolve_ambiguous_word(self, token, sentence: str, doc) -> str:
        """
        ğŸ§  Phase A3-5: 2ã‚±ãƒ¼ã‚¹è©¦è¡Œã‚·ã‚¹ãƒ†ãƒ 
        
        UnifiedStanzaRephraseMapperã®æ ¸å¿ƒæŠ€è¡“
        """
        word = token.text.lower()
        possible_pos = self.ambiguous_word_resolver['ambiguous_patterns'].get(word, [token.pos_])
        
        if len(possible_pos) <= 1:
            return possible_pos[0] if possible_pos else token.pos_
        
        # ã‚±ãƒ¼ã‚¹1: åè©è§£é‡ˆ
        noun_score = self._evaluate_syntactic_completeness(sentence, word, 'NOUN')
        
        # ã‚±ãƒ¼ã‚¹2: å‹•è©è§£é‡ˆ  
        verb_score = self._evaluate_syntactic_completeness(sentence, word, 'VERB')
        
        # æ§‹æ–‡å®Œå…¨æ€§ã«ã‚ˆã‚‹æœ€é©è§£é¸æŠ
        best_pos = 'VERB' if verb_score > noun_score else 'NOUN'
        self.logger.debug(f"ğŸ§  2ã‚±ãƒ¼ã‚¹è©¦è¡Œ: '{word}' NOUN={noun_score:.2f} VERB={verb_score:.2f} â†’ {best_pos}")
        
        return best_pos
    
    def _evaluate_syntactic_completeness(self, sentence: str, word: str, pos: str) -> float:
        """
        ğŸ§  Phase A3-5: æ§‹æ–‡å®Œå…¨æ€§è©•ä¾¡
        """
        # åŸºæœ¬çš„ãªæ§‹æ–‡å®Œå…¨æ€§è©•ä¾¡
        completeness = 0.0
        
        # åŸºæœ¬æ§‹é€ è©•ä¾¡
        if pos == 'VERB':
            completeness += 0.4  # å‹•è©ã¯æ–‡ã®ä¸­æ ¸
        elif pos == 'NOUN':
            completeness += 0.2  # åè©ã¯è£œåŠ©çš„
            
        # æ–‡è„ˆè©•ä¾¡ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        word_index = sentence.lower().find(word.lower())
        if word_index > 0:
            # å‰å¾Œã®èªã«ã‚ˆã‚‹è©•ä¾¡
            completeness += 0.1
            
        return completeness
    
    def _execute_enhanced_central_control(self, sentence: str) -> Dict[str, Any]:
        """
        ğŸ”¥ Phase A3-5: é«˜åº¦åˆ¶å¾¡æ©Ÿæ§‹ã«ã‚ˆã‚‹è§£æ
        
        è¨­è¨ˆä»•æ§˜æ›¸Phase 2.5ç†å¿µã®å®Ÿç¾
        """
        self.logger.debug(f"ğŸ”¥ é«˜åº¦åˆ¶å¾¡æ©Ÿæ§‹é–‹å§‹: '{sentence}'")
        
        # ä¸­å¤®åˆ¶å¾¡æ©Ÿæ§‹ã«ã‚ˆã‚‹æ–‡æ§‹é€ åˆ†é›¢
        structure_info = self._analyze_sentence_structure(sentence)
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè¡Œåˆ¶å¾¡
        execution_plan = self._create_execution_plan(structure_info)
        
        # æƒ…å ±çµ±åˆç®¡ç†
        integrated_result = self._execute_controlled_handlers(sentence, execution_plan)
        
        self.logger.debug(f"ğŸ”¥ é«˜åº¦åˆ¶å¾¡æ©Ÿæ§‹å®Œäº†: {len(integrated_result.get('slots', {}))}ã‚¹ãƒ­ãƒƒãƒˆ")
        return integrated_result
    
    def _analyze_sentence_structure(self, sentence: str) -> Dict[str, Any]:
        """
        ğŸ”¥ Phase A3-5: æ–‡æ§‹é€ åˆ†é›¢ç®¡ç†
        """
        return {
            'main_sentence': sentence,
            'sub_sentences': [],
            'complexity_level': 'basic',
            'hierarchy_map': {}
        }
    
    def _create_execution_plan(self, structure_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ”¥ Phase A3-5: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè¡Œè¨ˆç”»ä½œæˆ
        """
        return {
            'execution_order': self.handler_execution_order,
            'scope_limitations': {},
            'coordination_rules': {},
            'priority_settings': {}
        }
    
    def _execute_controlled_handlers(self, sentence: str, execution_plan: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ”¥ Phase A3-5: åˆ¶å¾¡ã•ã‚ŒãŸãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè¡Œ
        """
        # åŸºæœ¬å®Ÿè£…: æ—¢å­˜ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        context = self._initialize_management_context(sentence)
        pipeline_results = self._execute_pure_management_pipeline(context)
        return self._finalize_management_result(pipeline_results, sentence)
    
    def _apply_syntactic_completeness_evaluation(self, result: Dict[str, Any], sentence: str) -> Dict[str, Any]:
        """
        ğŸ§  Phase A3-5: æ§‹æ–‡å®Œå…¨æ€§è©•ä¾¡ã«ã‚ˆã‚‹å“è³ªå‘ä¸Š
        """
        # æ§‹æ–‡å®Œå…¨æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
        quality_score = self._calculate_enhanced_quality_score(result, sentence)
        
        # å“è³ªå‘ä¸Šå‡¦ç†
        enhanced_result = self._enhance_result_quality(result, quality_score)
        
        # å“è³ªã‚¹ã‚³ã‚¢è¨˜éŒ²
        enhanced_result['quality_score'] = quality_score
        
        self.logger.debug(f"ğŸ§  æ§‹æ–‡å®Œå…¨æ€§è©•ä¾¡å®Œäº†: ã‚¹ã‚³ã‚¢={quality_score:.2f}")
        return enhanced_result
    
    def _calculate_enhanced_quality_score(self, result: Dict[str, Any], sentence: str) -> float:
        """
        ğŸ§  Phase A3-5: å¼·åŒ–å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
        """
        score = 0.0
        
        # åŸºæœ¬æ§‹é€ è©•ä¾¡
        slots = result.get('slots', {})
        if 'S' in slots and 'V' in slots:
            score += 0.4
            
        # ä¿®é£¾èªçµ±åˆè©•ä¾¡
        modifier_count = sum(1 for k in slots.keys() if k.startswith('M'))
        score += min(0.3, modifier_count * 0.1)
        
        # æ„å‘³çš„ä¸€è²«æ€§è©•ä¾¡
        if result.get('confidence', 0) > 0.8:
            score += 0.3
            
        return min(1.0, score)
    
    def _enhance_result_quality(self, result: Dict[str, Any], quality_score: float) -> Dict[str, Any]:
        """
        ğŸ§  Phase A3-5: çµæœå“è³ªå‘ä¸Šå‡¦ç†
        """
        enhanced = result.copy()
        
        # å“è³ªã‚¹ã‚³ã‚¢ãŒä½ã„å ´åˆã®æ”¹å–„å‡¦ç†
        if quality_score < 0.7:
            self.logger.debug(f"ğŸ§  å“è³ªæ”¹å–„å®Ÿè¡Œ: ã‚¹ã‚³ã‚¢={quality_score:.2f}")
            # åŸºæœ¬çš„ãªå“è³ªæ”¹å–„å‡¦ç†
            enhanced = self._apply_basic_quality_improvements(enhanced)
            
        return enhanced
    
    def _apply_basic_quality_improvements(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ§  Phase A3-5: åŸºæœ¬å“è³ªæ”¹å–„å‡¦ç†
        """
        improved = result.copy()
        
        # ç©ºå€¤ã‚¹ãƒ­ãƒƒãƒˆé™¤å»
        if 'slots' in improved:
            improved['slots'] = {k: v for k, v in improved['slots'].items() if v and v.strip()}
            
        return improved
    
    def _initialize_management_context(self, sentence: str) -> Dict[str, Any]:
        """
        âœ… ç´”ç²‹ç®¡ç†æ©Ÿèƒ½: ç®¡ç†ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆæœŸåŒ–
        
        åˆ†è§£ä½œæ¥­ã¯å®Ÿè¡Œã›ãšã€ç®¡ç†ã«å¿…è¦ãªæƒ…å ±ã®ã¿æº–å‚™
        """
        context = {
            'sentence': sentence,
            'timestamp': self._get_timestamp(),
            'handler_execution_log': [],
            'quality_metrics': {},
            'error_log': [],
            'management_flags': {
                'force_handler_retry': False,
                'quality_enforcement': True,
                'debug_mode': False
            }
        }
        
        self.logger.debug(f"ğŸ¯ ç®¡ç†ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆæœŸåŒ–å®Œäº†: {len(context)}é …ç›®")
        return context
    
    def _execute_pure_management_pipeline(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ¯ Phase A3-3: çœŸã®ç´”ç²‹ç®¡ç†æ©Ÿèƒ½å®Ÿè£…
        
        å„ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å€‹åˆ¥å®Ÿè¡Œã—ã€çµæœã‚’ç®¡ç†ãƒ»çµ±åˆ
        ãƒ¬ã‚¬ã‚·ãƒ¼åˆ†è§£æ©Ÿèƒ½ã¸ã®å§”è­²ã‚’å®Œå…¨é™¤å»
        """
        pipeline_results = {
            'sentence': context['sentence'],
            'handler_results': {},
            'execution_log': context['handler_execution_log'],
            'quality_metrics': context['quality_metrics'],
            'unified_result': {}
        }
        
        self.logger.info(f"ğŸ”¥ Phase A3-3: çœŸã®ç®¡ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹ - {len(self.handler_execution_order)}ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å€‹åˆ¥å®Ÿè¡Œ")
        
        # ğŸ¯ çœŸã®ä¸­å¤®ç®¡ç†: å„ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å€‹åˆ¥å®Ÿè¡Œãƒ»ç®¡ç†
        sentence = context['sentence']
        accumulated_result = {
            'sentence': sentence,
            'slots': {},
            'sub_slots': {},
            'grammar_info': {
                'detected_patterns': [],
                'handler_contributions': {},
                'control_flags': {}
            }
        }
        
        # ğŸ”¥ Phase A3-3: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å€‹åˆ¥å®Ÿè¡Œãƒ«ãƒ¼ãƒ—ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼å§”è­²å»ƒæ­¢ï¼‰
        doc = self.grammar_mapper.nlp(sentence)  # å¿…è¦ãªNLPå‰å‡¦ç†
        
        for handler_name in self.handler_execution_order:
            try:
                self.logger.debug(f"ğŸ¯ å®Ÿè¡Œä¸­: {handler_name}ãƒãƒ³ãƒ‰ãƒ©ãƒ¼")
                
                # æ–°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œï¼ˆ_handle_* ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
                handler_method = getattr(self.grammar_mapper, f'_handle_{handler_name}', None)
                if handler_method:
                    handler_result = handler_method(sentence, doc, accumulated_result)
                    
                    # çµæœçµ±åˆç®¡ç†
                    if handler_result:
                        accumulated_result = self._merge_handler_result(accumulated_result, handler_result, handler_name)
                        
                    pipeline_results['handler_results'][handler_name] = handler_result
                    pipeline_results['execution_log'].append({
                        'handler': handler_name,
                        'status': 'success',
                        'slots_added': len(handler_result.get('slots', {})) if handler_result else 0
                    })
                    
                    self.logger.debug(f"âœ… {handler_name}ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Œäº†")
                    
                else:
                    # ãƒ¬ã‚¬ã‚·ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å‡¦ç†ï¼ˆbasic_five_patternã®ã¿ï¼‰
                    if handler_name == 'basic_five_pattern':
                        self.logger.debug(f"ğŸ”§ Phase A3-4: ãƒ¬ã‚¬ã‚·ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ä¿®æ­£å®Ÿè¡Œ: {handler_name}")
                        
                        # basic_five_patternã®ç‰¹æ®Šå‡¦ç†
                        try:
                            analysis_sentence = sentence
                            if accumulated_result.get('relative_clause_info', {}).get('found'):
                                main_sentence = accumulated_result['relative_clause_info']['main_sentence']
                                analysis_sentence = main_sentence
                                doc = self.grammar_mapper.nlp(main_sentence)
                            
                            # ğŸ”¥ Phase A3-4: å†…éƒ¨5æ–‡å‹å‡¦ç†ã‚’æ­£ã—ãå®Ÿè¡Œ
                            # æ¶ˆè²»æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼‰
                            filtered_tokens = []
                            consumed_tokens = getattr(self.grammar_mapper, '_consumed_tokens', set())
                            
                            for i, token in enumerate(doc):
                                if i not in consumed_tokens:
                                    filtered_tokens.append(token)
                            
                            # ğŸ¯ Phase A3-2b: ç‹¬ç«‹å®Ÿè£…ãƒ¡ã‚½ãƒƒãƒ‰ä½¿ç”¨ï¼ˆPure Central Controllerï¼‰
                            enhanced_tokens = self._pure_convert_spacy_to_dict_tokens(filtered_tokens)
                            core_elements = self.grammar_mapper._identify_core_elements(enhanced_tokens)
                            sentence_pattern = self.grammar_mapper._determine_sentence_pattern(core_elements, enhanced_tokens)
                            grammar_elements = self.grammar_mapper._assign_grammar_roles(enhanced_tokens, sentence_pattern, core_elements)
                            
                            # ğŸ¯ Phase A3-4: çµæœå¤‰æ›ç®¡ç†
                            basic_pattern_result = self.grammar_mapper._convert_to_rephrase_format(
                                grammar_elements, sentence_pattern, accumulated_result.get('sub_slots', {})
                            )
                            
                            # ğŸ”¥ Phase A3-4: ç®¡ç†æ¥­å‹™ - çµæœçµ±åˆ
                            if 'main_slots' in basic_pattern_result:
                                accumulated_result['slots'].update(basic_pattern_result['main_slots'])
                            
                            pipeline_results['handler_results'][handler_name] = basic_pattern_result
                            pipeline_results['execution_log'].append({
                                'handler': handler_name,
                                'status': 'success_legacy_fixed',
                                'slots_added': len(basic_pattern_result.get('main_slots', {})),
                                'phase': 'A3-4'
                            })
                            
                            self.logger.debug(f"âœ… Phase A3-4: {handler_name}ãƒ¬ã‚¬ã‚·ãƒ¼å‡¦ç†ä¿®æ­£å®Œäº†")
                            
                        except Exception as e:
                            self.logger.error(f"âŒ Phase A3-4: ãƒ¬ã‚¬ã‚·ãƒ¼{handler_name}ã‚¨ãƒ©ãƒ¼: {e}")
                            # ğŸ”§ Phase A3-4: ã‚¨ãƒ©ãƒ¼æ™‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç®¡ç†
                            pipeline_results['execution_log'].append({
                                'handler': handler_name,
                                'status': 'legacy_error_A3-4',
                                'error': str(e),
                                'fallback_applied': True
                            })
                    else:
                        self.logger.warning(f"âš ï¸ Phase A3-4: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰æœªç™ºè¦‹: _handle_{handler_name}")
                        pipeline_results['execution_log'].append({
                            'handler': handler_name,
                            'status': 'method_not_found_A3-4'
                        })
                    
            except Exception as e:
                self.logger.error(f"âŒ {handler_name}ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
                pipeline_results['execution_log'].append({
                    'handler': handler_name,
                    'status': 'error',
                    'error': str(e)
                })
        
        # ğŸ¯ ç®¡ç†æ¥­å‹™: æœ€çµ‚çµæœçµ±åˆ
        pipeline_results['unified_result'] = accumulated_result
        
        self.logger.info("ğŸ”¥ Phase A3-3: çœŸã®ç®¡ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº† - ãƒ¬ã‚¬ã‚·ãƒ¼å§”è­²å»ƒæ­¢")
        return pipeline_results
    
    def _merge_handler_result(self, accumulated_result: Dict[str, Any], handler_result: Dict[str, Any], handler_name: str) -> Dict[str, Any]:
        """
        ğŸ¯ Phase A3-3: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼çµæœçµ±åˆç®¡ç†
        
        å„ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®çµæœã‚’é©åˆ‡ã«çµ±åˆã™ã‚‹ç®¡ç†æ©Ÿèƒ½
        """
        if not handler_result:
            return accumulated_result
            
        # ã‚¹ãƒ­ãƒƒãƒˆçµ±åˆç®¡ç†
        if 'slots' in handler_result:
            accumulated_result['slots'].update(handler_result['slots'])
            
        # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆçµ±åˆç®¡ç†  
        if 'sub_slots' in handler_result:
            accumulated_result['sub_slots'].update(handler_result['sub_slots'])
            
        # æ–‡æ³•æƒ…å ±çµ±åˆç®¡ç†
        if 'grammar_info' in handler_result:
            grammar_info = handler_result['grammar_info']
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³æƒ…å ±çµ±åˆ
            if 'detected_patterns' in grammar_info:
                accumulated_result['grammar_info']['detected_patterns'].extend(grammar_info['detected_patterns'])
                
            # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è²¢çŒ®æƒ…å ±çµ±åˆ
            if 'handler_contributions' in grammar_info:
                accumulated_result['grammar_info']['handler_contributions'].update(grammar_info['handler_contributions'])
                
            # åˆ¶å¾¡ãƒ•ãƒ©ã‚°çµ±åˆ
            if 'control_flags' in grammar_info:
                accumulated_result['grammar_info']['control_flags'].update(grammar_info['control_flags'])
        
        self.logger.debug(f"ğŸ¯ {handler_name}çµæœçµ±åˆå®Œäº†: {len(accumulated_result['slots'])}ã‚¹ãƒ­ãƒƒãƒˆ")
        return accumulated_result
    
    def _finalize_management_result(self, pipeline_result: Dict[str, Any], sentence: str) -> Dict[str, Any]:
        """
        ğŸ¯ Phase A3-4: Pure Managementå“è³ªä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ 
        
        å€‹åˆ¥ãƒãƒ³ãƒ‰ãƒ©ãƒ¼çµæœã®æœ€çµ‚çµ±åˆï¼ˆç®¡ç†æ¥­å‹™ï¼‰
        å“è³ªæ¤œè¨¼ãƒ»æœ€é©åŒ–æ©Ÿèƒ½ã‚’è¿½åŠ 
        """
        # ğŸ”¥ Phase A3-4: çœŸã®çµ±åˆç®¡ç†æ¥­å‹™
        unified_result = pipeline_result['unified_result']
        
        # ğŸ¯ Phase A3-4: å“è³ªç®¡ç† - çµæœæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        quality_score = self._calculate_result_quality(unified_result, pipeline_result['execution_log'])
        
        # ğŸ”§ Phase A3-4: å“è³ªç®¡ç† - ã‚¹ãƒ­ãƒƒãƒˆæœ€é©åŒ–
        optimized_result = self._optimize_slot_allocation(unified_result)
        
        # ç®¡ç†æ¥­å‹™: ãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›å½¢å¼ã¸ã®å¤‰æ›
        final_result = optimized_result.copy()
        
        # ç®¡ç†æ¥­å‹™: å†…éƒ¨ç®¡ç†ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆçµæœã«ã¯å«ã‚ãªã„ã€ãƒ­ã‚°ã®ã¿ï¼‰
        management_info = {
            'controller': 'PureCentralController_A3-4',
            'execution_log': pipeline_result['execution_log'],
            'quality_metrics': {
                'quality_score': quality_score,
                'slots_optimized': len(optimized_result.get('slots', {})) != len(unified_result.get('slots', {}))
            },
            'management_timestamp': self._get_timestamp(),
            'handlers_executed': len(pipeline_result['handler_results']),
            'legacy_delegation_removed': True,
            'a3_4_improvements': True
        }
        
        # å†…éƒ¨ãƒ­ã‚°è¨˜éŒ²ã®ã¿ï¼ˆçµæœæ§‹é€ ã¯å¤‰æ›´ã—ãªã„ï¼‰
        self.logger.debug(f"ğŸ”¥ Phase A3-4 å“è³ªç®¡ç†æƒ…å ±: {management_info}")
        
        self.logger.info(f"ğŸ”¥ Phase A3-4: Pure Managementå“è³ªä¿è¨¼å®Œäº† - {len(final_result.get('slots', {}))}ã‚¹ãƒ­ãƒƒãƒˆ (å“è³ªã‚¹ã‚³ã‚¢: {quality_score:.2f})")
        return final_result
    
    def _calculate_result_quality(self, result: Dict[str, Any], execution_log: List[Dict]) -> float:
        """
        ğŸ¯ Phase A3-4: çµæœå“è³ªè¨ˆç®—
        """
        quality_score = 1.0
        
        # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ•°ã«ã‚ˆã‚‹æ¸›ç‚¹
        error_count = sum(1 for log in execution_log if log.get('status', '').endswith('error'))
        quality_score -= (error_count * 0.1)
        
        # æˆåŠŸãƒãƒ³ãƒ‰ãƒ©ãƒ¼æ•°ã«ã‚ˆã‚‹è©•ä¾¡
        success_count = sum(1 for log in execution_log if 'success' in log.get('status', ''))
        if success_count > 0:
            quality_score += (success_count * 0.05)
        
        # ã‚¹ãƒ­ãƒƒãƒˆå®Œå…¨æ€§ã«ã‚ˆã‚‹è©•ä¾¡
        slots = result.get('slots', {})
        if 'S' in slots and 'V' in slots:
            quality_score += 0.1  # åŸºæœ¬æ§‹é€ ãƒœãƒ¼ãƒŠã‚¹
            
        return max(0.0, min(1.0, quality_score))
    
    def _optimize_slot_allocation(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ¯ Phase A3-4: ã‚¹ãƒ­ãƒƒãƒˆé…ç½®æœ€é©åŒ–
        """
        optimized = result.copy()
        
        # é‡è¤‡ã‚¹ãƒ­ãƒƒãƒˆé™¤å»
        if 'slots' in optimized:
            slots = optimized['slots']
            # M3é‡è¤‡å•é¡Œã®ä¿®æ­£ãªã©ã€æ—¢çŸ¥ã®å•é¡Œã‚’ç®¡ç†æ¥­å‹™ã¨ã—ã¦æœ€é©åŒ–
            # ä»Šå›ã¯åŸºæœ¬çš„ãªæœ€é©åŒ–ã®ã¿
            optimized['slots'] = {k: v for k, v in slots.items() if v}  # ç©ºå€¤é™¤å»
            
        return optimized
    
    def _quality_assurance_check(self, result: Dict[str, Any]) -> None:
        """
        âœ… ç´”ç²‹ç®¡ç†æ©Ÿèƒ½: å“è³ªä¿è¨¼ãƒã‚§ãƒƒã‚¯
        
        çµæœã®å“è³ªã‚’è©•ä¾¡ã—ã€å¿…è¦ã«å¿œã˜ã¦è­¦å‘Šã‚’ç™ºè¡Œ
        """
        slots = result.get('slots', {})
        confidence = result.get('confidence', 0.0)
        
        # å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        slot_coverage = len(slots) / 10.0  # æœ€å¤§10ã‚¹ãƒ­ãƒƒãƒˆæƒ³å®š
        has_main_verb = 'V' in slots
        has_subject = 'S' in slots
        
        quality_score = (confidence + slot_coverage) / 2.0
        
        if quality_score < self.quality_thresholds['confidence_minimum']:
            self.logger.warning(f"ğŸ”¥ å“è³ªè­¦å‘Š: ã‚¹ã‚³ã‚¢{quality_score:.2f} < é–¾å€¤{self.quality_thresholds['confidence_minimum']}")
        
        if not has_main_verb:
            self.logger.warning("ğŸ”¥ å“è³ªè­¦å‘Š: ä¸»å‹•è©ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
        self.logger.debug(f"ğŸ¯ å“è³ªãƒã‚§ãƒƒã‚¯å®Œäº†: ã‚¹ã‚³ã‚¢{quality_score:.2f}")
    
    def _create_error_result(self, sentence: str, error_message: str) -> Dict[str, Any]:
        """
        âœ… ç´”ç²‹ç®¡ç†æ©Ÿèƒ½: ã‚¨ãƒ©ãƒ¼çµæœç”Ÿæˆ
        """
        return {
            'sentence': sentence,
            'slots': {},
            'sub_slots': {},
            'error': error_message,
            'management_info': {
                'controller': 'PureCentralController',
                'error_timestamp': self._get_timestamp(),
                'error_source': 'pure_management_pipeline'
            }
        }
    
    def _get_timestamp(self) -> str:
        """ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç”Ÿæˆ"""
        from datetime import datetime
        return datetime.now().isoformat()


def main():
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    mapper = DynamicGrammarMapper()
    test_sentence = "The book was written by John."
    result = mapper.analyze_sentence(test_sentence)
    print(f"ãƒ†ã‚¹ãƒˆçµæœ: {result}")

if __name__ == "__main__":
    main()
