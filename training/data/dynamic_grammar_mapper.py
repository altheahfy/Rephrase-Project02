#!/usr/bin/env python3
"""
å‹•çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ  v2.0
==================

èªæ•°ã«ä¾å­˜ã—ãªã„æ–‡æ³•è¦ç´ ãƒ™ãƒ¼ã‚¹ã®èªè­˜ã‚·ã‚¹ãƒ†ãƒ 
- å“è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã¯ãªãæ–‡æ³•çš„å½¹å‰²ã§èªè­˜
- ä¿®é£¾èªã®å‹•çš„æ¤œå‡ºã¨é™¤å¤–
- æ ¸è¦ç´ ï¼ˆä¸»èªãƒ»å‹•è©ãƒ»ç›®çš„èªãƒ»è£œèªï¼‰ã®ç‰¹å®š
- 5æ–‡å‹ã®å‹•çš„åˆ¤å®š

è¨­è¨ˆæ€æƒ³:
1. æ–‡ã®ã€Œæ ¸ã€ã‚’ç‰¹å®šï¼ˆä¸»èª + å‹•è©ï¼‰
2. å‹•è©ã®æ€§è³ªã‹ã‚‰æ–‡å‹ã‚’æ¨å®š
3. æ®‹ã‚Šã®è¦ç´ ã‚’æ–‡æ³•çš„å½¹å‰²ã§åˆ†é¡
4. ä¿®é£¾èªã¯åˆ¥é€”å‡¦ç†
"""

import spacy
import logging
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass

# ğŸ†• Phase 1.2: æ–‡å‹èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³è¿½åŠ 
from sentence_type_detector import SentenceTypeDetector

@dataclass
class GrammarElement:
    """æ–‡æ³•è¦ç´ ã®å®šç¾©"""
    text: str
    tokens: List[Dict]
    role: str  # S, V, O1, O2, C1, C2, M1, M2, M3, Aux
    start_idx: int
    end_idx: int
    confidence: float
    
    # ğŸ†• Orderæ©Ÿèƒ½é–¢é€£ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ (Phase 1.1)
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®šã«ã‚ˆã‚Šæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¸ã®å½±éŸ¿ã‚’æœ€å°åŒ–
    slot_display_order: int = 0      # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆé †åº
    display_order: int = 0           # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå†…é †åº  
    v_group_key: str = ""            # å‹•è©ã‚°ãƒ«ãƒ¼ãƒ—ã‚­ãƒ¼
    sentence_type: str = ""          # æ–‡å‹ (statement/wh_question/yes_no_question)
    is_subslot: bool = False         # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆãƒ•ãƒ©ã‚°
    parent_slot: str = ""            # è¦ªã‚¹ãƒ­ãƒƒãƒˆ (ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”¨)
    subslot_id: str = ""             # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆID (sub-s, sub-vç­‰)

class DynamicGrammarMapper:
    """
    å‹•çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ 
    èªæ•°ã«ä¾å­˜ã—ãªã„æ–‡æ³•èªè­˜ã‚’å®Ÿç¾
    """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        try:
            self.nlp = spacy.load("en_core_web_sm")
            print("âœ… spaCyå‹•çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except OSError:
            print("âŒ spaCyè‹±èªãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            raise
        
        self.logger = logging.getLogger(__name__)
        
        # ğŸ†• Phase 1.2: æ–‡å‹èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        self.sentence_type_detector = SentenceTypeDetector()
        print("âœ… æ–‡å‹èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
        
        # å‹•è©åˆ†é¡è¾æ›¸
        self.linking_verbs = {
            'be', 'am', 'is', 'are', 'was', 'were', 'being', 'been',
            'become', 'became', 'becoming',
            'seem', 'seemed', 'seeming', 'seems',
            'appear', 'appeared', 'appearing', 'appears',
            'look', 'looked', 'looking', 'looks',
            'sound', 'sounded', 'sounding', 'sounds',
            'feel', 'felt', 'feeling', 'feels',
            'taste', 'tasted', 'tasting', 'tastes',
            'smell', 'smelled', 'smelling', 'smells',
            'remain', 'remained', 'remaining', 'remains',
            'stay', 'stayed', 'staying', 'stays'
        }
        
        self.ditransitive_verbs = {
            'give', 'gave', 'given', 'giving', 'gives',
            'tell', 'told', 'telling', 'tells',
            'show', 'showed', 'shown', 'showing', 'shows',
            'send', 'sent', 'sending', 'sends',
            'teach', 'taught', 'teaching', 'teaches',
            'buy', 'bought', 'buying', 'buys',
            'bring', 'brought', 'bringing', 'brings',
            'offer', 'offered', 'offering', 'offers',
            'lend', 'lent', 'lending', 'lends',
            'sell', 'sold', 'selling', 'sells'
        }
        
        self.objective_complement_verbs = {
            'make', 'made', 'making', 'makes',
            'call', 'called', 'calling', 'calls',
            'consider', 'considered', 'considering', 'considers',
            'find', 'found', 'finding', 'finds',
            'keep', 'kept', 'keeping', 'keeps',
            'leave', 'left', 'leaving', 'leaves',
            'elect', 'elected', 'electing', 'elects',
            'name', 'named', 'naming', 'names',
            'choose', 'chose', 'chosen', 'choosing', 'chooses'
        }
        
        # å‹•è©/åè©åŒå½¢èªãƒªã‚¹ãƒˆï¼ˆstanzaã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ç¶™æ‰¿ï¼‰
        # ğŸ†• äººé–“çš„æ–‡æ³•èªè­˜: æ›–æ˜§èªã®å€™è£œãƒªã‚¹ãƒˆ
        self.ambiguous_words = {
            'lives': ['NOUN', 'VERB'],    # lifeè¤‡æ•°å½¢ vs liveä¸‰äººç§°å˜æ•°
            'works': ['NOUN', 'VERB'],    # workè¤‡æ•°å½¢ vs workä¸‰äººç§°å˜æ•°  
            'runs': ['NOUN', 'VERB'],     # runè¤‡æ•°å½¢ vs runä¸‰äººç§°å˜æ•°
            'goes': ['NOUN', 'VERB'],     # goè¤‡æ•°å½¢ vs goä¸‰äººç§°å˜æ•°
            'comes': ['NOUN', 'VERB'],    # comeè¤‡æ•°å½¢ vs comeä¸‰äººç§°å˜æ•°
            'stays': ['NOUN', 'VERB'],    # stayè¤‡æ•°å½¢ vs stayä¸‰äººç§°å˜æ•°
            'plays': ['NOUN', 'VERB'],    # playè¤‡æ•°å½¢ vs playä¸‰äººç§°å˜æ•°
            'looks': ['NOUN', 'VERB'],    # lookè¤‡æ•°å½¢ vs lookä¸‰äººç§°å˜æ•°
            'walks': ['NOUN', 'VERB'],    # walkè¤‡æ•°å½¢ vs walkä¸‰äººç§°å˜æ•°
            'talks': ['NOUN', 'VERB'],    # talkè¤‡æ•°å½¢ vs talkä¸‰äººç§°å˜æ•°
            'moves': ['NOUN', 'VERB'],    # moveè¤‡æ•°å½¢ vs moveä¸‰äººç§°å˜æ•°
            'drives': ['NOUN', 'VERB'],   # driveè¤‡æ•°å½¢ vs driveä¸‰äººç§°å˜æ•°
            'flies': ['NOUN', 'VERB'],    # flyè¤‡æ•°å½¢ vs flyä¸‰äººç§°å˜æ•°
            'rides': ['NOUN', 'VERB'],    # rideè¤‡æ•°å½¢ vs rideä¸‰äººç§°å˜æ•°
            'sits': ['NOUN', 'VERB']      # sitè¤‡æ•°å½¢ vs sitä¸‰äººç§°å˜æ•°
        }
    
    def analyze_sentence(self, sentence: str) -> Dict[str, Any]:
        """
        æ–‡ç« ã‚’å‹•çš„ã«è§£æã—ã¦Rephraseã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ ã‚’ç”Ÿæˆ
        
        Args:
            sentence (str): è§£æå¯¾è±¡ã®æ–‡ç« 
            
        Returns:
            Dict[str, Any]: Rephraseã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ 
        """
        try:
            # ğŸ†• Phase 1.2: æ–‡å‹èªè­˜
            sentence_type = self.sentence_type_detector.detect_sentence_type(sentence)
            sentence_type_confidence = self.sentence_type_detector.get_detection_confidence(sentence)
            
            # 1. spaCyåŸºæœ¬è§£æ
            doc = self.nlp(sentence)
            tokens = self._extract_tokens(doc)
            
            # 1.5. é–¢ä¿‚ç¯€æ§‹é€ ã®æ¤œå‡º
            relative_clause_info = self._detect_relative_clause(tokens, sentence)
            
            # ğŸ”§ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆã‚’äº‹å‰é™¤å¤–ã‚ˆã‚Šå‰ã«å®Ÿè¡Œï¼ˆcarç­‰ã®è¦ç´ ã‚’ä¿æŒã™ã‚‹ãŸã‚ï¼‰
            sub_slots = {}
            original_tokens = tokens.copy()  # å…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¿å­˜
            if relative_clause_info['found']:
                self.logger.debug(f"é–¢ä¿‚ç¯€æ¤œå‡º: {relative_clause_info['type']} (ä¿¡é ¼åº¦: {relative_clause_info['confidence']})")
                # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆï¼ˆå…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ï¼‰
                processed_tokens, sub_slots = self._process_relative_clause(original_tokens, relative_clause_info)
            
            # ğŸ”§ é–¢ä¿‚ç¯€å†…è¦ç´ ã®äº‹å‰é™¤å¤–ï¼ˆãƒ¡ã‚¤ãƒ³æ–‡æ³•è§£æç”¨ï¼‰
            excluded_indices = self._identify_relative_clause_elements(tokens, relative_clause_info)
            
            # 2. é™¤å¤–ã•ã‚Œã¦ã„ãªã„è¦ç´ ã®ã¿ã§ã‚³ã‚¢è¦ç´ ã‚’ç‰¹å®š
            filtered_tokens = [token for i, token in enumerate(tokens) if i not in excluded_indices]
            core_elements = self._identify_core_elements(filtered_tokens)
            
            # 3. å‹•è©ã®æ€§è³ªã‹ã‚‰æ–‡å‹ã‚’æ¨å®šï¼ˆé™¤å¤–ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ï¼‰
            sentence_pattern = self._determine_sentence_pattern(core_elements, filtered_tokens)
            
            # 4. æ–‡æ³•è¦ç´ ã‚’å‹•çš„ã«å‰²ã‚Šå½“ã¦ï¼ˆé™¤å¤–ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ï¼‰
            grammar_elements = self._assign_grammar_roles(filtered_tokens, sentence_pattern, core_elements, relative_clause_info)
            
            # 5. Rephraseã‚¹ãƒ­ãƒƒãƒˆå½¢å¼ã«å¤‰æ›
            rephrase_result = self._convert_to_rephrase_format(grammar_elements, sentence_pattern, sub_slots)
            
            # ğŸ†• Phase 1.2: æ–‡å‹æƒ…å ±ã‚’çµæœã«è¿½åŠ 
            rephrase_result['sentence_type'] = sentence_type
            rephrase_result['sentence_type_confidence'] = sentence_type_confidence
            
            return rephrase_result
            
        except Exception as e:
            self.logger.error(f"å‹•çš„æ–‡æ³•è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_error_result(sentence, str(e))
    
    def _extract_tokens(self, doc) -> List[Dict]:
        """spaCyãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã‚’æŠ½å‡º"""
        tokens = []
        for token in doc:
            token_info = {
                'text': token.text,
                'pos': token.pos_,
                'tag': token.tag_,
                'lemma': token.lemma_,
                'dep': token.dep_,  # ä¾å­˜é–¢ä¿‚
                'head': token.head.text,
                'head_idx': token.head.i,  # ğŸ†• ä¾å­˜é–¢ä¿‚ã®ãƒ˜ãƒƒãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                'is_stop': token.is_stop,
                'is_alpha': token.is_alpha,
                'index': token.i
            }
            tokens.append(token_info)
        return tokens
    
    def _identify_core_elements(self, tokens: List[Dict]) -> Dict[str, Any]:
        """
        æ–‡ã®æ ¸è¦ç´ ï¼ˆä¸»èªãƒ»å‹•è©ï¼‰ã‚’ç‰¹å®š
        ã“ã‚ŒãŒå…¨ã¦ã®æ–‡å‹èªè­˜ã®åŸºç›¤ã¨ãªã‚‹
        """
        core = {
            'subject': None,
            'verb': None,
            'subject_indices': [],
            'verb_indices': [],
            'auxiliary': None,
            'auxiliary_indices': []
        }
        
        # å‹•è©ã‚’æ¢ã™ï¼ˆæœ€ã‚‚é‡è¦ï¼‰
        main_verb_idx = self._find_main_verb(tokens)
        if main_verb_idx is not None:
            core['verb'] = tokens[main_verb_idx]
            core['verb_indices'] = [main_verb_idx]
            
            # åŠ©å‹•è©ã‚’æ¢ã™
            aux_idx = self._find_auxiliary(tokens, main_verb_idx)
            if aux_idx is not None:
                core['auxiliary'] = tokens[aux_idx]
                core['auxiliary_indices'] = [aux_idx]
        
        # ä¸»èªã‚’æ¢ã™ï¼ˆå‹•è©ã®å‰ã§æœ€ã‚‚é©åˆ‡ãªåè©å¥ï¼‰
        if main_verb_idx is not None:
            subject_indices = self._find_subject(tokens, main_verb_idx)
            if subject_indices:
                core['subject_indices'] = subject_indices
                core['subject'] = ' '.join([tokens[i]['text'] for i in subject_indices])
        
        return core
    
    def _find_main_verb(self, tokens: List[Dict]) -> Optional[int]:
        """
        ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’ç‰¹å®šï¼ˆäººé–“çš„æ–‡æ³•èªè­˜ï¼‰
        å“è©æƒ…å ±ã¨èªé †ã®ã¿ã‚’ä½¿ç”¨ã€ä¾å­˜é–¢ä¿‚ã¯ä½¿ã‚ãªã„
        """
        # POSãƒ™ãƒ¼ã‚¹ã¨æ–‡è„ˆãƒ™ãƒ¼ã‚¹ã®ä¸¡æ–¹ã‚’å–å¾—
        pos_candidates = []
        for i, token in enumerate(tokens):
            # å‹•è©ã®å“è©ã‚¿ã‚°
            if (token['tag'].startswith('VB') and token['pos'] == 'VERB') or token['pos'] == 'AUX':
                pos_candidates.append((i, token))
        
        # æ–‡è„ˆçš„å‹•è©è­˜åˆ¥ï¼ˆPOSèª¤èªè­˜å¯¾ç­–ï¼‰
        contextual_candidates = self._find_contextual_verbs(tokens)
        
        # ä¸¡æ–¹ã‚’çµ±åˆï¼ˆé‡è¤‡é™¤å»ï¼‰
        verb_candidates = pos_candidates.copy()
        for i, token in contextual_candidates:
            # æ—¢ã«å­˜åœ¨ã—ãªã„å ´åˆã®ã¿è¿½åŠ 
            if not any(existing_i == i for existing_i, _ in verb_candidates):
                verb_candidates.append((i, token))
        
        if not verb_candidates:
            return None
        
        # äººé–“çš„åˆ¤å®šï¼šé–¢ä¿‚ç¯€ã‚’é™¤å¤–ã—ã¦ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’ç‰¹å®š
        non_relative_verbs = []
        
        for i, token in verb_candidates:
            # é–¢ä¿‚ä»£åè©ã®ç›´å¾Œã®å‹•è©ã¯é–¢ä¿‚ç¯€å†…å‹•è©ã¨ã—ã¦é™¤å¤–
            is_in_relative_clause = False
            
            # å‰ã®å˜èªã‚’ç¢ºèª
            for j in range(max(0, i-5), i):  # 5èªå‰ã¾ã§ç¢ºèª
                prev_token = tokens[j]
                if prev_token['text'].lower() in ['who', 'whom', 'which', 'that', 'whose', 'where', 'when']:
                    # whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†: å‹•è©/åè©åŒå½¢èªã¯é–¢ä¿‚ç¯€å¤–ã®ãƒ¡ã‚¤ãƒ³å‹•è©ã¨ã—ã¦æ‰±ã†
                    if (prev_token['text'].lower() == 'whose' and 
                        token['text'].lower() in self.ambiguous_words and
                        token.get('contextual_override', False)):
                        # whoseæ§‹æ–‡ã§ã®åŒå½¢èªå‹•è©ã¯é–¢ä¿‚ç¯€å¤–ã¨ã—ã¦æ‰±ã†
                        is_in_relative_clause = False
                        break
                    
                    # é–¢ä¿‚ä»£åè©ã‹ã‚‰å‹•è©ã¾ã§ã®è·é›¢ãŒè¿‘ã„å ´åˆã€é–¢ä¿‚ç¯€å†…å‹•è©
                    if i - j <= 4:  # 4èªä»¥å†…ãªã‚‰é–¢ä¿‚ç¯€å†…
                        is_in_relative_clause = True
                        break
            
            if not is_in_relative_clause:
                non_relative_verbs.append((i, token))
        
        if non_relative_verbs:
            # ãƒ¡ã‚¤ãƒ³å‹•è©å€™è£œã‹ã‚‰åŠ©å‹•è©ã§ãªã„ã‚‚ã®ã‚’å„ªå…ˆ
            main_verbs = [(i, token) for i, token in non_relative_verbs if not self._is_auxiliary_verb(token)]
            if main_verbs:
                # æ–‡ã®å¾ŒåŠã«ã‚ã‚‹ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’å„ªå…ˆï¼ˆé–¢ä¿‚ç¯€ã®å¾Œï¼‰
                return main_verbs[-1][0]
            return non_relative_verbs[-1][0]
        
        # æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦ã€ã©ã®å‹•è©ã§ã‚‚é¸æŠ
        return verb_candidates[-1][0]

    def _find_contextual_verbs(self, tokens: List[Dict]) -> List[Tuple[int, Dict]]:
        """
        äººé–“çš„æ–‡æ³•èªè­˜ã«ã‚ˆã‚‹å‹•è©è­˜åˆ¥
        æ§‹æ–‡çš„æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã§æœ€é©ãªå“è©ã‚’æ±ºå®š
        """
        contextual_verbs = []
        sentence_text = ' '.join([token['text'] for token in tokens])
        
        for i, token in enumerate(tokens):
            # æ—¢ã«å‹•è©ã¨ã—ã¦èªè­˜ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®
            if token['pos'] == 'VERB':
                contextual_verbs.append((i, token))
                continue
            
            # ğŸ†• äººé–“çš„å“è©æ±ºå®š: æ§‹æ–‡çš„æ•´åˆæ€§ã«ã‚ˆã‚‹é¸æŠ
            if token['text'].lower() in self.ambiguous_words:
                optimal_pos = self._resolve_ambiguous_word(token, tokens, i, sentence_text)
                
                if optimal_pos == 'VERB':
                    verb_token = token.copy()
                    verb_token['pos'] = 'VERB'
                    verb_token['human_grammar_correction'] = True
                    verb_token['resolution_method'] = 'syntactic_consistency'
                    contextual_verbs.append((i, verb_token))
                    self.logger.debug(f"ğŸ§  äººé–“çš„å“è©æ±ºå®š: '{token['text']}' â†’ VERB (æ§‹æ–‡æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯)")
                continue
            
            # ãã®ä»–ã®å‹•è©å€™è£œï¼ˆaux, modalå«ã‚€ï¼‰
            if token['pos'] in ['AUX', 'MODAL']:
                contextual_verbs.append((i, token))
        
        return contextual_verbs

    def _resolve_ambiguous_word(self, token: Dict, tokens: List[Dict], position: int, sentence: str) -> str:
        """
        äººé–“çš„å“è©æ±ºå®š: æ§‹æ–‡çš„æ•´åˆæ€§ã«ã‚ˆã‚‹æ›–æ˜§èªè§£æ±º
        
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆã®4æ®µéšã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:
        â‘ æ›–æ˜§èªãƒªã‚¹ãƒˆã®ç¢ºèª
        â‘¡ä¸¡ã‚±ãƒ¼ã‚¹è©¦è¡Œ
        â‘¢æ§‹æ–‡å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
        â‘£æœ€é©è§£æ¡ç”¨
        """
        word_text = token['text'].lower()
        
        if word_text not in self.ambiguous_words:
            return token['pos']  # é€šå¸¸ã®spaCyåˆ¤å®š
        
        candidates = self.ambiguous_words[word_text]
        best_pos = token['pos']  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯spaCyåˆ¤å®š
        best_score = 0
        
        self.logger.debug(f"ğŸ§  æ›–æ˜§èªè§£æ±ºé–‹å§‹: '{token['text']}' å€™è£œ={candidates}")
        
        # å„å€™è£œã‚’è©¦è¡Œã—ã¦æ§‹æ–‡çš„æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯
        for candidate_pos in candidates:
            score = self._evaluate_syntactic_consistency(token, candidate_pos, tokens, position, sentence)
            self.logger.debug(f"  ã‚±ãƒ¼ã‚¹è©¦è¡Œ: {candidate_pos} â†’ ã‚¹ã‚³ã‚¢={score}")
            
            if score > best_score:
                best_pos = candidate_pos
                best_score = score
        
        self.logger.debug(f"ğŸ§  æœ€é©è§£æ¡ç”¨: '{token['text']}' â†’ {best_pos} (ã‚¹ã‚³ã‚¢={best_score})")
        return best_pos

    def _evaluate_syntactic_consistency(self, ambiguous_token: Dict, candidate_pos: str, 
                                       tokens: List[Dict], position: int, sentence: str) -> float:
        """
        æ§‹æ–‡çš„æ•´åˆæ€§ã®è©•ä¾¡
        
        äººé–“çš„æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹:
        - ã‚±ãƒ¼ã‚¹1è©¦è¡Œ: åè©ã¨ã—ã¦è§£é‡ˆ â†’ æ–‡æ§‹é€ ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
        - ã‚±ãƒ¼ã‚¹2è©¦è¡Œ: å‹•è©ã¨ã—ã¦è§£é‡ˆ â†’ æ–‡æ§‹é€ ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
        - ã‚ˆã‚Šå®Œå…¨ãªæ§‹é€ ã‚’æŒã¤ã‚±ãƒ¼ã‚¹ã‚’é¸æŠ
        """
        # ä»®æƒ³çš„ã«ãƒˆãƒ¼ã‚¯ãƒ³ã®å“è©ã‚’å¤‰æ›´
        test_tokens = [t.copy() for t in tokens]
        test_tokens[position]['pos'] = candidate_pos
        
        # æ§‹æ–‡æ§‹é€ ã®è©•ä¾¡
        structure_score = self._analyze_sentence_structure_completeness(test_tokens, sentence)
        
        return structure_score

    def _analyze_sentence_structure_completeness(self, tokens: List[Dict], sentence: str) -> float:
        """
        æ–‡æ§‹é€ ã®å®Œå…¨æ€§ã‚’åˆ†æï¼ˆé–¢ä¿‚ç¯€å­˜åœ¨å‰æç‰ˆï¼‰
        
        äººé–“çš„æ€è€ƒ:
        - é–¢ä¿‚è©ãŒã‚ã‚‹ãªã‚‰ã€é–¢ä¿‚ç¯€ + ãƒ¡ã‚¤ãƒ³æ–‡ã®ä¸¡æ–¹ãŒå¿…è¦
        - é–¢ä¿‚ç¯€ã®ã¿ã§çµ‚ã‚ã‚‹ â†’ æ§‹é€ çš„ã«ä¸å®Œå…¨
        - é–¢ä¿‚ç¯€ + ãƒ¡ã‚¤ãƒ³æ–‡ â†’ æ§‹é€ çš„ã«å®Œå…¨
        """
        score = 0.0
        
        # ğŸ†• CRITICAL: é–¢ä¿‚è©ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        has_relative_pronoun = self._has_relative_pronoun(sentence)
        
        if has_relative_pronoun:
            self.logger.debug(f"    ğŸ” é–¢ä¿‚ç¯€æ–‡ã¨ã—ã¦è©•ä¾¡é–‹å§‹")
            
            # é–¢ä¿‚ç¯€ + ãƒ¡ã‚¤ãƒ³æ–‡ã®åˆ†é›¢è©•ä¾¡
            relative_clause_complete = self._check_relative_clause_completeness(tokens, sentence)
            main_clause_complete = self._check_main_clause_completeness(tokens, sentence)
            
            self.logger.debug(f"    é–¢ä¿‚ç¯€å®Œå…¨æ€§: {relative_clause_complete}")
            self.logger.debug(f"    ãƒ¡ã‚¤ãƒ³æ–‡å®Œå…¨æ€§: {main_clause_complete}")
            
            # é–¢ä¿‚ç¯€æ§‹æ–‡ã§ã¯ä¸¡æ–¹ãŒå¿…è¦
            if relative_clause_complete and main_clause_complete:
                score = 100.0  # å®Œå…¨ãªé–¢ä¿‚ç¯€æ§‹æ–‡
                self.logger.debug(f"    âœ… å®Œå…¨ãªé–¢ä¿‚ç¯€æ§‹æ–‡: +100")
            elif relative_clause_complete and not main_clause_complete:
                score = 20.0   # é–¢ä¿‚ç¯€ã®ã¿ï¼ˆæ§‹é€ çš„ã«ä¸å®Œå…¨ï¼‰
                self.logger.debug(f"    âŒ é–¢ä¿‚ç¯€ã®ã¿ï¼ˆãƒ¡ã‚¤ãƒ³æ–‡æ¬ å¦‚ï¼‰: +20")
            elif not relative_clause_complete and main_clause_complete:
                score = 30.0   # ãƒ¡ã‚¤ãƒ³æ–‡ã®ã¿ï¼ˆé–¢ä¿‚ç¯€ç„¡è¦–ã¯ä¸è‡ªç„¶ï¼‰
                self.logger.debug(f"    âŒ ãƒ¡ã‚¤ãƒ³æ–‡ã®ã¿ï¼ˆé–¢ä¿‚ç¯€ç„¡è¦–ï¼‰: +30")
            else:
                score = 0.0    # ä¸¡æ–¹ä¸å®Œå…¨
                self.logger.debug(f"    âŒ ä¸¡æ–¹ä¸å®Œå…¨: +0")
        else:
            # é€šå¸¸æ–‡ã®è©•ä¾¡
            if self._has_main_verb(tokens) and self._has_subject_structure(tokens):
                score = 100.0
                self.logger.debug(f"    âœ… é€šå¸¸æ–‡å®Œå…¨: +100")
        
        self.logger.debug(f"    ç·åˆã‚¹ã‚³ã‚¢: {score}/100")
        return score

    def _has_relative_pronoun(self, sentence: str) -> bool:
        """é–¢ä¿‚ä»£åè©ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯"""
        relative_pronouns = ['who', 'whom', 'which', 'that', 'whose', 'where', 'when']
        sentence_lower = sentence.lower()
        return any(pronoun in sentence_lower for pronoun in relative_pronouns)

    def _check_relative_clause_completeness(self, tokens: List[Dict], sentence: str) -> bool:
        """é–¢ä¿‚ç¯€ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯"""
        # whoseæ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³: whose + åè© + å‹•è© + (è£œèª)
        if 'whose' in sentence.lower():
            return self._check_whose_clause_completeness(tokens)
        # ä»–ã®é–¢ä¿‚ä»£åè©ãƒ‘ã‚¿ãƒ¼ãƒ³
        return self._check_general_relative_clause_completeness(tokens)

    def _check_main_clause_completeness(self, tokens: List[Dict], sentence: str) -> bool:
        """ãƒ¡ã‚¤ãƒ³æ–‡ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆé–¢ä¿‚ç¯€ã‚’é™¤ã„ã¦ï¼‰"""
        # é–¢ä¿‚ç¯€ä»¥å¤–ã®éƒ¨åˆ†ã«ãƒ¡ã‚¤ãƒ³å‹•è©ãŒå­˜åœ¨ã™ã‚‹ã‹
        main_verbs = []
        for i, token in enumerate(tokens):
            # äººé–“çš„POSåˆ¤å®šã‚’ä½¿ç”¨ï¼ˆæ›–æ˜§èªã®å“è©å¤‰æ›´ã‚’åæ˜ ï¼‰
            corrected_pos = self._resolve_ambiguous_word(token, tokens, i, sentence)
            if corrected_pos in ['VERB', 'AUX']:
                if not self._is_likely_in_relative_clause(token, tokens):
                    main_verbs.append(token)
                    self.logger.debug(f"      ãƒ¡ã‚¤ãƒ³å‹•è©å€™è£œ: '{token['text']}' (pos={corrected_pos})")
        
        return len(main_verbs) > 0

    def _check_whose_clause_completeness(self, tokens: List[Dict]) -> bool:
        """whoseæ§‹æ–‡ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯: whose + åè© + å‹•è© + (è£œèª)"""
        whose_idx = None
        possessed_noun = False
        relative_verb = False
        
        for i, token in enumerate(tokens):
            if token['text'].lower() == 'whose':
                whose_idx = i
            elif whose_idx is not None and i == whose_idx + 1:
                if token['pos'] in ['NOUN', 'PROPN']:
                    possessed_noun = True
            elif whose_idx is not None and i > whose_idx + 1 and token['pos'] in ['VERB', 'AUX']:
                relative_verb = True
                break
        
        return whose_idx is not None and possessed_noun and relative_verb

    def _check_general_relative_clause_completeness(self, tokens: List[Dict]) -> bool:
        """ä¸€èˆ¬çš„ãªé–¢ä¿‚ç¯€ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯"""
        has_relative_pronoun = False
        has_relative_verb = False
        
        for i, token in enumerate(tokens):
            if token['text'].lower() in ['who', 'which', 'that', 'whom']:
                has_relative_pronoun = True
            elif has_relative_pronoun and token['pos'] in ['VERB', 'AUX']:
                has_relative_verb = True
                break
        
        return has_relative_pronoun and has_relative_verb

    def _is_likely_main_verb_by_position(self, token: Dict, tokens: List[Dict], position: int) -> bool:
        """ä½ç½®çš„ã«ãƒ¡ã‚¤ãƒ³å‹•è©ã§ã‚ã‚‹å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        # whoseç¯€ã®å¾Œã«æ¥ã‚‹å‹•è©ã¯ãƒ¡ã‚¤ãƒ³å‹•è©ã®å¯èƒ½æ€§ãŒé«˜ã„
        for i in range(position):
            if tokens[i]['text'].lower() in ['whose', 'who', 'which', 'that']:
                # é–¢ä¿‚ä»£åè©ã‚ˆã‚Šå¾Œã«ã‚ã‚‹æ›–æ˜§èª
                # ã‹ã¤ã€é–¢ä¿‚ç¯€å†…å‹•è©ï¼ˆbeå‹•è©ç­‰ï¼‰ã®å¾Œã«ã‚ã‚‹
                relative_verb_found = False
                for j in range(i + 1, position):
                    if tokens[j]['pos'] in ['VERB', 'AUX']:
                        relative_verb_found = True
                        break
                
                if relative_verb_found:
                    return True  # é–¢ä¿‚ç¯€å‹•è©ã®å¾Œ â†’ ãƒ¡ã‚¤ãƒ³å‹•è©ã®å¯èƒ½æ€§
        
        return False

    def _has_main_verb(self, tokens: List[Dict]) -> bool:
        """ãƒ¡ã‚¤ãƒ³å‹•è©ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯"""
        for token in tokens:
            if token['pos'] in ['VERB', 'AUX']:
                return True
        return False

    def _is_likely_in_relative_clause(self, token: Dict, tokens: List[Dict]) -> bool:
        """ãƒˆãƒ¼ã‚¯ãƒ³ãŒé–¢ä¿‚ç¯€å†…ã«ã‚ã‚‹å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        token_idx = None
        for i, t in enumerate(tokens):
            if t['text'] == token['text'] and t.get('index', i) == token.get('index', -1):
                token_idx = i
                break
        
        if token_idx is None:
            return False
        
        # é–¢ä¿‚ä»£åè©ã®å¾Œã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        for i in range(token_idx):
            if tokens[i]['text'].lower() in ['who', 'whom', 'which', 'that', 'whose']:
                return True
        
        return False

    def _has_main_verb_outside_relative_clause(self, tokens: List[Dict]) -> bool:
        """ãƒ¡ã‚¤ãƒ³æ–‡ï¼ˆé–¢ä¿‚ç¯€å¤–ï¼‰ã«å‹•è©ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        for token in tokens:
            # é–¢ä¿‚ç¯€å†…ã§ã¯ãªã„å‹•è©ã‚’æ¢ã™
            if (token['pos'] in ['VERB', 'AUX'] and 
                not token.get('is_in_relative_clause', False)):
                return True
        return False

    def _has_subject_structure(self, tokens: List[Dict]) -> bool:
        """ä¸»èªæ§‹é€ ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯"""
        for token in tokens:
            if token['pos'] in ['NOUN', 'PROPN', 'PRON']:
                return True
        return False

    def _is_relative_clause_structurally_complete(self, tokens: List[Dict]) -> bool:
        """é–¢ä¿‚ç¯€æ§‹é€ ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯"""
        # ç°¡æ˜“å®Ÿè£…: é–¢ä¿‚ä»£åè©ãŒã‚ã‚Œã°é–¢ä¿‚ç¯€ã¨ã—ã¦èªè­˜
        has_relative_pronoun = any(
            token['text'].lower() in ['who', 'whom', 'which', 'that', 'whose'] 
            for token in tokens
        )
        return has_relative_pronoun

    def _is_modifier_placement_valid(self, tokens: List[Dict]) -> bool:
        """ä¿®é£¾èªé…ç½®ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        # ç°¡æ˜“å®Ÿè£…: åŸºæœ¬çš„ã«å¦¥å½“ã¨ã™ã‚‹
        return True

    def _get_human_corrected_pos(self, token: Dict) -> str:
        """
        äººé–“çš„å“è©åˆ¤å®šã®çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
        
        é©å‘½çš„äºŒé‡è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨:
        1. æ›–æ˜§èªãƒªã‚¹ãƒˆã®ç¢ºèª
        2. ä¸¡ã‚±ãƒ¼ã‚¹è©¦è¡Œï¼ˆNOUN/VERBï¼‰
        3. æ§‹æ–‡å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
        4. æœ€é©è§£æ¡ç”¨
        """
        if token['text'].lower() not in self.ambiguous_words:
            return token['pos']  # é€šå¸¸ã®spaCyåˆ¤å®š
        
        # ğŸ§  é©å‘½çš„äºŒé‡è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®é©ç”¨
        # Note: ç°¡æ˜“ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã§äºŒé‡è©•ä¾¡ã‚’å®Ÿè¡Œ
        word_text = token['text'].lower()
        
        # VERBå€™è£œã¨NOUNå€™è£œã§æ§‹æ–‡çš„æ•´åˆæ€§ã‚’æ¯”è¼ƒ
        verb_score = self._evaluate_word_as_verb_simple(token, word_text)
        noun_score = self._evaluate_word_as_noun_simple(token, word_text)
        
        if verb_score > noun_score:
            self.logger.debug(f"ğŸ§  äººé–“çš„åˆ¤å®š: '{token['text']}' â†’ VERB (ã‚¹ã‚³ã‚¢: {verb_score} vs {noun_score})")
            return 'VERB'
        else:
            self.logger.debug(f"ğŸ§  äººé–“çš„åˆ¤å®š: '{token['text']}' â†’ NOUN (ã‚¹ã‚³ã‚¢: {verb_score} vs {noun_score})")
            return 'NOUN'

    def _evaluate_word_as_verb_simple(self, token: Dict, word_text: str) -> float:
        """èªã‚’å‹•è©ã¨ã—ã¦è©•ä¾¡ã™ã‚‹ç°¡æ˜“ã‚¹ã‚³ã‚¢"""
        score = 0.0
        
        # åŸºæœ¬çš„ãªå‹•è©ã‚‰ã—ã•ãƒã‚§ãƒƒã‚¯
        if word_text.endswith('s'):  # ä¸‰äººç§°å˜æ•°å½¢
            score += 30.0
            
        # whoseæ§‹æ–‡ã§ã®å‹•è©åˆ¤å®šï¼ˆlivesç­‰ï¼‰
        if word_text in ['lives', 'works', 'runs', 'goes', 'comes']:
            score += 50.0
            
        return score
    
    def _evaluate_word_as_noun_simple(self, token: Dict, word_text: str) -> float:
        """èªã‚’åè©ã¨ã—ã¦è©•ä¾¡ã™ã‚‹ç°¡æ˜“ã‚¹ã‚³ã‚¢"""
        score = 0.0
        
        # åŸºæœ¬çš„ãªåè©ã‚‰ã—ã•ãƒã‚§ãƒƒã‚¯
        if word_text.endswith('s'):  # è¤‡æ•°å½¢
            score += 20.0
            
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®spaCyåˆ¤å®šã‚’å°Šé‡
        if token['pos'] == 'NOUN':
            score += 10.0
            
        return score

    def _is_likely_verb_in_context(self, token: Dict, word_text: str) -> bool:
        """æ–‡è„ˆãƒ™ãƒ¼ã‚¹ã®å‹•è©åˆ¤å®š"""
        # ç°¡æ˜“å®Ÿè£…: ambiguous_wordsãƒªã‚¹ãƒˆã«ã‚ã‚‹èªã¯å‹•è©ã¨ã—ã¦æ‰±ã†
        # (ã‚ˆã‚Šé«˜ç²¾åº¦ãªå®Ÿè£…ã¯ä»Šå¾Œã®æ”¹å–„ã§)
        return word_text in self.ambiguous_words
        
        return contextual_verbs
    
    def _is_verb_in_whose_context(self, token: Dict, tokens: List[Dict], 
                                 position: int, sentence: str) -> bool:
        """
        whoseæ§‹æ–‡ã§ã®å‹•è©/åè©åŒå½¢èªåˆ¤å®š
        stanzaã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ã‚’POSãƒ™ãƒ¼ã‚¹ã§å†å®Ÿè£…
        """
        import re
        word = token['text'].lower()
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: whose [åè©] is [å½¢å®¹è©] [å‹•è©] (here|there|å ´æ‰€)
        pattern1 = rf'whose\s+\w+\s+is\s+\w+\s+{word}\s+(here|there|in\s+\w+)'
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: whose [åè©] [ä¿®é£¾èª]* [å‹•è©] (å ´æ‰€è¡¨ç¾)
        pattern2 = rf'whose\s+\w+.*?\s+{word}\s+(here|there|in|at|on)\s+\w+'
        
        if re.search(pattern1, sentence.lower()) or re.search(pattern2, sentence.lower()):
            # æ–‡ä¸­ã«whoseãŒã‚ã‚Šã€è©²å½“ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆ
            return True
        
        # ã‚ˆã‚Šä¸€èˆ¬çš„ãªåˆ¤å®š: whoseå¾Œã§ã€å ´æ‰€è¡¨ç¾ã®å‰ã«ã‚ã‚‹åŒå½¢èª
        if 'whose' in sentence.lower():
            # whoseå¾Œã®ä½ç½®ç¢ºèª
            whose_pos = None
            for i, t in enumerate(tokens):
                if t['text'].lower() == 'whose':
                    whose_pos = i
                    break
            
            if whose_pos is not None and position > whose_pos:
                # whoseå¾Œã§ã€å ´æ‰€è¡¨ç¾ãŒã‚ã‚‹å ´åˆ
                for j in range(position + 1, len(tokens)):
                    next_token = tokens[j]['text'].lower()
                    if next_token in ['here', 'there', 'in', 'at', 'on']:
                        return True
        
        return False

    def _identify_relative_clause_elements(self, tokens: List[Dict], relative_info: Dict) -> set:
        """
        é–¢ä¿‚ç¯€å†…ã®è¦ç´ ã‚’äº‹å‰ã«ç‰¹å®šï¼ˆå…ˆè¡Œè©ã¯ä¿æŒã€é–¢ä¿‚ç¯€éƒ¨åˆ†ã®ã¿é™¤å¤–ï¼‰
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆã®æ–¹æ³•è«–ï¼š
        â‘ é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒé–¢ä¿‚ç¯€ã®éƒ¨åˆ†ã‚’æ­£ã—ãåˆ‡ã‚Šå–ã‚‹
        â‘¡5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®åˆ¤æ–­ç”¨ã«å…ˆè¡Œè©ã ã‘æ®‹ã™ï¼ˆã€Œå¾Œã«""ã«ã™ã¹ãã€æƒ…å ±ä»˜ãï¼‰
        """
        excluded_indices = set()
        
        if not relative_info['found']:
            return excluded_indices
        
        # å…ˆè¡Œè©ã¯ä¿æŒã—ã€é–¢ä¿‚ç¯€éƒ¨åˆ†ã®ã¿ã‚’é™¤å¤–
        rel_start = relative_info.get('clause_start_idx', -1)  # é–¢ä¿‚ä»£åè©ã®ä½ç½®
        rel_end = relative_info.get('clause_end_idx', -1)
        antecedent_idx = relative_info.get('antecedent_idx', -1)  # å…ˆè¡Œè©ã¯ä¿æŒ
        
        if rel_start >= 0 and rel_end >= 0:
            # é–¢ä¿‚ä»£åè©ã‹ã‚‰é–¢ä¿‚ç¯€çµ‚äº†ã¾ã§é™¤å¤–ï¼ˆå…ˆè¡Œè©ã¨ãƒ¡ã‚¤ãƒ³å‹•è©ã¯ä¿è­·ï¼‰
            # rel_endã¯ã‚¯ãƒ©ã‚¦ã‚ºã®æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãªã®ã§ +1 ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
            for i in range(rel_start, rel_end + 1):
                if i < len(tokens):
                    # å…ˆè¡Œè©ã¯ä¿è­·ï¼ˆ5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã§åˆ¤æ–­ã«ä½¿ç”¨ï¼‰
                    if i != antecedent_idx:
                        excluded_indices.add(i)
            
            self.logger.debug(f"é–¢ä¿‚ç¯€è¦ç´ é™¤å¤–: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {rel_start}-{rel_end} (å…ˆè¡Œè©{antecedent_idx}ã¯ä¿æŒ)")
        
        return excluded_indices
        
        # ã‚ˆãèª¤èªè­˜ã•ã‚Œã‚‹å‹•è©ã®ãƒªã‚¹ãƒˆ
        common_verbs = {
            'lives', 'live', 'lived', 'living',
            'works', 'work', 'worked', 'working',
            'runs', 'run', 'ran', 'running',
            'goes', 'go', 'went', 'going',
            'comes', 'come', 'came', 'coming',
            'sits', 'sit', 'sat', 'sitting',
            'stands', 'stand', 'stood', 'standing',
            'plays', 'play', 'played', 'playing'
        }
        
        for i, token in enumerate(tokens):
            word = token['text'].lower()
            
            # è¾æ›¸ã«å«ã¾ã‚Œã‚‹ä¸€èˆ¬çš„ãªå‹•è©
            if word in common_verbs:
                contextual_verbs.append((i, token))
            
            # èªå°¾ã«ã‚ˆã‚‹å‹•è©åˆ¤å®šï¼ˆ-s, -ed, -ingï¼‰
            elif (word.endswith('s') and len(word) > 2 and 
                  not word.endswith('ss') and not word.endswith('us')):
                # ä¸‰äººç§°å˜æ•°å½¢ã‚‰ã—ã„èª
                if self._looks_like_verb_context(tokens, i):
                    contextual_verbs.append((i, token))
        
        return contextual_verbs
    
    def _looks_like_verb_context(self, tokens: List[Dict], index: int) -> bool:
        """
        å‹•è©ã‚‰ã—ã„æ–‡è„ˆã‹ã‚’åˆ¤å®š
        """
        if index == 0:
            return False
        
        # å‰ã®èªãŒåè©ãƒ»ä»£åè©ãªã‚‰å‹•è©ã®å¯èƒ½æ€§ãŒé«˜ã„
        prev_token = tokens[index - 1]
        if prev_token['pos'] in ['NOUN', 'PRON', 'PROPN']:
            return True
        
        # å¾Œã®èªãŒå‰¯è©ãªã‚‰å‹•è©ã®å¯èƒ½æ€§ãŒé«˜ã„
        if index < len(tokens) - 1:
            next_token = tokens[index + 1]
            if next_token['pos'] == 'ADV':
                return True
        
        return False
    
    def _find_auxiliary(self, tokens: List[Dict], main_verb_idx: int) -> Optional[int]:
        """åŠ©å‹•è©ã‚’ç‰¹å®š"""
        # ãƒ¡ã‚¤ãƒ³å‹•è©ã®å‰ã‚’æ¢ã™
        for i in range(main_verb_idx):
            token = tokens[i]
            if self._is_auxiliary_verb(token):
                return i
        return None
    
    def _find_subject(self, tokens: List[Dict], verb_idx: int) -> List[int]:
        """
        ä¸»èªã‚’ç‰¹å®šï¼ˆå‹•è©ã®å‰ã®åè©å¥ï¼‰
        è¤‡æ•°èªã®åè©å¥ã«å¯¾å¿œ
        é–¢ä¿‚ç¯€ã‚’å«ã‚€å ´åˆã¯é–¢ä¿‚ç¯€å…¨ä½“ã‚’ä¸»èªã«å«ã‚ã‚‹
        """
        subject_indices = []
        
        # ğŸ†• é–¢ä¿‚ç¯€ã‚’å«ã‚€ä¸»èªã®ç‰¹å®šï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        # ãƒˆãƒ¼ã‚¯ãƒ³ã«é–¢ä¿‚ç¯€ãƒãƒ¼ã‚«ãƒ¼ãŒã‚ã‚‹å ´åˆã®å‡¦ç†
        antecedent_idx = None
        relative_clause_end_idx = None
        
        for i, token in enumerate(tokens):
            if token.get('is_antecedent', False):
                antecedent_idx = i
            if token.get('is_relative_pronoun', False):
                # é–¢ä¿‚ç¯€ã®å®Ÿéš›ã®çµ‚äº†ä½ç½®ã‚’ä½¿ç”¨
                relative_clause_end_idx = token.get('relative_clause_end', verb_idx - 1)
                break
        
        # é–¢ä¿‚ç¯€ã‚’å«ã‚€ä¸»èªã®å ´åˆ
        if antecedent_idx is not None and relative_clause_end_idx is not None:
            # ğŸ”§ Rephraseã‚·ã‚¹ãƒ†ãƒ ä»•æ§˜: é–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆã§ã‚‚é€šå¸¸ã®ä¸»èªæ¤œå‡ºã‚’è¡Œã†
            # _assign_grammar_rolesã§ã€Œã‹ãŸã¾ã‚Šã€åˆ¤å®šã«ã‚ˆã‚Šç©ºã«ã™ã‚‹ã‹ã‚’æ±ºå®š
            self.logger.debug(f"é–¢ä¿‚ç¯€æ¤œå‡º: é€šå¸¸ã®ä¸»èªæ¤œå‡ºã‚’ç¶™ç¶šï¼ˆã‹ãŸã¾ã‚Šåˆ¤å®šã¯å¾Œã§å®Ÿè¡Œï¼‰")
            # return []  # ã“ã®æ—©æœŸãƒªã‚¿ãƒ¼ãƒ³ã‚’å‰Šé™¤
        
        # é€šå¸¸ã®ä¸»èªæ¤œå‡ºï¼ˆé–¢ä¿‚ç¯€ã‚ã‚Šãƒ»ãªã—ä¸¡å¯¾å¿œï¼‰
        # å‹•è©ã®å‰ã‚’å³ã‹ã‚‰å·¦ã«æ¢ã™
        for i in range(verb_idx - 1, -1, -1):
            token = tokens[i]
            
            # åŠ©å‹•è©ã¯é£›ã°ã™
            if self._is_auxiliary_verb(token):
                continue
            
            # åè©ãƒ»ä»£åè©ãƒ»å† è©ã‚’ä¸»èªã®ä¸€éƒ¨ã¨ã—ã¦åé›†
            if (token['pos'] in ['NOUN', 'PROPN', 'PRON'] or 
                token['tag'] in ['DT', 'PRP', 'PRP$', 'WP']):
                subject_indices.insert(0, i)  # é †åºã‚’ä¿ã¤ãŸã‚å‰ã«æŒ¿å…¥
            else:
                # ä¸»èªã®å¢ƒç•Œã«åˆ°é”
                break
        
        return subject_indices
    
    def _determine_sentence_pattern(self, core_elements: Dict, tokens: List[Dict]) -> str:
        """
        å‹•è©ã®æ€§è³ªã¨æ–‡è„ˆã‹ã‚‰æ–‡å‹ã‚’å‹•çš„ã«åˆ¤å®š
        """
        if not core_elements['verb']:
            return 'UNKNOWN'
        
        verb = core_elements['verb']
        verb_lemma = verb['lemma'].lower()
        verb_indices = core_elements['verb_indices'] + core_elements.get('auxiliary_indices', [])
        subject_indices = core_elements['subject_indices']
        
        # ä½¿ç”¨æ¸ˆã¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        used_indices = set(verb_indices + subject_indices)
        
        # æ®‹ã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åˆ†æ
        remaining_tokens = [
            (i, token) for i, token in enumerate(tokens) 
            if i not in used_indices and token['pos'] != 'PUNCT'
        ]
        
        # é€£çµå‹•è©ã®å ´åˆ â†’ SVCå€™è£œ
        if verb_lemma in self.linking_verbs:
            if remaining_tokens:
                # è£œèªãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                for i, token in remaining_tokens:
                    if self._can_be_complement(token):
                        return 'SVC'
            return 'SV'  # è£œèªãŒãªã„å ´åˆ
        
        # æˆä¸å‹•è©ã®å ´åˆ â†’ SVOOå€™è£œ
        if verb_lemma in self.ditransitive_verbs:
            if len(remaining_tokens) >= 2:
                return 'SVOO'
            elif len(remaining_tokens) == 1:
                return 'SVO'
        
        # ç›®çš„æ ¼è£œèªå‹•è©ã®å ´åˆ â†’ SVOCå€™è£œ
        if verb_lemma in self.objective_complement_verbs:
            if len(remaining_tokens) >= 2:
                return 'SVOC'
            elif len(remaining_tokens) == 1:
                return 'SVO'
        
        # ä¸€èˆ¬çš„ãªä»–å‹•è© â†’ SVO
        if remaining_tokens:
            # ç›®çš„èªå€™è£œãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            for i, token in remaining_tokens:
                if self._can_be_object(token):
                    return 'SVO'
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ â†’ SV
        return 'SV'
    
    def _assign_grammar_roles(self, tokens: List[Dict], pattern: str, core_elements: Dict, relative_info: Dict = None) -> List[GrammarElement]:
        """
        æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ã„ã¦æ–‡æ³•çš„å½¹å‰²ã‚’å‹•çš„ã«å‰²ã‚Šå½“ã¦
        é–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆã¯è©²å½“ã‚¹ãƒ­ãƒƒãƒˆã‚’ç©ºã«ã™ã‚‹
        """
        if relative_info is None:
            relative_info = {'found': False}
            
        elements = []
        used_indices = set()
        
        # ğŸ†• é–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆï¼šã€Œã‹ãŸã¾ã‚Šã€ã®æ–‡æ³•çš„å½¹å‰²ã‚’å‹•è©ã¨ã®é–¢ä¿‚ã‹ã‚‰æ¨å®š
        relative_slot_to_empty = None
        if relative_info['found']:
            relative_slot_to_empty = self._determine_chunk_grammatical_role(tokens, core_elements, relative_info)
        
        # ä¸»èªå‡¦ç†ï¼ˆé–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆã¯å¼·åˆ¶çš„ã«ä¸»èªè¦ç´ ã‚’ä½œæˆï¼‰
        if core_elements['subject_indices'] or (relative_info['found'] and relative_slot_to_empty == 'S'):
            if relative_slot_to_empty == 'S':
                # â‘£ é–¢ä¿‚ç¯€ãŒSä½ç½®ã«ã‚ã‚‹å ´åˆï¼šã€Œå¾Œã«""ã«ã™ã¹ãã€æƒ…å ±ã‚’é©ç”¨
                subject_element = GrammarElement(
                    text="",  # ç©ºæ–‡å­—åˆ—ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆã®â‘£ï¼‰
                    tokens=[],
                    role='S',
                    start_idx=relative_info.get('antecedent_idx', 0),
                    end_idx=relative_info.get('antecedent_idx', 0),
                    confidence=0.95
                )
                self.logger.debug(f"é–¢ä¿‚ç¯€ä¸»èªã‚’ç©ºã‚¹ãƒ­ãƒƒãƒˆã«å¤‰æ›: antecedent_idx={relative_info.get('antecedent_idx')}")
            elif core_elements['subject_indices']:
                # é€šå¸¸ã®ä¸»èªå‡¦ç†
                subject_text = self._clean_relative_clause_from_text(core_elements['subject'], relative_info)
                subject_element = GrammarElement(
                    text=subject_text,
                    tokens=[tokens[i] for i in core_elements['subject_indices']],
                    role='S',
                    start_idx=min(core_elements['subject_indices']),
                    end_idx=max(core_elements['subject_indices']),
                    confidence=0.95
                )
            
            elements.append(subject_element)
            if core_elements['subject_indices']:
                used_indices.update(core_elements['subject_indices'])
        
        # åŠ©å‹•è©
        if core_elements['auxiliary_indices']:
            aux_element = GrammarElement(
                text=core_elements['auxiliary']['text'],
                tokens=[core_elements['auxiliary']],
                role='Aux',
                start_idx=core_elements['auxiliary_indices'][0],
                end_idx=core_elements['auxiliary_indices'][0],
                confidence=0.95
            )
            elements.append(aux_element)
            used_indices.update(core_elements['auxiliary_indices'])
        
        # å‹•è©
        if core_elements['verb_indices']:
            verb_element = GrammarElement(
                text=core_elements['verb']['text'],
                tokens=[core_elements['verb']],
                role='V',
                start_idx=core_elements['verb_indices'][0],
                end_idx=core_elements['verb_indices'][0],
                confidence=0.95
            )
            elements.append(verb_element)
            used_indices.update(core_elements['verb_indices'])
        
        # æ®‹ã‚Šã®è¦ç´ ã‚’æ–‡å‹ã«å¿œã˜ã¦å‰²ã‚Šå½“ã¦
        remaining_tokens = [
            (i, token) for i, token in enumerate(tokens) 
            if i not in used_indices and token['pos'] != 'PUNCT'
        ]
        
        # æ–‡å‹åˆ¥ã®å‰²ã‚Šå½“ã¦ï¼ˆé–¢ä¿‚ç¯€æƒ…å ±ã‚’æ¸¡ã™ï¼‰
        if pattern == 'SVC':
            elements.extend(self._assign_svc_elements(remaining_tokens, relative_slot_to_empty))
        elif pattern == 'SVO':
            elements.extend(self._assign_svo_elements(remaining_tokens, relative_slot_to_empty))
        elif pattern == 'SVOO':
            elements.extend(self._assign_svoo_elements(remaining_tokens, relative_slot_to_empty))
        elif pattern == 'SVOC':
            elements.extend(self._assign_svoc_elements(remaining_tokens, relative_slot_to_empty))
        else:  # SV or other
            elements.extend(self._assign_modifiers(remaining_tokens))
        
        return elements
    
    def _assign_svc_elements(self, remaining_tokens: List[Tuple[int, Dict]], relative_slot_to_empty: str = None) -> List[GrammarElement]:
        """SVCæ–‡å‹ã®è¦ç´ ã‚’å‰²ã‚Šå½“ã¦ - è¤‡åˆå¥å¯¾å¿œ"""
        elements = []
        complement_assigned = False
        used_indices = set()
        
        i = 0
        while i < len(remaining_tokens):
            idx, token = remaining_tokens[i]
            
            # æ—¢ã«ä½¿ç”¨æ¸ˆã¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ã‚¹ã‚­ãƒƒãƒ—
            if idx in used_indices:
                i += 1
                continue
            
            if not complement_assigned and (self._can_be_complement(token) or token['tag'] == 'DT'):
                # C1ã¨ã—ã¦è¤‡åˆå¥ã‚’æ¤œå‡ºï¼ˆå† è©ã‹ã‚‰å§‹ã¾ã‚‹å ´åˆã‚‚å«ã‚€ï¼‰
                phrase_indices, phrase_text = self._find_noun_phrase(remaining_tokens, i)
                if phrase_indices:
                    elements.append(GrammarElement(
                        text=phrase_text,
                        tokens=[remaining_tokens[j][1] for j in range(i, i + len(phrase_indices))],
                        role='C1',
                        start_idx=min(phrase_indices),
                        end_idx=max(phrase_indices),
                        confidence=0.9
                    ))
                    used_indices.update(phrase_indices)
                    complement_assigned = True
                    i += len(phrase_indices)
                else:
                    i += 1
            else:
                # ä¿®é£¾èªã¨ã—ã¦å‡¦ç†
                if idx not in used_indices:
                    elements.append(self._create_modifier_element(idx, token))
                i += 1
        
        return elements
    
    def _assign_svo_elements(self, remaining_tokens: List[Tuple[int, Dict]], relative_slot_to_empty: str = None) -> List[GrammarElement]:
        """SVOæ–‡å‹ã®è¦ç´ ã‚’å‰²ã‚Šå½“ã¦ï¼ˆé–¢ä¿‚ç¯€å¯¾å¿œï¼‰"""
        elements = []
        object_assigned = False
        
        # é–¢ä¿‚ç¯€ã«ã‚ˆã‚Šç›®çš„èªã‚¹ãƒ­ãƒƒãƒˆã‚’æŠ½å‡º
        object_text = ""
        if relative_slot_to_empty == 'O1':
            # O1ã«é–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆã¯ç©ºæ–‡å­—åˆ—
            object_text = ""
        else:
            # é€šå¸¸ã®ç›®çš„èªå‡¦ç† - è¤‡æ•°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã¾ã¨ã‚ã‚‹
            object_tokens = []
            for i, token in remaining_tokens:
                if self._can_be_object(token) or token['pos'] in ['DET', 'ADJ']:
                    object_tokens.append((i, token))
                elif object_tokens:  # ç›®çš„èªå¥ãŒçµ‚äº†
                    break
            
            if object_tokens:
                object_text = ' '.join([token['text'] for _, token in object_tokens])
                used_indices = {i for i, _ in object_tokens}
                
                elements.append(GrammarElement(
                    text=object_text,
                    tokens=[token for _, token in object_tokens],
                    role='O1',
                    start_idx=object_tokens[0][0],
                    end_idx=object_tokens[-1][0],
                    confidence=0.9
                ))
                object_assigned = True
                
                # æ®‹ã‚Šã®è¦ç´ ã‚’ä¿®é£¾èªã¨ã—ã¦å‡¦ç†
                for i, token in remaining_tokens:
                    if i not in used_indices:
                        elements.append(self._create_modifier_element(i, token))
            
        # é–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆã¯ç©ºã®O1è¦ç´ ã‚’ä½œæˆ
        if relative_slot_to_empty == 'O1' and not object_assigned:
            elements.append(GrammarElement(
                text="",  # ç©ºæ–‡å­—åˆ—
                tokens=[],
                role='O1',
                start_idx=0,
                end_idx=0,
                confidence=0.9
            ))
            
            # æ®‹ã‚Šã‚’ä¿®é£¾èªã¨ã—ã¦å‡¦ç†
            for i, token in remaining_tokens:
                elements.append(self._create_modifier_element(i, token))
        
        return elements
    
    def _assign_svoo_elements(self, remaining_tokens: List[Tuple[int, Dict]], relative_slot_to_empty: str = None) -> List[GrammarElement]:
        """SVOOæ–‡å‹ã®è¦ç´ ã‚’å‰²ã‚Šå½“ã¦ - O1/O2åˆ†é›¢å¯¾å¿œ"""
        elements = []
        o1_assigned = False
        o2_assigned = False
        used_indices = set()
        
        i = 0
        while i < len(remaining_tokens):
            idx, token = remaining_tokens[i]
            
            # æ—¢ã«ä½¿ç”¨æ¸ˆã¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ã‚¹ã‚­ãƒƒãƒ—
            if idx in used_indices:
                i += 1
                continue
            
            if not o1_assigned and (self._can_be_object(token) or token['tag'] == 'DT'):
                # SVOOæ–‡å‹ã®O1ã¯é€šå¸¸å˜ä¸€èªï¼ˆä»£åè©ãªã©ï¼‰
                if token['pos'] == 'PRON':
                    # ä»£åè©ã®å ´åˆã¯å˜èªã®ã¿ã§O1
                    elements.append(GrammarElement(
                        text=token['text'],
                        tokens=[token],
                        role='O1',
                        start_idx=idx,
                        end_idx=idx,
                        confidence=0.9
                    ))
                    used_indices.add(idx)
                    o1_assigned = True
                    i += 1
                else:
                    # ä»£åè©ä»¥å¤–ã‚‚å˜èªã®ã¿ã§O1ã¨ã—ã¦æ‰±ã†
                    elements.append(GrammarElement(
                        text=token['text'],
                        tokens=[token],
                        role='O1',
                        start_idx=idx,
                        end_idx=idx,
                        confidence=0.85
                    ))
                    used_indices.add(idx)
                    o1_assigned = True
                    i += 1
            elif not o2_assigned and (self._can_be_object(token) or token['tag'] == 'DT'):
                # O2ã¨ã—ã¦è¤‡åˆå¥ã‚’æ¤œå‡ºï¼ˆå† è©ã‹ã‚‰å§‹ã¾ã‚‹å ´åˆã‚‚å«ã‚€ï¼‰
                phrase_indices, phrase_text = self._find_noun_phrase(remaining_tokens, i)
                if phrase_indices:
                    elements.append(GrammarElement(
                        text=phrase_text,
                        tokens=[remaining_tokens[j][1] for j in range(i, i + len(phrase_indices))],
                        role='O2',
                        start_idx=min(phrase_indices),
                        end_idx=max(phrase_indices),
                        confidence=0.85
                    ))
                    used_indices.update(phrase_indices)
                    o2_assigned = True
                    i += len(phrase_indices)
                else:
                    i += 1
            else:
                # ä¿®é£¾èªã¨ã—ã¦å‡¦ç†
                if idx not in used_indices:
                    elements.append(self._create_modifier_element(idx, token))
                i += 1
        
        return elements
    
    def _assign_svoc_elements(self, remaining_tokens: List[Tuple[int, Dict]], relative_slot_to_empty: str = None) -> List[GrammarElement]:
        """SVOCæ–‡å‹ã®è¦ç´ ã‚’å‰²ã‚Šå½“ã¦"""
        elements = []
        object_assigned = False
        complement_assigned = False
        
        for i, token in remaining_tokens:
            if not object_assigned and self._can_be_object(token):
                elements.append(GrammarElement(
                    text=token['text'],
                    tokens=[token],
                    role='O1',
                    start_idx=i,
                    end_idx=i,
                    confidence=0.85
                ))
                object_assigned = True
            elif not complement_assigned and (self._can_be_complement(token) or object_assigned):
                elements.append(GrammarElement(
                    text=token['text'],
                    tokens=[token],
                    role='C2',  # ğŸ”§ SVOCã®Cã¯C2ã«ä¿®æ­£
                    start_idx=i,
                    end_idx=i,
                    confidence=0.85
                ))
                complement_assigned = True
            else:
                # ä¿®é£¾èªã¨ã—ã¦å‡¦ç†
                elements.append(self._create_modifier_element(i, token))
        
        return elements
    
    def _assign_modifiers(self, remaining_tokens: List[Tuple[int, Dict]]) -> List[GrammarElement]:
        """ä¿®é£¾èªã‚’å‰²ã‚Šå½“ã¦"""
        elements = []
        for i, token in remaining_tokens:
            elements.append(self._create_modifier_element(i, token))
        return elements
    
    def _create_modifier_element(self, idx: int, token: Dict) -> GrammarElement:
        """ä¿®é£¾èªè¦ç´ ã‚’ä½œæˆ"""
        # ä¿®é£¾èªã®ç¨®é¡ã‚’åˆ¤å®š
        if token['pos'] in ['ADV', 'PART']:
            role = 'M1'  # å‰¯è©çš„ä¿®é£¾
        elif token['pos'] in ['ADP'] or token['tag'] in ['IN', 'TO']:
            role = 'M2'  # å‰ç½®è©å¥
        else:
            role = 'M3'  # ãã®ä»–ã®ä¿®é£¾
        
        return GrammarElement(
            text=token['text'],
            tokens=[token],
            role=role,
            start_idx=idx,
            end_idx=idx,
            confidence=0.7
        )
    
    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    def _is_auxiliary_verb(self, token: Dict) -> bool:
        """åŠ©å‹•è©åˆ¤å®š"""
        aux_words = {'be', 'am', 'is', 'are', 'was', 'were', 'being', 'been',
                     'have', 'has', 'had', 'having',
                     'do', 'does', 'did', 'doing',
                     'will', 'would', 'shall', 'should', 'can', 'could',
                     'may', 'might', 'must', 'ought'}
        return token['lemma'].lower() in aux_words or token['tag'] in ['MD']
    
    def _find_noun_phrase(self, tokens: List[Tuple[int, Dict]], start_idx: int) -> Tuple[List[int], str]:
        """
        æŒ‡å®šä½ç½®ã‹ã‚‰è¤‡åˆåè©å¥ã‚’æ¤œå‡º
        
        Args:
            tokens: ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆ [(index, token), ...]
            start_idx: æ¤œç´¢é–‹å§‹ä½ç½®
            
        Returns:
            Tuple[List[int], str]: (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚¹ãƒˆ, çµåˆã—ãŸãƒ•ãƒ¬ãƒ¼ã‚º)
        """
        phrase_indices = []
        phrase_tokens = []
        
        # é–‹å§‹ä½ç½®ã‹ã‚‰é€£ç¶šã™ã‚‹åè©å¥è¦ç´ ã‚’åé›†
        for i in range(start_idx, len(tokens)):
            idx, token = tokens[i]
            
            # åè©å¥ã®æ§‹æˆè¦ç´ ã‹ãƒã‚§ãƒƒã‚¯
            if (token['pos'] in ['NOUN', 'PROPN', 'PRON'] or 
                token['tag'] in ['DT', 'CD', 'JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS']):
                phrase_indices.append(idx)
                phrase_tokens.append(token['text'])
            else:
                # åè©å¥ã®çµ‚äº†
                break
        
        if phrase_indices:
            phrase_text = ' '.join(phrase_tokens)
            return phrase_indices, phrase_text
        else:
            return [], ""

    def _can_be_object(self, token: Dict) -> bool:
        """ç›®çš„èªã«ãªã‚Œã‚‹ã‹ã®åˆ¤å®š"""
        return token['pos'] in ['NOUN', 'PROPN', 'PRON'] or token['tag'] in ['PRP', 'DT']
    
    def _can_be_complement(self, token: Dict) -> bool:
        """è£œèªã«ãªã‚Œã‚‹ã‹ã®åˆ¤å®š"""
        return token['pos'] in ['ADJ', 'NOUN', 'PROPN', 'PRON'] or token['tag'] in ['JJ', 'NN', 'NNS', 'PRP']
    
    def _convert_to_rephrase_format(self, elements: List[GrammarElement], pattern: str, sub_slots: Dict = None) -> Dict[str, Any]:
        """Rephraseã‚¹ãƒ­ãƒƒãƒˆå½¢å¼ã«å¤‰æ›"""
        if sub_slots is None:
            sub_slots = {}
        
        # ğŸ”§ é–¢ä¿‚ç¯€ã®æœ‰ç„¡ã‚’ç¢ºèªã—ã¦ã‚¹ãƒ­ãƒƒãƒˆç•ªå·ã‚’èª¿æ•´
        has_relative_clause = bool(sub_slots)
        
        # é–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆã¯ä¿®é£¾èªã®ã‚¹ãƒ­ãƒƒãƒˆç•ªå·ã‚’ã‚·ãƒ•ãƒˆ
        if has_relative_clause:
            for element in elements:
                if element.role == 'M1':
                    element.role = 'M2'  # M1 â†’ M2
                elif element.role == 'M2':
                    element.role = 'M3'  # M2 â†’ M3
            
        slots = []
        slot_phrases = []
        slot_display_order = []
        display_order = []
        phrase_types = []
        subslot_ids = []
        
        # ğŸ”§ main_slotsè¾æ›¸å½¢å¼ã‚‚ç”Ÿæˆ
        main_slots = {}
        
        # è¦ç´ ã‚’ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
        elements.sort(key=lambda x: x.start_idx)
        
        role_order = {'S': 1, 'Aux': 2, 'V': 3, 'O1': 4, 'O2': 5, 'C1': 6, 'C2': 7, 'M1': 8, 'M2': 9, 'M3': 10}
        
        for i, element in enumerate(elements):
            # ã‚¹ãƒ­ãƒƒãƒˆåã®èª¿æ•´
            slot_name = element.role
            # çµ±ä¸€å½¢å¼: å¸¸ã«O1, O2, C1, C2ã‚’ä½¿ç”¨
            
            slots.append(slot_name)
            slot_phrases.append(element.text)
            
            # ğŸ”§ main_slotsè¾æ›¸ã«è¿½åŠ 
            main_slots[element.role] = element.text
            
            order = role_order.get(element.role, 99)
            slot_display_order.append(order)
            display_order.append(order)
            
            # å“è©ã‚¿ã‚¤ãƒ—ã®åˆ¤å®š
            if element.role in ['S', 'O1', 'O2']:
                phrase_types.append('åè©å¥')
            elif element.role == 'V':
                phrase_types.append('å‹•è©å¥')
            elif element.role in ['C1', 'C2']:
                phrase_types.append('è£œèªå¥')
            else:
                phrase_types.append('ä¿®é£¾å¥')
            
            subslot_ids.append(i)
        
        return {
            'Slot': slots,
            'SlotPhrase': slot_phrases,
            'Slot_display_order': slot_display_order,
            'display_order': display_order,
            'PhraseType': phrase_types,
            'SubslotID': subslot_ids,
            'main_slots': main_slots,  # ğŸ”§ è¾æ›¸å½¢å¼è¿½åŠ 
            'sub_slots': sub_slots,    # ï¿½ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆè¿½åŠ 
            'slots': main_slots,       # ğŸ”§ çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ äº’æ›æ€§
            'pattern_detected': pattern,
            'confidence': 0.9,
            'analysis_method': 'dynamic_grammar',
            'lexical_tokens': len([e for e in elements if e.role != 'PUNCT'])
        }
    
    def _create_error_result(self, sentence: str, error: str) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼çµæœã‚’ä½œæˆ"""
        return {
            'Slot': [],
            'SlotPhrase': [],
            'Slot_display_order': [],
            'display_order': [],
            'PhraseType': [],
            'SubslotID': [],
            'main_slots': {},    # ğŸ”§ è¾æ›¸å½¢å¼è¿½åŠ 
            'sub_slots': {},     # ğŸ”§ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆï¼ˆç¾åœ¨ã¯ç©ºï¼‰
            'slots': {},         # ğŸ”§ çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ äº’æ›æ€§
            'error': error,
            'sentence': sentence,
            'analysis_method': 'dynamic_grammar'
        }

    # ============================================
    # é–¢ä¿‚ç¯€å‡¦ç†ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤
    # ============================================
    
    def _detect_relative_clause(self, tokens: List[Dict], sentence: str) -> Dict[str, Any]:
        """é–¢ä¿‚ç¯€æ§‹é€ ã®æ¤œå‡º"""
        result = {
            'found': False,
            'type': None,
            'confidence': 0.0,
            'relative_pronoun_idx': None,
            'antecedent_idx': None,
            'clause_start_idx': None,
            'clause_end_idx': None
        }
        
        sentence_lower = sentence.lower()
        
        # é–¢ä¿‚ä»£åè©ã®æ¤œå‡º
        relative_pronouns = ['who', 'whom', 'which', 'that', 'whose', 'where', 'when', 'why', 'how']
        
        for rel_pronoun in relative_pronouns:
            if rel_pronoun in sentence_lower:
                # ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆã§é–¢ä¿‚ä»£åè©ã‚’æ¢ã™
                for i, token in enumerate(tokens):
                    if token['text'].lower() == rel_pronoun:
                        result.update({
                            'found': True,
                            'type': f'{rel_pronoun}_clause',
                            'confidence': 0.8,
                            'relative_pronoun_idx': i
                        })
                        
                        # å…ˆè¡Œè©ã‚’æ¢ã™ï¼ˆé–¢ä¿‚ä»£åè©ã®ç›´å‰ã®åè©ï¼‰
                        if i > 0 and tokens[i-1]['pos'] in ['NOUN', 'PROPN']:
                            result['antecedent_idx'] = i - 1
                            result['confidence'] = 0.9
                        
                        # é–¢ä¿‚ç¯€ã®ç¯„å›²ã‚’æ±ºå®šï¼ˆæ”¹è‰¯ç‰ˆï¼‰
                        result['clause_start_idx'] = i
                        result['clause_end_idx'] = self._find_relative_clause_end(tokens, i, rel_pronoun)
                        
                        self.logger.debug(f"é–¢ä¿‚ç¯€æ¤œå‡º: {rel_pronoun} at position {i}, end at {result['clause_end_idx']}")
                        break
                
                if result['found']:
                    break
        
        return result
    
    def _find_relative_clause_end(self, tokens: List[Dict], rel_start_idx: int, rel_type: str) -> int:
        """é–¢ä¿‚ç¯€ã®çµ‚äº†ä½ç½®ã‚’ç‰¹å®šï¼ˆäººé–“çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ ï¼‰"""
        
        # whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†
        if rel_type == 'whose':
            return self._find_whose_clause_end(tokens, rel_start_idx)
        
        # ğŸ†• whoæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†ï¼ˆTest 12æˆåŠŸæ‰‹æ³•ã‚’é©ç”¨ï¼‰
        if rel_type == 'who':
            return self._find_who_clause_end(tokens, rel_start_idx)
        
        # ğŸ†• ä¸€èˆ¬çš„ãªé–¢ä¿‚ç¯€ï¼ˆwhich/thatï¼‰ã®çµ‚äº†ä½ç½®ã‚’å“è©ãƒ™ãƒ¼ã‚¹ã§ç‰¹å®š
        # æˆ¦ç•¥: é–¢ä¿‚ä»£åè© + å‹•è© + [ä¿®é£¾èª/ç›®çš„èª/è£œèª] ã¾ã§ã‚’é–¢ä¿‚ç¯€ã¨ã™ã‚‹
        
        clause_end = rel_start_idx
        
        # Step 1: é–¢ä¿‚ä»£åè©ã®å¾Œã®å‹•è©ã‚’æ¢ã™
        rel_verb_idx = None
        for i in range(rel_start_idx + 1, min(rel_start_idx + 4, len(tokens))):
            if i < len(tokens) and tokens[i]['pos'] in ['VERB', 'AUX']:
                rel_verb_idx = i
                break
        
        if rel_verb_idx is None:
            return rel_start_idx + 1
        
        clause_end = rel_verb_idx
        
        # Step 2: å‹•è©ã®å¾Œã®è¦ç´ ã‚’é–¢ä¿‚ç¯€ã«å«ã‚ã‚‹ï¼ˆğŸ†• äººé–“çš„æ§‹é€ åˆ¤å®šï¼‰
        for i in range(rel_verb_idx + 1, len(tokens)):
            if i < len(tokens):
                token = tokens[i]
                
                # ğŸ†• äººé–“çš„å“è©åˆ¤å®šã‚’é©ç”¨
                actual_pos = self._get_human_corrected_pos(token)
                
                # ğŸ†• æ§‹é€ çš„åˆ¤å®š: æ–°ã—ã„å‹•è©å‡ºç¾ã§ä¸Šä½æ–‡é–‹å§‹
                if actual_pos in ['VERB', 'AUX']:
                    self.logger.debug(f"ğŸ§  ä¸Šä½æ–‡å‹•è©æ¤œå‡ºã«ã‚ˆã‚Šé–¢ä¿‚ç¯€çµ‚äº†: '{token['text']}' â†’ {actual_pos}")
                    break
                
                # é–¢ä¿‚ç¯€å†…ã®è¦ç´ ã¨ã—ã¦å«ã‚ã‚‹æ¡ä»¶
                if actual_pos in ['ADV', 'ADJ', 'NOUN', 'PROPN', 'NUM']:
                    clause_end = i
                    self.logger.debug(f"é–¢ä¿‚ç¯€ã«å«ã‚ã‚‹: '{token['text']}' (corrected_pos={actual_pos})")
                else:
                    # ãã®ä»–ã®å“è©ã§é–¢ä¿‚ç¯€çµ‚äº†
                    break
        
        self.logger.debug(f"é–¢ä¿‚ç¯€çµ‚äº†ä½ç½®({rel_type}): {clause_end} ('{tokens[clause_end]['text'] if clause_end < len(tokens) else 'EOF'}')")
        return clause_end
    
    def _find_whose_clause_end(self, tokens: List[Dict], whose_idx: int) -> int:
        """whoseæ§‹æ–‡ã®é–¢ä¿‚ç¯€çµ‚äº†ä½ç½®ã‚’ç‰¹å®šï¼ˆæ§‹é€ çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰
        
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆã®æ§‹é€ çš„ãƒ­ã‚¸ãƒƒã‚¯ï¼š
        whose â†’ å…ˆè¡Œè©ç‰¹å®š â†’ ä¿®é£¾å¯¾è±¡åè© â†’ é–¢ä¿‚ç¯€å†…å‹•è© â†’ è£œèª/ç›®çš„èª â†’ 
        æ–°ã—ã„å‹•è©å‡ºç¾æ™‚ç‚¹ã§ä¸Šä½æ–‡é–‹å§‹ã¨åˆ¤å®šã—ã¦ã‚¹ãƒˆãƒƒãƒ—
        """
        
        clause_end = whose_idx
        
        # Step 1: whose ã®ç›´å¾Œã®ä¿®é£¾å¯¾è±¡åè©ã‚’æ¢ã™
        possessed_noun_idx = None
        for i in range(whose_idx + 1, min(whose_idx + 3, len(tokens))):
            if tokens[i]['pos'] in ['NOUN', 'PROPN']:
                possessed_noun_idx = i
                self.logger.debug(f"whoseä¿®é£¾å¯¾è±¡: '{tokens[i]['text']}' at {i}")
                break
        
        if possessed_noun_idx is None:
            self.logger.debug(f"whoseæ§‹æ–‡: ä¿®é£¾å¯¾è±¡åè©ãŒè¦‹ã¤ã‹ã‚‰ãªã„")
            return whose_idx + 1
        
        # Step 2: é–¢ä¿‚ç¯€å†…å‹•è©ã‚’æ¢ã™
        relcl_verb_idx = None
        for i in range(possessed_noun_idx + 1, min(possessed_noun_idx + 4, len(tokens))):
            if tokens[i]['pos'] in ['VERB', 'AUX']:
                relcl_verb_idx = i
                self.logger.debug(f"é–¢ä¿‚ç¯€å†…å‹•è©: '{tokens[i]['text']}' at {i}")
                break
        
        if relcl_verb_idx is None:
            self.logger.debug(f"whoseæ§‹æ–‡: é–¢ä¿‚ç¯€å‹•è©ãŒè¦‹ã¤ã‹ã‚‰ãªã„")
            return possessed_noun_idx
        
        # Step 3: é–¢ä¿‚ç¯€å†…ã®è£œèª/ç›®çš„èªã‚’é †æ¬¡å‡¦ç†
        clause_end = relcl_verb_idx
        
        for i in range(relcl_verb_idx + 1, len(tokens)):
            token = tokens[i]
            
            # ğŸ†• äººé–“çš„å“è©åˆ¤å®šã‚’ä½¿ç”¨ï¼ˆå¾ªç’°å‚ç…§å›é¿ï¼‰
            sentence_text = ' '.join([t['text'] for t in tokens])
            if token['text'].lower() in self.ambiguous_words:
                # æ§‹é€ çš„åˆ¤å®šã§å‹•è©ã‹ã©ã†ã‹ç›´æ¥ãƒã‚§ãƒƒã‚¯
                if self._is_likely_main_verb_by_position(token, tokens, i):
                    corrected_pos = 'VERB'
                else:
                    corrected_pos = token['pos']  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            else:
                corrected_pos = token['pos']
            
            # ğŸ†• æ§‹é€ çš„åˆ¤å®š: æ–°ã—ã„å‹•è©ãŒå‡ºç¾ã—ãŸã‚‰ä¸Šä½æ–‡é–‹å§‹
            if corrected_pos in ['VERB', 'AUX'] and i > relcl_verb_idx:
                self.logger.debug(f"ä¸Šä½æ–‡å‹•è©æ¤œå‡ºã«ã‚ˆã‚Šé–¢ä¿‚ç¯€çµ‚äº†: '{token['text']}' at {i} (äººé–“çš„åˆ¤å®š: {corrected_pos})")
                break
            
            # é–¢ä¿‚ç¯€å†…è¦ç´ ã¨ã—ã¦å«ã‚ã‚‹
            if corrected_pos in ['ADJ', 'NOUN', 'PROPN', 'ADV']:
                clause_end = i
                self.logger.debug(f"é–¢ä¿‚ç¯€è¦ç´ : '{token['text']}' at {i}")
            else:
                # ãã®ä»–ã®å“è©ã§é–¢ä¿‚ç¯€çµ‚äº†
                break
        
        self.logger.debug(f"whoseå¥çµ‚äº†ä½ç½®(æ§‹é€ çš„): {clause_end} ('{tokens[clause_end]['text'] if clause_end < len(tokens) else 'EOF'}')")
        return clause_end
    
    def _find_who_clause_end(self, tokens: List[Dict], who_idx: int) -> int:
        """whoæ§‹æ–‡ã®é–¢ä¿‚ç¯€çµ‚äº†ä½ç½®ã‚’ç‰¹å®šï¼ˆTest 12æˆåŠŸæ‰‹æ³•ã‚’é©ç”¨ï¼‰
        
        Test 12ãƒ‘ã‚¿ãƒ¼ãƒ³: The man whose car is red lives here.
        â†’ "The man who runs fast is strong."
        
        whoæ§‹æ–‡ã®æ§‹é€ çš„ãƒ­ã‚¸ãƒƒã‚¯ï¼š
        who â†’ é–¢ä¿‚ç¯€å†…å‹•è© â†’ ä¿®é£¾èªï¼ˆå‰¯è©/å½¢å®¹è©ï¼‰ â†’ ä¸Šä½æ–‡å‹•è©å‡ºç¾ã§ã‚¹ãƒˆãƒƒãƒ—
        
        æœŸå¾…å€¤: "The man who runs fast" â†’ sub-s="The man who", sub-v="runs", sub-m2="fast"
        """
        
        clause_end = who_idx
        
        # Step 1: whoç›´å¾Œã®å‹•è©ã‚’æ¢ã™ï¼ˆé–¢ä¿‚ç¯€å†…å‹•è©ï¼‰
        relcl_verb_idx = None
        for i in range(who_idx + 1, min(who_idx + 3, len(tokens))):
            if i < len(tokens):
                token = tokens[i]
                # ğŸ†• äººé–“çš„å“è©åˆ¤å®šã‚’é©ç”¨
                corrected_pos = self._get_human_corrected_pos(token)
                
                if corrected_pos in ['VERB', 'AUX']:
                    relcl_verb_idx = i
                    clause_end = i
                    self.logger.debug(f"whoå¥å†…å‹•è©ç™ºè¦‹: '{token['text']}' at {i} (äººé–“çš„åˆ¤å®š: {corrected_pos})")
                    break
        
        if relcl_verb_idx is None:
            self.logger.debug("whoå¥å†…å‹•è©ãŒè¦‹ã¤ã‹ã‚‰ãªã„")
            return who_idx + 1
        
        # Step 2: é–¢ä¿‚ç¯€å†…å‹•è©ã®å¾Œã®ä¿®é£¾èªã‚’é–¢ä¿‚ç¯€ã«å«ã‚ã‚‹ï¼ˆ"fast"ç­‰ï¼‰
        for i in range(relcl_verb_idx + 1, len(tokens)):
            if i < len(tokens):
                token = tokens[i]
                
                # ğŸ†• äººé–“çš„å“è©åˆ¤å®šã‚’é©ç”¨ï¼ˆæ›–æ˜§èªã®å ´åˆï¼‰
                corrected_pos = self._get_human_corrected_pos(token)
                
                # ğŸ†• æ§‹é€ çš„åˆ¤å®š: æ–°ã—ã„å‹•è©ãŒå‡ºç¾ã—ãŸã‚‰ä¸Šä½æ–‡é–‹å§‹
                if corrected_pos in ['VERB', 'AUX'] and i > relcl_verb_idx:
                    self.logger.debug(f"ä¸Šä½æ–‡å‹•è©æ¤œå‡ºã«ã‚ˆã‚Šwhoå¥çµ‚äº†: '{token['text']}' at {i} (äººé–“çš„åˆ¤å®š: {corrected_pos})")
                    break
                
                # é–¢ä¿‚ç¯€å†…è¦ç´ ã¨ã—ã¦å«ã‚ã‚‹ï¼ˆå‰¯è©ã€å½¢å®¹è©ã€åè©ï¼‰
                if corrected_pos in ['ADV', 'ADJ', 'NOUN', 'PROPN']:
                    clause_end = i
                    self.logger.debug(f"whoå¥å†…è¦ç´ : '{token['text']}' at {i} (corrected_pos={corrected_pos})")
                else:
                    # ãã®ä»–ã®å“è©ã§é–¢ä¿‚ç¯€çµ‚äº†
                    break
        
        self.logger.debug(f"whoå¥çµ‚äº†ä½ç½®(æ§‹é€ çš„): {clause_end} ('{tokens[clause_end]['text'] if clause_end < len(tokens) else 'EOF'}')")
        return clause_end
    
    def _process_relative_clause(self, tokens: List[Dict], relative_info: Dict) -> Tuple[List[Dict], Dict]:
        """é–¢ä¿‚ç¯€ã®å‡¦ç†ã¨ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ï¼ˆRephraseä»•æ§˜æº–æ‹ ï¼‰
        
        æ­£ã—ã„Rephraseçš„åˆ†è§£:
        - é–¢ä¿‚ç¯€ã‚’å«ã‚€ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã¯ç©ºæ–‡å­—åˆ—
        - ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«å…ˆè¡Œè©+é–¢ä¿‚ä»£åè©ã€å‹•è©ã€ä¿®é£¾èªã‚’æ ¼ç´
        """
        self.logger.debug(f"é–¢ä¿‚ç¯€å‡¦ç†: {relative_info['type']} (ä¿¡é ¼åº¦: {relative_info['confidence']})")
        
        # é–¢ä¿‚ç¯€ã®ç¯„å›²ã‚’ç‰¹å®š
        rel_pronoun_idx = relative_info.get('relative_pronoun_idx')
        clause_end_idx = relative_info.get('clause_end_idx')
        antecedent_idx = relative_info.get('antecedent_idx')
        
        if rel_pronoun_idx is None or clause_end_idx is None or antecedent_idx is None:
            return tokens, {}
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ã«ãƒãƒ¼ã‚«ãƒ¼ã‚’è¿½åŠ 
        tokens[rel_pronoun_idx]['is_relative_pronoun'] = True
        tokens[rel_pronoun_idx]['relative_clause_type'] = relative_info['type']
        tokens[rel_pronoun_idx]['relative_clause_end'] = clause_end_idx
        tokens[antecedent_idx]['is_antecedent'] = True
        
        # Rephraseçš„ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£å®Ÿè£…
        sub_slots = self._create_rephrase_subslots(tokens, relative_info)
        
        self.logger.debug(f"ç”Ÿæˆã•ã‚ŒãŸã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ: {sub_slots}")
        
        return tokens, sub_slots

    def _create_rephrase_subslots(self, tokens: List[Dict], relative_info: Dict) -> Dict:
        """Rephraseä»•æ§˜ã«æº–æ‹ ã—ãŸã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ
        
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆã®æ–¹æ³•ï¼š5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ç›´æ¥ä½¿ç”¨
        """
        rel_pronoun_idx = relative_info['relative_pronoun_idx']
        clause_end_idx = relative_info['clause_end_idx']
        antecedent_idx = relative_info['antecedent_idx']
        
        # ğŸ†• é–¢ä¿‚ç¯€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ½å‡ºï¼ˆé–¢ä¿‚ä»£åè©ã‹ã‚‰é–¢ä¿‚ç¯€çµ‚äº†ã¾ã§ï¼‰
        # clause_end_idxã¯é–¢ä¿‚ç¯€æœ€å¾Œã®è¦ç´ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãªã®ã§ +1 ã—ã¦ã‚¹ãƒ©ã‚¤ã‚·ãƒ³ã‚°
        rel_tokens = tokens[rel_pronoun_idx:clause_end_idx + 1]
        
        # ğŸ†• é–¢ä¿‚ä»£åè©ã®å½¹å‰²ã‚’åˆ¤å®šï¼ˆä¸»èª/ç›®çš„èªï¼‰
        rel_clause_type = relative_info.get('type', '')
        rel_pronoun_role = self._determine_relative_pronoun_role_enhanced(rel_tokens, rel_clause_type)
        self.logger.debug(f"é–¢ä¿‚ä»£åè©å½¹å‰²åˆ¤å®š: {rel_pronoun_role}")
        
        # ğŸ†• 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã§é–¢ä¿‚ç¯€å†…ã‚’è§£æ
        sub_slots = self._analyze_relative_clause_structure_enhanced(rel_tokens, rel_clause_type, rel_pronoun_role)
        
        # ğŸ†• å…ˆè¡Œè©å¥å…¨ä½“ã‚’å–å¾—ï¼ˆThe man ãªã©ï¼‰
        antecedent_phrase = self._extract_full_antecedent_phrase(tokens, antecedent_idx)
        
        # ğŸ†• é–¢ä¿‚ä»£åè©ã®å½¹å‰²ã«åŸºã¥ãé©åˆ‡ãªé…ç½®
        rel_pronoun_text = rel_tokens[0]['text']
        if rel_pronoun_role == 'subject':
            # é–¢ä¿‚ä»£åè©ãŒä¸»èª â†’ sub-s
            sub_slots['sub-s'] = f"{antecedent_phrase} {rel_pronoun_text}"
        elif rel_pronoun_role == 'object':
            # é–¢ä¿‚ä»£åè©ãŒç›®çš„èª â†’ sub-o1  
            sub_slots['sub-o1'] = f"{antecedent_phrase} {rel_pronoun_text}"
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆwhoseç­‰ï¼‰
            if 'sub-s' in sub_slots:
                sub_slots['sub-s'] = f"{antecedent_phrase} {sub_slots['sub-s']}"
        
        return sub_slots
    
    def _determine_relative_pronoun_role_enhanced(self, rel_tokens: List[Dict], clause_type: str) -> str:
        """é–¢ä¿‚ä»£åè©ã®å½¹å‰²ã‚’åˆ¤å®šï¼ˆä¸»èª/ç›®çš„èªï¼‰- å¼·åŒ–ç‰ˆ
        
        äººé–“çš„æ–‡æ³•èªè­˜:
        - å‹•è©å‰ã«ä»–ã®ä¸»èªãŒã‚ã‚‹ã‹ï¼Ÿ â†’ ã‚ã‚‹å ´åˆã€é–¢ä¿‚ä»£åè©ã¯ç›®çš„èª
        - å‹•è©å‰ã«ä¸»èªãŒãªã„ â†’ é–¢ä¿‚ä»£åè©ã¯ä¸»èª
        """
        if not rel_tokens or len(rel_tokens) < 2:
            return 'subject'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        # å‹•è©ã‚’æ¢ã™
        verb_idx = None
        for i, token in enumerate(rel_tokens):
            if token['pos'] == 'VERB' and i > 0:  # é–¢ä¿‚ä»£åè©ä»¥å¤–
                verb_idx = i
                break
        
        if verb_idx is None:
            return 'subject'  # å‹•è©ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
        
        # whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†
        if clause_type == 'whose_clause':
            # whose ã®ç›´å¾Œã«åè©ãŒã‚ã‚Šã€ãã®å¾Œã«å‹•è© â†’ whose+åè©ãŒä¸»èª
            if len(rel_tokens) > 1 and rel_tokens[1]['pos'] in ['NOUN', 'PROPN']:
                return 'subject'
        
        # å‹•è©ã®å‰ã«ä»–ã®ä¸»èªãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        pre_verb_tokens = rel_tokens[1:verb_idx]  # é–¢ä¿‚ä»£åè©ã‚’é™¤ã
        has_other_subject = any(
            token['pos'] in ['NOUN', 'PRON', 'PROPN'] 
            for token in pre_verb_tokens
        )
        
        if has_other_subject:
            # å‹•è©å‰ã«ä»–ã®ä¸»èª â†’ é–¢ä¿‚ä»£åè©ã¯ç›®çš„èª
            self.logger.debug(f"é–¢ä¿‚ä»£åè©ã¯ç›®çš„èª: å‹•è©å‰ã«ä¸»èª {[t['text'] for t in pre_verb_tokens]}")
            return 'object'
        else:
            # å‹•è©å‰ã«ä¸»èªãªã— â†’ é–¢ä¿‚ä»£åè©ã¯ä¸»èª
            self.logger.debug(f"é–¢ä¿‚ä»£åè©ã¯ä¸»èª: å‹•è©å‰ã«ä¸»èªãªã—")
            return 'subject'

    def _analyze_relative_clause_structure_enhanced(self, rel_tokens: List[Dict], clause_type: str, rel_pronoun_role: str) -> Dict:
        """é–¢ä¿‚ç¯€å†…éƒ¨æ§‹é€ è§£æ - å¼·åŒ–ç‰ˆ
        
        é–¢ä¿‚ä»£åè©ã®å½¹å‰²ã‚’è€ƒæ…®ã—ãŸæ­£ç¢ºãªã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ
        """
        if not rel_tokens:
            return {}
        
        self.logger.debug(f"å¼·åŒ–ç‰ˆé–¢ä¿‚ç¯€è§£æ: {[t['text'] for t in rel_tokens]} (å½¹å‰²: {rel_pronoun_role})")
        
        sub_slots = {}
        
        # å‹•è©ã‚’ç‰¹å®š
        verb_token = None
        for i, token in enumerate(rel_tokens):
            if token['pos'] == 'VERB' and i > 0:  # é–¢ä¿‚ä»£åè©ä»¥å¤–
                verb_token = token
                sub_slots['sub-v'] = token['text']
                break
        
        if verb_token is None:
            return sub_slots
        
        # é–¢ä¿‚ä»£åè©ã®å½¹å‰²ãŒç›®çš„èªã®å ´åˆã€å‹•è©å‰ã®è¦ç´ ã‚’ sub-s ã«
        if rel_pronoun_role == 'object':
            for i, token in enumerate(rel_tokens):
                if i > 0 and token['pos'] in ['NOUN', 'PRON', 'PROPN'] and token != verb_token:
                    sub_slots['sub-s'] = token['text']
                    break
        
        # ä¿®é£¾èªã®æ¤œå‡ºï¼ˆADVã€å ´æ‰€å‰¯è©ã€å‰ç½®è©å¥ï¼‰
        for i, token in enumerate(rel_tokens):
            if i > 0 and token != verb_token and token['text'] not in [sub_slots.get('sub-s', '')]:
                # ğŸ†• å¼·åŒ–ã•ã‚ŒãŸä¿®é£¾èªæ¤œå‡º
                corrected_pos = self._get_human_corrected_pos(token)
                
                if (corrected_pos == 'ADV' or 
                    token['pos'] == 'ADV' or 
                    token['text'].lower() in ['there', 'here', 'everywhere', 'nowhere', 'fast', 'carefully', 'diligently', 'efficiently']):
                    
                    sub_slots['sub-m2'] = token['text']
                    self.logger.debug(f"ä¿®é£¾èªæ¤œå‡º: '{token['text']}' (pos={token['pos']}, corrected={corrected_pos}) â†’ sub-m2")
                    break
        
        return sub_slots

    def _extract_full_antecedent_phrase(self, tokens: List[Dict], antecedent_idx: int) -> str:
        """å…ˆè¡Œè©å¥å…¨ä½“ã‚’æŠ½å‡ºï¼ˆé™å®šè©ã€å½¢å®¹è©ã‚’å«ã‚€ï¼‰"""
        if antecedent_idx <= 0:
            return tokens[antecedent_idx]['text']
        
        # å…ˆè¡Œè©ã®å‰ã®ä¿®é£¾èªã‚’å«ã‚ã¦æŠ½å‡º
        phrase_tokens = []
        start_idx = max(0, antecedent_idx - 2)  # æœ€å¤§2èªå‰ã¾ã§ç¢ºèª
        
        for i in range(start_idx, antecedent_idx + 1):
            token = tokens[i]
            if token['pos'] in ['DET', 'ADJ', 'NOUN', 'PROPN']:
                phrase_tokens.append(token['text'])
        
        return ' '.join(phrase_tokens)
        
        # 3. é–¢ä¿‚ä»£åè©ã®å½¹å‰²ã‚’åˆ¤å®š
        verb_idx = None
        for i, token in enumerate(rel_clause_tokens):
            if token['tag'].startswith('VB') and token['pos'] == 'VERB':
                verb_idx = i
                break
        
        # 4. Rephraseçš„ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ ã‚’æ§‹ç¯‰
        sub_slots = {}
        
        if verb_idx is not None:
            rel_pronoun_role = self._determine_relative_pronoun_role(rel_clause_tokens, verb_idx)
            
            if rel_pronoun_role == 'subject':
                # é–¢ä¿‚ä»£åè©ãŒä¸»èªã®å ´åˆ
                sub_slots['sub-s'] = f"{antecedent_text} {rel_pronoun_text}"
            else:
                # é–¢ä¿‚ä»£åè©ãŒç›®çš„èªã®å ´åˆ
                sub_slots['sub-o1'] = f"{antecedent_text} {rel_pronoun_text}"
        else:
            # å‹•è©ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ä¸»èªæ‰±ã„
            sub_slots['sub-s'] = f"{antecedent_text} {rel_pronoun_text}"
        
        # 5. é–¢ä¿‚ç¯€å†…ã®ä»–ã®è¦ç´ ã‚’åˆ†æ
        self._analyze_relative_clause_elements(rel_clause_tokens, sub_slots)
        
        return sub_slots
    
    def _extract_antecedent_phrase(self, tokens: List[Dict], antecedent_idx: int, rel_pronoun_idx: int) -> str:
        """å…ˆè¡Œè©å¥ã‚’æŠ½å‡ºï¼ˆå† è©ãƒ»å½¢å®¹è©å«ã‚€ï¼‰"""
        # å…ˆè¡Œè©ã®å‰ã®ä¿®é£¾èªã‚‚å«ã‚ã¦æŠ½å‡º
        start_idx = antecedent_idx
        
        # å‰æ–¹ã®ä¿®é£¾èªã‚’æ¢ã™
        for i in range(antecedent_idx - 1, -1, -1):
            if tokens[i]['pos'] in ['DET', 'ADJ']:  # å† è©ãƒ»å½¢å®¹è©
                start_idx = i
            else:
                break
        
        # å…ˆè¡Œè©å¥ã‚’æ§‹ç¯‰
        antecedent_phrase = ' '.join([tokens[i]['text'] for i in range(start_idx, rel_pronoun_idx)])
        return antecedent_phrase.strip()
    
    def _analyze_relative_clause_elements(self, rel_tokens: List[Dict], sub_slots: Dict):
        """é–¢ä¿‚ç¯€å†…ã®è¦ç´ ã‚’Rephraseçš„ã«åˆ†æ"""
        if not rel_tokens:
            return
        
        # å‹•è©ã‚’æ¢ã™
        verb_idx = None
        for i, token in enumerate(rel_tokens):
            if token['tag'].startswith('VB') and token['pos'] == 'VERB':
                verb_idx = i
                sub_slots['sub-v'] = token['text']
                break
        
        if verb_idx is None:
            return
        
        # é–¢ä¿‚ä»£åè©ã®å½¹å‰²ã‚’åˆ¤å®šï¼ˆä¸»èªã‹ç›®çš„èªã‹ï¼‰
        rel_pronoun_role = self._determine_relative_pronoun_role(rel_tokens, verb_idx)
        clause_type = rel_tokens[0]['text'].lower()
        
        # whoseã®å ´åˆã®ç‰¹åˆ¥å‡¦ç†
        if clause_type == 'whose':
            # whose + åè© ã®å½¢ã‚’æ¢ã™
            whose_phrase = rel_tokens[0]['text']  # "whose"
            next_idx = 1
            while next_idx < len(rel_tokens) and next_idx < verb_idx:
                if rel_tokens[next_idx]['pos'] in ['NOUN', 'PROPN']:
                    whose_phrase += f" {rel_tokens[next_idx]['text']}"
                    break
                next_idx += 1
            
            # whoseå¥ã®å½¹å‰²ã‚’åˆ¤å®š
            if rel_pronoun_role == 'subject':
                sub_slots['sub-s'] = whose_phrase
            else:
                sub_slots['sub-o1'] = whose_phrase
        
        # å‹•è©å‰ã®è¦ç´ ã‚’åˆ†æï¼ˆä¸»èªï¼‰
        pre_verb_tokens = rel_tokens[:verb_idx]
        for token in pre_verb_tokens:
            if token['pos'] in ['NOUN', 'PRON', 'PROPN']:
                # é–¢ä¿‚ä»£åè©ãŒç›®çš„èªã®å ´åˆã€ã“ã“ã«ä¸»èªãŒã‚ã‚‹
                if rel_pronoun_role == 'object' and clause_type != 'whose':
                    sub_slots['sub-s'] = token['text']
        
        # å‹•è©å¾Œã®è¦ç´ ã‚’åˆ†æ
        post_verb_tokens = rel_tokens[verb_idx + 1:]
        modifier_count = 0
        
        for token in post_verb_tokens:
            if token['pos'] == 'ADV' or token['tag'] == 'EX':
                # å‰¯è©ã¾ãŸã¯å­˜åœ¨there â†’ ä¿®é£¾èªã¨ã—ã¦å‡¦ç†ï¼ˆå„ªå…ˆï¼‰
                modifier_count += 1
                if modifier_count == 1:
                    sub_slots['sub-m2'] = token['text']
                elif modifier_count == 2:
                    sub_slots['sub-m3'] = token['text']
            elif token['pos'] in ['NOUN', 'PRON', 'PROPN'] and token['tag'] != 'EX':
                # åè©é¡ï¼ˆå­˜åœ¨thereä»¥å¤–ï¼‰ â†’ ç›®çš„èªã¨ã—ã¦å‡¦ç†
                if 'sub-o1' not in sub_slots:
                    sub_slots['sub-o1'] = token['text']
                elif 'sub-o2' not in sub_slots:
                    sub_slots['sub-o2'] = token['text']
            elif token['pos'] == 'ADJ':
                # å½¢å®¹è© â†’ è£œèªã¨ã—ã¦å‡¦ç†
                sub_slots['sub-c1'] = token['text']
    
    def _determine_relative_pronoun_role(self, rel_tokens: List[Dict], verb_idx: int) -> str:
        """é–¢ä¿‚ä»£åè©ãŒä¸»èªã‹ç›®çš„èªã‹ã‚’åˆ¤å®š"""
        clause_type = rel_tokens[0]['text'].lower()
        
        # whoseã®å ´åˆã®ç‰¹åˆ¥å‡¦ç†
        if clause_type == 'whose':
            # whose + åè©ãŒå‹•è©ã®å‰ã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            whose_noun_idx = None
            for i in range(1, min(verb_idx, len(rel_tokens))):
                if rel_tokens[i]['pos'] in ['NOUN', 'PROPN']:
                    whose_noun_idx = i
                    break
            
            if whose_noun_idx is not None:
                # whose + åè©ãŒå‹•è©å‰ã«ã‚ã‚‹ãªã‚‰ä¸»èª
                return 'subject'
            else:
                # whose + åè©ãŒå‹•è©å¾Œã«ã‚ã‚‹ãªã‚‰ç›®çš„èª
                return 'object'
        
        # å‹•è©ã®å‰ã«ä»£åè©ãƒ»åè©ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆwhoseä»¥å¤–ã®å ´åˆï¼‰
        pre_verb_tokens = rel_tokens[1:verb_idx]  # é–¢ä¿‚ä»£åè©è‡ªä½“ã¯é™¤å¤–
        has_subject_before_verb = any(
            token['pos'] in ['NOUN', 'PRON', 'PROPN'] 
            for token in pre_verb_tokens
        )
        
        if has_subject_before_verb:
            # å‹•è©å‰ã«ä¸»èªãŒã‚ã‚‹ãªã‚‰ã€é–¢ä¿‚ä»£åè©ã¯ç›®çš„èª
            return 'object'
        else:
            # å‹•è©å‰ã«ä¸»èªãŒãªã„ãªã‚‰ã€é–¢ä¿‚ä»£åè©ã¯ä¸»èª
            return 'subject'

    def _determine_chunk_grammatical_role(self, tokens: List[Dict], core_elements: Dict, relative_info: Dict) -> str:
        """é–¢ä¿‚ç¯€ã‚’å«ã‚€ã€Œã‹ãŸã¾ã‚Šã€ã®æ–‡æ³•çš„å½¹å‰²ã‚’å‹•è©ã¨ã®é–¢ä¿‚ã‹ã‚‰æ¨å®š
        
        äººé–“çš„æ–‡æ³•èªè­˜ï¼š
        - å‹•è©ã®å‰ã®ã€Œã‹ãŸã¾ã‚Šã€â†’ ä¸»èªï¼ˆSï¼‰
        - å‹•è©ã®å¾Œã®ã€Œã‹ãŸã¾ã‚Šã€â†’ ç›®çš„èªï¼ˆO1ï¼‰ã¾ãŸã¯è£œèªï¼ˆC1ï¼‰
        - æ–‡æœ«ã®ã€Œã‹ãŸã¾ã‚Šã€â†’ ä¿®é£¾èªï¼ˆMï¼‰
        """
        if not relative_info['found']:
            return None
            
        # å…ˆè¡Œè©ã®ä½ç½®ã¨å‹•è©ã®ä½ç½®ã‚’æ¯”è¼ƒ
        antecedent_idx = relative_info.get('antecedent_idx')
        verb_indices = core_elements.get('verb_indices', [])
        
        if not antecedent_idx or not verb_indices:
            return None
            
        main_verb_idx = verb_indices[0] if verb_indices else len(tokens)
        
        # ä½ç½®é–¢ä¿‚ã«ã‚ˆã‚‹æ–‡æ³•çš„å½¹å‰²ã®åˆ¤å®š
        if antecedent_idx < main_verb_idx:
            # å‹•è©ã‚ˆã‚Šå‰ â†’ ä¸»èªã®å¯èƒ½æ€§ãŒé«˜ã„
            self.logger.debug(f"ã‹ãŸã¾ã‚Šä½ç½®åˆ¤å®š: å…ˆè¡Œè©{antecedent_idx} < å‹•è©{main_verb_idx} â†’ ä¸»èª(S)")
            return 'S'
        else:
            # å‹•è©ã‚ˆã‚Šå¾Œ â†’ ç›®çš„èªã¾ãŸã¯è£œèª
            # å‹•è©ã®æ€§è³ªã‹ã‚‰åˆ¤å®š
            if core_elements.get('verb') and core_elements['verb'].get('text'):
                verb_text = core_elements['verb']['text'].lower()
                if verb_text in ['is', 'are', 'was', 'were', 'am', 'be', 'become', 'seem']:
                    self.logger.debug(f"ã‹ãŸã¾ã‚Šä½ç½®åˆ¤å®š: beå‹•è© + å¾Œç¶š â†’ è£œèª(C1)")
                    return 'C1'
                else:
                    self.logger.debug(f"ã‹ãŸã¾ã‚Šä½ç½®åˆ¤å®š: ä¸€èˆ¬å‹•è© + å¾Œç¶š â†’ ç›®çš„èª(O1)")
                    return 'O1'
        
        return None

    def _determine_relative_slot_position(self, tokens: List[Dict], relative_info: Dict) -> str:
        """é–¢ä¿‚ç¯€ãŒã©ã®ã‚¹ãƒ­ãƒƒãƒˆä½ç½®ã«ã‚ã‚‹ã‹ã‚’åˆ¤å®š
        
        é‡è¦ï¼šé–¢ä¿‚ç¯€ã‚’å«ã‚€ã€Œã‹ãŸã¾ã‚Šã€ãŒã©ã®æ–‡æ³•çš„å½¹å‰²ã‚’æœãŸã™ã‹ã‚’åˆ¤å®šã—ã€
        ãã®ã‚¹ãƒ­ãƒƒãƒˆã‚’ç©ºã«ã—ã¦ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«ç§»å‹•ã•ã›ã‚‹
        """
        if not relative_info['found']:
            return None
            
        # æ—¢ã«å®Ÿè£…æ¸ˆã¿ã®ã€Œã‹ãŸã¾ã‚Šã€æ–‡æ³•çš„å½¹å‰²åˆ¤å®šã‚’ä½¿ç”¨
        # ã“ã®åˆ¤å®šã¯_assign_grammar_rolesã§å–å¾—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
        # ç¾åœ¨ã¯ç›´æ¥å®Ÿè£…
        antecedent_idx = relative_info.get('antecedent_idx')
        if antecedent_idx is None:
            return None
            
        # å‹•è©ä½ç½®ã‚’æ¢ã™
        main_verb_idx = None
        for i, token in enumerate(tokens):
            if token['pos'] in ['VERB', 'AUX'] and token['text'].lower() not in ['whose', 'which', 'who', 'that']:
                # é–¢ä¿‚ç¯€å†…ã®å‹•è©ã‚’é™¤å¤–ã—ã¦ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’ç‰¹å®š
                rel_start = relative_info.get('relative_pronoun_idx', -1)
                rel_end = relative_info.get('clause_end_idx', -1)
                if rel_start <= i <= rel_end:
                    continue  # é–¢ä¿‚ç¯€å†…ã®å‹•è©ã¯ã‚¹ã‚­ãƒƒãƒ—
                main_verb_idx = i
                break
        
        if main_verb_idx is None:
            return None
            
        # ä½ç½®é–¢ä¿‚ã«ã‚ˆã‚‹åˆ¤å®š
        if antecedent_idx < main_verb_idx:
            return 'S'  # ä¸»èª
        else:
            # å‹•è©ã®æ€§è³ªã‹ã‚‰åˆ¤å®š
            verb_token = tokens[main_verb_idx]
            if verb_token['text'].lower() in ['is', 'are', 'was', 'were', 'am', 'be', 'become', 'seem']:
                return 'C1'  # è£œèª
            else:
                return 'O1'  # ç›®çš„èª
    
    def _clean_relative_clause_from_text(self, text: str, relative_info: Dict) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é–¢ä¿‚ç¯€éƒ¨åˆ†ã‚’é™¤å»"""
        if not relative_info['found']:
            return text
        
        # ç°¡æ˜“å®Ÿè£…ï¼šé–¢ä¿‚ä»£åè©ä»¥é™ã‚’å‰Šé™¤
        rel_type = relative_info.get('type', '')
        if rel_type in text:
            parts = text.split(rel_type)
            return parts[0].strip()
        
        return text

    def _analyze_relative_clause_structure(self, rel_tokens: List[Dict], clause_type: str) -> Dict:
        """é–¢ä¿‚ç¯€å†…éƒ¨ã®æ§‹é€ ã‚’5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã§è§£æ
        
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆã®æ–¹æ³•ï¼š
        - 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®æŠ€è¡“ã‚’ãã®ã¾ã¾é–¢ä¿‚ç¯€å†…ã«é©ç”¨
        - é–¢ä¿‚ä»£åè©ã¨ã®çµåˆå•é¡Œã‚’ãƒ«ãƒ¼ãƒ«ã§è§£æ±º
        
        Args:
            rel_tokens: é–¢ä¿‚ç¯€ã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆ  
            clause_type: é–¢ä¿‚ç¯€ã®ç¨®é¡ï¼ˆwhose_clauseç­‰ï¼‰
            
        Returns:
            Dict: ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ 
        """
        if not rel_tokens:
            return {}
        
        self.logger.debug(f"5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã«ã‚ˆã‚‹é–¢ä¿‚ç¯€è§£æ: {[t['text'] for t in rel_tokens]}")
        
        # ğŸ†• 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ç›´æ¥é©ç”¨
        core_elements = self._identify_core_elements(rel_tokens)
        sentence_pattern = self._determine_sentence_pattern(core_elements, rel_tokens)
        
        self.logger.debug(f"é–¢ä¿‚ç¯€å†…æ–‡å‹: {sentence_pattern}")
        self.logger.debug(f"é–¢ä¿‚ç¯€å†…ã‚³ã‚¢è¦ç´ : ä¸»èª={core_elements.get('subject')}, å‹•è©={core_elements.get('verb')}")
        
        # ğŸ†• 5æ–‡å‹ã®çµæœã‚’ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«å¤‰æ›
        sub_slots = {}
        
        # ä¸»èªå‡¦ç†ï¼ˆé–¢ä¿‚ä»£åè©ã¨ã®çµåˆãƒ«ãƒ¼ãƒ«ï¼‰
        if core_elements.get('subject_indices'):
            rel_subject = core_elements['subject']
            if clause_type == 'whose_clause':
                # whose + åè© ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
                sub_slots['sub-s'] = f"whose {rel_subject}"
            else:
                # who, which, that ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
                sub_slots['sub-s'] = rel_tokens[0]['text']  # é–¢ä¿‚ä»£åè©è‡ªä½“
        
        # å‹•è©å‡¦ç†
        if core_elements.get('verb'):
            sub_slots['sub-v'] = core_elements['verb']['text']
        
        # 5æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãæ®‹ã‚Šè¦ç´ ã®å‡¦ç†
        if sentence_pattern == 'SVC':
            # beå‹•è© + è£œèª
            # æ®‹ã‚Šã®è¦ç´ ã‹ã‚‰è£œèªã‚’ç‰¹å®š
            used_indices = set(core_elements.get('subject_indices', []) + core_elements.get('verb_indices', []))
            for i, token in enumerate(rel_tokens):
                if i not in used_indices and token['pos'] in ['ADJ', 'NOUN', 'PROPN']:
                    sub_slots['sub-c1'] = token['text']
                    break
        elif sentence_pattern == 'SVO':
            # ä¸€èˆ¬å‹•è© + ç›®çš„èª
            used_indices = set(core_elements.get('subject_indices', []) + core_elements.get('verb_indices', []))
            for i, token in enumerate(rel_tokens):
                if i not in used_indices and token['pos'] in ['NOUN', 'PROPN']:
                    sub_slots['sub-o1'] = token['text']
                    break
        elif sentence_pattern == 'SV':
            # ğŸ†• è‡ªå‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³ (SV) + ä¿®é£¾èª
            # whoç¯€ã®ä¿®é£¾èªï¼ˆå‰¯è©ï¼‰ã‚’sub-m2ã¨ã—ã¦ç‰¹å®š
            used_indices = set(core_elements.get('subject_indices', []) + core_elements.get('verb_indices', []))
            for i, token in enumerate(rel_tokens):
                if i not in used_indices:
                    # ğŸ†• äººé–“çš„å“è©åˆ¤å®šã‚’é©ç”¨
                    corrected_pos = self._get_human_corrected_pos(token)
                    if corrected_pos in ['ADV', 'ADJ']:
                        sub_slots['sub-m2'] = token['text']
                        self.logger.debug(f"whoç¯€ä¿®é£¾èªæ¤œå‡º: '{token['text']}' â†’ sub-m2 (corrected_pos={corrected_pos})")
                        break
        
        # ğŸ†• ä¸€èˆ¬çš„ãªä¿®é£¾èªæ¤œå‡ºï¼ˆæ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã«é–¢ä¿‚ãªãï¼‰
        if 'sub-m2' not in sub_slots and clause_type == 'who_clause':
            # whoç¯€ç‰¹æœ‰ã®ä¿®é£¾èªæ¤œå‡º
            used_indices = set(core_elements.get('subject_indices', []) + core_elements.get('verb_indices', []))
            if 'sub-c1' in sub_slots:
                # è£œèªãŒã‚ã‚‹å ´åˆã¯è£œèªã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚‚é™¤å¤–
                for i, token in enumerate(rel_tokens):
                    if token['text'] == sub_slots['sub-c1']:
                        used_indices.add(i)
                        break
            
            for i, token in enumerate(rel_tokens):
                if i not in used_indices and i > 0:  # é–¢ä¿‚ä»£åè©ã‚’é™¤å¤–
                    corrected_pos = self._get_human_corrected_pos(token)
                    if corrected_pos in ['ADV', 'ADJ']:
                        sub_slots['sub-m2'] = token['text']
                        self.logger.debug(f"whoç¯€è¿½åŠ ä¿®é£¾èªæ¤œå‡º: '{token['text']}' â†’ sub-m2 (corrected_pos={corrected_pos})")
                        break
        
        return sub_slots
    
    def _find_verb_in_relative_clause(self, rel_tokens: List[Dict]) -> Optional[int]:
        """é–¢ä¿‚ç¯€å†…ã®å‹•è©ã‚’ç‰¹å®š
        
        5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®å‹•è©æ¤œå‡ºæŠ€è¡“ã‚’é©ç”¨
        """
        for i, token in enumerate(rel_tokens):
            if token['tag'].startswith('VB') and token['pos'] == 'VERB':
                return i
        return None
    
    def _analyze_post_verb_elements_in_relative(self, rel_tokens: List[Dict], verb_idx: int, sub_slots: Dict):
        """é–¢ä¿‚ç¯€å†…ã®å‹•è©å¾Œè¦ç´ ã‚’è§£æ
        
        5æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜æŠ€è¡“ã‚’é©ç”¨
        """
        if verb_idx >= len(rel_tokens) - 1:
            return
        
        post_verb_tokens = rel_tokens[verb_idx + 1:]
        
        # ç›®çš„èªã€è£œèªã€ä¿®é£¾èªã‚’é †æ¬¡è§£æ
        processed_positions = set()
        
        for i, token in enumerate(post_verb_tokens):
            if i in processed_positions:
                continue
                
            if token['pos'] in ['NOUN', 'PRON', 'PROPN']:
                # åè©é¡ â†’ ç›®çš„èªã¨ã—ã¦å‡¦ç†
                if 'sub-o1' not in sub_slots:
                    sub_slots['sub-o1'] = token['text']
                elif 'sub-o2' not in sub_slots:
                    sub_slots['sub-o2'] = token['text']
                processed_positions.add(i)
            elif token['pos'] == 'ADJ':
                # å½¢å®¹è© â†’ è£œèªã¨ã—ã¦å‡¦ç†
                sub_slots['sub-c1'] = token['text']
                processed_positions.add(i)
            elif token['pos'] == 'ADV':
                # å‰¯è© â†’ ä¿®é£¾èªã¨ã—ã¦å‡¦ç†
                if 'sub-m' not in sub_slots:
                    sub_slots['sub-m'] = token['text']
                else:
                    sub_slots['sub-m'] += f" {token['text']}"
                processed_positions.add(i)
        
        # é€£ç¶šã™ã‚‹è¦ç´ ã‚’ã¾ã¨ã‚ã‚‹ï¼ˆç°¡ç´ åŒ–ç‰ˆï¼‰
        self._consolidate_relative_clause_elements_simple(rel_tokens, verb_idx, sub_slots)

    def _consolidate_relative_clause_elements_simple(self, rel_tokens: List[Dict], verb_idx: int, sub_slots: Dict):
        """é–¢ä¿‚ç¯€å†…ã®è¦ç´ ã‚’çµ±åˆï¼ˆç°¡ç´ ç‰ˆï¼‰"""
        # ç¾åœ¨ã¯åŸºæœ¬çš„ãªé‡è¤‡é™¤å»ã®ã¿
        if 'sub-m' in sub_slots:
            # é‡è¤‡ã—ãŸä¿®é£¾èªã‚’é™¤å»
            m_words = sub_slots['sub-m'].split()
            unique_words = []
            for word in m_words:
                if word not in unique_words:
                    unique_words.append(word)
            sub_slots['sub-m'] = ' '.join(unique_words)

    def _consolidate_relative_clause_elements(self, rel_tokens: List[Dict], verb_idx: int, sub_slots: Dict):
        """é–¢ä¿‚ç¯€å†…ã®è¦ç´ ã‚’çµ±åˆ
        
        é€£ç¶šã™ã‚‹åè©å¥ã‚„ä¿®é£¾èªå¥ã‚’ä¸€ã¤ã«ã¾ã¨ã‚ã‚‹
        """
        if verb_idx >= len(rel_tokens) - 1:
            return
        
        post_verb_tokens = rel_tokens[verb_idx + 1:]
        current_phrase = []
        current_type = None
        
        for token in post_verb_tokens:
            if token['pos'] in ['NOUN', 'PRON', 'PROPN', 'DET', 'ADJ']:
                if current_type == 'noun_phrase':
                    current_phrase.append(token['text'])
                else:
                    # æ–°ã—ã„åè©å¥ã®é–‹å§‹
                    if current_phrase and current_type:
                        self._assign_phrase_to_subslot(current_phrase, current_type, sub_slots)
                    current_phrase = [token['text']]
                    current_type = 'noun_phrase'
            elif token['pos'] in ['ADV', 'ADP']:
                if current_type == 'adverbial_phrase':
                    current_phrase.append(token['text'])
                else:
                    # æ–°ã—ã„å‰¯è©å¥ã®é–‹å§‹
                    if current_phrase and current_type:
                        self._assign_phrase_to_subslot(current_phrase, current_type, sub_slots)
                    current_phrase = [token['text']]
                    current_type = 'adverbial_phrase'
            else:
                # ãã®ä»–ã®å“è©ã§å¥ãŒçµ‚äº†
                if current_phrase and current_type:
                    self._assign_phrase_to_subslot(current_phrase, current_type, sub_slots)
                current_phrase = []
                current_type = None
        
        # æœ€å¾Œã®å¥ã‚’å‡¦ç†
        if current_phrase and current_type:
            self._assign_phrase_to_subslot(current_phrase, current_type, sub_slots)
    
    def _assign_phrase_to_subslot(self, phrase: List[str], phrase_type: str, sub_slots: Dict):
        """å¥ã‚’ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«å‰²ã‚Šå½“ã¦"""
        phrase_text = ' '.join(phrase)
        
        if phrase_type == 'noun_phrase':
            if 'sub-o1' not in sub_slots:
                sub_slots['sub-o1'] = phrase_text
            elif 'sub-o2' not in sub_slots:
                sub_slots['sub-o2'] = phrase_text
            else:
                # è£œèªã¨ã—ã¦å‡¦ç†
                sub_slots['sub-c1'] = phrase_text
        elif phrase_type == 'adverbial_phrase':
            if 'sub-m' not in sub_slots:
                sub_slots['sub-m'] = phrase_text
            else:
                sub_slots['sub-m'] += f" {phrase_text}"

# ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°ã¨ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
def run_full_test_suite(test_data_path: str = None) -> Dict[str, Any]:
    """
    53ä¾‹æ–‡ã®å®Œå…¨ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    
    Args:
        test_data_path: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
    Returns:
        Dict: ãƒ†ã‚¹ãƒˆçµæœ
    """
    import json
    import os
    from datetime import datetime
    
    if test_data_path is None:
        test_data_path = os.path.join(
            os.path.dirname(__file__),
            "final_test_system",
            "final_54_test_data.json"
        )
    
    try:
        with open(test_data_path, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
    except FileNotFoundError:
        print(f"âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {test_data_path}")
        return {}
    
    mapper = DynamicGrammarMapper()
    results = {
        "timestamp": datetime.now().isoformat(),
        "test_system": "dynamic_grammar_mapper",
        "total_tests": len(test_data["data"]),
        "successful_tests": 0,
        "failed_tests": 0,
        "test_results": {}
    }
    
    print("=== å‹•çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ  53ä¾‹æ–‡ãƒ†ã‚¹ãƒˆ ===\n")
    
    for test_id, test_case in test_data["data"].items():
        sentence = test_case["sentence"]
        expected = test_case["expected"]
        
        print(f"ãƒ†ã‚¹ãƒˆ {test_id}: {sentence}")
        
        try:
            result = mapper.analyze_sentence(sentence)
            
            if 'error' in result:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
                results["failed_tests"] += 1
                status = "ERROR"
            else:
                print(f"âœ… æ–‡å‹: {result.get('pattern_detected', 'UNKNOWN')}")
                print(f"ğŸ“Š ã‚¹ãƒ­ãƒƒãƒˆ: {result['Slot']}")
                results["successful_tests"] += 1
                status = "SUCCESS"
            
            results["test_results"][test_id] = {
                "sentence": sentence,
                "expected": expected,
                "actual": result,
                "status": status
            }
            
        except Exception as e:
            print(f"âŒ ä¾‹å¤–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            results["failed_tests"] += 1
            results["test_results"][test_id] = {
                "sentence": sentence,
                "expected": expected,
                "actual": {"error": str(e)},
                "status": "EXCEPTION"
            }
        
        print("-" * 60)
    
    # çµæœã‚µãƒãƒªãƒ¼
    success_rate = results["successful_tests"] / results["total_tests"] * 100
    print(f"\n=== ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼ ===")
    print(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {results['total_tests']}")
    print(f"æˆåŠŸ: {results['successful_tests']}")
    print(f"å¤±æ•—: {results['failed_tests']}")
    print(f"æˆåŠŸç‡: {success_rate:.1f}%")
    
    return results

def save_test_results(results: Dict[str, Any], output_path: str = None) -> str:
    """
    ãƒ†ã‚¹ãƒˆçµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    
    Args:
        results: ãƒ†ã‚¹ãƒˆçµæœ
        output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆNone ã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        
    Returns:
        str: ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    import json
    from datetime import datetime
    
    if output_path is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"dynamic_grammar_test_results_{timestamp}.json"
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“ ãƒ†ã‚¹ãƒˆçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
    return output_path

def main():
    """å‹•çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--full-test":
        # 53ä¾‹æ–‡ã®å®Œå…¨ãƒ†ã‚¹ãƒˆ
        results = run_full_test_suite()
        save_test_results(results)
    else:
        # ç°¡æ˜“ãƒ†ã‚¹ãƒˆ
        mapper = DynamicGrammarMapper()
        
        test_sentences = [
            "The car is red.",
            "I love you.",
            "He has finished his homework.",
            "The students study hard for exams.",
            "The teacher explains grammar clearly to confused students daily.",
            "She made him very happy yesterday.",
            "The man who runs fast is strong."
        ]
        
        print("=== å‹•çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ  ç°¡æ˜“ãƒ†ã‚¹ãƒˆ ===\n")
        
        for sentence in test_sentences:
            print(f"ğŸ“ ãƒ†ã‚¹ãƒˆæ–‡: '{sentence}'")
            result = mapper.analyze_sentence(sentence)
            
            if 'error' in result:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
            else:
                print(f"âœ… æ–‡å‹: {result.get('pattern_detected', 'UNKNOWN')}")
                print(f"ğŸ“Š ã‚¹ãƒ­ãƒƒãƒˆ: {result['Slot']}")
                print(f"ğŸ“„ ãƒ•ãƒ¬ãƒ¼ã‚º: {result['SlotPhrase']}")
                print(f"ğŸ¯ ä¿¡é ¼åº¦: {result.get('confidence', 0.0)}")
            
            print("-" * 50)

if __name__ == "__main__":
    main()
