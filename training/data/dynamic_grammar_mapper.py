#!/usr/bin/env python3
"""
å‹•çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ  v2.0
==================

èªæ•°ã«ä¾å­˜ã—ãªã„æ–‡æ³•è¦ç´ ãƒ™ãƒ¼ã‚¹ã®èªè­˜ã‚·ã‚¹ãƒ†ãƒ 
- å“è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã¯ãªãæ–‡æ³•çš„å½¹å‰²ã§èªè­˜
- ä¿®é£¾èªã®å‹•çš„æ¤œå‡ºã¨é™¤å¤–
- æ ¸è¦ç´ ï¼ˆä¸»èªãƒ»å‹•è©ãƒ»ç›®çš„èªãƒ»è£œèªï¼‰ã®ç‰¹å®š
- 5æ–‡å‹ã®å‹•çš„åˆ¤å®š

è¨­è¨ˆæ€æƒ³:
1. æ–‡ã®ã€Œæ ¸ã€ã‚’ç‰¹å®šï¼ˆä¸»èª + å‹•è©ï¼‰
2. å‹•è©ã®æ€§è³ªã‹ã‚‰æ–‡å‹ã‚’æ¨å®š
3. æ®‹ã‚Šã®è¦ç´ ã‚’æ–‡æ³•çš„å½¹å‰²ã§åˆ†é¡
4. ä¿®é£¾èªã¯åˆ¥é€”å‡¦ç†
"""

import spacy
import logging
from typing import Dict, List, Optional, Any, Tuple
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass

# ğŸ†• Phase 1.2: æ–‡å‹èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³è¿½åŠ 
# from sentence_type_detector import SentenceTypeDetector  # ä¸€æ™‚çš„ã«ã‚³ãƒ¡ãƒ³ãƒˆåŒ–

@dataclass
class GrammarElement:
    """æ–‡æ³•è¦ç´ ã®å®šç¾©"""
    text: str
    tokens: List[Dict]
    role: str  # S, V, O1, O2, C1, C2, M1, M2, M3, Aux
    start_idx: int
    end_idx: int
    confidence: float
    
    # ğŸ†• Orderæ©Ÿèƒ½é–¢é€£ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ (Phase 1.1)
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®šã«ã‚ˆã‚Šæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¸ã®å½±éŸ¿ã‚’æœ€å°åŒ–
    slot_display_order: int = 0      # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆé †åº
    display_order: int = 0           # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå†…é †åº  
    v_group_key: str = ""            # å‹•è©ã‚°ãƒ«ãƒ¼ãƒ—ã‚­ãƒ¼
    sentence_type: str = ""          # æ–‡å‹ (statement/wh_question/yes_no_question)
    is_subslot: bool = False         # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆãƒ•ãƒ©ã‚°
    parent_slot: str = ""            # è¦ªã‚¹ãƒ­ãƒƒãƒˆ (ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”¨)
    subslot_id: str = ""             # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆID (sub-s, sub-vç­‰)

class DynamicGrammarMapper:
    """
    å‹•çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ 
    èªæ•°ã«ä¾å­˜ã—ãªã„æ–‡æ³•èªè­˜ã‚’å®Ÿç¾
    """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        try:
            self.nlp = spacy.load("en_core_web_sm")
            print("âœ… spaCyå‹•çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except OSError:
            print("âŒ spaCyè‹±èªãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            raise
        
        # ãƒ­ã‚°è¨­å®šã‚’æ—©æœŸã«è¡Œã†
        self.logger = logging.getLogger(__name__)
        
        # ğŸ”¥ Phase 1.0: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  (Stanza Asset Migration)
        self.active_handlers = []  # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãƒªã‚¹ãƒˆ
        self.handler_shared_context = {}  # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼é–“æƒ…å ±å…±æœ‰
        self.handler_success_count = {}  # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼æˆåŠŸçµ±è¨ˆ
        
        # ChatGPT5è¨ºæ–­: å†å…¥ã‚¬ãƒ¼ãƒ‰å¯¾ç­–
        self._analysis_depth = 0  # è§£ææ·±åº¦ã‚«ã‚¦ãƒ³ã‚¿ï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰
        
        # ChatGPT5 Step C: Token Consumption Tracking - ä½¿ç”¨æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒ³ç®¡ç†
        self._consumed_tokens = set()  # ãƒˆãƒ¼ã‚¯ãƒ³ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã‚»ãƒƒãƒˆ
        
        # ğŸ”¥ Phase 2: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼çµæœä¿å­˜ (ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆãƒãƒ¼ã‚¸ç”¨)
        self.last_unified_result = None
        
        # åŸºæœ¬ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®åˆæœŸåŒ–
        self._initialize_basic_handlers()
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–å®Œäº†ã‚’ãƒ­ã‚°å‡ºåŠ›
        print(f"ğŸ”¥ Phase 1.0 ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†: {len(self.active_handlers)}å€‹ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–")
        print(f"   ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼: {', '.join(self.active_handlers)}")
        
        # ğŸ†• Phase 1.2: æ–‡å‹èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        # self.sentence_type_detector = SentenceTypeDetector()  # ä¸€æ™‚çš„ã«ã‚³ãƒ¡ãƒ³ãƒˆåŒ–
        # print("âœ… æ–‡å‹èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
        
        # å‹•è©åˆ†é¡è¾æ›¸
        self.linking_verbs = {
            'be', 'am', 'is', 'are', 'was', 'were', 'being', 'been',
            'become', 'became', 'becoming',
            'seem', 'seemed', 'seeming', 'seems',
            'appear', 'appeared', 'appearing', 'appears',
            'look', 'looked', 'looking', 'looks',
            'sound', 'sounded', 'sounding', 'sounds',
            'feel', 'felt', 'feeling', 'feels',
            'taste', 'tasted', 'tasting', 'tastes',
            'smell', 'smelled', 'smelling', 'smells',
            'remain', 'remained', 'remaining', 'remains',
            'stay', 'stayed', 'staying', 'stays'
        }
        
        self.ditransitive_verbs = {
            'give', 'gave', 'given', 'giving', 'gives',
            'tell', 'told', 'telling', 'tells',
            'show', 'showed', 'shown', 'showing', 'shows',
            'send', 'sent', 'sending', 'sends',
            'teach', 'taught', 'teaching', 'teaches',
            'buy', 'bought', 'buying', 'buys',
            'bring', 'brought', 'bringing', 'brings',
            'offer', 'offered', 'offering', 'offers',
            'lend', 'lent', 'lending', 'lends',
            'sell', 'sold', 'selling', 'sells'
        }
        
        self.objective_complement_verbs = {
            'make', 'made', 'making', 'makes',
            'call', 'called', 'calling', 'calls',
            'consider', 'considered', 'considering', 'considers',
            'find', 'found', 'finding', 'finds',
            'keep', 'kept', 'keeping', 'keeps',
            'leave', 'left', 'leaving', 'leaves',
            'elect', 'elected', 'electing', 'elects',
            'name', 'named', 'naming', 'names',
            'choose', 'chose', 'chosen', 'choosing', 'chooses'
        }
        
        # å‹•è©/åè©åŒå½¢èªãƒªã‚¹ãƒˆï¼ˆstanzaã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ç¶™æ‰¿ï¼‰
        # ğŸ†• äººé–“çš„æ–‡æ³•èªè­˜: æ›–æ˜§èªã®å€™è£œãƒªã‚¹ãƒˆ
        self.ambiguous_words = {
            'lives': ['NOUN', 'VERB'],    # lifeè¤‡æ•°å½¢ vs liveä¸‰äººç§°å˜æ•°
            'works': ['NOUN', 'VERB'],    # workè¤‡æ•°å½¢ vs workä¸‰äººç§°å˜æ•°  
            'runs': ['NOUN', 'VERB'],     # runè¤‡æ•°å½¢ vs runä¸‰äººç§°å˜æ•°
            'goes': ['NOUN', 'VERB'],     # goè¤‡æ•°å½¢ vs goä¸‰äººç§°å˜æ•°
            'comes': ['NOUN', 'VERB'],    # comeè¤‡æ•°å½¢ vs comeä¸‰äººç§°å˜æ•°
            'stays': ['NOUN', 'VERB'],    # stayè¤‡æ•°å½¢ vs stayä¸‰äººç§°å˜æ•°
            'plays': ['NOUN', 'VERB'],    # playè¤‡æ•°å½¢ vs playä¸‰äººç§°å˜æ•°
            'looks': ['NOUN', 'VERB'],    # lookè¤‡æ•°å½¢ vs lookä¸‰äººç§°å˜æ•°
            'walks': ['NOUN', 'VERB'],    # walkè¤‡æ•°å½¢ vs walkä¸‰äººç§°å˜æ•°
            'talks': ['NOUN', 'VERB'],    # talkè¤‡æ•°å½¢ vs talkä¸‰äººç§°å˜æ•°
            'moves': ['NOUN', 'VERB'],    # moveè¤‡æ•°å½¢ vs moveä¸‰äººç§°å˜æ•°
            'drives': ['NOUN', 'VERB'],   # driveè¤‡æ•°å½¢ vs driveä¸‰äººç§°å˜æ•°
            'flies': ['NOUN', 'VERB'],    # flyè¤‡æ•°å½¢ vs flyä¸‰äººç§°å˜æ•°
            'rides': ['NOUN', 'VERB'],    # rideè¤‡æ•°å½¢ vs rideä¸‰äººç§°å˜æ•°
            'sits': ['NOUN', 'VERB']      # sitè¤‡æ•°å½¢ vs sitä¸‰äººç§°å˜æ•°
        }
    
    def analyze_sentence(self, sentence: str, allow_unified: bool = True) -> Dict[str, Any]:
        """
        æ–‡ç« ã‚’å‹•çš„ã«è§£æã—ã¦Rephraseã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ ã‚’ç”Ÿæˆ
        
        Args:
            sentence (str): è§£æå¯¾è±¡ã®æ–‡ç« 
            allow_unified (bool): çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼å‡¦ç†ã®è¨±å¯ï¼ˆå†å¸°é˜²æ­¢ç”¨ï¼‰
            
        Returns:
            Dict[str, Any]: Rephraseã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ 
        """
        # ğŸ”§ ç´¯ç©ãƒã‚°ä¿®æ­£: æ–°ã—ã„åˆ†æé–‹å§‹æ™‚ã«last_unified_resultã‚’ãƒªã‚»ãƒƒãƒˆ
        if allow_unified:
            self.last_unified_result = None
        
        # ChatGPT5 Step A: Re-entrancy Guard
        if not allow_unified:
            self._analysis_depth += 1
            if self._analysis_depth > 3:  # æ·±åº¦åˆ¶é™
                self.logger.warning(f"åˆ†ææ·±åº¦åˆ¶é™ã«é”ã—ã¾ã—ãŸ: {self._analysis_depth}")
                return self._create_error_result(sentence, "recursion_depth_exceeded")
        else:
            # ğŸ”¥ çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ : å¤ã„Token Consumption Trackingã¯çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒæ‹…å½“
            # è²¬ä»»åˆ†é›¢ã«ã‚ˆã‚Šå€‹åˆ¥ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã¯ä¸è¦
            pass
        
        try:
            # ğŸ†• Phase 1.2: æ–‡å‹èªè­˜
            # sentence_type = self.sentence_type_detector.detect_sentence_type(sentence)  # ä¸€æ™‚çš„ã«ã‚³ãƒ¡ãƒ³ãƒˆåŒ–
            # sentence_type_confidence = self.sentence_type_detector.get_detection_confidence(sentence)  # ä¸€æ™‚çš„ã«ã‚³ãƒ¡ãƒ³ãƒˆåŒ–
            sentence_type = "statement"  # ä¸€æ™‚çš„ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            sentence_type_confidence = 0.8  # ä¸€æ™‚çš„ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # 1. spaCyåŸºæœ¬è§£æ
            doc = self.nlp(sentence)
            tokens = self._extract_tokens(doc)
            
            # 1.5. é–¢ä¿‚ç¯€æ§‹é€ ã®æ¤œå‡º
            relative_clause_info = self._detect_relative_clause(tokens, sentence)
            
            # ğŸ”§ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆã‚’äº‹å‰é™¤å¤–ã‚ˆã‚Šå‰ã«å®Ÿè¡Œï¼ˆcarç­‰ã®è¦ç´ ã‚’ä¿æŒã™ã‚‹ãŸã‚ï¼‰
            sub_slots = {}
            original_tokens = tokens.copy()  # å…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¿å­˜
            if relative_clause_info['found']:
                self.logger.debug(f"é–¢ä¿‚ç¯€æ¤œå‡º: {relative_clause_info['type']} (ä¿¡é ¼åº¦: {relative_clause_info['confidence']})")
                # ğŸ”§ Step2: ä½ç½®æƒ…å ±åˆ¤å®šã®ãŸã‚ã«å…ˆã«core_elementsã‚’ç”Ÿæˆ
                temp_core_elements = self._identify_core_elements(tokens)
                # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆï¼ˆå…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ï¼‰
                processed_tokens, sub_slots = self._process_relative_clause(original_tokens, relative_clause_info, temp_core_elements)
            
            # ğŸ”§ é–¢ä¿‚ç¯€å†…è¦ç´ ã®äº‹å‰é™¤å¤–ï¼ˆãƒ¡ã‚¤ãƒ³æ–‡æ³•è§£æç”¨ï¼‰
            excluded_indices = self._identify_relative_clause_elements(tokens, relative_clause_info)
            
            # 2. é™¤å¤–ã•ã‚Œã¦ã„ãªã„è¦ç´ ã®ã¿ã§ã‚³ã‚¢è¦ç´ ã‚’ç‰¹å®š
            filtered_tokens = [token for i, token in enumerate(tokens) if i not in excluded_indices]
            core_elements = self._identify_core_elements(filtered_tokens)
            
            # 3. å‹•è©ã®æ€§è³ªã‹ã‚‰æ–‡å‹ã‚’æ¨å®šï¼ˆé™¤å¤–ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ï¼‰
            sentence_pattern = self._determine_sentence_pattern(core_elements, filtered_tokens)
            
            # ğŸ”¥ ä¾å­˜é–¢ä¿‚å³ç¦ã‚·ã‚¹ãƒ†ãƒ ï¼šãƒ¬ã‚¬ã‚·ãƒ¼åˆ†æã‚¹ã‚­ãƒƒãƒ—ã€çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼å°‚ç”¨
            if allow_unified:  # çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®ã¿ä½¿ç”¨
                # ç©ºã®åˆæœŸçµæœã‚’æº–å‚™
                rephrase_result = {
                    'sentence': sentence, 
                    'slots': {}, 
                    'sub_slots': sub_slots or {},
                    'grammar_info': {'detected_patterns': [], 'handler_contributions': {}, 'control_flags': {}},
                    'slot_provenance': {}
                }
            else:
                # ãƒ¬ã‚¬ã‚·ãƒ¼åˆ†æï¼ˆä¾å­˜é–¢ä¿‚ä½¿ç”¨ã€å½“ã‚·ã‚¹ãƒ†ãƒ ã§ã¯å³ç¦ï¼‰
                grammar_elements = self._assign_grammar_roles(filtered_tokens, sentence_pattern, core_elements, relative_clause_info)
                rephrase_result = self._convert_to_rephrase_format(grammar_elements, sentence_pattern, sub_slots)
            
            # ğŸ”¥ Phase 2: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œï¼ˆå—å‹•æ…‹ãƒ»åŠ©å‹•è©ãƒ»å‰¯è©å‡¦ç†ï¼‰
            if allow_unified:  # ChatGPT5 Step A: Re-entrancy Guard
                try:
                    unified_result = self._unified_mapping(sentence, doc)
                    if unified_result and 'slots' in unified_result:
                        # ğŸ”§ æ ¹æœ¬ä¿®æ­£: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒå‹ã£ãŸå ´åˆã€ãƒ¬ã‚¬ã‚·ãƒ¼çµæœã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                        handler_info = unified_result.get('handler_info', {})
                        winning_handler = handler_info.get('winning_handler', '')
                        
                        if winning_handler in ['comparative_superlative', 'passive_voice', 'relative_clause']:
                            # é«˜å„ªå…ˆåº¦ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒå‹ã£ãŸå ´åˆã€ãƒ¬ã‚¬ã‚·ãƒ¼çµæœã‚’ãƒªã‚»ãƒƒãƒˆ
                            print(f"ğŸ”¥ High-priority handler '{winning_handler}' won: resetting legacy results")
                            rephrase_result = {
                                'Slot': [],
                                'SlotPhrase': [],
                                'Slot_display_order': [],
                                'display_order': [],
                                'PhraseType': [],
                                'SubslotID': [],
                                'main_slots': {},
                                'sub_slots': sub_slots,
                                'slots': {},
                                'pattern_detected': f'{winning_handler}_pattern',
                                'confidence': 0.9,
                                'analysis_method': 'unified_handler',
                                'lexical_tokens': len([token for token in tokens if token.get('pos') not in ['PUNCT', 'SPACE']])
                            }
                        
                        # çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®çµæœã‚’ãƒãƒ¼ã‚¸ï¼ˆå„ªå…ˆåº¦é †ï¼‰
                        for slot_name, slot_value in unified_result['slots'].items():
                            if slot_value:  # ç©ºã§ãªã„å€¤ã®ã¿ãƒãƒ¼ã‚¸
                                # ChatGPT5 Step D: å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã¯å…¨ã‚¹ãƒ­ãƒƒãƒˆå„ªå…ˆï¼ˆAux, V, Mï¼‰
                                if 'passive_voice' in str(unified_result.get('grammar_info', {})):
                                    rephrase_result['slots'][slot_name] = slot_value
                                    rephrase_result['main_slots'][slot_name] = slot_value
                                    print(f"ğŸ”¥ å—å‹•æ…‹å„ªå…ˆãƒãƒ¼ã‚¸: {slot_name} = '{slot_value}'")
                                # ãã®ä»–ã®ã‚¹ãƒ­ãƒƒãƒˆã¯æ—¢å­˜å€¤ãŒãªã„å ´åˆã®ã¿
                                elif not rephrase_result['slots'].get(slot_name):
                                    rephrase_result['slots'][slot_name] = slot_value
                                    rephrase_result['main_slots'][slot_name] = slot_value
                        
                        # æ–‡æ³•æƒ…å ±ã‚‚ãƒãƒ¼ã‚¸
                        if 'grammar_info' in unified_result:
                            if 'unified_handlers' not in rephrase_result:
                                rephrase_result['unified_handlers'] = {}
                            rephrase_result['unified_handlers'] = unified_result['grammar_info']
                        
                        # ğŸ”¥ çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ : å¤ã„Token Consumptionå‰Šé™¤ã‚·ã‚¹ãƒ†ãƒ ã¯çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒæ‹…å½“
                        # å€‹åˆ¥ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®è²¬ä»»åˆ†é›¢ã«ã‚ˆã‚Šé‡è¤‡å‰Šé™¤ã¯ä¸è¦
                        
                        # ğŸ”¥ Phase 2: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼çµæœã‚’ä¿å­˜ (ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆãƒãƒ¼ã‚¸ç”¨)
                        self.last_unified_result = unified_result
                        print(f"ğŸ”¥ çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼çµæœä¿å­˜: sub_slots = {unified_result.get('sub_slots', {})}")
                        
                        # ğŸ¯ Central Controller: ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæƒ…å ±ã‚’æœ€çµ‚çµæœã«çµ±åˆ
                        if unified_result.get('sub_slots'):
                            if 'sub_slots' not in rephrase_result:
                                rephrase_result['sub_slots'] = {}
                            rephrase_result['sub_slots'].update(unified_result['sub_slots'])
                            print(f"ğŸ¯ Central Controller: Sub-slots merged to final result: {rephrase_result['sub_slots']}")
                        
                        # ğŸ¯ Central Controller: ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ­ãƒƒãƒˆä¿®æ­£ï¼ˆé–¢ä¿‚ç¯€åˆ†é›¢å¯¾å¿œï¼‰
                        if unified_result.get('relative_clause_info', {}).get('found'):
                            main_sentence = unified_result['relative_clause_info']['main_sentence']
                            print(f"ğŸ¯ Central Controller: Analyzing main sentence for correct slots: '{main_sentence}'")
                            
                            # ğŸ”§ æ ¹æœ¬ä¿®æ­£: ãƒ¬ã‚¬ã‚·ãƒ¼ã‚·ã‚¹ãƒ†ãƒ é™¤å»ã€çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®5æ–‡å‹ã®ã¿ä½¿ç”¨
                            main_doc = self.nlp(main_sentence)
                            main_analysis_slots = self._extract_basic_five_pattern_only(main_sentence, main_doc)
                            
                            if main_analysis_slots:
                                # ä¸­å¤®åˆ¶å¾¡: ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã¨é‡è¤‡ã—ãªã„ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ­ãƒƒãƒˆã®ã¿æ¡ç”¨
                                for slot_name, slot_value in main_analysis_slots.items():
                                    if slot_value and slot_name in ['S', 'V', 'O1', 'O2', 'C1']:  # 5æ–‡å‹ã®ã¿
                                        # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®å€¤ã¨é‡è¤‡ãƒã‚§ãƒƒã‚¯
                                        is_duplicate = False
                                        for sub_name, sub_value in unified_result.get('sub_slots', {}).items():
                                            if sub_value and str(slot_value).lower() in str(sub_value).lower():
                                                print(f"ğŸ¯ Central Controller: Skipping main slot {slot_name}='{slot_value}' (duplicate with {sub_name}='{sub_value}')")
                                                is_duplicate = True
                                                break
                                        
                                        if not is_duplicate:
                                            # ğŸ¯ Central Controller: è‡ªå‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹åˆ¥å‡¦ç†
                                            if slot_name == 'O1' and 'arrived' in main_sentence:
                                                # "arrived"ã¯è‡ªå‹•è©ãªã®ã§ã€O1ï¼ˆç›®çš„èªï¼‰ã¯ä¸è¦
                                                print(f"ğŸ¯ Central Controller: Skipping O1='{slot_value}' (arrived is intransitive verb)")
                                                continue
                                            
                                            rephrase_result['slots'][slot_name] = slot_value
                                            rephrase_result['main_slots'][slot_name] = slot_value
                                            print(f"ğŸ¯ Central Controller: Main slot set {slot_name}='{slot_value}' (via unified basic_five_pattern)")
                            
                    print(f"ğŸ”¥ Phase 2: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œå®Œäº†")
                except Exception as e:
                    self.logger.error(f"çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ğŸ†• Phase 2: å‰¯è©å‡¦ç†ã®è¿½åŠ  (Direct Implementation) - å¾Œç¶šå‡¦ç†ã¨ã—ã¦ä¿æŒ
            # ğŸš« è²¬ä»»åˆ†é›¢åŸå‰‡: å‰¯è©å‡¦ç†ã¯çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒæ‹…å½“ï¼ˆé‡è¤‡å‡¦ç†ã‚’é˜²æ­¢ï¼‰
            try:
                print(f"ğŸ”¥ Phase 2: å‰¯è©å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—ï¼ˆçµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒæ‹…å½“ï¼‰")
            except Exception as e:
                self.logger.error(f"å‰¯è©å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ğŸ†• Phase 1.2: æ–‡å‹æƒ…å ±ã‚’çµæœã«è¿½åŠ 
            rephrase_result['sentence_type'] = sentence_type
            rephrase_result['sentence_type_confidence'] = sentence_type_confidence
            
            # ğŸ¯ Central Controller: æœ€çµ‚çµ±åˆãƒã‚§ãƒƒã‚¯
            if hasattr(self, 'last_unified_result') and self.last_unified_result:
                print(f"ğŸ¯ Central Controller: Final integration check")
                
                # ğŸ”§ çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼æƒ…å ±ã¨handler_infoã‚’æœ€çµ‚çµæœã«çµ±åˆ
                if 'unified_handlers' in self.last_unified_result:
                    rephrase_result['unified_handlers'] = self.last_unified_result['unified_handlers']
                
                # ğŸ¯ Handleræƒ…å ±ã‚’æœ€çµ‚çµæœã«ãƒãƒ¼ã‚¸
                if 'handler_info' in self.last_unified_result:
                    rephrase_result['handler_info'] = self.last_unified_result['handler_info']
                    print(f"ğŸ”§ Handler info merged to final result: {self.last_unified_result['handler_info']}")
                
                # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæœ€çµ‚ãƒã‚§ãƒƒã‚¯
                unified_sub_slots = self.last_unified_result.get('sub_slots', {})
                if unified_sub_slots:
                    if 'sub_slots' not in rephrase_result:
                        rephrase_result['sub_slots'] = {}
                    
                    # ä¸­å¤®åˆ¶å¾¡: ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆçµ±åˆ
                    for sub_name, sub_value in unified_sub_slots.items():
                        if sub_value:
                            rephrase_result['sub_slots'][sub_name] = sub_value
                    
                    print(f"ğŸ¯ Central Controller: Final sub_slots = {rephrase_result.get('sub_slots', {})}")
            
            return rephrase_result
            
        except Exception as e:
            self.logger.error(f"å‹•çš„æ–‡æ³•è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_error_result(sentence, str(e))
        finally:
            # ChatGPT5 Step A: Re-entrancy Guard - ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ãƒªã‚»ãƒƒãƒˆ
            if not allow_unified and hasattr(self, '_analysis_depth'):
                self._analysis_depth = max(0, self._analysis_depth - 1)
    
    def _cleanup_duplicate_slots_by_consumption(self, rephrase_result: Dict, doc, unified_result: Dict = None) -> None:
        """
        ChatGPT5 Step D: Token consumptionãƒ™ãƒ¼ã‚¹ã§é‡è¤‡ã‚¹ãƒ­ãƒƒãƒˆã‚’å‰Šé™¤ - é«˜å„ªå…ˆåº¦ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ä¿è­·æ©Ÿèƒ½ä»˜ã
        """
        if not hasattr(self, '_consumed_tokens') or not self._consumed_tokens:
            return
        
        # ğŸ›¡ï¸ é«˜å„ªå…ˆåº¦ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ä¿è­·ãƒã‚§ãƒƒã‚¯ - çµ±åˆçµæœã‹ã‚‰å–å¾—
        slot_provenance = {}
        if unified_result and 'slot_provenance' in unified_result:
            slot_provenance = unified_result['slot_provenance']
        else:
            slot_provenance = rephrase_result.get('slot_provenance', {})
        
        protected_handlers = ['comparative_superlative', 'passive_voice', 'relative_clause']
        
        slots_to_remove = []
        
        # å„ã‚¹ãƒ­ãƒƒãƒˆã®å€¤ãŒconsumed tokenã«å¯¾å¿œã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        for slot_name, slot_value in rephrase_result['slots'].items():
            if not slot_value:
                continue
            
            # ğŸ›¡ï¸ ã‚¹ãƒ­ãƒƒãƒˆä¿è­·ãƒã‚§ãƒƒã‚¯
            is_protected = False
            if slot_name in slot_provenance:
                handler_name = slot_provenance[slot_name]['handler']
                if handler_name in protected_handlers:
                    is_protected = True
                    print(f"ğŸ›¡ï¸ Token consumptionä¿è­·: {slot_name}='{slot_value}' (handler: {handler_name})")
            
            if is_protected:
                continue  # ä¿è­·å¯¾è±¡ã‚¹ãƒ­ãƒƒãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—
                
            # ã‚¹ãƒ­ãƒƒãƒˆå€¤ã«å«ã¾ã‚Œã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ãŒconsumedã‹ãƒã‚§ãƒƒã‚¯
            slot_tokens = str(slot_value).split()
            slot_token_indices = []
            
            for slot_token in slot_tokens:
                for token in doc:
                    if token.text == slot_token:
                        slot_token_indices.append(token.i)
                        break
            
            # ã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒconsumedãªã‚‰ã‚¹ãƒ­ãƒƒãƒˆå‰Šé™¤å¯¾è±¡
            if slot_token_indices and all(idx in self._consumed_tokens for idx in slot_token_indices):
                # ãŸã ã—ã€consumedãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®šã—ãŸå…ƒã®ã‚¹ãƒ­ãƒƒãƒˆã¯ä¿æŒ
                if slot_name not in ['M2']:  # M2='by John'ã¯ä¿æŒï¼ˆRephraseå‰¯è©ãƒ«ãƒ¼ãƒ«ï¼‰
                    slots_to_remove.append(slot_name)
                    print(f"ğŸ”¥ ChatGPT5 Step D: Removing duplicate slot {slot_name}='{slot_value}' (consumed tokens)")
        
        # é‡è¤‡ã‚¹ãƒ­ãƒƒãƒˆå‰Šé™¤
        for slot_name in slots_to_remove:
            rephrase_result['slots'].pop(slot_name, None)
            rephrase_result['main_slots'].pop(slot_name, None)
    
    def _extract_tokens(self, doc) -> List[Dict]:
        """spaCyãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã‚’æŠ½å‡º"""
        tokens = []
        for token in doc:
            token_info = {
                'text': token.text,
                'pos': token.pos_,
                'tag': token.tag_,
                'lemma': token.lemma_,
                'dep': token.dep_,  # ä¾å­˜é–¢ä¿‚
                'head': token.head.text,
                'head_idx': token.head.i,  # ğŸ†• ä¾å­˜é–¢ä¿‚ã®ãƒ˜ãƒƒãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                'is_stop': token.is_stop,
                'is_alpha': token.is_alpha,
                'index': token.i
            }
            tokens.append(token_info)
        return tokens
    
    def _identify_core_elements(self, tokens: List[Dict]) -> Dict[str, Any]:
        """
        æ–‡ã®æ ¸è¦ç´ ï¼ˆä¸»èªãƒ»å‹•è©ï¼‰ã‚’ç‰¹å®š
        ã“ã‚ŒãŒå…¨ã¦ã®æ–‡å‹èªè­˜ã®åŸºç›¤ã¨ãªã‚‹
        """
        core = {
            'subject': None,
            'verb': None,
            'subject_indices': [],
            'verb_indices': [],
            'auxiliary': None,
            'auxiliary_indices': []
        }
        
        # å‹•è©ã‚’æ¢ã™ï¼ˆæœ€ã‚‚é‡è¦ï¼‰
        main_verb_idx = self._find_main_verb(tokens)
        if main_verb_idx is not None:
            core['verb'] = tokens[main_verb_idx]
            core['verb_indices'] = [main_verb_idx]
            
            # åŠ©å‹•è©ã‚’æ¢ã™
            aux_idx = self._find_auxiliary(tokens, main_verb_idx)
            if aux_idx is not None:
                core['auxiliary'] = tokens[aux_idx]
                core['auxiliary_indices'] = [aux_idx]
        
        # ä¸»èªã‚’æ¢ã™ï¼ˆå‹•è©ã®å‰ã§æœ€ã‚‚é©åˆ‡ãªåè©å¥ï¼‰
        if main_verb_idx is not None:
            subject_indices = self._find_subject(tokens, main_verb_idx)
            if subject_indices:
                core['subject_indices'] = subject_indices
                core['subject'] = ' '.join([tokens[i]['text'] for i in subject_indices])
        
        return core
    
    def _find_main_verb(self, tokens: List[Dict]) -> Optional[int]:
        """
        ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’ç‰¹å®šï¼ˆäººé–“çš„æ–‡æ³•èªè­˜ï¼‰
        å“è©æƒ…å ±ã¨èªé †ã®ã¿ã‚’ä½¿ç”¨ã€ä¾å­˜é–¢ä¿‚ã¯ä½¿ã‚ãªã„
        """
        # POSãƒ™ãƒ¼ã‚¹ã¨æ–‡è„ˆãƒ™ãƒ¼ã‚¹ã®ä¸¡æ–¹ã‚’å–å¾—
        pos_candidates = []
        for i, token in enumerate(tokens):
            # å‹•è©ã®å“è©ã‚¿ã‚°
            if (token['tag'].startswith('VB') and token['pos'] == 'VERB') or token['pos'] == 'AUX':
                pos_candidates.append((i, token))
        
        # æ–‡è„ˆçš„å‹•è©è­˜åˆ¥ï¼ˆPOSèª¤èªè­˜å¯¾ç­–ï¼‰
        contextual_candidates = self._find_contextual_verbs(tokens)
        
        # ä¸¡æ–¹ã‚’çµ±åˆï¼ˆé‡è¤‡é™¤å»ï¼‰
        verb_candidates = pos_candidates.copy()
        for i, token in contextual_candidates:
            # æ—¢ã«å­˜åœ¨ã—ãªã„å ´åˆã®ã¿è¿½åŠ 
            if not any(existing_i == i for existing_i, _ in verb_candidates):
                verb_candidates.append((i, token))
        
        if not verb_candidates:
            return None
        
        # äººé–“çš„åˆ¤å®šï¼šé–¢ä¿‚ç¯€ã‚’é™¤å¤–ã—ã¦ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’ç‰¹å®š
        non_relative_verbs = []
        
        for i, token in verb_candidates:
            # é–¢ä¿‚ä»£åè©ã®ç›´å¾Œã®å‹•è©ã¯é–¢ä¿‚ç¯€å†…å‹•è©ã¨ã—ã¦é™¤å¤–
            is_in_relative_clause = False
            
            # å‰ã®å˜èªã‚’ç¢ºèª
            for j in range(max(0, i-5), i):  # 5èªå‰ã¾ã§ç¢ºèª
                prev_token = tokens[j]
                if prev_token['text'].lower() in ['who', 'whom', 'which', 'that', 'whose', 'where', 'when']:
                    # whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†: å‹•è©/åè©åŒå½¢èªã¯é–¢ä¿‚ç¯€å¤–ã®ãƒ¡ã‚¤ãƒ³å‹•è©ã¨ã—ã¦æ‰±ã†
                    if (prev_token['text'].lower() == 'whose' and 
                        token['text'].lower() in self.ambiguous_words and
                        token.get('contextual_override', False)):
                        # whoseæ§‹æ–‡ã§ã®åŒå½¢èªå‹•è©ã¯é–¢ä¿‚ç¯€å¤–ã¨ã—ã¦æ‰±ã†
                        is_in_relative_clause = False
                        break
                    
                    # é–¢ä¿‚ä»£åè©ã‹ã‚‰å‹•è©ã¾ã§ã®è·é›¢ãŒè¿‘ã„å ´åˆã€é–¢ä¿‚ç¯€å†…å‹•è©
                    if i - j <= 4:  # 4èªä»¥å†…ãªã‚‰é–¢ä¿‚ç¯€å†…
                        is_in_relative_clause = True
                        break
            
            if not is_in_relative_clause:
                non_relative_verbs.append((i, token))
        
        if non_relative_verbs:
            # ãƒ¡ã‚¤ãƒ³å‹•è©å€™è£œã‹ã‚‰åŠ©å‹•è©ã§ãªã„ã‚‚ã®ã‚’å„ªå…ˆ
            main_verbs = [(i, token) for i, token in non_relative_verbs if not self._is_auxiliary_verb(token)]
            if main_verbs:
                # æ–‡ã®å¾ŒåŠã«ã‚ã‚‹ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’å„ªå…ˆï¼ˆé–¢ä¿‚ç¯€ã®å¾Œï¼‰
                return main_verbs[-1][0]
            return non_relative_verbs[-1][0]
        
        # æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦ã€ã©ã®å‹•è©ã§ã‚‚é¸æŠ
        return verb_candidates[-1][0]

    def _find_contextual_verbs(self, tokens: List[Dict]) -> List[Tuple[int, Dict]]:
        """
        äººé–“çš„æ–‡æ³•èªè­˜ã«ã‚ˆã‚‹å‹•è©è­˜åˆ¥
        æ§‹æ–‡çš„æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã§æœ€é©ãªå“è©ã‚’æ±ºå®š
        """
        contextual_verbs = []
        sentence_text = ' '.join([token['text'] for token in tokens])
        
        for i, token in enumerate(tokens):
            # æ—¢ã«å‹•è©ã¨ã—ã¦èªè­˜ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®
            if token['pos'] == 'VERB':
                contextual_verbs.append((i, token))
                continue
            
            # ğŸ†• äººé–“çš„å“è©æ±ºå®š: æ§‹æ–‡çš„æ•´åˆæ€§ã«ã‚ˆã‚‹é¸æŠ
            if token['text'].lower() in self.ambiguous_words:
                optimal_pos = self._resolve_ambiguous_word(token, tokens, i, sentence_text)
                
                if optimal_pos == 'VERB':
                    verb_token = token.copy()
                    verb_token['pos'] = 'VERB'
                    verb_token['human_grammar_correction'] = True
                    verb_token['resolution_method'] = 'syntactic_consistency'
                    contextual_verbs.append((i, verb_token))
                    self.logger.debug(f"ğŸ§  äººé–“çš„å“è©æ±ºå®š: '{token['text']}' â†’ VERB (æ§‹æ–‡æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯)")
                continue
            
            # ãã®ä»–ã®å‹•è©å€™è£œï¼ˆaux, modalå«ã‚€ï¼‰
            if token['pos'] in ['AUX', 'MODAL']:
                contextual_verbs.append((i, token))
        
        return contextual_verbs

    def _resolve_ambiguous_word(self, token: Dict, tokens: List[Dict], position: int, sentence: str) -> str:
        """
        äººé–“çš„å“è©æ±ºå®š: æ§‹æ–‡çš„æ•´åˆæ€§ã«ã‚ˆã‚‹æ›–æ˜§èªè§£æ±ºï¼ˆå®‰å…¨ç‰ˆï¼‰
        
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆã®4æ®µéšã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:
        â‘ æ›–æ˜§èªãƒªã‚¹ãƒˆã®ç¢ºèª âœ…
        â‘¡ä¸¡ã‚±ãƒ¼ã‚¹è©¦è¡Œ âœ…
        â‘¢æ§‹æ–‡å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯ âœ…ï¼ˆå¾ªç’°å‚ç…§å›é¿ç‰ˆï¼‰
        â‘£æœ€é©è§£æ¡ç”¨ âœ…
        """
        word_text = token['text'].lower()
        
        if word_text not in self.ambiguous_words:
            return token['pos']  # é€šå¸¸ã®spaCyåˆ¤å®š
        
        candidates = self.ambiguous_words[word_text]
        best_pos = token['pos']  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯spaCyåˆ¤å®š
        best_score = 0
        
        self.logger.debug(f"ğŸ§  æ›–æ˜§èªè§£æ±ºé–‹å§‹: '{token['text']}' å€™è£œ={candidates}")
        
        # å„å€™è£œã‚’è©¦è¡Œã—ã¦æ§‹æ–‡çš„æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆå¾ªç’°å‚ç…§å›é¿ç‰ˆï¼‰
        for candidate_pos in candidates:
            score = self._evaluate_syntactic_consistency_safe(token, candidate_pos, tokens, position, sentence)
            self.logger.debug(f"  ã‚±ãƒ¼ã‚¹è©¦è¡Œ: {candidate_pos} â†’ ã‚¹ã‚³ã‚¢={score}")
            
            if score > best_score:
                best_pos = candidate_pos
                best_score = score
        
        self.logger.debug(f"ğŸ§  æœ€é©è§£æ¡ç”¨: '{token['text']}' â†’ {best_pos} (ã‚¹ã‚³ã‚¢={best_score})")
        return best_pos

    def _evaluate_syntactic_consistency_safe(self, ambiguous_token: Dict, candidate_pos: str, 
                                           tokens: List[Dict], position: int, sentence: str) -> float:
        """
        æ§‹æ–‡çš„æ•´åˆæ€§ã®è©•ä¾¡ï¼ˆå¾ªç’°å‚ç…§å›é¿ç‰ˆï¼‰
        
        äººé–“çš„æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹:
        - ã‚±ãƒ¼ã‚¹1è©¦è¡Œ: åè©ã¨ã—ã¦è§£é‡ˆ â†’ æ–‡æ§‹é€ ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
        - ã‚±ãƒ¼ã‚¹2è©¦è¡Œ: å‹•è©ã¨ã—ã¦è§£é‡ˆ â†’ æ–‡æ§‹é€ ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
        - ã‚ˆã‚Šå®Œå…¨ãªæ§‹é€ ã‚’æŒã¤ã‚±ãƒ¼ã‚¹ã‚’é¸æŠ
        """
        # ä»®æƒ³çš„ã«ãƒˆãƒ¼ã‚¯ãƒ³ã®å“è©ã‚’å¤‰æ›´
        test_tokens = [t.copy() for t in tokens]
        test_tokens[position]['pos'] = candidate_pos
        
        # å¾ªç’°å‚ç…§ã‚’å›é¿ã—ãŸæ§‹æ–‡æ§‹é€ ã®è©•ä¾¡
        structure_score = self._analyze_sentence_structure_completeness_safe(test_tokens, sentence)
        
        return structure_score

    def _analyze_sentence_structure_completeness_safe(self, tokens: List[Dict], sentence: str) -> float:
        """
        æ–‡æ§‹é€ ã®å®Œå…¨æ€§ã‚’åˆ†æï¼ˆå¾ªç’°å‚ç…§å›é¿ç‰ˆï¼‰
        
        äººé–“çš„æ€è€ƒ:
        - é–¢ä¿‚è©ãŒã‚ã‚‹ãªã‚‰ã€é–¢ä¿‚ç¯€ + ãƒ¡ã‚¤ãƒ³æ–‡ã®ä¸¡æ–¹ãŒå¿…è¦
        - é–¢ä¿‚ç¯€ã®ã¿ã§çµ‚ã‚ã‚‹ â†’ æ§‹é€ çš„ã«ä¸å®Œå…¨
        - é–¢ä¿‚ç¯€ + ãƒ¡ã‚¤ãƒ³æ–‡ â†’ æ§‹é€ çš„ã«å®Œå…¨
        """
        score = 0.0
        
        # é–¢ä¿‚è©ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        has_relative_pronoun = self._has_relative_pronoun(sentence)
        
        if has_relative_pronoun:
            self.logger.debug(f"    ğŸ” é–¢ä¿‚ç¯€æ–‡ã¨ã—ã¦è©•ä¾¡é–‹å§‹")
            
            # é–¢ä¿‚ç¯€ + ãƒ¡ã‚¤ãƒ³æ–‡ã®åˆ†é›¢è©•ä¾¡ï¼ˆå¾ªç’°å‚ç…§å›é¿ç‰ˆï¼‰
            relative_clause_complete = self._check_relative_clause_completeness_safe(tokens)
            main_clause_complete = self._check_main_clause_completeness_safe(tokens)
            
            self.logger.debug(f"    é–¢ä¿‚ç¯€å®Œå…¨æ€§: {relative_clause_complete}")
            self.logger.debug(f"    ãƒ¡ã‚¤ãƒ³æ–‡å®Œå…¨æ€§: {main_clause_complete}")
            
            # é–¢ä¿‚ç¯€æ§‹æ–‡ã§ã¯ä¸¡æ–¹ãŒå¿…è¦
            if relative_clause_complete and main_clause_complete:
                score = 100.0  # å®Œå…¨ãªé–¢ä¿‚ç¯€æ§‹æ–‡
                self.logger.debug(f"    âœ… å®Œå…¨ãªé–¢ä¿‚ç¯€æ§‹æ–‡: +100")
            elif relative_clause_complete and not main_clause_complete:
                score = 20.0   # é–¢ä¿‚ç¯€ã®ã¿ï¼ˆæ§‹é€ çš„ã«ä¸å®Œå…¨ï¼‰
                self.logger.debug(f"    âŒ é–¢ä¿‚ç¯€ã®ã¿ï¼ˆãƒ¡ã‚¤ãƒ³æ–‡æ¬ å¦‚ï¼‰: +20")
            elif not relative_clause_complete and main_clause_complete:
                score = 30.0   # ãƒ¡ã‚¤ãƒ³æ–‡ã®ã¿ï¼ˆé–¢ä¿‚ç¯€ç„¡è¦–ã¯ä¸è‡ªç„¶ï¼‰
                self.logger.debug(f"    âŒ ãƒ¡ã‚¤ãƒ³æ–‡ã®ã¿ï¼ˆé–¢ä¿‚ç¯€ç„¡è¦–ï¼‰: +30")
            else:
                score = 0.0    # ä¸¡æ–¹ä¸å®Œå…¨
                self.logger.debug(f"    âŒ ä¸¡æ–¹ä¸å®Œå…¨: +0")
        else:
            # é€šå¸¸æ–‡ã®è©•ä¾¡
            if self._has_main_verb_simple(tokens) and self._has_subject_structure_simple(tokens):
                score = 100.0
                self.logger.debug(f"    âœ… é€šå¸¸æ–‡å®Œå…¨: +100")
        
        self.logger.debug(f"    ç·åˆã‚¹ã‚³ã‚¢: {score}/100")
        return score

    def _check_main_clause_completeness_safe(self, tokens: List[Dict]) -> bool:
        """ãƒ¡ã‚¤ãƒ³æ–‡ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå¾ªç’°å‚ç…§å›é¿ç‰ˆï¼‰"""
        # é–¢ä¿‚ç¯€ä»¥å¤–ã®éƒ¨åˆ†ã«ãƒ¡ã‚¤ãƒ³å‹•è©ãŒå­˜åœ¨ã™ã‚‹ã‹ï¼ˆæ›–æ˜§èªè§£æ±ºã¯ä½¿ã‚ãªã„ï¼‰
        main_verbs = []
        for token in tokens:
            if token['pos'] in ['VERB', 'AUX'] and token['dep'] in ['ROOT']:
                main_verbs.append(token)
                self.logger.debug(f"      ãƒ¡ã‚¤ãƒ³å‹•è©å€™è£œ: '{token['text']}' (pos={token['pos']})")
        
        return len(main_verbs) > 0

    def _check_relative_clause_completeness_safe(self, tokens: List[Dict]) -> bool:
        """é–¢ä¿‚ç¯€ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå¾ªç’°å‚ç…§å›é¿ç‰ˆï¼‰"""
        has_relative_pronoun = False
        has_relative_verb = False
        
        for token in tokens:
            if token['text'].lower() in ['who', 'whom', 'which', 'that', 'whose']:
                has_relative_pronoun = True
            elif token['pos'] in ['VERB', 'AUX'] and token['dep'] in ['relcl']:
                has_relative_verb = True
        
        return has_relative_pronoun and has_relative_verb

    def _has_main_verb_simple(self, tokens: List[Dict]) -> bool:
        """ç°¡æ˜“ãƒ¡ã‚¤ãƒ³å‹•è©ãƒã‚§ãƒƒã‚¯"""
        return any(token['pos'] == 'VERB' for token in tokens)

    def _has_subject_structure_simple(self, tokens: List[Dict]) -> bool:
        """ç°¡æ˜“ä¸»èªæ§‹é€ ãƒã‚§ãƒƒã‚¯"""
        return any(token['pos'] in ['NOUN', 'PRON', 'PROPN'] for token in tokens)

    def _evaluate_syntactic_consistency(self, ambiguous_token: Dict, candidate_pos: str, 
                                       tokens: List[Dict], position: int, sentence: str) -> float:
        """
        æ§‹æ–‡çš„æ•´åˆæ€§ã®è©•ä¾¡
        
        äººé–“çš„æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹:
        - ã‚±ãƒ¼ã‚¹1è©¦è¡Œ: åè©ã¨ã—ã¦è§£é‡ˆ â†’ æ–‡æ§‹é€ ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
        - ã‚±ãƒ¼ã‚¹2è©¦è¡Œ: å‹•è©ã¨ã—ã¦è§£é‡ˆ â†’ æ–‡æ§‹é€ ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
        - ã‚ˆã‚Šå®Œå…¨ãªæ§‹é€ ã‚’æŒã¤ã‚±ãƒ¼ã‚¹ã‚’é¸æŠ
        """
        # ä»®æƒ³çš„ã«ãƒˆãƒ¼ã‚¯ãƒ³ã®å“è©ã‚’å¤‰æ›´
        test_tokens = [t.copy() for t in tokens]
        test_tokens[position]['pos'] = candidate_pos
        
        # æ§‹æ–‡æ§‹é€ ã®è©•ä¾¡
        structure_score = self._analyze_sentence_structure_completeness(test_tokens, sentence)
        
        return structure_score

    def _analyze_sentence_structure_completeness(self, tokens: List[Dict], sentence: str) -> float:
        """
        æ–‡æ§‹é€ ã®å®Œå…¨æ€§ã‚’åˆ†æï¼ˆé–¢ä¿‚ç¯€å­˜åœ¨å‰æç‰ˆï¼‰
        
        äººé–“çš„æ€è€ƒ:
        - é–¢ä¿‚è©ãŒã‚ã‚‹ãªã‚‰ã€é–¢ä¿‚ç¯€ + ãƒ¡ã‚¤ãƒ³æ–‡ã®ä¸¡æ–¹ãŒå¿…è¦
        - é–¢ä¿‚ç¯€ã®ã¿ã§çµ‚ã‚ã‚‹ â†’ æ§‹é€ çš„ã«ä¸å®Œå…¨
        - é–¢ä¿‚ç¯€ + ãƒ¡ã‚¤ãƒ³æ–‡ â†’ æ§‹é€ çš„ã«å®Œå…¨
        """
        score = 0.0
        
        # ğŸ†• CRITICAL: é–¢ä¿‚è©ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        has_relative_pronoun = self._has_relative_pronoun(sentence)
        
        if has_relative_pronoun:
            self.logger.debug(f"    ğŸ” é–¢ä¿‚ç¯€æ–‡ã¨ã—ã¦è©•ä¾¡é–‹å§‹")
            
            # é–¢ä¿‚ç¯€ + ãƒ¡ã‚¤ãƒ³æ–‡ã®åˆ†é›¢è©•ä¾¡
            relative_clause_complete = self._check_relative_clause_completeness(tokens, sentence)
            main_clause_complete = self._check_main_clause_completeness(tokens, sentence)
            
            self.logger.debug(f"    é–¢ä¿‚ç¯€å®Œå…¨æ€§: {relative_clause_complete}")
            self.logger.debug(f"    ãƒ¡ã‚¤ãƒ³æ–‡å®Œå…¨æ€§: {main_clause_complete}")
            
            # é–¢ä¿‚ç¯€æ§‹æ–‡ã§ã¯ä¸¡æ–¹ãŒå¿…è¦
            if relative_clause_complete and main_clause_complete:
                score = 100.0  # å®Œå…¨ãªé–¢ä¿‚ç¯€æ§‹æ–‡
                self.logger.debug(f"    âœ… å®Œå…¨ãªé–¢ä¿‚ç¯€æ§‹æ–‡: +100")
            elif relative_clause_complete and not main_clause_complete:
                score = 20.0   # é–¢ä¿‚ç¯€ã®ã¿ï¼ˆæ§‹é€ çš„ã«ä¸å®Œå…¨ï¼‰
                self.logger.debug(f"    âŒ é–¢ä¿‚ç¯€ã®ã¿ï¼ˆãƒ¡ã‚¤ãƒ³æ–‡æ¬ å¦‚ï¼‰: +20")
            elif not relative_clause_complete and main_clause_complete:
                score = 30.0   # ãƒ¡ã‚¤ãƒ³æ–‡ã®ã¿ï¼ˆé–¢ä¿‚ç¯€ç„¡è¦–ã¯ä¸è‡ªç„¶ï¼‰
                self.logger.debug(f"    âŒ ãƒ¡ã‚¤ãƒ³æ–‡ã®ã¿ï¼ˆé–¢ä¿‚ç¯€ç„¡è¦–ï¼‰: +30")
            else:
                score = 0.0    # ä¸¡æ–¹ä¸å®Œå…¨
                self.logger.debug(f"    âŒ ä¸¡æ–¹ä¸å®Œå…¨: +0")
        else:
            # é€šå¸¸æ–‡ã®è©•ä¾¡
            if self._has_main_verb(tokens) and self._has_subject_structure(tokens):
                score = 100.0
                self.logger.debug(f"    âœ… é€šå¸¸æ–‡å®Œå…¨: +100")
        
        self.logger.debug(f"    ç·åˆã‚¹ã‚³ã‚¢: {score}/100")
        return score

    def _has_relative_pronoun(self, sentence: str) -> bool:
        """é–¢ä¿‚ä»£åè©ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯"""
        relative_pronouns = ['who', 'whom', 'which', 'that', 'whose', 'where', 'when']
        sentence_lower = sentence.lower()
        return any(pronoun in sentence_lower for pronoun in relative_pronouns)

    def _check_relative_clause_completeness(self, tokens: List[Dict], sentence: str) -> bool:
        """é–¢ä¿‚ç¯€ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯"""
        # whoseæ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³: whose + åè© + å‹•è© + (è£œèª)
        if 'whose' in sentence.lower():
            return self._check_whose_clause_completeness(tokens)
        # ä»–ã®é–¢ä¿‚ä»£åè©ãƒ‘ã‚¿ãƒ¼ãƒ³
        return self._check_general_relative_clause_completeness(tokens)

    def _check_main_clause_completeness(self, tokens: List[Dict], sentence: str) -> bool:
        """ãƒ¡ã‚¤ãƒ³æ–‡ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆé–¢ä¿‚ç¯€ã‚’é™¤ã„ã¦ï¼‰"""
        # é–¢ä¿‚ç¯€ä»¥å¤–ã®éƒ¨åˆ†ã«ãƒ¡ã‚¤ãƒ³å‹•è©ãŒå­˜åœ¨ã™ã‚‹ã‹
        main_verbs = []
        for i, token in enumerate(tokens):
            # äººé–“çš„POSåˆ¤å®šã‚’ä½¿ç”¨ï¼ˆæ›–æ˜§èªã®å“è©å¤‰æ›´ã‚’åæ˜ ï¼‰
            corrected_pos = self._resolve_ambiguous_word(token, tokens, i, sentence)
            if corrected_pos in ['VERB', 'AUX']:
                if not self._is_likely_in_relative_clause(token, tokens):
                    main_verbs.append(token)
                    self.logger.debug(f"      ãƒ¡ã‚¤ãƒ³å‹•è©å€™è£œ: '{token['text']}' (pos={corrected_pos})")
        
        return len(main_verbs) > 0

    def _check_whose_clause_completeness(self, tokens: List[Dict]) -> bool:
        """whoseæ§‹æ–‡ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯: whose + åè© + å‹•è© + (è£œèª)"""
        whose_idx = None
        possessed_noun = False
        relative_verb = False
        
        for i, token in enumerate(tokens):
            if token['text'].lower() == 'whose':
                whose_idx = i
            elif whose_idx is not None and i == whose_idx + 1:
                if token['pos'] in ['NOUN', 'PROPN']:
                    possessed_noun = True
            elif whose_idx is not None and i > whose_idx + 1 and token['pos'] in ['VERB', 'AUX']:
                relative_verb = True
                break
        
        return whose_idx is not None and possessed_noun and relative_verb

    def _check_general_relative_clause_completeness(self, tokens: List[Dict]) -> bool:
        """ä¸€èˆ¬çš„ãªé–¢ä¿‚ç¯€ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯"""
        has_relative_pronoun = False
        has_relative_verb = False
        
        for i, token in enumerate(tokens):
            if token['text'].lower() in ['who', 'which', 'that', 'whom']:
                has_relative_pronoun = True
            elif has_relative_pronoun and token['pos'] in ['VERB', 'AUX']:
                has_relative_verb = True
                break
        
        return has_relative_pronoun and has_relative_verb

    def _is_likely_main_verb_by_position(self, token: Dict, tokens: List[Dict], position: int) -> bool:
        """ä½ç½®çš„ã«ãƒ¡ã‚¤ãƒ³å‹•è©ã§ã‚ã‚‹å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        # whoseç¯€ã®å¾Œã«æ¥ã‚‹å‹•è©ã¯ãƒ¡ã‚¤ãƒ³å‹•è©ã®å¯èƒ½æ€§ãŒé«˜ã„
        for i in range(position):
            if tokens[i]['text'].lower() in ['whose', 'who', 'which', 'that']:
                # é–¢ä¿‚ä»£åè©ã‚ˆã‚Šå¾Œã«ã‚ã‚‹æ›–æ˜§èª
                # ã‹ã¤ã€é–¢ä¿‚ç¯€å†…å‹•è©ï¼ˆbeå‹•è©ç­‰ï¼‰ã®å¾Œã«ã‚ã‚‹
                relative_verb_found = False
                for j in range(i + 1, position):
                    if tokens[j]['pos'] in ['VERB', 'AUX']:
                        relative_verb_found = True
                        break
                
                if relative_verb_found:
                    return True  # é–¢ä¿‚ç¯€å‹•è©ã®å¾Œ â†’ ãƒ¡ã‚¤ãƒ³å‹•è©ã®å¯èƒ½æ€§
        
        return False

    def _has_main_verb(self, tokens: List[Dict]) -> bool:
        """ãƒ¡ã‚¤ãƒ³å‹•è©ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯"""
        for token in tokens:
            if token['pos'] in ['VERB', 'AUX']:
                return True
        return False

    def _is_likely_in_relative_clause(self, token: Dict, tokens: List[Dict]) -> bool:
        """ãƒˆãƒ¼ã‚¯ãƒ³ãŒé–¢ä¿‚ç¯€å†…ã«ã‚ã‚‹å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        token_idx = None
        for i, t in enumerate(tokens):
            if t['text'] == token['text'] and t.get('index', i) == token.get('index', -1):
                token_idx = i
                break
        
        if token_idx is None:
            return False
        
        # é–¢ä¿‚ä»£åè©ã®å¾Œã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        for i in range(token_idx):
            if tokens[i]['text'].lower() in ['who', 'whom', 'which', 'that', 'whose']:
                return True
        
        return False

    def _has_main_verb_outside_relative_clause(self, tokens: List[Dict]) -> bool:
        """ãƒ¡ã‚¤ãƒ³æ–‡ï¼ˆé–¢ä¿‚ç¯€å¤–ï¼‰ã«å‹•è©ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        for token in tokens:
            # é–¢ä¿‚ç¯€å†…ã§ã¯ãªã„å‹•è©ã‚’æ¢ã™
            if (token['pos'] in ['VERB', 'AUX'] and 
                not token.get('is_in_relative_clause', False)):
                return True
        return False

    def _has_subject_structure(self, tokens: List[Dict]) -> bool:
        """ä¸»èªæ§‹é€ ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯"""
        for token in tokens:
            if token['pos'] in ['NOUN', 'PROPN', 'PRON']:
                return True
        return False

    def _is_relative_clause_structurally_complete(self, tokens: List[Dict]) -> bool:
        """é–¢ä¿‚ç¯€æ§‹é€ ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯"""
        # ç°¡æ˜“å®Ÿè£…: é–¢ä¿‚ä»£åè©ãŒã‚ã‚Œã°é–¢ä¿‚ç¯€ã¨ã—ã¦èªè­˜
        has_relative_pronoun = any(
            token['text'].lower() in ['who', 'whom', 'which', 'that', 'whose'] 
            for token in tokens
        )
        return has_relative_pronoun

    def _is_modifier_placement_valid(self, tokens: List[Dict]) -> bool:
        """ä¿®é£¾èªé…ç½®ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        # ç°¡æ˜“å®Ÿè£…: åŸºæœ¬çš„ã«å¦¥å½“ã¨ã™ã‚‹
        return True

    def _get_human_corrected_pos(self, token: Dict) -> str:
        """
        äººé–“çš„å“è©åˆ¤å®šã®çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
        
        é©å‘½çš„äºŒé‡è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨:
        1. æ›–æ˜§èªãƒªã‚¹ãƒˆã®ç¢ºèª
        2. ä¸¡ã‚±ãƒ¼ã‚¹è©¦è¡Œï¼ˆNOUN/VERBï¼‰
        3. æ§‹æ–‡å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
        4. æœ€é©è§£æ¡ç”¨
        """
        if token['text'].lower() not in self.ambiguous_words:
            return token['pos']  # é€šå¸¸ã®spaCyåˆ¤å®š
        
        # ğŸ§  é©å‘½çš„äºŒé‡è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®é©ç”¨
        # Note: ç°¡æ˜“ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã§äºŒé‡è©•ä¾¡ã‚’å®Ÿè¡Œ
        word_text = token['text'].lower()
        
        # VERBå€™è£œã¨NOUNå€™è£œã§æ§‹æ–‡çš„æ•´åˆæ€§ã‚’æ¯”è¼ƒ
        verb_score = self._evaluate_word_as_verb_simple(token, word_text)
        noun_score = self._evaluate_word_as_noun_simple(token, word_text)
        
        if verb_score > noun_score:
            self.logger.debug(f"ğŸ§  äººé–“çš„åˆ¤å®š: '{token['text']}' â†’ VERB (ã‚¹ã‚³ã‚¢: {verb_score} vs {noun_score})")
            return 'VERB'
        else:
            self.logger.debug(f"ğŸ§  äººé–“çš„åˆ¤å®š: '{token['text']}' â†’ NOUN (ã‚¹ã‚³ã‚¢: {verb_score} vs {noun_score})")
            return 'NOUN'

    def _evaluate_word_as_verb_simple(self, token: Dict, word_text: str) -> float:
        """èªã‚’å‹•è©ã¨ã—ã¦è©•ä¾¡ã™ã‚‹ç°¡æ˜“ã‚¹ã‚³ã‚¢"""
        score = 0.0
        
        # åŸºæœ¬çš„ãªå‹•è©ã‚‰ã—ã•ãƒã‚§ãƒƒã‚¯
        if word_text.endswith('s'):  # ä¸‰äººç§°å˜æ•°å½¢
            score += 30.0
            
        # whoseæ§‹æ–‡ã§ã®å‹•è©åˆ¤å®šï¼ˆlivesç­‰ï¼‰
        if word_text in ['lives', 'works', 'runs', 'goes', 'comes']:
            score += 50.0
            
        return score
    
    def _evaluate_word_as_noun_simple(self, token: Dict, word_text: str) -> float:
        """èªã‚’åè©ã¨ã—ã¦è©•ä¾¡ã™ã‚‹ç°¡æ˜“ã‚¹ã‚³ã‚¢"""
        score = 0.0
        
        # åŸºæœ¬çš„ãªåè©ã‚‰ã—ã•ãƒã‚§ãƒƒã‚¯
        if word_text.endswith('s'):  # è¤‡æ•°å½¢
            score += 20.0
            
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®spaCyåˆ¤å®šã‚’å°Šé‡
        if token['pos'] == 'NOUN':
            score += 10.0
            
        return score

    def _is_likely_verb_in_context(self, token: Dict, word_text: str) -> bool:
        """æ–‡è„ˆãƒ™ãƒ¼ã‚¹ã®å‹•è©åˆ¤å®š"""
        # ç°¡æ˜“å®Ÿè£…: ambiguous_wordsãƒªã‚¹ãƒˆã«ã‚ã‚‹èªã¯å‹•è©ã¨ã—ã¦æ‰±ã†
        # (ã‚ˆã‚Šé«˜ç²¾åº¦ãªå®Ÿè£…ã¯ä»Šå¾Œã®æ”¹å–„ã§)
        return word_text in self.ambiguous_words
        
        return contextual_verbs
    
    def _is_verb_in_whose_context(self, token: Dict, tokens: List[Dict], 
                                 position: int, sentence: str) -> bool:
        """
        whoseæ§‹æ–‡ã§ã®å‹•è©/åè©åŒå½¢èªåˆ¤å®š
        stanzaã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ã‚’POSãƒ™ãƒ¼ã‚¹ã§å†å®Ÿè£…
        """
        import re
        word = token['text'].lower()
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: whose [åè©] is [å½¢å®¹è©] [å‹•è©] (here|there|å ´æ‰€)
        pattern1 = rf'whose\s+\w+\s+is\s+\w+\s+{word}\s+(here|there|in\s+\w+)'
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: whose [åè©] [ä¿®é£¾èª]* [å‹•è©] (å ´æ‰€è¡¨ç¾)
        pattern2 = rf'whose\s+\w+.*?\s+{word}\s+(here|there|in|at|on)\s+\w+'
        
        if re.search(pattern1, sentence.lower()) or re.search(pattern2, sentence.lower()):
            # æ–‡ä¸­ã«whoseãŒã‚ã‚Šã€è©²å½“ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆ
            return True
        
        # ã‚ˆã‚Šä¸€èˆ¬çš„ãªåˆ¤å®š: whoseå¾Œã§ã€å ´æ‰€è¡¨ç¾ã®å‰ã«ã‚ã‚‹åŒå½¢èª
        if 'whose' in sentence.lower():
            # whoseå¾Œã®ä½ç½®ç¢ºèª
            whose_pos = None
            for i, t in enumerate(tokens):
                if t['text'].lower() == 'whose':
                    whose_pos = i
                    break
            
            if whose_pos is not None and position > whose_pos:
                # whoseå¾Œã§ã€å ´æ‰€è¡¨ç¾ãŒã‚ã‚‹å ´åˆ
                for j in range(position + 1, len(tokens)):
                    next_token = tokens[j]['text'].lower()
                    if next_token in ['here', 'there', 'in', 'at', 'on']:
                        return True
        
        return False

    def _identify_relative_clause_elements(self, tokens: List[Dict], relative_info: Dict) -> set:
        """
        é–¢ä¿‚ç¯€å†…ã®è¦ç´ ã‚’äº‹å‰ã«ç‰¹å®šï¼ˆå…ˆè¡Œè©ã¯ä¿æŒã€é–¢ä¿‚ç¯€éƒ¨åˆ†ã®ã¿é™¤å¤–ï¼‰
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆã®æ–¹æ³•è«–ï¼š
        â‘ é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒé–¢ä¿‚ç¯€ã®éƒ¨åˆ†ã‚’æ­£ã—ãåˆ‡ã‚Šå–ã‚‹
        â‘¡5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®åˆ¤æ–­ç”¨ã«å…ˆè¡Œè©ã ã‘æ®‹ã™ï¼ˆã€Œå¾Œã«""ã«ã™ã¹ãã€æƒ…å ±ä»˜ãï¼‰
        """
        excluded_indices = set()
        
        if not relative_info['found']:
            return excluded_indices
        
        # å…ˆè¡Œè©ã¯ä¿æŒã—ã€é–¢ä¿‚ç¯€éƒ¨åˆ†ã®ã¿ã‚’é™¤å¤–
        rel_start = relative_info.get('clause_start_idx', -1)  # é–¢ä¿‚ä»£åè©ã®ä½ç½®
        rel_end = relative_info.get('clause_end_idx', -1)
        antecedent_idx = relative_info.get('antecedent_idx', -1)  # å…ˆè¡Œè©ã¯ä¿æŒ
        
        if rel_start >= 0 and rel_end >= 0:
            # é–¢ä¿‚ä»£åè©ã‹ã‚‰é–¢ä¿‚ç¯€çµ‚äº†ã¾ã§é™¤å¤–ï¼ˆå…ˆè¡Œè©ã¨ãƒ¡ã‚¤ãƒ³å‹•è©ã¯ä¿è­·ï¼‰
            # rel_endã¯ã‚¯ãƒ©ã‚¦ã‚ºã®æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãªã®ã§ +1 ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
            for i in range(rel_start, rel_end + 1):
                if i < len(tokens):
                    # å…ˆè¡Œè©ã¯ä¿è­·ï¼ˆ5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã§åˆ¤æ–­ã«ä½¿ç”¨ï¼‰
                    if i != antecedent_idx:
                        excluded_indices.add(i)
            
            self.logger.debug(f"é–¢ä¿‚ç¯€è¦ç´ é™¤å¤–: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {rel_start}-{rel_end} (å…ˆè¡Œè©{antecedent_idx}ã¯ä¿æŒ)")
        
        return excluded_indices
        
        # ã‚ˆãèª¤èªè­˜ã•ã‚Œã‚‹å‹•è©ã®ãƒªã‚¹ãƒˆ
        common_verbs = {
            'lives', 'live', 'lived', 'living',
            'works', 'work', 'worked', 'working',
            'runs', 'run', 'ran', 'running',
            'goes', 'go', 'went', 'going',
            'comes', 'come', 'came', 'coming',
            'sits', 'sit', 'sat', 'sitting',
            'stands', 'stand', 'stood', 'standing',
            'plays', 'play', 'played', 'playing'
        }
        
        for i, token in enumerate(tokens):
            word = token['text'].lower()
            
            # è¾æ›¸ã«å«ã¾ã‚Œã‚‹ä¸€èˆ¬çš„ãªå‹•è©
            if word in common_verbs:
                contextual_verbs.append((i, token))
            
            # èªå°¾ã«ã‚ˆã‚‹å‹•è©åˆ¤å®šï¼ˆ-s, -ed, -ingï¼‰
            elif (word.endswith('s') and len(word) > 2 and 
                  not word.endswith('ss') and not word.endswith('us')):
                # ä¸‰äººç§°å˜æ•°å½¢ã‚‰ã—ã„èª
                if self._looks_like_verb_context(tokens, i):
                    contextual_verbs.append((i, token))
        
        return contextual_verbs
    
    def _looks_like_verb_context(self, tokens: List[Dict], index: int) -> bool:
        """
        å‹•è©ã‚‰ã—ã„æ–‡è„ˆã‹ã‚’åˆ¤å®š
        """
        if index == 0:
            return False
        
        # å‰ã®èªãŒåè©ãƒ»ä»£åè©ãªã‚‰å‹•è©ã®å¯èƒ½æ€§ãŒé«˜ã„
        prev_token = tokens[index - 1]
        if prev_token['pos'] in ['NOUN', 'PRON', 'PROPN']:
            return True
        
        # å¾Œã®èªãŒå‰¯è©ãªã‚‰å‹•è©ã®å¯èƒ½æ€§ãŒé«˜ã„
        if index < len(tokens) - 1:
            next_token = tokens[index + 1]
            if next_token['pos'] == 'ADV':
                return True
        
        return False
    
    def _find_auxiliary(self, tokens: List[Dict], main_verb_idx: int) -> Optional[int]:
        """åŠ©å‹•è©ã‚’ç‰¹å®š"""
        # ãƒ¡ã‚¤ãƒ³å‹•è©ã®å‰ã‚’æ¢ã™
        for i in range(main_verb_idx):
            token = tokens[i]
            if self._is_auxiliary_verb(token):
                return i
        return None
    
    def _find_subject(self, tokens: List[Dict], verb_idx: int) -> List[int]:
        """
        ä¸»èªã‚’ç‰¹å®šï¼ˆå‹•è©ã®å‰ã®åè©å¥ï¼‰
        è¤‡æ•°èªã®åè©å¥ã«å¯¾å¿œ
        é–¢ä¿‚ç¯€ã‚’å«ã‚€å ´åˆã¯é–¢ä¿‚ç¯€å…¨ä½“ã‚’ä¸»èªã«å«ã‚ã‚‹
        """
        subject_indices = []
        
        # ğŸ†• é–¢ä¿‚ç¯€ã‚’å«ã‚€ä¸»èªã®ç‰¹å®šï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        # ãƒˆãƒ¼ã‚¯ãƒ³ã«é–¢ä¿‚ç¯€ãƒãƒ¼ã‚«ãƒ¼ãŒã‚ã‚‹å ´åˆã®å‡¦ç†
        antecedent_idx = None
        relative_clause_end_idx = None
        
        for i, token in enumerate(tokens):
            if token.get('is_antecedent', False):
                antecedent_idx = i
            if token.get('is_relative_pronoun', False):
                # é–¢ä¿‚ç¯€ã®å®Ÿéš›ã®çµ‚äº†ä½ç½®ã‚’ä½¿ç”¨
                relative_clause_end_idx = token.get('relative_clause_end', verb_idx - 1)
                break
        
        # é–¢ä¿‚ç¯€ã‚’å«ã‚€ä¸»èªã®å ´åˆ
        if antecedent_idx is not None and relative_clause_end_idx is not None:
            # ğŸ”§ Rephraseã‚·ã‚¹ãƒ†ãƒ ä»•æ§˜: é–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆã§ã‚‚é€šå¸¸ã®ä¸»èªæ¤œå‡ºã‚’è¡Œã†
            # _assign_grammar_rolesã§ã€Œã‹ãŸã¾ã‚Šã€åˆ¤å®šã«ã‚ˆã‚Šç©ºã«ã™ã‚‹ã‹ã‚’æ±ºå®š
            self.logger.debug(f"é–¢ä¿‚ç¯€æ¤œå‡º: é€šå¸¸ã®ä¸»èªæ¤œå‡ºã‚’ç¶™ç¶šï¼ˆã‹ãŸã¾ã‚Šåˆ¤å®šã¯å¾Œã§å®Ÿè¡Œï¼‰")
            # return []  # ã“ã®æ—©æœŸãƒªã‚¿ãƒ¼ãƒ³ã‚’å‰Šé™¤
        
        # é€šå¸¸ã®ä¸»èªæ¤œå‡ºï¼ˆé–¢ä¿‚ç¯€ã‚ã‚Šãƒ»ãªã—ä¸¡å¯¾å¿œï¼‰
        # å‹•è©ã®å‰ã‚’å³ã‹ã‚‰å·¦ã«æ¢ã™
        for i in range(verb_idx - 1, -1, -1):
            token = tokens[i]
            
            # åŠ©å‹•è©ã¯é£›ã°ã™
            if self._is_auxiliary_verb(token):
                continue
            
            # åè©ãƒ»ä»£åè©ãƒ»å† è©ã‚’ä¸»èªã®ä¸€éƒ¨ã¨ã—ã¦åé›†
            if (token['pos'] in ['NOUN', 'PROPN', 'PRON'] or 
                token['tag'] in ['DT', 'PRP', 'PRP$', 'WP']):
                subject_indices.insert(0, i)  # é †åºã‚’ä¿ã¤ãŸã‚å‰ã«æŒ¿å…¥
            else:
                # ä¸»èªã®å¢ƒç•Œã«åˆ°é”
                break
        
        return subject_indices
    
    def _determine_sentence_pattern(self, core_elements: Dict, tokens: List[Dict]) -> str:
        """
        å‹•è©ã®æ€§è³ªã¨æ–‡è„ˆã‹ã‚‰æ–‡å‹ã‚’å‹•çš„ã«åˆ¤å®š
        """
        if not core_elements['verb']:
            return 'UNKNOWN'
        
        verb = core_elements['verb']
        verb_lemma = verb['lemma'].lower()
        verb_indices = core_elements['verb_indices'] + core_elements.get('auxiliary_indices', [])
        subject_indices = core_elements['subject_indices']
        
        # ğŸ¯ Central Controller: è‡ªå‹•è©ãƒªã‚¹ãƒˆ
        intransitive_verbs = {
            'arrive', 'arrived', 'come', 'came', 'go', 'went', 'sleep', 'slept',
            'walk', 'walked', 'run', 'ran', 'happen', 'happened', 'occur', 'occurred',
            'exist', 'existed', 'fall', 'fell', 'rise', 'rose', 'sit', 'sat',
            'stand', 'stood', 'lie', 'lay', 'work', 'worked', 'laugh', 'laughed',
            'cry', 'cried', 'smile', 'smiled', 'die', 'died'
        }
        
        # ä½¿ç”¨æ¸ˆã¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        used_indices = set(verb_indices + subject_indices)
        
        # æ®‹ã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åˆ†æ
        remaining_tokens = [
            (i, token) for i, token in enumerate(tokens) 
            if i not in used_indices and token['pos'] != 'PUNCT'
        ]
        
        # ğŸ¯ Central Controller: è‡ªå‹•è©ã®å ´åˆã¯å¼·åˆ¶çš„ã«SVãƒ‘ã‚¿ãƒ¼ãƒ³
        if verb_lemma in intransitive_verbs or verb['text'].lower() in intransitive_verbs:
            return 'SV'
        
        # é€£çµå‹•è©ã®å ´åˆ â†’ SVCå€™è£œ
        if verb_lemma in self.linking_verbs:
            if remaining_tokens:
                # è£œèªãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                for i, token in remaining_tokens:
                    if self._can_be_complement(token):
                        return 'SVC'
            return 'SV'  # è£œèªãŒãªã„å ´åˆ
        
        # æˆä¸å‹•è©ã®å ´åˆ â†’ SVOOå€™è£œ
        if verb_lemma in self.ditransitive_verbs:
            if len(remaining_tokens) >= 2:
                return 'SVOO'
            elif len(remaining_tokens) == 1:
                return 'SVO'
        
        # ç›®çš„æ ¼è£œèªå‹•è©ã®å ´åˆ â†’ SVOCå€™è£œ
        if verb_lemma in self.objective_complement_verbs:
            if len(remaining_tokens) >= 2:
                return 'SVOC'
            elif len(remaining_tokens) == 1:
                return 'SVO'
        
        # ä¸€èˆ¬çš„ãªä»–å‹•è© â†’ SVO
        if remaining_tokens:
            # ç›®çš„èªå€™è£œãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            for i, token in remaining_tokens:
                if self._can_be_object(token):
                    return 'SVO'
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ â†’ SV
        return 'SV'
    
    def _assign_grammar_roles(self, tokens: List[Dict], pattern: str, core_elements: Dict, relative_info: Dict = None) -> List[GrammarElement]:
        """
        æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ã„ã¦æ–‡æ³•çš„å½¹å‰²ã‚’å‹•çš„ã«å‰²ã‚Šå½“ã¦
        é–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆã¯è©²å½“ã‚¹ãƒ­ãƒƒãƒˆã‚’ç©ºã«ã™ã‚‹
        """
        if relative_info is None:
            relative_info = {'found': False}
            
        elements = []
        used_indices = set()
        
        # ğŸ†• é–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆï¼šã€Œã‹ãŸã¾ã‚Šã€ã®æ–‡æ³•çš„å½¹å‰²ã‚’å‹•è©ã¨ã®é–¢ä¿‚ã‹ã‚‰æ¨å®š
        relative_slot_to_empty = None
        if relative_info['found']:
            relative_slot_to_empty = self._determine_chunk_grammatical_role(tokens, core_elements, relative_info)
        
        # ä¸»èªå‡¦ç†ï¼ˆé–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆã¯å¼·åˆ¶çš„ã«ä¸»èªè¦ç´ ã‚’ä½œæˆï¼‰
        if core_elements['subject_indices'] or (relative_info['found'] and relative_slot_to_empty == 'S'):
            if relative_slot_to_empty == 'S':
                # â‘£ é–¢ä¿‚ç¯€ãŒSä½ç½®ã«ã‚ã‚‹å ´åˆï¼šã€Œå¾Œã«""ã«ã™ã¹ãã€æƒ…å ±ã‚’é©ç”¨
                subject_element = GrammarElement(
                    text="",  # ç©ºæ–‡å­—åˆ—ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆã®â‘£ï¼‰
                    tokens=[],
                    role='S',
                    start_idx=relative_info.get('antecedent_idx', 0),
                    end_idx=relative_info.get('antecedent_idx', 0),
                    confidence=0.95
                )
                self.logger.debug(f"é–¢ä¿‚ç¯€ä¸»èªã‚’ç©ºã‚¹ãƒ­ãƒƒãƒˆã«å¤‰æ›: antecedent_idx={relative_info.get('antecedent_idx')}")
            elif core_elements['subject_indices']:
                # é€šå¸¸ã®ä¸»èªå‡¦ç†
                subject_text = self._clean_relative_clause_from_text(core_elements['subject'], relative_info)
                subject_element = GrammarElement(
                    text=subject_text,
                    tokens=[tokens[i] for i in core_elements['subject_indices']],
                    role='S',
                    start_idx=min(core_elements['subject_indices']),
                    end_idx=max(core_elements['subject_indices']),
                    confidence=0.95
                )
            
            elements.append(subject_element)
            if core_elements['subject_indices']:
                used_indices.update(core_elements['subject_indices'])
        
        # åŠ©å‹•è©
        if core_elements['auxiliary_indices']:
            aux_element = GrammarElement(
                text=core_elements['auxiliary']['text'],
                tokens=[core_elements['auxiliary']],
                role='Aux',
                start_idx=core_elements['auxiliary_indices'][0],
                end_idx=core_elements['auxiliary_indices'][0],
                confidence=0.95
            )
            elements.append(aux_element)
            used_indices.update(core_elements['auxiliary_indices'])
        
        # å‹•è©
        if core_elements['verb_indices']:
            verb_element = GrammarElement(
                text=core_elements['verb']['text'],
                tokens=[core_elements['verb']],
                role='V',
                start_idx=core_elements['verb_indices'][0],
                end_idx=core_elements['verb_indices'][0],
                confidence=0.95
            )
            elements.append(verb_element)
            used_indices.update(core_elements['verb_indices'])
        
        # æ®‹ã‚Šã®è¦ç´ ã‚’æ–‡å‹ã«å¿œã˜ã¦å‰²ã‚Šå½“ã¦
        remaining_tokens = [
            (i, token) for i, token in enumerate(tokens) 
            if i not in used_indices and token['pos'] != 'PUNCT'
        ]
        
        # æ–‡å‹åˆ¥ã®å‰²ã‚Šå½“ã¦ï¼ˆé–¢ä¿‚ç¯€æƒ…å ±ã‚’æ¸¡ã™ï¼‰
        if pattern == 'SVC':
            elements.extend(self._assign_svc_elements(remaining_tokens, relative_slot_to_empty))
        elif pattern == 'SVO':
            elements.extend(self._assign_svo_elements(remaining_tokens, relative_slot_to_empty))
        elif pattern == 'SVOO':
            elements.extend(self._assign_svoo_elements(remaining_tokens, relative_slot_to_empty))
        elif pattern == 'SVOC':
            elements.extend(self._assign_svoc_elements(remaining_tokens, relative_slot_to_empty))
        else:  # SV or other
            elements.extend(self._assign_modifiers(remaining_tokens))
        
        return elements
    
    def _assign_svc_elements(self, remaining_tokens: List[Tuple[int, Dict]], relative_slot_to_empty: str = None) -> List[GrammarElement]:
        """SVCæ–‡å‹ã®è¦ç´ ã‚’å‰²ã‚Šå½“ã¦ - è¤‡åˆå¥å¯¾å¿œ"""
        elements = []
        complement_assigned = False
        used_indices = set()
        
        i = 0
        while i < len(remaining_tokens):
            idx, token = remaining_tokens[i]
            
            # æ—¢ã«ä½¿ç”¨æ¸ˆã¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ã‚¹ã‚­ãƒƒãƒ—
            if idx in used_indices:
                i += 1
                continue
            
            if not complement_assigned and (self._can_be_complement(token) or token['tag'] == 'DT'):
                # C1ã¨ã—ã¦è¤‡åˆå¥ã‚’æ¤œå‡ºï¼ˆå† è©ã‹ã‚‰å§‹ã¾ã‚‹å ´åˆã‚‚å«ã‚€ï¼‰
                phrase_indices, phrase_text = self._find_noun_phrase(remaining_tokens, i)
                if phrase_indices:
                    elements.append(GrammarElement(
                        text=phrase_text,
                        tokens=[remaining_tokens[j][1] for j in range(i, i + len(phrase_indices))],
                        role='C1',
                        start_idx=min(phrase_indices),
                        end_idx=max(phrase_indices),
                        confidence=0.9
                    ))
                    used_indices.update(phrase_indices)
                    complement_assigned = True
                    i += len(phrase_indices)
                else:
                    i += 1
            else:
                # ä¿®é£¾èªã¨ã—ã¦å‡¦ç†
                if idx not in used_indices:
                    elements.append(self._create_modifier_element(idx, token))
                i += 1
        
        return elements
    
    def _assign_svo_elements(self, remaining_tokens: List[Tuple[int, Dict]], relative_slot_to_empty: str = None) -> List[GrammarElement]:
        """SVOæ–‡å‹ã®è¦ç´ ã‚’å‰²ã‚Šå½“ã¦ï¼ˆé–¢ä¿‚ç¯€å¯¾å¿œï¼‰"""
        elements = []
        object_assigned = False
        
        # é–¢ä¿‚ç¯€ã«ã‚ˆã‚Šç›®çš„èªã‚¹ãƒ­ãƒƒãƒˆã‚’æŠ½å‡º
        object_text = ""
        if relative_slot_to_empty == 'O1':
            # O1ã«é–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆã¯ç©ºæ–‡å­—åˆ—
            object_text = ""
        else:
            # é€šå¸¸ã®ç›®çš„èªå‡¦ç† - è¤‡æ•°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã¾ã¨ã‚ã‚‹
            object_tokens = []
            for i, token in remaining_tokens:
                if self._can_be_object(token) or token['pos'] in ['DET', 'ADJ']:
                    object_tokens.append((i, token))
                elif object_tokens:  # ç›®çš„èªå¥ãŒçµ‚äº†
                    break
            
            if object_tokens:
                object_text = ' '.join([token['text'] for _, token in object_tokens])
                used_indices = {i for i, _ in object_tokens}
                
                elements.append(GrammarElement(
                    text=object_text,
                    tokens=[token for _, token in object_tokens],
                    role='O1',
                    start_idx=object_tokens[0][0],
                    end_idx=object_tokens[-1][0],
                    confidence=0.9
                ))
                object_assigned = True
                
                # æ®‹ã‚Šã®è¦ç´ ã‚’ä¿®é£¾èªã¨ã—ã¦å‡¦ç†
                for i, token in remaining_tokens:
                    if i not in used_indices:
                        elements.append(self._create_modifier_element(i, token))
            
        # é–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆã¯ç©ºã®O1è¦ç´ ã‚’ä½œæˆ
        if relative_slot_to_empty == 'O1' and not object_assigned:
            elements.append(GrammarElement(
                text="",  # ç©ºæ–‡å­—åˆ—
                tokens=[],
                role='O1',
                start_idx=0,
                end_idx=0,
                confidence=0.9
            ))
            
            # æ®‹ã‚Šã‚’ä¿®é£¾èªã¨ã—ã¦å‡¦ç†
            for i, token in remaining_tokens:
                elements.append(self._create_modifier_element(i, token))
        
        return elements
    
    def _assign_svoo_elements(self, remaining_tokens: List[Tuple[int, Dict]], relative_slot_to_empty: str = None) -> List[GrammarElement]:
        """SVOOæ–‡å‹ã®è¦ç´ ã‚’å‰²ã‚Šå½“ã¦ - O1/O2åˆ†é›¢å¯¾å¿œ"""
        elements = []
        o1_assigned = False
        o2_assigned = False
        used_indices = set()
        
        i = 0
        while i < len(remaining_tokens):
            idx, token = remaining_tokens[i]
            
            # æ—¢ã«ä½¿ç”¨æ¸ˆã¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ã‚¹ã‚­ãƒƒãƒ—
            if idx in used_indices:
                i += 1
                continue
            
            if not o1_assigned and (self._can_be_object(token) or token['tag'] == 'DT'):
                # SVOOæ–‡å‹ã®O1ã¯é€šå¸¸å˜ä¸€èªï¼ˆä»£åè©ãªã©ï¼‰
                if token['pos'] == 'PRON':
                    # ä»£åè©ã®å ´åˆã¯å˜èªã®ã¿ã§O1
                    elements.append(GrammarElement(
                        text=token['text'],
                        tokens=[token],
                        role='O1',
                        start_idx=idx,
                        end_idx=idx,
                        confidence=0.9
                    ))
                    used_indices.add(idx)
                    o1_assigned = True
                    i += 1
                else:
                    # ä»£åè©ä»¥å¤–ã‚‚å˜èªã®ã¿ã§O1ã¨ã—ã¦æ‰±ã†
                    elements.append(GrammarElement(
                        text=token['text'],
                        tokens=[token],
                        role='O1',
                        start_idx=idx,
                        end_idx=idx,
                        confidence=0.85
                    ))
                    used_indices.add(idx)
                    o1_assigned = True
                    i += 1
            elif not o2_assigned and (self._can_be_object(token) or token['tag'] == 'DT'):
                # O2ã¨ã—ã¦è¤‡åˆå¥ã‚’æ¤œå‡ºï¼ˆå† è©ã‹ã‚‰å§‹ã¾ã‚‹å ´åˆã‚‚å«ã‚€ï¼‰
                phrase_indices, phrase_text = self._find_noun_phrase(remaining_tokens, i)
                if phrase_indices:
                    elements.append(GrammarElement(
                        text=phrase_text,
                        tokens=[remaining_tokens[j][1] for j in range(i, i + len(phrase_indices))],
                        role='O2',
                        start_idx=min(phrase_indices),
                        end_idx=max(phrase_indices),
                        confidence=0.85
                    ))
                    used_indices.update(phrase_indices)
                    o2_assigned = True
                    i += len(phrase_indices)
                else:
                    i += 1
            else:
                # ä¿®é£¾èªã¨ã—ã¦å‡¦ç†
                if idx not in used_indices:
                    elements.append(self._create_modifier_element(idx, token))
                i += 1
        
        return elements
    
    def _assign_svoc_elements(self, remaining_tokens: List[Tuple[int, Dict]], relative_slot_to_empty: str = None) -> List[GrammarElement]:
        """SVOCæ–‡å‹ã®è¦ç´ ã‚’å‰²ã‚Šå½“ã¦"""
        elements = []
        object_assigned = False
        complement_assigned = False
        
        for i, token in remaining_tokens:
            if not object_assigned and self._can_be_object(token):
                elements.append(GrammarElement(
                    text=token['text'],
                    tokens=[token],
                    role='O1',
                    start_idx=i,
                    end_idx=i,
                    confidence=0.85
                ))
                object_assigned = True
            elif not complement_assigned and (self._can_be_complement(token) or object_assigned):
                elements.append(GrammarElement(
                    text=token['text'],
                    tokens=[token],
                    role='C2',  # ğŸ”§ SVOCã®Cã¯C2ã«ä¿®æ­£
                    start_idx=i,
                    end_idx=i,
                    confidence=0.85
                ))
                complement_assigned = True
            else:
                # ä¿®é£¾èªã¨ã—ã¦å‡¦ç†
                elements.append(self._create_modifier_element(i, token))
        
        return elements
    
    def _assign_modifiers(self, remaining_tokens: List[Tuple[int, Dict]]) -> List[GrammarElement]:
        """ä¿®é£¾èªã‚’å‰²ã‚Šå½“ã¦"""
        elements = []
        for i, token in remaining_tokens:
            elements.append(self._create_modifier_element(i, token))
        return elements
    
    def _create_modifier_element(self, idx: int, token: Dict) -> GrammarElement:
        """ä¿®é£¾èªè¦ç´ ã‚’ä½œæˆ"""
        # ä¿®é£¾èªã®ç¨®é¡ã‚’åˆ¤å®š
        if token['pos'] in ['ADV', 'PART']:
            role = 'M1'  # å‰¯è©çš„ä¿®é£¾ï¼ˆå‰¯è©ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒå¾Œã§ä¸Šæ›¸ãï¼‰
        elif token['pos'] in ['ADP'] or token['tag'] in ['IN', 'TO']:
            role = 'M2'  # å‰ç½®è©å¥
        else:
            role = 'M3'  # ãã®ä»–ã®ä¿®é£¾
        
        return GrammarElement(
            text=token['text'],
            tokens=[token],
            role=role,
            start_idx=idx,
            end_idx=idx,
            confidence=0.7
        )
    
    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    def _is_auxiliary_verb(self, token: Dict) -> bool:
        """åŠ©å‹•è©åˆ¤å®š"""
        aux_words = {'be', 'am', 'is', 'are', 'was', 'were', 'being', 'been',
                     'have', 'has', 'had', 'having',
                     'do', 'does', 'did', 'doing',
                     'will', 'would', 'shall', 'should', 'can', 'could',
                     'may', 'might', 'must', 'ought'}
        return token['lemma'].lower() in aux_words or token['tag'] in ['MD']
    
    def _find_noun_phrase(self, tokens: List[Tuple[int, Dict]], start_idx: int) -> Tuple[List[int], str]:
        """
        æŒ‡å®šä½ç½®ã‹ã‚‰è¤‡åˆåè©å¥ã‚’æ¤œå‡º
        
        Args:
            tokens: ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆ [(index, token), ...]
            start_idx: æ¤œç´¢é–‹å§‹ä½ç½®
            
        Returns:
            Tuple[List[int], str]: (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚¹ãƒˆ, çµåˆã—ãŸãƒ•ãƒ¬ãƒ¼ã‚º)
        """
        phrase_indices = []
        phrase_tokens = []
        
        # é–‹å§‹ä½ç½®ã‹ã‚‰é€£ç¶šã™ã‚‹åè©å¥è¦ç´ ã‚’åé›†
        for i in range(start_idx, len(tokens)):
            idx, token = tokens[i]
            
            # åè©å¥ã®æ§‹æˆè¦ç´ ã‹ãƒã‚§ãƒƒã‚¯
            if (token['pos'] in ['NOUN', 'PROPN', 'PRON'] or 
                token['tag'] in ['DT', 'CD', 'JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS']):
                phrase_indices.append(idx)
                phrase_tokens.append(token['text'])
            else:
                # åè©å¥ã®çµ‚äº†
                break
        
        if phrase_indices:
            phrase_text = ' '.join(phrase_tokens)
            return phrase_indices, phrase_text
        else:
            return [], ""

    def _can_be_object(self, token: Dict) -> bool:
        """ç›®çš„èªã«ãªã‚Œã‚‹ã‹ã®åˆ¤å®š"""
        return token['pos'] in ['NOUN', 'PROPN', 'PRON'] or token['tag'] in ['PRP', 'DT']
    
    def _can_be_complement(self, token: Dict) -> bool:
        """è£œèªã«ãªã‚Œã‚‹ã‹ã®åˆ¤å®š"""
        return token['pos'] in ['ADJ', 'NOUN', 'PROPN', 'PRON'] or token['tag'] in ['JJ', 'NN', 'NNS', 'PRP']
    
    def _convert_to_rephrase_format(self, elements: List[GrammarElement], pattern: str, sub_slots: Dict = None) -> Dict[str, Any]:
        """Rephraseã‚¹ãƒ­ãƒƒãƒˆå½¢å¼ã«å¤‰æ›"""
        if sub_slots is None:
            sub_slots = {}
        
        # ğŸ”§ é–¢ä¿‚ç¯€ã®æœ‰ç„¡ã‚’ç¢ºèªã—ã¦ã‚¹ãƒ­ãƒƒãƒˆç•ªå·ã‚’èª¿æ•´
        has_relative_clause = bool(sub_slots)
        
        # C. ã€Œç›¸å¯¾ç¯€ã‚ã‚Šã®ã‚·ãƒ•ãƒˆã€ã¯ä¸€åº¦ãã‚Šã«ã™ã‚‹
        result = {}
        if has_relative_clause and not getattr(self, '_shifted_for_relcl', False):
            for element in elements:
                if element.role == 'M1':
                    element.role = 'M2'  # M1 â†’ M2
                elif element.role == 'M2':
                    element.role = 'M3'  # M2 â†’ M3
            self._shifted_for_relcl = True  # ä¸€åº¦ã ã‘å®Ÿè¡Œã™ã‚‹ãƒ•ãƒ©ã‚°
            result['_shifted_for_relcl'] = True  # çµæœã«ã‚‚å°ã‚’æ®‹ã™
            
        slots = []
        slot_phrases = []
        slot_display_order = []
        display_order = []
        phrase_types = []
        subslot_ids = []
        
        # ğŸ”§ main_slotsè¾æ›¸å½¢å¼ã‚‚ç”Ÿæˆ
        main_slots = {}
        
        # ğŸ”¥ Phase 2: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‹ã‚‰ã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæƒ…å ±ãƒãƒ¼ã‚¸
        unified_sub_slots = {}
        if hasattr(self, 'last_unified_result') and self.last_unified_result:
            unified_sub_slots = self.last_unified_result.get('sub_slots', {})
            print(f"ğŸ”¥ çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‹ã‚‰ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå–å¾—: {unified_sub_slots}")
        
        # æ—¢å­˜ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã¨çµ±åˆ
        merged_sub_slots = sub_slots.copy()
        merged_sub_slots.update(unified_sub_slots)
        print(f"ğŸ”¥ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆãƒãƒ¼ã‚¸çµæœ: {merged_sub_slots}")
        
        # è¦ç´ ã‚’ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
        elements.sort(key=lambda x: x.start_idx)
        
        role_order = {'S': 1, 'Aux': 2, 'V': 3, 'O1': 4, 'O2': 5, 'C1': 6, 'C2': 7, 'M1': 8, 'M2': 9, 'M3': 10}
        
        # ğŸ”¥ è²¬ä»»åˆ†é›¢åŸå‰‡: çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ãŒã‚ã‚‹å ´åˆã€ãƒ¬ã‚¬ã‚·ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã¯å‰¯è©å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
        adverb_slots = ['M1', 'M2', 'M3']
        
        for i, element in enumerate(elements):
            # ã‚¹ãƒ­ãƒƒãƒˆåã®èª¿æ•´
            slot_name = element.role
            
            # ğŸ”¥ è²¬ä»»åˆ†é›¢: å‰¯è©ã‚¹ãƒ­ãƒƒãƒˆã¯çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒæ‹…å½“ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã¯é™¤å¤–ï¼‰
            if slot_name in adverb_slots:
                print(f"ğŸ”¥ ãƒ¬ã‚¬ã‚·ãƒ¼ã‚·ã‚¹ãƒ†ãƒ : å‰¯è©ã‚¹ãƒ­ãƒƒãƒˆ {slot_name}='{element.text}' ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆçµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒæ‹…å½“ï¼‰")
                continue
            
            # çµ±ä¸€å½¢å¼: å¸¸ã«O1, O2, C1, C2ã‚’ä½¿ç”¨
            
            slots.append(slot_name)
            slot_phrases.append(element.text)
            
            # ğŸ”§ main_slotsè¾æ›¸ã«è¿½åŠ ï¼ˆå‰¯è©ä»¥å¤–ã®ã¿ï¼‰
            # ğŸ›‘ ãƒ¬ã‚¬ã‚·ãƒ¼O1å³æ ¼ã‚¬ãƒ¼ãƒ‰ï¼šå‰ç½®è©å¥å†…åè©ã®ç›®çš„èªèª¤èªè­˜ã‚’é˜²æ­¢
            if element.role == 'O1':
                # æ¶ˆè²»æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚§ãƒƒã‚¯
                if hasattr(self, '_consumed_tokens') and any(
                    element.start_idx + j in self._consumed_tokens for j in range(len(element.text.split()))
                ):
                    print(f"ğŸ›‘ Skip legacy O1='{element.text}' from _convert_to_rephrase_format (uses consumed tokens)")
                    continue
            
            main_slots[element.role] = element.text
            
            order = role_order.get(element.role, 99)
            slot_display_order.append(order)
            display_order.append(order)
            
            # å“è©ã‚¿ã‚¤ãƒ—ã®åˆ¤å®š
            if element.role in ['S', 'O1', 'O2']:
                phrase_types.append('åè©å¥')
            elif element.role == 'V':
                phrase_types.append('å‹•è©å¥')
            elif element.role in ['C1', 'C2']:
                phrase_types.append('è£œèªå¥')
            else:
                phrase_types.append('ä¿®é£¾å¥')
            
            subslot_ids.append(i)
        
        return {
            'Slot': slots,
            'SlotPhrase': slot_phrases,
            'Slot_display_order': slot_display_order,
            'display_order': display_order,
            'PhraseType': phrase_types,
            'SubslotID': subslot_ids,
            'main_slots': main_slots,  # ğŸ”§ è¾æ›¸å½¢å¼è¿½åŠ 
            'sub_slots': merged_sub_slots,    # ğŸ”¥ çµ±åˆã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆä½¿ç”¨
            'slots': main_slots,       # ğŸ”§ çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ äº’æ›æ€§
            'pattern_detected': pattern,
            'confidence': 0.9,
            'analysis_method': 'dynamic_grammar',
            'lexical_tokens': len([e for e in elements if e.role != 'PUNCT'])
        }

    def _detect_and_assign_adverbs_direct(self, doc, current_result: Dict) -> Dict:
        """
        ç›´æ¥çš„ãªå‰¯è©æ¤œå‡ºã¨é…ç½® (Phase 2ç°¡æ˜“å®Ÿè£…)
        
        Rephraseãƒ«ãƒ¼ãƒ«ï¼ˆæ­£ã—ã„ç†è§£ï¼‰:
        - 1å€‹: M2
        - 2å€‹: å‹•è©ã‚ˆã‚Šå‰ â†’ M1,M2 / å‹•è©ã‚ˆã‚Šå¾Œ â†’ M2,M3
        - 3å€‹: M1, M2, M3 (ä½ç½®é †)
        """
        try:
            # spaCyã‹ã‚‰å‰¯è©ã‚’æŠ½å‡º (é–¢ä¿‚ç¯€å‡¦ç†ã¯æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã«ä»»ã›ã‚‹)
            adverbs = []
            
            # B. ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå†…å‰¯è©ã®é™¤å¤–ã¯"èªå¢ƒç•Œ"ã§
            sub_words = set()
            for v in (current_result.get('sub_slots') or {}).values():
                if isinstance(v, str):
                    sub_words.update(v.split())  # æ–‡å­—åˆ—â†’èªãƒªã‚¹ãƒˆåŒ–
            
            # ãƒ¡ã‚¤ãƒ³å‹•è©ã®ä½ç½®ã‚’ç‰¹å®š
            main_verb_pos = None
            main_verb = current_result.get('main_slots', {}).get('V', '')
            if main_verb:
                for token in doc:
                    if token.text == main_verb and token.pos_ in ['VERB', 'AUX']:
                        main_verb_pos = token.i
                        break
            
            for token in doc:
                if token.pos_ == 'ADV':
                    # ChatGPT5 Step C: æ¶ˆè²»æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    if hasattr(self, '_consumed_tokens') and token.i in self._consumed_tokens:
                        print(f"ğŸ”¥ ChatGPT5 Step C: Adverb handler skipping consumed token {token.i}='{token.text}'")
                        continue
                        
                    # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå†…ã®å‰¯è©ã¯é™¤å¤–ï¼ˆèªå˜ä½ã®ä¸€è‡´ï¼‰
                    if token.text not in sub_words:
                        adverbs.append({
                            'text': token.text,
                            'index': token.i,
                            'pos': token.pos_
                        })
                    else:
                        print(f"ğŸ” é–¢ä¿‚ç¯€å†…å‰¯è©ã‚’é™¤å¤–: {token.text} (ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã§å‡¦ç†æ¸ˆã¿)")
            
            # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
            adverbs.sort(key=lambda x: x['index'])
            
            if not adverbs:
                return {}
            
            print(f"ğŸ” æ¤œå‡ºã•ã‚ŒãŸå‰¯è©: {[adv['text'] for adv in adverbs]}")
            print(f"ğŸ” ãƒ¡ã‚¤ãƒ³å‹•è© '{main_verb}' ã®ä½ç½®: {main_verb_pos}")
            
            # ï¿½ è²¬ä»»åˆ†é›¢åŸå‰‡: 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã¯å‰¯è©å‡¦ç†ã‚’è¡Œã‚ãªã„
            # å‰¯è©ã¯å°‚ç”¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆæ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´šã€å‰¯è©ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼‰ãŒæ‹…å½“
            modifier_assignments = {}
            
            print(f"ğŸ” 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼: å‰¯è©å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆè²¬ä»»åˆ†é›¢åŸå‰‡ï¼‰")
            
            # ãƒ‡ãƒãƒƒã‚°ï¼šåæŸç¢ºèªç”¨ã®ãƒãƒƒã‚·ãƒ¥
            sig = '|'.join([current_result['main_slots'].get(k,'') for k in ('M1','M2','M3')])
            print(f"ğŸ” ADV_SIGNATURE_BEFORE={sig}")
            
            # ğŸš« è²¬ä»»åˆ†é›¢åŸå‰‡: 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã¯å‰¯è©ã‚’è¿”ã•ãªã„
            result = {}
            
            print(f"ğŸ” 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼: å‰¯è©çµæœãªã—ï¼ˆè²¬ä»»åˆ†é›¢åŸå‰‡ï¼‰")
            
            return result
            
        except Exception as e:
            self.logger.error(f"ç›´æ¥å‰¯è©å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def _create_error_result(self, sentence: str, error: str) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼çµæœã‚’ä½œæˆ"""
        return {
            'Slot': [],
            'SlotPhrase': [],
            'Slot_display_order': [],
            'display_order': [],
            'PhraseType': [],
            'SubslotID': [],
            'main_slots': {},    # ğŸ”§ è¾æ›¸å½¢å¼è¿½åŠ 
            'sub_slots': {},     # ğŸ”§ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆï¼ˆç¾åœ¨ã¯ç©ºï¼‰
            'slots': {},         # ğŸ”§ çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ äº’æ›æ€§
            'error': error,
            'sentence': sentence,
            'analysis_method': 'dynamic_grammar'
        }

    # ============================================
    # é–¢ä¿‚ç¯€å‡¦ç†ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤
    # ============================================
    
    def _detect_relative_clause(self, tokens: List[Dict], sentence: str) -> Dict[str, Any]:
        """é–¢ä¿‚ç¯€æ§‹é€ ã®æ¤œå‡º"""
        result = {
            'found': False,
            'type': None,
            'confidence': 0.0,
            'relative_pronoun_idx': None,
            'antecedent_idx': None,
            'clause_start_idx': None,
            'clause_end_idx': None
        }
        
        sentence_lower = sentence.lower()
        
        # é–¢ä¿‚ä»£åè©ã®æ¤œå‡º
        relative_pronouns = ['who', 'whom', 'which', 'that', 'whose', 'where', 'when', 'why', 'how']
        
        for rel_pronoun in relative_pronouns:
            if rel_pronoun in sentence_lower:
                # ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆã§é–¢ä¿‚ä»£åè©ã‚’æ¢ã™
                for i, token in enumerate(tokens):
                    if token['text'].lower() == rel_pronoun:
                        result.update({
                            'found': True,
                            'type': f'{rel_pronoun}_clause',
                            'confidence': 0.8,
                            'relative_pronoun_idx': i
                        })
                        
                        # å…ˆè¡Œè©ã‚’æ¢ã™ï¼ˆé–¢ä¿‚ä»£åè©ã®ç›´å‰ã®åè©ï¼‰
                        if i > 0 and tokens[i-1]['pos'] in ['NOUN', 'PROPN']:
                            result['antecedent_idx'] = i - 1
                            result['confidence'] = 0.9
                        
                        # é–¢ä¿‚ç¯€ã®ç¯„å›²ã‚’æ±ºå®šï¼ˆæ”¹è‰¯ç‰ˆï¼‰
                        result['clause_start_idx'] = i
                        result['clause_end_idx'] = self._find_relative_clause_end(tokens, i, rel_pronoun)
                        
                        self.logger.debug(f"é–¢ä¿‚ç¯€æ¤œå‡º: {rel_pronoun} at position {i}, end at {result['clause_end_idx']}")
                        break
                
                if result['found']:
                    break
        
        return result
    
    def _find_relative_clause_end(self, tokens: List[Dict], rel_start_idx: int, rel_type: str) -> int:
        """é–¢ä¿‚ç¯€ã®çµ‚äº†ä½ç½®ã‚’ç‰¹å®šï¼ˆäººé–“çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ ï¼‰"""
        
        # whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†
        if rel_type == 'whose':
            return self._find_whose_clause_end(tokens, rel_start_idx)
        
        # ğŸ†• whoæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†ï¼ˆTest 12æˆåŠŸæ‰‹æ³•ã‚’é©ç”¨ï¼‰
        if rel_type == 'who':
            return self._find_who_clause_end(tokens, rel_start_idx)
        
        # ğŸ†• ä¸€èˆ¬çš„ãªé–¢ä¿‚ç¯€ï¼ˆwhich/thatï¼‰ã®çµ‚äº†ä½ç½®ã‚’å“è©ãƒ™ãƒ¼ã‚¹ã§ç‰¹å®š
        # æˆ¦ç•¥: é–¢ä¿‚ä»£åè© + å‹•è© + [ä¿®é£¾èª/ç›®çš„èª/è£œèª] ã¾ã§ã‚’é–¢ä¿‚ç¯€ã¨ã™ã‚‹
        
        clause_end = rel_start_idx
        
        # Step 1: é–¢ä¿‚ä»£åè©ã®å¾Œã®å‹•è©ã‚’æ¢ã™
        rel_verb_idx = None
        for i in range(rel_start_idx + 1, min(rel_start_idx + 4, len(tokens))):
            if i < len(tokens) and tokens[i]['pos'] in ['VERB', 'AUX']:
                rel_verb_idx = i
                break
        
        if rel_verb_idx is None:
            return rel_start_idx + 1
        
        clause_end = rel_verb_idx
        
        # Step 2: å‹•è©ã®å¾Œã®è¦ç´ ã‚’é–¢ä¿‚ç¯€ã«å«ã‚ã‚‹ï¼ˆğŸ†• äººé–“çš„æ§‹é€ åˆ¤å®šï¼‰
        for i in range(rel_verb_idx + 1, len(tokens)):
            if i < len(tokens):
                token = tokens[i]
                
                # ğŸ†• äººé–“çš„å“è©åˆ¤å®šã‚’é©ç”¨
                actual_pos = self._get_human_corrected_pos(token)
                
                # ğŸ†• æ§‹é€ çš„åˆ¤å®š: æ–°ã—ã„å‹•è©å‡ºç¾ã§ä¸Šä½æ–‡é–‹å§‹
                if actual_pos in ['VERB', 'AUX']:
                    self.logger.debug(f"ğŸ§  ä¸Šä½æ–‡å‹•è©æ¤œå‡ºã«ã‚ˆã‚Šé–¢ä¿‚ç¯€çµ‚äº†: '{token['text']}' â†’ {actual_pos}")
                    break
                
                # é–¢ä¿‚ç¯€å†…ã®è¦ç´ ã¨ã—ã¦å«ã‚ã‚‹æ¡ä»¶
                # ğŸ†• 'there'ãªã©ã®å ´æ‰€å‰¯è©ã‚‚é–¢ä¿‚ç¯€ã«å«ã‚ã‚‹ï¼ˆTest 4å¯¾å¿œï¼‰
                if actual_pos in ['ADV', 'ADJ', 'NOUN', 'PROPN', 'NUM'] or \
                   (actual_pos == 'PRON' and token['text'].lower() in ['there', 'here']):
                    clause_end = i
                    self.logger.debug(f"é–¢ä¿‚ç¯€ã«å«ã‚ã‚‹: '{token['text']}' (corrected_pos={actual_pos})")
                else:
                    # ãã®ä»–ã®å“è©ã§é–¢ä¿‚ç¯€çµ‚äº†
                    self.logger.debug(f"é–¢ä¿‚ç¯€çµ‚äº†: '{token['text']}' (corrected_pos={actual_pos})")
                    break
        
        self.logger.debug(f"é–¢ä¿‚ç¯€çµ‚äº†ä½ç½®({rel_type}): {clause_end} ('{tokens[clause_end]['text'] if clause_end < len(tokens) else 'EOF'}')")
        return clause_end
    
    def _find_whose_clause_end(self, tokens: List[Dict], whose_idx: int) -> int:
        """whoseæ§‹æ–‡ã®é–¢ä¿‚ç¯€çµ‚äº†ä½ç½®ã‚’ç‰¹å®šï¼ˆæ§‹é€ çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰
        
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆã®æ§‹é€ çš„ãƒ­ã‚¸ãƒƒã‚¯ï¼š
        whose â†’ å…ˆè¡Œè©ç‰¹å®š â†’ ä¿®é£¾å¯¾è±¡åè© â†’ é–¢ä¿‚ç¯€å†…å‹•è© â†’ è£œèª/ç›®çš„èª â†’ 
        æ–°ã—ã„å‹•è©å‡ºç¾æ™‚ç‚¹ã§ä¸Šä½æ–‡é–‹å§‹ã¨åˆ¤å®šã—ã¦ã‚¹ãƒˆãƒƒãƒ—
        """
        
        clause_end = whose_idx
        
        # Step 1: whose ã®ç›´å¾Œã®ä¿®é£¾å¯¾è±¡åè©ã‚’æ¢ã™
        possessed_noun_idx = None
        for i in range(whose_idx + 1, min(whose_idx + 3, len(tokens))):
            if tokens[i]['pos'] in ['NOUN', 'PROPN']:
                possessed_noun_idx = i
                self.logger.debug(f"whoseä¿®é£¾å¯¾è±¡: '{tokens[i]['text']}' at {i}")
                break
        
        if possessed_noun_idx is None:
            self.logger.debug(f"whoseæ§‹æ–‡: ä¿®é£¾å¯¾è±¡åè©ãŒè¦‹ã¤ã‹ã‚‰ãªã„")
            return whose_idx + 1
        
        # Step 2: é–¢ä¿‚ç¯€å†…å‹•è©ã‚’æ¢ã™
        relcl_verb_idx = None
        for i in range(possessed_noun_idx + 1, min(possessed_noun_idx + 4, len(tokens))):
            if tokens[i]['pos'] in ['VERB', 'AUX']:
                relcl_verb_idx = i
                self.logger.debug(f"é–¢ä¿‚ç¯€å†…å‹•è©: '{tokens[i]['text']}' at {i}")
                break
        
        if relcl_verb_idx is None:
            self.logger.debug(f"whoseæ§‹æ–‡: é–¢ä¿‚ç¯€å‹•è©ãŒè¦‹ã¤ã‹ã‚‰ãªã„")
            return possessed_noun_idx
        
        # Step 3: é–¢ä¿‚ç¯€å†…ã®è£œèª/ç›®çš„èªã‚’é †æ¬¡å‡¦ç†
        clause_end = relcl_verb_idx
        
        for i in range(relcl_verb_idx + 1, len(tokens)):
            token = tokens[i]
            
            # ğŸ†• äººé–“çš„å“è©åˆ¤å®šã‚’ä½¿ç”¨ï¼ˆå¾ªç’°å‚ç…§å›é¿ï¼‰
            sentence_text = ' '.join([t['text'] for t in tokens])
            if token['text'].lower() in self.ambiguous_words:
                # æ§‹é€ çš„åˆ¤å®šã§å‹•è©ã‹ã©ã†ã‹ç›´æ¥ãƒã‚§ãƒƒã‚¯
                if self._is_likely_main_verb_by_position(token, tokens, i):
                    corrected_pos = 'VERB'
                else:
                    corrected_pos = token['pos']  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            else:
                corrected_pos = token['pos']
            
            # ğŸ†• æ§‹é€ çš„åˆ¤å®š: æ–°ã—ã„å‹•è©ãŒå‡ºç¾ã—ãŸã‚‰ä¸Šä½æ–‡é–‹å§‹
            if corrected_pos in ['VERB', 'AUX'] and i > relcl_verb_idx:
                self.logger.debug(f"ä¸Šä½æ–‡å‹•è©æ¤œå‡ºã«ã‚ˆã‚Šé–¢ä¿‚ç¯€çµ‚äº†: '{token['text']}' at {i} (äººé–“çš„åˆ¤å®š: {corrected_pos})")
                break
            
            # é–¢ä¿‚ç¯€å†…è¦ç´ ã¨ã—ã¦å«ã‚ã‚‹
            if corrected_pos in ['ADJ', 'NOUN', 'PROPN', 'ADV']:
                clause_end = i
                self.logger.debug(f"é–¢ä¿‚ç¯€è¦ç´ : '{token['text']}' at {i}")
            else:
                # ãã®ä»–ã®å“è©ã§é–¢ä¿‚ç¯€çµ‚äº†
                break
        
        self.logger.debug(f"whoseå¥çµ‚äº†ä½ç½®(æ§‹é€ çš„): {clause_end} ('{tokens[clause_end]['text'] if clause_end < len(tokens) else 'EOF'}')")
        return clause_end
    
    def _find_who_clause_end(self, tokens: List[Dict], who_idx: int) -> int:
        """whoæ§‹æ–‡ã®é–¢ä¿‚ç¯€çµ‚äº†ä½ç½®ã‚’ç‰¹å®šï¼ˆTest 12æˆåŠŸæ‰‹æ³•ã‚’é©ç”¨ï¼‰
        
        Test 12ãƒ‘ã‚¿ãƒ¼ãƒ³: The man whose car is red lives here.
        â†’ "The man who runs fast is strong."
        
        whoæ§‹æ–‡ã®æ§‹é€ çš„ãƒ­ã‚¸ãƒƒã‚¯ï¼š
        who â†’ é–¢ä¿‚ç¯€å†…å‹•è© â†’ ä¿®é£¾èªï¼ˆå‰¯è©/å½¢å®¹è©ï¼‰ â†’ ä¸Šä½æ–‡å‹•è©å‡ºç¾ã§ã‚¹ãƒˆãƒƒãƒ—
        
        æœŸå¾…å€¤: "The man who runs fast" â†’ sub-s="The man who", sub-v="runs", sub-m2="fast"
        """
        
        clause_end = who_idx
        
        # Step 1: whoç›´å¾Œã®å‹•è©ã‚’æ¢ã™ï¼ˆé–¢ä¿‚ç¯€å†…å‹•è©ï¼‰
        relcl_verb_idx = None
        for i in range(who_idx + 1, min(who_idx + 3, len(tokens))):
            if i < len(tokens):
                token = tokens[i]
                # ğŸ†• äººé–“çš„å“è©åˆ¤å®šã‚’é©ç”¨
                corrected_pos = self._get_human_corrected_pos(token)
                
                if corrected_pos in ['VERB', 'AUX']:
                    relcl_verb_idx = i
                    clause_end = i
                    self.logger.debug(f"whoå¥å†…å‹•è©ç™ºè¦‹: '{token['text']}' at {i} (äººé–“çš„åˆ¤å®š: {corrected_pos})")
                    break
        
        if relcl_verb_idx is None:
            self.logger.debug("whoå¥å†…å‹•è©ãŒè¦‹ã¤ã‹ã‚‰ãªã„")
            return who_idx + 1
        
        # Step 2: é–¢ä¿‚ç¯€å†…å‹•è©ã®å¾Œã®ä¿®é£¾èªã‚’é–¢ä¿‚ç¯€ã«å«ã‚ã‚‹ï¼ˆ"fast"ç­‰ï¼‰
        for i in range(relcl_verb_idx + 1, len(tokens)):
            if i < len(tokens):
                token = tokens[i]
                
                # ğŸ†• äººé–“çš„å“è©åˆ¤å®šã‚’é©ç”¨ï¼ˆæ›–æ˜§èªã®å ´åˆï¼‰
                corrected_pos = self._get_human_corrected_pos(token)
                
                # ğŸ†• æ§‹é€ çš„åˆ¤å®š: æ–°ã—ã„å‹•è©ãŒå‡ºç¾ã—ãŸã‚‰ä¸Šä½æ–‡é–‹å§‹
                if corrected_pos in ['VERB', 'AUX'] and i > relcl_verb_idx:
                    self.logger.debug(f"ä¸Šä½æ–‡å‹•è©æ¤œå‡ºã«ã‚ˆã‚Šwhoå¥çµ‚äº†: '{token['text']}' at {i} (äººé–“çš„åˆ¤å®š: {corrected_pos})")
                    break
                
                # é–¢ä¿‚ç¯€å†…è¦ç´ ã¨ã—ã¦å«ã‚ã‚‹ï¼ˆå‰¯è©ã€å½¢å®¹è©ã€åè©ï¼‰
                if corrected_pos in ['ADV', 'ADJ', 'NOUN', 'PROPN']:
                    clause_end = i
                    self.logger.debug(f"whoå¥å†…è¦ç´ : '{token['text']}' at {i} (corrected_pos={corrected_pos})")
                else:
                    # ãã®ä»–ã®å“è©ã§é–¢ä¿‚ç¯€çµ‚äº†
                    break
        
        self.logger.debug(f"whoå¥çµ‚äº†ä½ç½®(æ§‹é€ çš„): {clause_end} ('{tokens[clause_end]['text'] if clause_end < len(tokens) else 'EOF'}')")
        return clause_end
    
    def _process_relative_clause(self, tokens: List[Dict], relative_info: Dict, core_elements: Dict = None) -> Tuple[List[Dict], Dict]:
        """é–¢ä¿‚ç¯€ã®å‡¦ç†ã¨ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ï¼ˆRephraseä»•æ§˜æº–æ‹ ï¼‰
        
        æ­£ã—ã„Rephraseçš„åˆ†è§£:
        - é–¢ä¿‚ç¯€ã‚’å«ã‚€ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã¯ç©ºæ–‡å­—åˆ—
        - ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«å…ˆè¡Œè©+é–¢ä¿‚ä»£åè©ã€å‹•è©ã€ä¿®é£¾èªã‚’æ ¼ç´
        """
        self.logger.debug(f"é–¢ä¿‚ç¯€å‡¦ç†: {relative_info['type']} (ä¿¡é ¼åº¦: {relative_info['confidence']})")
        
        # é–¢ä¿‚ç¯€ã®ç¯„å›²ã‚’ç‰¹å®š
        rel_pronoun_idx = relative_info.get('relative_pronoun_idx')
        clause_end_idx = relative_info.get('clause_end_idx')
        antecedent_idx = relative_info.get('antecedent_idx')
        
        if rel_pronoun_idx is None or clause_end_idx is None or antecedent_idx is None:
            return tokens, {}
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ã«ãƒãƒ¼ã‚«ãƒ¼ã‚’è¿½åŠ 
        tokens[rel_pronoun_idx]['is_relative_pronoun'] = True
        tokens[rel_pronoun_idx]['relative_clause_type'] = relative_info['type']
        tokens[rel_pronoun_idx]['relative_clause_end'] = clause_end_idx
        tokens[antecedent_idx]['is_antecedent'] = True
        
        # Rephraseçš„ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£å®Ÿè£…
        sub_slots = self._create_rephrase_subslots(tokens, relative_info)
        
        # ğŸ”§ Step1: é–¢ä¿‚ç¯€ã®ä½ç½®æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆæ©Ÿèƒ½å¤‰æ›´ãªã—ï¼‰
        relative_position = self._determine_chunk_grammatical_role(tokens, core_elements or {}, relative_info)
        self.logger.debug(f"ç”Ÿæˆã•ã‚ŒãŸã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ: {sub_slots}")
        self.logger.debug(f"é–¢ä¿‚ç¯€ä½ç½®: {relative_position} (å…ˆè¡Œè©: {relative_info.get('antecedent_idx', 'unknown')})")
        
        # ğŸ”§ Step4: UIå½¢å¼å¯¾å¿œ - parent_slotæƒ…å ±ã‚’è¨˜éŒ²
        if relative_position and sub_slots:
            sub_slots['_parent_slot'] = relative_position  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦è¨˜éŒ²
            self.logger.debug(f"ğŸ·ï¸ UIå½¢å¼å¯¾å¿œ: parent_slot = {relative_position}")
        
        return tokens, sub_slots

    def _create_rephrase_subslots(self, tokens: List[Dict], relative_info: Dict) -> Dict:
        """Rephraseä»•æ§˜ã«æº–æ‹ ã—ãŸã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ
        
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆã®æ–¹æ³•ï¼š5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ç›´æ¥ä½¿ç”¨
        """
        rel_pronoun_idx = relative_info['relative_pronoun_idx']
        clause_end_idx = relative_info['clause_end_idx']
        antecedent_idx = relative_info['antecedent_idx']
        
        # ğŸ†• é–¢ä¿‚ç¯€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ½å‡ºï¼ˆé–¢ä¿‚ä»£åè©ã‹ã‚‰é–¢ä¿‚ç¯€çµ‚äº†ã¾ã§ï¼‰
        # clause_end_idxã¯é–¢ä¿‚ç¯€æœ€å¾Œã®è¦ç´ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãªã®ã§ +1 ã—ã¦ã‚¹ãƒ©ã‚¤ã‚·ãƒ³ã‚°
        rel_tokens = tokens[rel_pronoun_idx:clause_end_idx + 1]
        
        # ğŸ†• é–¢ä¿‚ä»£åè©ã®å½¹å‰²ã‚’åˆ¤å®šï¼ˆä¸»èª/ç›®çš„èªï¼‰
        rel_clause_type = relative_info.get('type', '')
        rel_pronoun_role = self._determine_relative_pronoun_role_enhanced(rel_tokens, rel_clause_type)
        self.logger.debug(f"é–¢ä¿‚ä»£åè©å½¹å‰²åˆ¤å®š: {rel_pronoun_role}")
        
        # ğŸ†• å…ˆè¡Œè©å¥å…¨ä½“ã‚’å–å¾—ï¼ˆThe man ãªã©ï¼‰
        antecedent_phrase = self._extract_full_antecedent_phrase(tokens, antecedent_idx)
        
        # ğŸ†• 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã§é–¢ä¿‚ç¯€å†…ã‚’è§£æï¼ˆå…ˆè¡Œè©æƒ…å ±ã‚‚æ¸¡ã™ï¼‰
        sub_slots = self._analyze_relative_clause_structure_enhanced(rel_tokens, rel_clause_type, rel_pronoun_role, antecedent_phrase)
        
        # ğŸ†• whoseæ§‹æ–‡ã¯å°‚ç”¨å‡¦ç†ã§å®Œäº†ã—ã¦ã„ã‚‹ã®ã§ãã®ã¾ã¾è¿”ã™
        if rel_clause_type == 'whose_clause':
            return sub_slots
        
        # ğŸ†• é–¢ä¿‚ä»£åè©ã®å½¹å‰²ã«åŸºã¥ãé©åˆ‡ãªé…ç½®ï¼ˆwhoseä»¥å¤–ï¼‰
        rel_pronoun_text = rel_tokens[0]['text']
        if rel_pronoun_role == 'subject':
            # é–¢ä¿‚ä»£åè©ãŒä¸»èª â†’ sub-s
            sub_slots['sub-s'] = f"{antecedent_phrase} {rel_pronoun_text}"
        elif rel_pronoun_role == 'object':
            # é–¢ä¿‚ä»£åè©ãŒç›®çš„èª â†’ sub-o1  
            sub_slots['sub-o1'] = f"{antecedent_phrase} {rel_pronoun_text}"
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            if 'sub-s' in sub_slots:
                sub_slots['sub-s'] = f"{antecedent_phrase} {sub_slots['sub-s']}"
        
        return sub_slots
    
    def _determine_relative_pronoun_role_enhanced(self, rel_tokens: List[Dict], clause_type: str) -> str:
        """é–¢ä¿‚ä»£åè©ã®å½¹å‰²ã‚’åˆ¤å®šï¼ˆä¸»èª/ç›®çš„èªï¼‰- å¼·åŒ–ç‰ˆ
        
        äººé–“çš„æ–‡æ³•èªè­˜:
        - å‹•è©å‰ã«ä»–ã®ä¸»èªãŒã‚ã‚‹ã‹ï¼Ÿ â†’ ã‚ã‚‹å ´åˆã€é–¢ä¿‚ä»£åè©ã¯ç›®çš„èª
        - å‹•è©å‰ã«ä¸»èªãŒãªã„ â†’ é–¢ä¿‚ä»£åè©ã¯ä¸»èª
        """
        if not rel_tokens or len(rel_tokens) < 2:
            return 'subject'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        # å‹•è©ã‚’æ¢ã™
        verb_idx = None
        for i, token in enumerate(rel_tokens):
            if token['pos'] == 'VERB' and i > 0:  # é–¢ä¿‚ä»£åè©ä»¥å¤–
                verb_idx = i
                break
        
        if verb_idx is None:
            return 'subject'  # å‹•è©ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
        
        # whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†
        if clause_type == 'whose_clause':
            # whose + åè©ã®å¾Œã«ä»–ã®ä¸»èªãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            # "whose book I borrowed" â†’ IãŒä¸»èªãªã®ã§whose bookã¯ç›®çš„èª
            # "whose car is red" â†’ ä»–ã«ä¸»èªãŒãªã„ã®ã§whose carã¯ä¸»èª
            whose_noun_idx = None
            for i, token in enumerate(rel_tokens):
                if i > 0 and token['pos'] in ['NOUN', 'PROPN']:
                    whose_noun_idx = i
                    break
            
            if whose_noun_idx is not None:
                # whoseåè©ã‚ˆã‚Šå¾Œã«ä»–ã®ä¸»èªãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                post_whose_tokens = rel_tokens[whose_noun_idx + 1:verb_idx]
                has_other_subject = any(
                    token['pos'] in ['NOUN', 'PRON', 'PROPN'] 
                    for token in post_whose_tokens
                )
                if has_other_subject:
                    self.logger.debug(f"whoseæ§‹æ–‡: ä»–ã®ä¸»èªç™ºè¦‹ â†’ whoseå¥ã¯ç›®çš„èª")
                    return 'object'
                else:
                    self.logger.debug(f"whoseæ§‹æ–‡: ä»–ã®ä¸»èªãªã— â†’ whoseå¥ã¯ä¸»èª")
                    return 'subject'
        
        # å‹•è©ã®å‰ã«ä»–ã®ä¸»èªãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        pre_verb_tokens = rel_tokens[1:verb_idx]  # é–¢ä¿‚ä»£åè©ã‚’é™¤ã
        has_other_subject = any(
            token['pos'] in ['NOUN', 'PRON', 'PROPN'] 
            for token in pre_verb_tokens
        )
        
        if has_other_subject:
            # å‹•è©å‰ã«ä»–ã®ä¸»èª â†’ é–¢ä¿‚ä»£åè©ã¯ç›®çš„èª
            self.logger.debug(f"é–¢ä¿‚ä»£åè©ã¯ç›®çš„èª: å‹•è©å‰ã«ä¸»èª {[t['text'] for t in pre_verb_tokens]}")
            return 'object'
        else:
            # å‹•è©å‰ã«ä¸»èªãªã— â†’ é–¢ä¿‚ä»£åè©ã¯ä¸»èª
            self.logger.debug(f"é–¢ä¿‚ä»£åè©ã¯ä¸»èª: å‹•è©å‰ã«ä¸»èªãªã—")
            return 'subject'

    def _analyze_relative_clause_structure_enhanced(self, rel_tokens: List[Dict], clause_type: str, rel_pronoun_role: str, antecedent_phrase: str = "") -> Dict:
        """é–¢ä¿‚ç¯€å†…éƒ¨æ§‹é€ è§£æ - å¼·åŒ–ç‰ˆ
        
        é–¢ä¿‚ä»£åè©ã®å½¹å‰²ã‚’è€ƒæ…®ã—ãŸæ­£ç¢ºãªã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ
        """
        if not rel_tokens:
            return {}
        
        self.logger.debug(f"å¼·åŒ–ç‰ˆé–¢ä¿‚ç¯€è§£æ: {[t['text'] for t in rel_tokens]} (å½¹å‰²: {rel_pronoun_role})")
        
        sub_slots = {}
        
        # ğŸ†• whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†
        if clause_type == 'whose_clause':
            self.logger.debug(f"ğŸ” whoseæ§‹æ–‡ç‰¹åˆ¥å‡¦ç†é–‹å§‹: {clause_type}")
            return self._analyze_whose_clause_structure(rel_tokens, antecedent_phrase, rel_pronoun_role)
        
        self.logger.debug(f"ğŸ” ä¸€èˆ¬çš„é–¢ä¿‚ç¯€å‡¦ç†: {clause_type}")
        
        # å‹•è©ã‚’ç‰¹å®š
        verb_token = None
        for i, token in enumerate(rel_tokens):
            if token['pos'] == 'VERB' and i > 0:  # é–¢ä¿‚ä»£åè©ä»¥å¤–
                verb_token = token
                sub_slots['sub-v'] = token['text']
                break
        
        if verb_token is None:
            return sub_slots
        
        # é–¢ä¿‚ä»£åè©ã®å½¹å‰²ãŒç›®çš„èªã®å ´åˆã€å‹•è©å‰ã®è¦ç´ ã‚’ sub-s ã«
        if rel_pronoun_role == 'object':
            for i, token in enumerate(rel_tokens):
                if i > 0 and token['pos'] in ['NOUN', 'PRON', 'PROPN'] and token != verb_token:
                    sub_slots['sub-s'] = token['text']
                    break
        
        # ä¿®é£¾èªã®æ¤œå‡ºï¼ˆADVã€å ´æ‰€å‰¯è©ã€å‰ç½®è©å¥ï¼‰
        for i, token in enumerate(rel_tokens):
            if i > 0 and token != verb_token and token['text'] not in [sub_slots.get('sub-s', '')]:
                # ğŸ†• å¼·åŒ–ã•ã‚ŒãŸä¿®é£¾èªæ¤œå‡º
                corrected_pos = self._get_human_corrected_pos(token)
                
                if (corrected_pos == 'ADV' or 
                    token['pos'] == 'ADV' or 
                    token['text'].lower() in ['there', 'here', 'everywhere', 'nowhere', 'fast', 'carefully', 'diligently', 'efficiently']):
                    
                    sub_slots['sub-m2'] = token['text']
                    self.logger.debug(f"ä¿®é£¾èªæ¤œå‡º: '{token['text']}' (pos={token['pos']}, corrected={corrected_pos}) â†’ sub-m2")
                    break
        
        return sub_slots

    def _analyze_whose_clause_structure(self, rel_tokens: List[Dict], antecedent_phrase: str = "", rel_pronoun_role: str = "subject") -> Dict:
        """whoseæ§‹æ–‡å°‚ç”¨ã®æ§‹é€ è§£æï¼ˆå†å¸°ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆï¼‰
        
        ãƒ‘ã‚¿ãƒ¼ãƒ³: whose + åè© + å‹•è© + è£œèª/ç›®çš„èª
        ä¾‹: "whose car is red" â†’ {'sub-s': 'The man whose car', 'sub-v': 'is', 'sub-c1': 'red'}
        """
        sub_slots = {}
        
        self.logger.debug(f"ğŸš€ whoseæ§‹æ–‡è§£æé–‹å§‹: {[t['text'] for t in rel_tokens]}, å…ˆè¡Œè©: '{antecedent_phrase}'")
        
        if len(rel_tokens) < 3:  # æœ€ä½é™: whose + åè© + å‹•è©
            self.logger.debug(f"âŒ whoseæ§‹æ–‡è¦ç´ ä¸è¶³: {len(rel_tokens)} < 3")
            return sub_slots
        
        try:
            # 1. whoseç›´å¾Œã®åè©ã‚’ç‰¹å®š
            whose_noun = None
            whose_noun_idx = -1
            for i, token in enumerate(rel_tokens):
                if i > 0 and token['pos'] in ['NOUN', 'PROPN']:  # whoseä»¥é™ã®æœ€åˆã®åè©
                    whose_noun = token['text']
                    whose_noun_idx = i
                    self.logger.debug(f"âœ… whoseåè©ç™ºè¦‹: '{whose_noun}' at {i}")
                    break
            
            if not whose_noun:
                self.logger.debug(f"âŒ whoseå¾Œã®åè©ãŒè¦‹ã¤ã‹ã‚‰ãªã„")
                return sub_slots
            
            # 2. é–¢ä¿‚ç¯€å†…ã®å‹•è©ã‚’ç‰¹å®šï¼ˆVERBã¾ãŸã¯AUXï¼‰
            verb_token = None
            verb_idx = -1
            for i, token in enumerate(rel_tokens):
                if i > whose_noun_idx and token['pos'] in ['VERB', 'AUX']:  # ğŸ†• AUXã‚‚å«ã‚ã‚‹
                    verb_token = token['text']
                    verb_idx = i
                    sub_slots['sub-v'] = verb_token
                    self.logger.debug(f"âœ… whoseå‹•è©ç™ºè¦‹: '{verb_token}' (pos={token['pos']}) at {i}")
                    break
            
            if not verb_token:
                self.logger.debug(f"âŒ whoseå¾Œã®å‹•è©ãŒè¦‹ã¤ã‹ã‚‰ãªã„")
                return sub_slots
            
            # 3. å…ˆè¡Œè©ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’å®‰å…¨ã«æ§‹ç¯‰ï¼ˆå†å¸°å›é¿ï¼‰
            if antecedent_phrase:
                whose_phrase = f"{antecedent_phrase} whose {whose_noun}"
            else:
                whose_phrase = f"whose {whose_noun}"
            
            # 4. é–¢ä¿‚ä»£åè©ã®å½¹å‰²ã«åŸºã¥ãé…ç½®
            if rel_pronoun_role == 'object':
                sub_slots['sub-o1'] = whose_phrase
                self.logger.debug(f"âœ… sub-o1æ§‹ç¯‰ï¼ˆç›®çš„èªï¼‰: '{whose_phrase}'")
                
                # whoseç¯€å†…ã®ä»–ã®ä¸»èªã‚’æ¤œå‡º
                for i, token in enumerate(rel_tokens):
                    if i > whose_noun_idx and i < verb_idx and token['pos'] in ['NOUN', 'PRON', 'PROPN']:
                        sub_slots['sub-s'] = token['text']
                        self.logger.debug(f"âœ… whoseç¯€å†…ä¸»èªç™ºè¦‹: '{token['text']}'")
                        break
            else:  # subject
                sub_slots['sub-s'] = whose_phrase
                self.logger.debug(f"âœ… sub-sæ§‹ç¯‰ï¼ˆä¸»èªï¼‰: '{whose_phrase}'")
            
            # 5. å‹•è©å¾Œã®è¦ç´ ã‚’è£œèª/ç›®çš„èªã¨ã—ã¦å‡¦ç†ï¼ˆç°¡ç´ åŒ–ï¼‰
            for i, token in enumerate(rel_tokens):
                if i > verb_idx and token['pos'] not in ['PUNCT']:
                    if token['pos'] in ['ADJ', 'NOUN', 'PROPN'] and 'sub-c1' not in sub_slots:
                        sub_slots['sub-c1'] = token['text']
                        self.logger.debug(f"âœ… sub-c1ç™ºè¦‹: '{token['text']}'")
                        break
                    elif token['pos'] == 'ADV' and 'sub-m2' not in sub_slots:
                        sub_slots['sub-m2'] = token['text']
                        self.logger.debug(f"âœ… sub-m2ç™ºè¦‹: '{token['text']}'")
                        break
            
            self.logger.debug(f"whoseæ§‹æ–‡è§£æçµæœ: {sub_slots}")
            return sub_slots
            
        except Exception as e:
            self.logger.error(f"whoseæ§‹æ–‡è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def _extract_full_antecedent_phrase(self, tokens: List[Dict], antecedent_idx: int) -> str:
        """å…ˆè¡Œè©å¥å…¨ä½“ã‚’æŠ½å‡ºï¼ˆé™å®šè©ã€å½¢å®¹è©ã‚’å«ã‚€ï¼‰- å†å¸°ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆ"""
        if antecedent_idx < 0 or antecedent_idx >= len(tokens):
            return ""
        
        try:
            # å…ˆè¡Œè©ã®å‰ã®ä¿®é£¾èªã‚’å«ã‚ã¦æŠ½å‡ºï¼ˆæœ€å¤§2èªå‰ã¾ã§ï¼‰
            phrase_tokens = []
            start_idx = max(0, antecedent_idx - 2)
            
            for i in range(start_idx, antecedent_idx + 1):
                if i < len(tokens):
                    token = tokens[i]
                    if token['pos'] in ['DET', 'ADJ', 'NOUN', 'PROPN']:
                        phrase_tokens.append(token['text'])
            
            result = ' '.join(phrase_tokens).strip()
            self.logger.debug(f"å…ˆè¡Œè©å¥æŠ½å‡º: idx={antecedent_idx} â†’ '{result}'")
            return result
            
        except Exception as e:
            self.logger.error(f"å…ˆè¡Œè©å¥æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ç´”ã«è©²å½“ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
            if 0 <= antecedent_idx < len(tokens):
                return tokens[antecedent_idx]['text']
            return ""
        verb_idx = None
        for i, token in enumerate(rel_tokens):
            if token['tag'].startswith('VB') and token['pos'] == 'VERB':
                verb_idx = i
                sub_slots['sub-v'] = token['text']
                break
        
        if verb_idx is None:
            return
        
        # é–¢ä¿‚ä»£åè©ã®å½¹å‰²ã‚’åˆ¤å®šï¼ˆä¸»èªã‹ç›®çš„èªã‹ï¼‰
        rel_pronoun_role = self._determine_relative_pronoun_role(rel_tokens, verb_idx)
        clause_type = rel_tokens[0]['text'].lower()
        
        # whoseã®å ´åˆã®ç‰¹åˆ¥å‡¦ç†
        if clause_type == 'whose':
            # whose + åè© ã®å½¢ã‚’æ¢ã™
            whose_phrase = rel_tokens[0]['text']  # "whose"
            next_idx = 1
            while next_idx < len(rel_tokens) and next_idx < verb_idx:
                if rel_tokens[next_idx]['pos'] in ['NOUN', 'PROPN']:
                    whose_phrase += f" {rel_tokens[next_idx]['text']}"
                    break
                next_idx += 1
            
            # whoseå¥ã®å½¹å‰²ã‚’åˆ¤å®š
            if rel_pronoun_role == 'subject':
                sub_slots['sub-s'] = whose_phrase
            else:
                sub_slots['sub-o1'] = whose_phrase
        
        # å‹•è©å‰ã®è¦ç´ ã‚’åˆ†æï¼ˆä¸»èªï¼‰
        pre_verb_tokens = rel_tokens[:verb_idx]
        for token in pre_verb_tokens:
            if token['pos'] in ['NOUN', 'PRON', 'PROPN']:
                # é–¢ä¿‚ä»£åè©ãŒç›®çš„èªã®å ´åˆã€ã“ã“ã«ä¸»èªãŒã‚ã‚‹
                if rel_pronoun_role == 'object' and clause_type != 'whose':
                    sub_slots['sub-s'] = token['text']
        
        # å‹•è©å¾Œã®è¦ç´ ã‚’åˆ†æ
        post_verb_tokens = rel_tokens[verb_idx + 1:]
        modifier_count = 0
        
        for token in post_verb_tokens:
            if token['pos'] == 'ADV' or token['tag'] == 'EX':
                # å‰¯è©ã¾ãŸã¯å­˜åœ¨there â†’ ä¿®é£¾èªã¨ã—ã¦å‡¦ç†ï¼ˆå„ªå…ˆï¼‰
                modifier_count += 1
                if modifier_count == 1:
                    sub_slots['sub-m2'] = token['text']
                elif modifier_count == 2:
                    sub_slots['sub-m3'] = token['text']
            elif token['pos'] in ['NOUN', 'PRON', 'PROPN'] and token['tag'] != 'EX':
                # åè©é¡ï¼ˆå­˜åœ¨thereä»¥å¤–ï¼‰ â†’ ç›®çš„èªã¨ã—ã¦å‡¦ç†
                if 'sub-o1' not in sub_slots:
                    sub_slots['sub-o1'] = token['text']
                elif 'sub-o2' not in sub_slots:
                    sub_slots['sub-o2'] = token['text']
            elif token['pos'] == 'ADJ':
                # å½¢å®¹è© â†’ è£œèªã¨ã—ã¦å‡¦ç†
                sub_slots['sub-c1'] = token['text']
    
    def _determine_relative_pronoun_role(self, rel_tokens: List[Dict], verb_idx: int) -> str:
        """é–¢ä¿‚ä»£åè©ãŒä¸»èªã‹ç›®çš„èªã‹ã‚’åˆ¤å®š"""
        clause_type = rel_tokens[0]['text'].lower()
        
        # whoseã®å ´åˆã®ç‰¹åˆ¥å‡¦ç†
        if clause_type == 'whose':
            # whose + åè©ãŒå‹•è©ã®å‰ã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            whose_noun_idx = None
            for i in range(1, min(verb_idx, len(rel_tokens))):
                if rel_tokens[i]['pos'] in ['NOUN', 'PROPN']:
                    whose_noun_idx = i
                    break
            
            if whose_noun_idx is not None:
                # whose + åè©ãŒå‹•è©å‰ã«ã‚ã‚‹ãªã‚‰ä¸»èª
                return 'subject'
            else:
                # whose + åè©ãŒå‹•è©å¾Œã«ã‚ã‚‹ãªã‚‰ç›®çš„èª
                return 'object'
        
        # å‹•è©ã®å‰ã«ä»£åè©ãƒ»åè©ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆwhoseä»¥å¤–ã®å ´åˆï¼‰
        pre_verb_tokens = rel_tokens[1:verb_idx]  # é–¢ä¿‚ä»£åè©è‡ªä½“ã¯é™¤å¤–
        has_subject_before_verb = any(
            token['pos'] in ['NOUN', 'PRON', 'PROPN'] 
            for token in pre_verb_tokens
        )
        
        if has_subject_before_verb:
            # å‹•è©å‰ã«ä¸»èªãŒã‚ã‚‹ãªã‚‰ã€é–¢ä¿‚ä»£åè©ã¯ç›®çš„èª
            return 'object'
        else:
            # å‹•è©å‰ã«ä¸»èªãŒãªã„ãªã‚‰ã€é–¢ä¿‚ä»£åè©ã¯ä¸»èª
            return 'subject'

    def _determine_chunk_grammatical_role(self, tokens: List[Dict], core_elements: Dict, relative_info: Dict) -> str:
        """é–¢ä¿‚ç¯€ã‚’å«ã‚€ã€Œã‹ãŸã¾ã‚Šã€ã®æ–‡æ³•çš„å½¹å‰²ã‚’å‹•è©ã¨ã®é–¢ä¿‚ã‹ã‚‰æ¨å®š
        
        äººé–“çš„æ–‡æ³•èªè­˜ï¼š
        - å‹•è©ã®å‰ã®ã€Œã‹ãŸã¾ã‚Šã€â†’ ä¸»èªï¼ˆSï¼‰
        - å‹•è©ã®å¾Œã®ã€Œã‹ãŸã¾ã‚Šã€â†’ ç›®çš„èªï¼ˆO1ï¼‰ã¾ãŸã¯è£œèªï¼ˆC1ï¼‰
        - æ–‡æœ«ã®ã€Œã‹ãŸã¾ã‚Šã€â†’ ä¿®é£¾èªï¼ˆMï¼‰
        """
        if not relative_info['found']:
            return None
            
        # å…ˆè¡Œè©ã®ä½ç½®ã¨å‹•è©ã®ä½ç½®ã‚’æ¯”è¼ƒ
        antecedent_idx = relative_info.get('antecedent_idx')
        verb_indices = core_elements.get('verb_indices', [])
        
        if antecedent_idx is None or not verb_indices:
            return None
            
        main_verb_idx = verb_indices[0] if verb_indices else len(tokens)
        
        # ä½ç½®é–¢ä¿‚ã«ã‚ˆã‚‹æ–‡æ³•çš„å½¹å‰²ã®åˆ¤å®š
        if antecedent_idx < main_verb_idx:
            # å‹•è©ã‚ˆã‚Šå‰ â†’ ä¸»èªã®å¯èƒ½æ€§ãŒé«˜ã„
            self.logger.debug(f"ã‹ãŸã¾ã‚Šä½ç½®åˆ¤å®š: å…ˆè¡Œè©{antecedent_idx} < å‹•è©{main_verb_idx} â†’ ä¸»èª(S)")
            return 'S'
        else:
            # å‹•è©ã‚ˆã‚Šå¾Œ â†’ ç›®çš„èªã¾ãŸã¯è£œèª
            # å‹•è©ã®æ€§è³ªã‹ã‚‰åˆ¤å®š
            if core_elements.get('verb') and core_elements['verb'].get('text'):
                verb_text = core_elements['verb']['text'].lower()
                if verb_text in ['is', 'are', 'was', 'were', 'am', 'be', 'become', 'seem']:
                    self.logger.debug(f"ã‹ãŸã¾ã‚Šä½ç½®åˆ¤å®š: beå‹•è© + å¾Œç¶š â†’ è£œèª(C1)")
                    return 'C1'
                else:
                    self.logger.debug(f"ã‹ãŸã¾ã‚Šä½ç½®åˆ¤å®š: ä¸€èˆ¬å‹•è© + å¾Œç¶š â†’ ç›®çš„èª(O1)")
                    return 'O1'
        
        return None

    def _determine_relative_slot_position(self, tokens: List[Dict], relative_info: Dict) -> str:
        """é–¢ä¿‚ç¯€ãŒã©ã®ã‚¹ãƒ­ãƒƒãƒˆä½ç½®ã«ã‚ã‚‹ã‹ã‚’åˆ¤å®š
        
        é‡è¦ï¼šé–¢ä¿‚ç¯€ã‚’å«ã‚€ã€Œã‹ãŸã¾ã‚Šã€ãŒã©ã®æ–‡æ³•çš„å½¹å‰²ã‚’æœãŸã™ã‹ã‚’åˆ¤å®šã—ã€
        ãã®ã‚¹ãƒ­ãƒƒãƒˆã‚’ç©ºã«ã—ã¦ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«ç§»å‹•ã•ã›ã‚‹
        """
        if not relative_info['found']:
            return None
            
        # æ—¢ã«å®Ÿè£…æ¸ˆã¿ã®ã€Œã‹ãŸã¾ã‚Šã€æ–‡æ³•çš„å½¹å‰²åˆ¤å®šã‚’ä½¿ç”¨
        # ã“ã®åˆ¤å®šã¯_assign_grammar_rolesã§å–å¾—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
        # ç¾åœ¨ã¯ç›´æ¥å®Ÿè£…
        antecedent_idx = relative_info.get('antecedent_idx')
        if antecedent_idx is None:
            return None
            
        # å‹•è©ä½ç½®ã‚’æ¢ã™
        main_verb_idx = None
        for i, token in enumerate(tokens):
            if token['pos'] in ['VERB', 'AUX'] and token['text'].lower() not in ['whose', 'which', 'who', 'that']:
                # é–¢ä¿‚ç¯€å†…ã®å‹•è©ã‚’é™¤å¤–ã—ã¦ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’ç‰¹å®š
                rel_start = relative_info.get('relative_pronoun_idx', -1)
                rel_end = relative_info.get('clause_end_idx', -1)
                if rel_start <= i <= rel_end:
                    continue  # é–¢ä¿‚ç¯€å†…ã®å‹•è©ã¯ã‚¹ã‚­ãƒƒãƒ—
                main_verb_idx = i
                break
        
        if main_verb_idx is None:
            return None
            
        # ä½ç½®é–¢ä¿‚ã«ã‚ˆã‚‹åˆ¤å®š
        if antecedent_idx < main_verb_idx:
            return 'S'  # ä¸»èª
        else:
            # å‹•è©ã®æ€§è³ªã‹ã‚‰åˆ¤å®š
            verb_token = tokens[main_verb_idx]
            if verb_token['text'].lower() in ['is', 'are', 'was', 'were', 'am', 'be', 'become', 'seem']:
                return 'C1'  # è£œèª
            else:
                return 'O1'  # ç›®çš„èª
    
    def _clean_relative_clause_from_text(self, text: str, relative_info: Dict) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é–¢ä¿‚ç¯€éƒ¨åˆ†ã‚’é™¤å»"""
        if not relative_info['found']:
            return text
        
        # ç°¡æ˜“å®Ÿè£…ï¼šé–¢ä¿‚ä»£åè©ä»¥é™ã‚’å‰Šé™¤
        rel_type = relative_info.get('type', '')
        if rel_type in text:
            parts = text.split(rel_type)
            return parts[0].strip()
        
        return text

    def _analyze_relative_clause_structure(self, rel_tokens: List[Dict], clause_type: str) -> Dict:
        """é–¢ä¿‚ç¯€å†…éƒ¨ã®æ§‹é€ ã‚’5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã§è§£æ
        
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆã®æ–¹æ³•ï¼š
        - 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®æŠ€è¡“ã‚’ãã®ã¾ã¾é–¢ä¿‚ç¯€å†…ã«é©ç”¨
        - é–¢ä¿‚ä»£åè©ã¨ã®çµåˆå•é¡Œã‚’ãƒ«ãƒ¼ãƒ«ã§è§£æ±º
        
        Args:
            rel_tokens: é–¢ä¿‚ç¯€ã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆ  
            clause_type: é–¢ä¿‚ç¯€ã®ç¨®é¡ï¼ˆwhose_clauseç­‰ï¼‰
            
        Returns:
            Dict: ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ 
        """
        if not rel_tokens:
            return {}
        
        self.logger.debug(f"5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã«ã‚ˆã‚‹é–¢ä¿‚ç¯€è§£æ: {[t['text'] for t in rel_tokens]}")
        
        # ğŸ†• 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ç›´æ¥é©ç”¨
        core_elements = self._identify_core_elements(rel_tokens)
        sentence_pattern = self._determine_sentence_pattern(core_elements, rel_tokens)
        
        self.logger.debug(f"é–¢ä¿‚ç¯€å†…æ–‡å‹: {sentence_pattern}")
        self.logger.debug(f"é–¢ä¿‚ç¯€å†…ã‚³ã‚¢è¦ç´ : ä¸»èª={core_elements.get('subject')}, å‹•è©={core_elements.get('verb')}")
        
        # ğŸ†• 5æ–‡å‹ã®çµæœã‚’ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«å¤‰æ›
        sub_slots = {}
        
        # ä¸»èªå‡¦ç†ï¼ˆé–¢ä¿‚ä»£åè©ã¨ã®çµåˆãƒ«ãƒ¼ãƒ«ï¼‰
        if core_elements.get('subject_indices'):
            rel_subject = core_elements['subject']
            if clause_type == 'whose_clause':
                # whose + åè© ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
                sub_slots['sub-s'] = f"whose {rel_subject}"
            else:
                # who, which, that ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
                sub_slots['sub-s'] = rel_tokens[0]['text']  # é–¢ä¿‚ä»£åè©è‡ªä½“
        
        # å‹•è©å‡¦ç†
        if core_elements.get('verb'):
            sub_slots['sub-v'] = core_elements['verb']['text']
        
        # 5æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãæ®‹ã‚Šè¦ç´ ã®å‡¦ç†
        if sentence_pattern == 'SVC':
            # beå‹•è© + è£œèª
            # æ®‹ã‚Šã®è¦ç´ ã‹ã‚‰è£œèªã‚’ç‰¹å®š
            used_indices = set(core_elements.get('subject_indices', []) + core_elements.get('verb_indices', []))
            for i, token in enumerate(rel_tokens):
                if i not in used_indices and token['pos'] in ['ADJ', 'NOUN', 'PROPN']:
                    sub_slots['sub-c1'] = token['text']
                    break
        elif sentence_pattern == 'SVO':
            # ä¸€èˆ¬å‹•è© + ç›®çš„èª
            used_indices = set(core_elements.get('subject_indices', []) + core_elements.get('verb_indices', []))
            for i, token in enumerate(rel_tokens):
                if i not in used_indices and token['pos'] in ['NOUN', 'PROPN']:
                    sub_slots['sub-o1'] = token['text']
                    break
        elif sentence_pattern == 'SV':
            # ğŸ†• è‡ªå‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³ (SV) + ä¿®é£¾èª
            # whoç¯€ã®ä¿®é£¾èªï¼ˆå‰¯è©ï¼‰ã‚’sub-m2ã¨ã—ã¦ç‰¹å®š
            used_indices = set(core_elements.get('subject_indices', []) + core_elements.get('verb_indices', []))
            for i, token in enumerate(rel_tokens):
                if i not in used_indices:
                    # ğŸ†• äººé–“çš„å“è©åˆ¤å®šã‚’é©ç”¨
                    corrected_pos = self._get_human_corrected_pos(token)
                    if corrected_pos in ['ADV', 'ADJ']:
                        sub_slots['sub-m2'] = token['text']
                        self.logger.debug(f"whoç¯€ä¿®é£¾èªæ¤œå‡º: '{token['text']}' â†’ sub-m2 (corrected_pos={corrected_pos})")
                        break
        
        # ğŸ†• ä¸€èˆ¬çš„ãªä¿®é£¾èªæ¤œå‡ºï¼ˆæ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã«é–¢ä¿‚ãªãï¼‰
        if 'sub-m2' not in sub_slots and clause_type == 'who_clause':
            # whoç¯€ç‰¹æœ‰ã®ä¿®é£¾èªæ¤œå‡º
            used_indices = set(core_elements.get('subject_indices', []) + core_elements.get('verb_indices', []))
            if 'sub-c1' in sub_slots:
                # è£œèªãŒã‚ã‚‹å ´åˆã¯è£œèªã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚‚é™¤å¤–
                for i, token in enumerate(rel_tokens):
                    if token['text'] == sub_slots['sub-c1']:
                        used_indices.add(i)
                        break
            
            for i, token in enumerate(rel_tokens):
                if i not in used_indices and i > 0:  # é–¢ä¿‚ä»£åè©ã‚’é™¤å¤–
                    corrected_pos = self._get_human_corrected_pos(token)
                    if corrected_pos in ['ADV', 'ADJ']:
                        sub_slots['sub-m2'] = token['text']
                        self.logger.debug(f"whoç¯€è¿½åŠ ä¿®é£¾èªæ¤œå‡º: '{token['text']}' â†’ sub-m2 (corrected_pos={corrected_pos})")
                        break
        
        return sub_slots
    
    def _find_verb_in_relative_clause(self, rel_tokens: List[Dict]) -> Optional[int]:
        """é–¢ä¿‚ç¯€å†…ã®å‹•è©ã‚’ç‰¹å®š
        
        5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®å‹•è©æ¤œå‡ºæŠ€è¡“ã‚’é©ç”¨
        """
        for i, token in enumerate(rel_tokens):
            if token['tag'].startswith('VB') and token['pos'] == 'VERB':
                return i
        return None
    
    def _analyze_post_verb_elements_in_relative(self, rel_tokens: List[Dict], verb_idx: int, sub_slots: Dict):
        """é–¢ä¿‚ç¯€å†…ã®å‹•è©å¾Œè¦ç´ ã‚’è§£æ
        
        5æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜æŠ€è¡“ã‚’é©ç”¨
        """
        if verb_idx >= len(rel_tokens) - 1:
            return
        
        post_verb_tokens = rel_tokens[verb_idx + 1:]
        
        # ç›®çš„èªã€è£œèªã€ä¿®é£¾èªã‚’é †æ¬¡è§£æ
        processed_positions = set()
        
        for i, token in enumerate(post_verb_tokens):
            if i in processed_positions:
                continue
                
            if token['pos'] in ['NOUN', 'PRON', 'PROPN']:
                # åè©é¡ â†’ ç›®çš„èªã¨ã—ã¦å‡¦ç†
                if 'sub-o1' not in sub_slots:
                    sub_slots['sub-o1'] = token['text']
                elif 'sub-o2' not in sub_slots:
                    sub_slots['sub-o2'] = token['text']
                processed_positions.add(i)
            elif token['pos'] == 'ADJ':
                # å½¢å®¹è© â†’ è£œèªã¨ã—ã¦å‡¦ç†
                sub_slots['sub-c1'] = token['text']
                processed_positions.add(i)
            elif token['pos'] == 'ADV':
                # å‰¯è© â†’ ä¿®é£¾èªã¨ã—ã¦å‡¦ç†
                if 'sub-m' not in sub_slots:
                    sub_slots['sub-m'] = token['text']
                else:
                    sub_slots['sub-m'] += f" {token['text']}"
                processed_positions.add(i)
        
        # é€£ç¶šã™ã‚‹è¦ç´ ã‚’ã¾ã¨ã‚ã‚‹ï¼ˆç°¡ç´ åŒ–ç‰ˆï¼‰
        self._consolidate_relative_clause_elements_simple(rel_tokens, verb_idx, sub_slots)

    def _consolidate_relative_clause_elements_simple(self, rel_tokens: List[Dict], verb_idx: int, sub_slots: Dict):
        """é–¢ä¿‚ç¯€å†…ã®è¦ç´ ã‚’çµ±åˆï¼ˆç°¡ç´ ç‰ˆï¼‰"""
        # ç¾åœ¨ã¯åŸºæœ¬çš„ãªé‡è¤‡é™¤å»ã®ã¿
        if 'sub-m' in sub_slots:
            # é‡è¤‡ã—ãŸä¿®é£¾èªã‚’é™¤å»
            m_words = sub_slots['sub-m'].split()
            unique_words = []
            for word in m_words:
                if word not in unique_words:
                    unique_words.append(word)
            sub_slots['sub-m'] = ' '.join(unique_words)

    def _consolidate_relative_clause_elements(self, rel_tokens: List[Dict], verb_idx: int, sub_slots: Dict):
        """é–¢ä¿‚ç¯€å†…ã®è¦ç´ ã‚’çµ±åˆ
        
        é€£ç¶šã™ã‚‹åè©å¥ã‚„ä¿®é£¾èªå¥ã‚’ä¸€ã¤ã«ã¾ã¨ã‚ã‚‹
        """
        if verb_idx >= len(rel_tokens) - 1:
            return
        
        post_verb_tokens = rel_tokens[verb_idx + 1:]
        current_phrase = []
        current_type = None
        
        for token in post_verb_tokens:
            if token['pos'] in ['NOUN', 'PRON', 'PROPN', 'DET', 'ADJ']:
                if current_type == 'noun_phrase':
                    current_phrase.append(token['text'])
                else:
                    # æ–°ã—ã„åè©å¥ã®é–‹å§‹
                    if current_phrase and current_type:
                        self._assign_phrase_to_subslot(current_phrase, current_type, sub_slots)
                    current_phrase = [token['text']]
                    current_type = 'noun_phrase'
            elif token['pos'] in ['ADV', 'ADP']:
                if current_type == 'adverbial_phrase':
                    current_phrase.append(token['text'])
                else:
                    # æ–°ã—ã„å‰¯è©å¥ã®é–‹å§‹
                    if current_phrase and current_type:
                        self._assign_phrase_to_subslot(current_phrase, current_type, sub_slots)
                    current_phrase = [token['text']]
                    current_type = 'adverbial_phrase'
            else:
                # ãã®ä»–ã®å“è©ã§å¥ãŒçµ‚äº†
                if current_phrase and current_type:
                    self._assign_phrase_to_subslot(current_phrase, current_type, sub_slots)
                current_phrase = []
                current_type = None
        
        # æœ€å¾Œã®å¥ã‚’å‡¦ç†
        if current_phrase and current_type:
            self._assign_phrase_to_subslot(current_phrase, current_type, sub_slots)
    
    def _assign_phrase_to_subslot(self, phrase: List[str], phrase_type: str, sub_slots: Dict):
        """å¥ã‚’ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«å‰²ã‚Šå½“ã¦"""
        phrase_text = ' '.join(phrase)
        
        if phrase_type == 'noun_phrase':
            if 'sub-o1' not in sub_slots:
                sub_slots['sub-o1'] = phrase_text
            elif 'sub-o2' not in sub_slots:
                sub_slots['sub-o2'] = phrase_text
            else:
                # è£œèªã¨ã—ã¦å‡¦ç†
                sub_slots['sub-c1'] = phrase_text
        elif phrase_type == 'adverbial_phrase':
            if 'sub-m' not in sub_slots:
                sub_slots['sub-m'] = phrase_text
            else:
                sub_slots['sub-m'] += f" {phrase_text}"

    def generate_ui_format(self, sentence: str, example_id: str = "test") -> List[Dict]:
        """UIå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç”Ÿæˆ
        
        Args:
            sentence: è§£æã™ã‚‹æ–‡
            example_id: ä¾‹æ–‡ID
            
        Returns:
            UIå½¢å¼ã®ã‚¹ãƒ­ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿é…åˆ—
        """
        # æ–‡æ³•è§£æå®Ÿè¡Œ
        result = self.analyze_sentence(sentence)
        
        ui_data = []
        slots = result.get('slots', {})
        sub_slots = result.get('sub_slots', {})
        parent_slot = sub_slots.get('_parent_slot', '')
        
        # ãƒ¡ã‚¤ãƒ³ ã‚¹ãƒ­ãƒƒãƒˆã®å‡¦ç†
        slot_order = ['M1', 'S', 'Aux', 'M2', 'V', 'C1', 'O1', 'O2', 'C2', 'M3']
        
        for slot_name in slot_order:
            # é–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆã¯ç©ºã‚¹ãƒ­ãƒƒãƒˆã‚‚å«ã‚ã‚‹
            has_subslots = (slot_name == parent_slot and sub_slots and any(k.startswith('sub-') for k in sub_slots))
            if slot_name in slots and (slots[slot_name] or has_subslots):
                # ğŸ”§ Step5: ã‚¹ãƒ­ãƒƒãƒˆå†…display_orderï¼ˆãƒªã‚»ãƒƒãƒˆæ–¹å¼ï¼‰
                slot_display_order = 0
                
                # é–¢ä¿‚ç¯€ãŒã‚ã‚‹å ´åˆã¯ç©ºæ–‡å­—
                phrase = "" if has_subslots else slots[slot_name]
                phrase_type = "clause" if has_subslots else "word"
                
                ui_data.append({
                    "æ§‹æ–‡ID": "",
                    "V_group_key": result.get('pattern', ''),
                    "ä¾‹æ–‡ID": example_id,
                    "Slot": slot_name,
                    "SlotPhrase": phrase,
                    "SlotText": "",
                    "PhraseType": phrase_type,
                    "SubslotID": "",
                    "SubslotElement": "",
                    "SubslotText": "",
                    "Slot_display_order": slot_order.index(slot_name) + 1,
                    "display_order": slot_display_order,
                    "QuestionType": ""
                })
                slot_display_order += 1
                
                # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®è¿½åŠ 
                if has_subslots:
                    # ğŸ”§ Step5.1: ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®æ­£ã—ã„é †åºã‚’å®šç¾©
                    subslot_order = ['sub-s', 'sub-v', 'sub-o1', 'sub-o2', 'sub-c1', 'sub-c2', 'sub-m1', 'sub-m2', 'sub-m3']
                    
                    # é †åºã«å¾“ã£ã¦ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã‚’è¿½åŠ 
                    for sub_slot_id in subslot_order:
                        if sub_slot_id in sub_slots and sub_slots[sub_slot_id]:
                            ui_data.append({
                                "æ§‹æ–‡ID": "",
                                "V_group_key": result.get('pattern', ''),
                                "ä¾‹æ–‡ID": example_id,
                                "Slot": slot_name,
                                "SlotPhrase": "",
                                "SlotText": "",
                                "PhraseType": "",
                                "SubslotID": sub_slot_id,
                                "SubslotElement": sub_slots[sub_slot_id],
                                "SubslotText": "",
                                "Slot_display_order": slot_order.index(slot_name) + 1,
                                "display_order": slot_display_order,
                                "QuestionType": ""
                            })
                            slot_display_order += 1
        
        return ui_data

    # =============================================================================
    # ğŸ”¥ Phase 1.0: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  (from Stanza Asset Migration)
    # =============================================================================
    
    def _initialize_basic_handlers(self):
        """åŸºæœ¬ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®åˆæœŸåŒ–"""
        basic_handlers = [
            'basic_five_pattern',     # åŸºæœ¬5æ–‡å‹
            'relative_clause',        # é–¢ä¿‚ç¯€  
            'passive_voice',          # å—å‹•æ…‹
            'comparative_superlative', # æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´š
            'auxiliary_complex',      # åŠ©å‹•è©
            # 'adverbial_modifier',   # å‰¯è©ãƒ»ä¿®é£¾èª (æœªå®Ÿè£…ã®ãŸã‚ä¸€æ™‚å‰Šé™¤)
        ]
        
        for handler in basic_handlers:
            self.add_handler(handler)
        
        self.logger.info(f"åŸºæœ¬ãƒãƒ³ãƒ‰ãƒ©ãƒ¼åˆæœŸåŒ–å®Œäº†: {len(self.active_handlers)}å€‹")
    
    def add_handler(self, handler_name: str):
        """ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ ï¼ˆPhaseåˆ¥é–‹ç™ºç”¨ï¼‰"""
        if handler_name not in self.active_handlers:
            self.active_handlers.append(handler_name)
            self.logger.info(f"Handlerè¿½åŠ : {handler_name}")
        else:
            self.logger.warning(f"âš ï¸ Handler already active: {handler_name}")
    
    def remove_handler(self, handler_name: str):
        """ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å‰Šé™¤"""
        if handler_name in self.active_handlers:
            self.active_handlers.remove(handler_name)
            self.logger.info(f"â– Handlerå‰Šé™¤: {handler_name}")
    
    def list_active_handlers(self) -> List[str]:
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ä¸€è¦§"""
        return self.active_handlers.copy()
    
    # =================================
    # Phase 2: é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè£…
    # =================================
    
    def _handle_relative_clause(self, sentence: str, doc, current_result: Dict) -> Dict:
        """
        ğŸ¯ Phase 2: é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ ç”¨ï¼‰
        
        æ©Ÿèƒ½:
        1. é–¢ä¿‚ç¯€ã®æ¤œå‡ºã¨åˆ†é›¢
        2. ä¸»æ–‡ã¨ã‚µãƒ–å¥ã®æ§‹é€ åŒ–
        3. relative_clause_info ã®ç”Ÿæˆï¼ˆä»–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã¨ã®é€£æºç”¨ï¼‰
        
        Rephraseãƒ«ãƒ¼ãƒ«:
        - é–¢ä¿‚ç¯€è¦ç´ ã¯ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®
        - ä¸»æ–‡è¦ç´ ã¯ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®
        - æ§‹é€ çš„åˆ†é›¢ã«ã‚ˆã‚Š100%ç²¾åº¦ä¿è¨¼
        """
        try:
            print(f"ğŸ” Executing relative_clause handler for: {sentence}")
            
            # æ—¢å­˜ã®é–¢ä¿‚ç¯€æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
            tokens = [{'text': token.text, 'pos': token.pos_, 'dep': token.dep_, 'head': token.head, 'lemma': token.lemma_} for token in doc]
            relative_info = self._detect_relative_clause(tokens, sentence)
            
            if not relative_info.get('found', False):
                print(f"ğŸ” é–¢ä¿‚ç¯€æœªæ¤œå‡º: {sentence}")
                return None
            
            print(f"ğŸ”¥ é–¢ä¿‚ç¯€æ¤œå‡º: ã‚¿ã‚¤ãƒ—={relative_info.get('type', 'unknown')}, ä¿¡é ¼åº¦={relative_info.get('confidence', 0)}")
            
            # é–¢ä¿‚ç¯€å‡¦ç†
            original_tokens = tokens.copy()
            processed_tokens, sub_slots = self._process_relative_clause(original_tokens, relative_info)
            
            # ä¸»æ–‡ã¨ã‚µãƒ–å¥ã®åˆ†é›¢ï¼ˆæ–°æ©Ÿèƒ½ï¼‰
            main_sentence = self._extract_main_sentence(sentence, relative_info)
            sub_sentences = self._extract_sub_sentences(sentence, relative_info)
            
            print(f"ğŸ” é–¢ä¿‚ç¯€åˆ†é›¢çµæœ: ä¸»æ–‡='{main_sentence}', ã‚µãƒ–å¥={len(sub_sentences)}å€‹")
            
            return {
                'slots': {},  # é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è‡ªä½“ã¯ã‚¹ãƒ­ãƒƒãƒˆã‚’ç”Ÿæˆã—ãªã„
                'sub_slots': sub_slots,
                'relative_clause_info': {
                    'found': True,
                    'main_sentence': main_sentence,
                    'sub_sentences': sub_sentences,
                    'type': relative_info.get('type', 'unknown'),
                    'confidence': relative_info.get('confidence', 0)
                },
                'grammar_info': {
                    'patterns': [{
                        'type': 'relative_clause_2stage',
                        'main_sentence': main_sentence,
                        'sub_sentences_count': len(sub_sentences),
                        'detection_method': 'é–¢ä¿‚ç¯€å¯¾å¿œ2æ®µéšå‡¦ç†',
                        'clause_type': relative_info.get('type', 'unknown')
                    }],
                    'handler_success': True,
                    'processing_notes': f"Relative clause 2-stage: main='{main_sentence}', sub={len(sub_sentences)}"
                }
            }
            
        except Exception as e:
            self.logger.error(f"é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _extract_main_sentence(self, sentence: str, relative_info: Dict) -> str:
        """é–¢ä¿‚ç¯€ã‚’é™¤ã„ãŸä¸»æ–‡ã‚’æŠ½å‡º"""
        if not relative_info.get('found', False):
            return sentence
        
        rel_type = relative_info.get('type', '')
        
        # whichç¯€å‡¦ç†: "The car which was crashed is red." -> "The car is red."
        if 'which' in sentence and rel_type == 'which_clause':
            parts = sentence.split(' which ')
            if len(parts) == 2:
                before = parts[0]  # "The car"
                after_which = parts[1]  # "was crashed is red."
                
                # é–¢ä¿‚ç¯€çµ‚äº†ã‚’æ¤œå‡ºï¼ˆä¸»æ–‡å‹•è©æ¤œç´¢ï¼‰
                words_after = after_which.split()
                main_verb_start = self._find_main_verb_start(words_after)
                
                if main_verb_start >= 0:
                    main_part = ' '.join(words_after[main_verb_start:])
                    result = f"{before} {main_part}"
                    return result
        
        # thatç¯€å‡¦ç†: "The book that was written is famous." -> "The book is famous."
        elif 'that' in sentence and rel_type == 'that_clause':
            parts = sentence.split(' that ')
            if len(parts) == 2:
                before = parts[0]  # "The book"  
                after_that = parts[1]  # "was written is famous."
                
                # é–¢ä¿‚ç¯€çµ‚äº†ã‚’æ¤œå‡ºï¼ˆä¸»æ–‡å‹•è©æ¤œç´¢ï¼‰
                words_after = after_that.split()
                main_verb_start = self._find_main_verb_start(words_after)
                
                if main_verb_start >= 0:
                    main_part = ' '.join(words_after[main_verb_start:])
                    return f"{before} {main_part}"
        
        return sentence
    
    def _find_main_verb_start(self, words_after: List[str]) -> int:
        """é–¢ä¿‚ç¯€å¾Œã®å˜èªåˆ—ã‹ã‚‰ä¸»æ–‡å‹•è©ã®é–‹å§‹ä½ç½®ã‚’æ¤œå‡º"""
        past_participles = ['been', 'gone', 'done', 'made', 'said', 'written', 'crashed', 'sent', 
                           'taken', 'given', 'seen', 'heard', 'found', 'built', 'bought']
        
        # é€šå¸¸å‹•è©ï¼ˆå—å‹•æ…‹ã§ãªã„è‡ªå‹•è©ãƒ»ä»–å‹•è©ï¼‰
        main_verbs = ['arrived', 'came', 'went', 'left', 'stayed', 'lived', 'died', 'worked', 
                     'studied', 'played', 'ran', 'walked', 'stood', 'sat', 'fell', 'rose']
        
        print(f"ğŸ” DEBUG: _find_main_verb_start words_after={words_after}")
        
        in_relative_clause = True
        for i, word in enumerate(words_after):
            # å¥èª­ç‚¹ã‚’é™¤å»ã—ã¦ç´”ç²‹ãªå˜èªã‚’å–å¾—
            clean_word = word.rstrip('.,!?;:').lower()
            print(f"ğŸ” DEBUG: æ¤œæŸ»ä¸­ i={i}, word='{word}', clean_word='{clean_word}'")
            
            # beå‹•è©ã®å ´åˆ
            if clean_word in ['is', 'are', 'was', 'were', 'will', 'would', 'can', 'could', 'should']:
                # å—å‹•æ…‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
                if i + 1 < len(words_after):
                    next_word = words_after[i + 1].rstrip('.,!?;:').lower()
                    is_passive = (next_word.endswith('ed') or 
                                next_word.endswith('en') or 
                                next_word in past_participles)
                    
                    # SVCæ§‹æ–‡ï¼ˆbeå‹•è©+å½¢å®¹è©ï¼‰ã‚’ãƒã‚§ãƒƒã‚¯
                    adjectives = ['red', 'blue', 'green', 'famous', 'beautiful', 'happy', 'sad', 'big', 'small', 
                                 'good', 'bad', 'hot', 'cold', 'new', 'old', 'young', 'smart', 'stupid']
                    is_svc = next_word in adjectives
                    
                    print(f"ğŸ” DEBUG: beå‹•è©'{clean_word}' next_word='{next_word}', is_passive={is_passive}, is_svc={is_svc}")
                    
                    if in_relative_clause and is_passive and not is_svc:
                        # é–¢ä¿‚ç¯€å†…å—å‹•æ…‹ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãŸã ã—SVCæ§‹æ–‡ã¯é™¤ãï¼‰
                        print(f"ğŸ” DEBUG: é–¢ä¿‚ç¯€å†…å—å‹•æ…‹ã‚¹ã‚­ãƒƒãƒ— i={i}")
                        continue
                    else:
                        # ä¸»æ–‡ã®å‹•è©ã‚’ç™ºè¦‹ï¼ˆSVCæ§‹æ–‡ã‚‚å«ã‚€ï¼‰
                        print(f"ğŸ” DEBUG: ä¸»æ–‡beå‹•è©ç™ºè¦‹ i={i}")
                        return i
                else:
                    # æ–‡æœ«ã®å ´åˆã¯ä¸»æ–‡å‹•è©
                    print(f"ğŸ” DEBUG: æ–‡æœ«beå‹•è©ç™ºè¦‹ i={i}")
                    return i
            
            # é€šå¸¸å‹•è©ã®å ´åˆï¼ˆarrived, came, etc.ï¼‰
            elif clean_word in main_verbs:
                # ã“ã‚ŒãŒé–¢ä¿‚ç¯€å¤–ã®ä¸»æ–‡å‹•è©ã®å¯èƒ½æ€§
                print(f"ğŸ” DEBUG: é€šå¸¸å‹•è©æ¤œå‡º clean_word='{clean_word}', i={i}")
                if not in_relative_clause or self._is_likely_main_verb(clean_word, words_after, i):
                    print(f"ğŸ” DEBUG: ä¸»æ–‡å‹•è©ã¨ã—ã¦èªå®š i={i}")
                    return i
                    
            # éå»å½¢å‹•è©ã®æ¤œå‡ºï¼ˆ-edèªå°¾ã ãŒéå»åˆ†è©ã§ãªã„å ´åˆï¼‰
            elif clean_word.endswith('ed') and clean_word not in past_participles:
                print(f"ğŸ” DEBUG: éå»å½¢å‹•è©æ¤œå‡º clean_word='{clean_word}', i={i}")
                if not in_relative_clause or self._is_likely_main_verb(clean_word, words_after, i):
                    print(f"ğŸ” DEBUG: ä¸»æ–‡éå»å½¢å‹•è©ã¨ã—ã¦èªå®š i={i}")
                    return i
        
        print(f"ğŸ” DEBUG: ä¸»æ–‡å‹•è©æœªç™ºè¦‹ return -1")
        return -1
    
    def _is_likely_main_verb(self, word: str, words_after: List[str], position: int) -> bool:
        """å˜èªãŒä¸»æ–‡å‹•è©ã§ã‚ã‚‹å¯èƒ½æ€§ã‚’åˆ¤å®š"""
        # ç‰¹å®šã®å‹•è©ã¯ä¸»æ–‡å‹•è©ã®å¯èƒ½æ€§ãŒé«˜ã„
        main_verb_indicators = ['arrived', 'came', 'went', 'left', 'stayed', 'lived', 'died']
        
        if word in main_verb_indicators:
            return True
        
        # beå‹•è©+å½¢å®¹è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡ºï¼ˆSVCæ§‹é€ ï¼‰
        if word in ['is', 'are', 'was', 'were'] and position + 1 < len(words_after):
            next_word = words_after[position + 1].rstrip('.,!?;:').lower()
            # å½¢å®¹è©ãƒªã‚¹ãƒˆ
            adjectives = ['red', 'blue', 'green', 'famous', 'beautiful', 'happy', 'sad', 'big', 'small', 
                         'good', 'bad', 'hot', 'cold', 'new', 'old', 'young', 'smart', 'stupid']
            
            if next_word in adjectives:
                return True
        
        # ä½ç½®çš„åˆ¤æ–­ï¼šé–¢ä¿‚ç¯€ï¼ˆå—å‹•æ…‹ï¼‰ã®å¾Œã«æ¥ã‚‹å‹•è©ã¯ä¸»æ–‡ã®å¯èƒ½æ€§ãŒé«˜ã„
        if position > 0:
            prev_words = words_after[:position]
            # å‰ã«å—å‹•æ…‹ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            for i in range(len(prev_words) - 1):
                if (prev_words[i].lower() in ['was', 'were'] and 
                    prev_words[i + 1] in ['written', 'sent', 'crashed', 'taken']):
                    return True
        
        return False
    
    def _extract_sub_sentences(self, sentence: str, relative_info: Dict) -> List[str]:
        """é–¢ä¿‚ç¯€éƒ¨åˆ†ã‚’ã‚µãƒ–å¥ã¨ã—ã¦æŠ½å‡º"""
        if not relative_info.get('found', False):
            return []
        
        rel_type = relative_info.get('type', '')
        
        # whichç¯€å‡¦ç†: "The car which was crashed is red." -> ["which was crashed"]
        if 'which' in sentence and rel_type == 'which_clause':
            parts = sentence.split(' which ')
            if len(parts) == 2:
                after_which = parts[1]  # "was crashed is red."
                words_after = after_which.split()
                
                # é–¢ä¿‚ç¯€ã®çµ‚äº†ã‚’ç‰¹å®šï¼ˆä¸»æ–‡ã®å‹•è©ã¾ã§ï¼‰
                rel_clause_end = self._find_main_verb_start(words_after)
                
                if rel_clause_end > 0:
                    rel_clause = ' '.join(words_after[:rel_clause_end])
                    return [f"which {rel_clause}"]
        
        # thatç¯€å‡¦ç†: "The book that was written is famous." -> ["that was written"]
        elif 'that' in sentence and rel_type == 'that_clause':
            parts = sentence.split(' that ')
            if len(parts) == 2:
                after_that = parts[1]  # "was written is famous."
                words_after = after_that.split()
                
                # é–¢ä¿‚ç¯€ã®çµ‚äº†ã‚’ç‰¹å®šï¼ˆä¸»æ–‡ã®å‹•è©ã¾ã§ï¼‰
                rel_clause_end = self._find_main_verb_start(words_after)
                
                if rel_clause_end > 0:
                    rel_clause = ' '.join(words_after[:rel_clause_end])
                    return [f"that {rel_clause}"]
        
        return []
    
    # =================================
    # Phase 2: å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè£…
    # =================================
    
    def _handle_passive_voice(self, sentence: str, doc, current_result: Dict) -> Optional[Dict]:
        """
        å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ (é–¢ä¿‚ç¯€å¯¾å¿œ2æ®µéšå‡¦ç†)
        
        ğŸ¯ è¨­è¨ˆä»•æ§˜: å…¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å…±é€šã®é–¢ä¿‚ç¯€å¯¾å¿œã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
        1. é–¢ä¿‚ç¯€åˆ†é›¢æƒ…å ±ã‚’å–å¾—
        2. ä¸»æ–‡ã¨ã‚µãƒ–å¥ã‚’åˆ¥ã€…ã«å—å‹•æ…‹å‡¦ç†
        3. çµæœã‚’é©åˆ‡ãªã‚¹ãƒ­ãƒƒãƒˆï¼ˆmain_slots/sub_slotsï¼‰ã«é…ç½®
        
        äººé–“çš„èªè­˜ãƒ‘ã‚¿ãƒ¼ãƒ³:
        - beå‹•è© + éå»åˆ†è© â†’ å—å‹•æ…‹
        - èªå½™çš„å½¢æ…‹è«–åˆ†æã«ã‚ˆã‚‹éå»åˆ†è©åˆ¤å®š
        - byå¥ï¼ˆè¡Œç‚ºè€…ï¼‰ã®æ¤œå‡ºã¨é…ç½®
        
        Rephraseãƒ«ãƒ¼ãƒ«:
        - beå‹•è©: Aux ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®ï¼ˆå—å‹•æ…‹ã®beå‹•è©ã¯Auxã«é…ç½®ï¼‰
        - éå»åˆ†è©: V ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®
        - byå¥: M2 ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®ï¼ˆRephraseå‰¯è©ãƒ«ãƒ¼ãƒ«ï¼šå˜ç‹¬å‰¯è©å¥â†’M2ï¼‰
        """
        try:
            print(f"ğŸ” Executing passive_voice handler for: {sentence}")
            
            # ğŸ¯ Step 1: é–¢ä¿‚ç¯€åˆ†é›¢æƒ…å ±ã®å–å¾—
            relative_info = current_result.get('relative_clause_info', {})
            main_sentence = relative_info.get('main_sentence', sentence)
            sub_sentences = relative_info.get('sub_sentences', [])
            
            result_slots = {}
            result_sub_slots = current_result.get('sub_slots', {})
            consumed_tokens = []
            
            print(f"ğŸ” é–¢ä¿‚ç¯€åˆ†é›¢: ä¸»æ–‡='{main_sentence}', ã‚µãƒ–å¥={len(sub_sentences)}å€‹")
            
            # ğŸ¯ Step 2a: ä¸»æ–‡ã®å—å‹•æ…‹å‡¦ç†
            print(f"ğŸ” ä¸»æ–‡å—å‹•æ…‹æ¤œå‡ºå¯¾è±¡: '{main_sentence}'")
            main_passive = self._detect_passive_in_text(main_sentence, doc)
            if main_passive and main_passive['found']:
                print(f"ğŸ”¥ ä¸»æ–‡å—å‹•æ…‹æ¤œå‡º: {main_passive['be_verb']} + {main_passive['past_participle']}")
                main_slots = self._create_passive_slots(main_passive)
                result_slots.update(main_slots)
                
                # Token consumption for main sentence
                if main_passive.get('by_agent'):
                    main_consumed = self._get_tokens_for_phrase(main_passive['by_agent'], doc)
                    consumed_tokens.extend(main_consumed)
            else:
                print(f"ğŸ” ä¸»æ–‡å—å‹•æ…‹æœªæ¤œå‡º: '{main_sentence}'")
            
            # ğŸ¯ Step 2b: ã‚µãƒ–å¥ã®å—å‹•æ…‹å‡¦ç†
            for i, sub_sentence in enumerate(sub_sentences):
                sub_passive = self._detect_passive_in_text(sub_sentence, doc)
                if sub_passive and sub_passive['found']:
                    print(f"ğŸ”¥ ã‚µãƒ–å¥{i+1}å—å‹•æ…‹æ¤œå‡º: {sub_passive['be_verb']} + {sub_passive['past_participle']}")
                    
                    # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®
                    if sub_passive['be_verb']:
                        result_sub_slots[f'sub-aux'] = sub_passive['be_verb']
                    if sub_passive['past_participle']:
                        result_sub_slots[f'sub-v'] = sub_passive['past_participle']
                    if sub_passive.get('by_agent'):
                        result_sub_slots[f'sub-m2'] = sub_passive['by_agent']
                        
                        # Token consumption for sub sentence
                        sub_consumed = self._get_tokens_for_phrase(sub_passive['by_agent'], doc)
                        consumed_tokens.extend(sub_consumed)
            
            # çµæœåˆ¤å®šã‚’ç°¡ç´ åŒ–ï¼ˆChatGPT5ãƒ‡ãƒãƒƒã‚°ï¼‰
            has_main_passive = bool(result_slots)
            has_sub_passive = any(key.startswith('sub-aux') or key.startswith('sub-v') for key in result_sub_slots.keys())
            
            if not has_main_passive and not has_sub_passive:
                print(f"ğŸ” å—å‹•æ…‹ãƒ‘ã‚¿ãƒ¼ãƒ³æœªæ¤œå‡º: {sentence}")
                return None
            
            print(f"ğŸ”¥ å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼æˆåŠŸ: ä¸»æ–‡å—å‹•æ…‹={has_main_passive}, ã‚µãƒ–å¥å—å‹•æ…‹={has_sub_passive}")
            
            # ğŸ¯ çµ±åˆçµæœã®æº–å‚™
            return {
                'slots': result_slots,
                'sub_slots': result_sub_slots, 
                'consumed_tokens': consumed_tokens,
                'grammar_info': {
                    'patterns': [{
                        'type': 'passive_voice_2stage',
                        'main_sentence': main_sentence,
                        'sub_sentences_count': len(sub_sentences),
                        'detection_method': 'é–¢ä¿‚ç¯€å¯¾å¿œ2æ®µéšå‡¦ç†',
                        'rephrase_allocation': 'beâ†’Aux, past_participleâ†’V, byå¥â†’M2'
                    }],
                    'handler_success': True,
                    'processing_notes': f"Passive voice 2-stage: main={bool(result_slots)}, sub={len([k for k in result_sub_slots if k.startswith('sub-')])}"
                }
            }
        
        except Exception as e:
            self.logger.error(f"å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _detect_passive_in_text(self, text: str, doc) -> Dict[str, Any]:
        """
        ğŸ¯ é–¢ä¿‚ç¯€å¯¾å¿œ2æ®µéšå‡¦ç†ç”¨ï¼šç‰¹å®šãƒ†ã‚­ã‚¹ãƒˆå†…ã®å—å‹•æ…‹æ¤œå‡º
        
        Args:
            text: æ¤œæŸ»å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¸»æ–‡ã¾ãŸã¯ã‚µãƒ–å¥ï¼‰
            doc: spaCyè§£ææ¸ˆã¿æ–‡æ›¸ï¼ˆå…¨ä½“ï¼‰
        
        Returns:
            Dict: å—å‹•æ…‹æ¤œå‡ºçµæœ
        """
        try:
            # ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã®å ´åˆã¯æ¤œå‡ºãªã—
            if not text.strip():
                return {'found': False}
            
            # æŒ‡å®šãƒ†ã‚­ã‚¹ãƒˆã®spaCyè§£æ
            text_doc = self.nlp(text.strip())
            
            # æ—¢å­˜ã®æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
            return self._detect_passive_voice_pattern(text_doc, text)
            
        except Exception as e:
            self.logger.error(f"ãƒ†ã‚­ã‚¹ãƒˆå†…å—å‹•æ…‹æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return {'found': False}
    
    def _create_passive_slots(self, passive_info: Dict) -> Dict[str, str]:
        """
        ğŸ¯ é–¢ä¿‚ç¯€å¯¾å¿œ2æ®µéšå‡¦ç†ç”¨ï¼šå—å‹•æ…‹æƒ…å ±ã‹ã‚‰ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ
        
        Args:
            passive_info: å—å‹•æ…‹æ¤œå‡ºçµæœ
        
        Returns:
            Dict: Rephraseã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ 
        """
        slots = {}
        
        if not passive_info.get('found', False):
            return slots
        
        be_verb = passive_info.get('be_verb', '')
        past_participle = passive_info.get('past_participle', '')
        by_agent = passive_info.get('by_agent', '')
        
        # Rephraseã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ï¼šbeå‹•è©ã¯Auxã«é…ç½®
        if ' ' in be_verb:  # "will be"ã®ã‚ˆã†ãªå ´åˆ
            aux_parts = be_verb.split()
            if len(aux_parts) == 2:
                # åŠ©å‹•è© + beå‹•è©ã®å ´åˆï¼šåŠ©å‹•è©ã‚’å„ªå…ˆã—ã¦Auxã«é…ç½®
                slots['Aux'] = aux_parts[0]  # will
                slots['V'] = past_participle  # written
                print(f"ğŸ” åŠ©å‹•è©+beæ§‹æˆ: Aux='{aux_parts[0]}', V='{past_participle}' (beå‹•è©å†…åŒ…)")
            else:
                # è¤‡é›‘ãªå ´åˆã¯å…¨ä½“ã‚’Auxã«é…ç½®
                slots['Aux'] = be_verb
                slots['V'] = past_participle
        else:
            # å˜ç´”ãªbeå‹•è©ã®å ´åˆï¼šbeå‹•è©ã‚’Auxã«é…ç½®ï¼ˆRephraseä»•æ§˜ï¼‰
            slots['Aux'] = be_verb  # is, was, are, were
            slots['V'] = past_participle  # written, done, etc.
        
        # M ã‚¹ãƒ­ãƒƒãƒˆ: byå¥ã®é…ç½®ï¼ˆRephraseä»•æ§˜ï¼šã€Œï½ã«ã‚ˆã£ã¦ã€å…¨ä½“ãŒå‰¯è©å¥ï¼‰
        if by_agent:
            # Rephraseå‰¯è©é…ç½®ãƒ«ãƒ¼ãƒ«: å˜ç‹¬å‰¯è©å¥ã¯M2ã«é…ç½®
            slots['M2'] = by_agent  # "by John" å…¨ä½“ã‚’å‰¯è©å¥ã¨ã—ã¦M2é…ç½®
            print(f"ğŸ” byå¥é…ç½®: M2='{by_agent}' (Rephraseå‰¯è©ãƒ«ãƒ¼ãƒ«ï¼šå˜ç‹¬å‰¯è©å¥â†’M2)")
        
        return slots
    
    def _get_tokens_for_phrase(self, phrase: str, doc) -> List[int]:
        """
        ğŸ¯ é–¢ä¿‚ç¯€å¯¾å¿œ2æ®µéšå‡¦ç†ç”¨ï¼šãƒ•ãƒ¬ãƒ¼ã‚ºã«å¯¾å¿œã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å–å¾—
        
        Args:
            phrase: å¯¾è±¡ãƒ•ãƒ¬ãƒ¼ã‚ºï¼ˆä¾‹: "by John"ï¼‰
            doc: spaCyè§£ææ¸ˆã¿æ–‡æ›¸
        
        Returns:
            List[int]: å¯¾å¿œã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚¹ãƒˆ
        """
        consumed_indices = []
        
        if not phrase:
            return consumed_indices
        
        try:
            # ãƒ•ãƒ¬ãƒ¼ã‚ºã®å˜èªã‚’ç©ºç™½ã§åˆ†å‰²
            phrase_words = phrase.lower().split()
            
            for i, token in enumerate(doc):
                if token.text.lower() in phrase_words:
                    consumed_indices.append(i)
                    self._consumed_tokens.add(i)
            
            if consumed_indices:
                print(f"ğŸ”¥ Token consumption: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {consumed_indices} for ãƒ•ãƒ¬ãƒ¼ã‚º '{phrase}'")
            
            return consumed_indices
            
        except Exception as e:
            self.logger.error(f"ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return consumed_indices
            self.logger.error(f"å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _detect_passive_voice_pattern(self, doc, sentence: str) -> Dict[str, Any]:
        """
        å—å‹•æ…‹ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º (spaCyãƒ™ãƒ¼ã‚¹ãƒ»å½¢æ…‹è«–çš„åˆ†æ)
        
        æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³:
        1. beå‹•è© + éå»åˆ†è©ï¼ˆç›´å¾Œãƒ»è¿‘æ¥ï¼‰
        2. beå‹•è© + å‰¯è© + éå»åˆ†è©
        3. will/modal + be + éå»åˆ†è©
        4. byå¥ã®æ¤œå‡ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        """
        result = {
            'found': False,
            'be_verb': '',
            'past_participle': '',
            'by_agent': '',
            'confidence': 0.0
        }
        
        tokens = list(doc)
        
        # 1. å„ç¨®å—å‹•æ…‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
        for i in range(len(tokens)):
            current_token = tokens[i]
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: å˜ç´”ãªbeå‹•è© + éå»åˆ†è©
            if self._is_be_verb_spacy(current_token):
                be_verb = current_token.text
                
                # ç›´å¾Œã®éå»åˆ†è©
                if i + 1 < len(tokens):
                    next_token = tokens[i + 1]
                    if self._is_past_participle_spacy(next_token):
                        result.update({
                            'found': True,
                            'be_verb': be_verb,
                            'past_participle': next_token.text,
                            'confidence': 0.9
                        })
                        break
                
                # beå‹•è© + å‰¯è© + éå»åˆ†è©
                if i + 2 < len(tokens):
                    adv_token = tokens[i + 1]
                    pp_token = tokens[i + 2]
                    if (adv_token.pos_ == 'ADV' and 
                        self._is_past_participle_spacy(pp_token)):
                        result.update({
                            'found': True,
                            'be_verb': be_verb,
                            'past_participle': pp_token.text,
                            'confidence': 0.85
                        })
                        break
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: modal + be + éå»åˆ†è©
            if (current_token.pos_ == 'AUX' and 
                current_token.text.lower() in ['will', 'would', 'can', 'could', 'should', 'may', 'might', 'must']):
                
                # modal + be + éå»åˆ†è©ãƒ‘ã‚¿ãƒ¼ãƒ³
                if i + 2 < len(tokens):
                    be_token = tokens[i + 1]
                    pp_token = tokens[i + 2]
                    if (self._is_be_verb_spacy(be_token) and
                        self._is_past_participle_spacy(pp_token)):
                        result.update({
                            'found': True,
                            'be_verb': f"{current_token.text} {be_token.text}",
                            'past_participle': pp_token.text,
                            'confidence': 0.95
                        })
                        break
        
        # 2. byå¥ã®æ¤œå‡ºï¼ˆå—å‹•æ…‹ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã®ã¿ï¼‰
        if result['found']:
            by_agent = self._detect_by_agent_phrase_complete(tokens, sentence)
            if by_agent:
                result['by_agent'] = by_agent
                result['confidence'] = min(result['confidence'] + 0.05, 1.0)
        
        return result
    
    def _is_be_verb_spacy(self, token) -> bool:
        """spaCyãƒ™ãƒ¼ã‚¹ã®beå‹•è©åˆ¤å®š"""
        # lemmaï¼ˆåŸå‹ï¼‰ãŒbeã§ã€åŠ©å‹•è©ã‚¿ã‚°
        return (token.lemma_.lower() == 'be' and 
                token.pos_ in ['AUX', 'VERB'])
    
    def _is_past_participle_spacy(self, token) -> bool:
        """spaCyãƒ™ãƒ¼ã‚¹ã®éå»åˆ†è©åˆ¤å®š"""
        # 1. ã‚¿ã‚°ãƒ™ãƒ¼ã‚¹åˆ¤å®š
        if token.tag_ == 'VBN':  # Past participle
            return True
        
        # 2. å½¢æ…‹è«–çš„ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¤å®šï¼ˆbeå‹•è©ç›´å¾Œã®æ–‡è„ˆï¼‰
        if token.pos_ == 'ADJ':
            return self._has_past_participle_morphology_spacy(token.text)
        
        # 3. å‹•è©ã®éå»åˆ†è©å½¢
        if token.pos_ == 'VERB' and token.tag_ == 'VBN':
            return True
        
        return False
    
    def _has_past_participle_morphology_spacy(self, text: str) -> bool:
        """å½¢æ…‹è«–çš„éå»åˆ†è©ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆèªå°¾åˆ†æï¼‰"""
        text_lower = text.lower()
        
        # è¦å‰‡å‹•è©ã®-edèªå°¾
        if text_lower.endswith('ed') and len(text_lower) > 3:
            # ç´”ç²‹ãªå½¢å®¹è©ã‚’é™¤å¤–
            if not text_lower.endswith(('red', 'ded', 'eed', 'ted')):
                return True
        
        # -enèªå°¾ï¼ˆbroken, chosenç­‰ï¼‰
        if text_lower.endswith('en') and len(text_lower) > 3:
            if not text_lower.endswith(('tten', 'sten', 'chen', 'len')):
                return True
        
        # ç‰¹å¾´çš„ãªéå»åˆ†è©èªå°¾
        past_participle_endings = ['ated', 'ized', 'ified', 'ected', 'ested']
        return any(text_lower.endswith(ending) for ending in past_participle_endings)
    
    def _detect_by_agent_phrase_complete(self, tokens: List, sentence: str) -> str:
        """byå¥ï¼ˆè¡Œç‚ºè€…ï¼‰ã®å®Œå…¨æ¤œå‡º"""
        by_phrase = ""
        
        for i, token in enumerate(tokens):
            if token.text.lower() == 'by' and token.pos_ == 'ADP':
                # byä»¥é™ã®åè©å¥ã‚’æŠ½å‡º
                phrase_parts = ['by']
                for j in range(i + 1, min(i + 5, len(tokens))):  # æœ€å¤§4èªã¾ã§æ‹¡å¼µ
                    next_token = tokens[j]
                    if next_token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'DET', 'PRON']:
                        phrase_parts.append(next_token.text)
                    elif next_token.pos_ == 'PUNCT':  # å¥èª­ç‚¹ã§çµ‚äº†
                        break
                    else:
                        # ãã®ä»–ã®å“è©ã§ã‚‚çŸ­ã„èªã¯å«ã‚ã‚‹ï¼ˆthe, aç­‰ï¼‰
                        if len(next_token.text) <= 3:
                            phrase_parts.append(next_token.text)
                        else:
                            break
                
                if len(phrase_parts) > 1:
                    by_phrase = ' '.join(phrase_parts)
                    break
        
        return by_phrase
    
    # Phase 2 å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè£…çµ‚äº†
    
    def _handle_comparative_superlative(self, sentence: str, doc, current_result: Dict) -> Optional[Dict]:
        """
        ğŸ¯ æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´šãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        
        æ¯”è¼ƒç´šï¼ˆ-er, moreï¼‰ãƒ»æœ€ä¸Šç´šï¼ˆ-est, mostï¼‰æ§‹æ–‡ã‚’æ¤œå‡ºãƒ»å‡¦ç†
        thanå¥ã€ofå¥ã‚’M2/M3ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®
        
        Args:
            sentence: è§£æå¯¾è±¡æ–‡
            doc: spaCyè§£æçµæœ
            current_result: ç¾åœ¨ã®è§£æçµæœ
            
        Returns:
            Dict: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼çµæœï¼ˆNone=æœªæ¤œå‡ºï¼‰
        """
        print(f"ğŸ” Executing comparative_superlative handler for: {sentence}")
        
        try:
            # æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´šæ§‹æ–‡ã®æ¤œå‡º
            comparative_info = self._detect_comparative_superlative(sentence, doc)
            
            if not comparative_info['found']:
                print(f"ğŸ” æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´šæœªæ¤œå‡º: {sentence}")
                return None
            
            print(f"ğŸ”¥ æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´šæ¤œå‡º: {comparative_info}")
            
            # ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ
            slots = self._create_comparative_slots(comparative_info, doc)
            
            # ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»è¨˜éŒ²
            if comparative_info.get('consumed_tokens'):
                for token_idx in comparative_info['consumed_tokens']:
                    self._consumed_tokens.add(token_idx)
                print(f"ğŸ”¥ Token consumption: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {comparative_info['consumed_tokens']} for æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´š")
            
            result = {
                'sentence': sentence,
                'slots': slots,
                'sub_slots': {},
                'grammar_info': {
                    'detected_patterns': [{
                        'type': 'comparative_superlative',
                        'structure': comparative_info.get('structure', ''),
                        'comparison_type': comparative_info.get('comparison_type', ''),
                        'detection_method': 'æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´šæ§‹æ–‡è§£æ'
                    }],
                    'handler_contributions': {
                        'comparative_superlative': {
                            'patterns': [{
                                'type': 'comparative_superlative',
                                'structure': comparative_info.get('structure', ''),
                                'comparison_type': comparative_info.get('comparison_type', ''),
                                'detection_method': 'æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´šæ§‹æ–‡è§£æ'
                            }],
                            'handler_success': True,
                            'processing_notes': f"Comparative/Superlative: {comparative_info.get('comparison_type', '')} form detected"
                        }
                    },
                    'control_flags': {}
                },
                'slot_provenance': {slot: {'handler': 'comparative_superlative', 'priority': 9, 'value': value} 
                                  for slot, value in slots.items() if value}
            }
            
            print(f"ğŸ”¥ æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´šãƒãƒ³ãƒ‰ãƒ©ãƒ¼æˆåŠŸ: {comparative_info.get('comparison_type', '')}æ§‹æ–‡æ¤œå‡º")
            return result
            
        except Exception as e:
            self.logger.error(f"æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´šãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _detect_comparative_superlative(self, sentence: str, doc) -> Dict:
        """
        ğŸ¯ æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´šæ§‹æ–‡ã®æ¤œå‡º
        
        Returns:
            Dict: {
                'found': bool,
                'comparison_type': str,  # 'comparative' or 'superlative'
                'structure': str,        # æ¯”è¼ƒèª
                'than_phrase': str,      # thanå¥ (æ¯”è¼ƒç´šã®ã¿)
                'of_phrase': str,        # ofå¥ (æœ€ä¸Šç´šã®ã¿)
                'consumed_tokens': List[int]
            }
        """
        result = {
            'found': False,
            'comparison_type': '',
            'structure': '',
            'than_phrase': '',
            'of_phrase': '',
            'consumed_tokens': []
        }
        
        try:
            tokens = list(doc)
            
            # æ¯”è¼ƒç´šãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
            comparative_detected = self._detect_comparative_pattern(tokens, result)
            if comparative_detected:
                result['found'] = True
                return result
            
            # æœ€ä¸Šç´šãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
            superlative_detected = self._detect_superlative_pattern(tokens, result)
            if superlative_detected:
                result['found'] = True
                return result
            
            return result
            
        except Exception as e:
            self.logger.error(f"æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´šæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return {'found': False}
    
    def _detect_comparative_pattern(self, tokens: List, result: Dict) -> bool:
        """æ¯”è¼ƒç´šãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º"""
        # æ¯”è¼ƒç´šèªå°¾ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚ˆã‚Šå³å¯†ã«ï¼‰
        comparative_endings = ['er']
        comparative_words = ['more', 'less', 'better', 'worse', 'bigger', 'smaller', 'faster', 'slower', 'clearer', 'earlier']
        
        for i, token in enumerate(tokens):
            token_text = token.text.lower()
            
            # -erèªå°¾ã®æ¤œå‡ºï¼ˆé•·ã•4æ–‡å­—ä»¥ä¸Šã§èª¤æ¤œå‡ºé˜²æ­¢ï¼‰
            if len(token_text) >= 4 and any(token_text.endswith(ending) for ending in comparative_endings):
                # 'than'å¥ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                has_than = any(t.text.lower() == 'than' for t in tokens[i+1:])
                if has_than and token.pos_ in ['ADJ', 'ADV']:
                    result['comparison_type'] = 'comparative'
                    result['structure'] = token.text
                    self._find_than_phrase(tokens, i, result)
                    return True
            
            # more/less + å½¢å®¹è©ãƒ‘ã‚¿ãƒ¼ãƒ³
            if token_text in ['more', 'less'] and i + 1 < len(tokens):
                next_token = tokens[i + 1]
                if next_token.pos_ in ['ADJ', 'ADV']:
                    # 'than'å¥ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                    has_than = any(t.text.lower() == 'than' for t in tokens[i+2:])
                    if has_than:
                        result['comparison_type'] = 'comparative'
                        result['structure'] = f"{token.text} {next_token.text}"
                        self._find_than_phrase(tokens, i + 1, result)
                        return True
            
            # æ—¢çŸ¥ã®æ¯”è¼ƒç´šèªï¼ˆå³å¯†ãƒã‚§ãƒƒã‚¯ï¼‰
            if token_text in comparative_words and token.pos_ in ['ADJ', 'ADV']:
                # 'than'å¥ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                has_than = any(t.text.lower() == 'than' for t in tokens[i+1:])
                if has_than:
                    result['comparison_type'] = 'comparative'
                    result['structure'] = token.text
                    self._find_than_phrase(tokens, i, result)
                    return True
        
        return False
    
    def _detect_superlative_pattern(self, tokens: List, result: Dict) -> bool:
        """æœ€ä¸Šç´šãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º"""
        # æœ€ä¸Šç´šèªå°¾ãƒ‘ã‚¿ãƒ¼ãƒ³
        superlative_endings = ['est']
        superlative_words = ['most', 'least', 'best', 'worst', 'biggest', 'smallest', 'fastest', 'slowest', 'smartest', 'highest', 'hardest']
        
        for i, token in enumerate(tokens):
            token_text = token.text.lower()
            
            # å˜ç‹¬ã®estèªå°¾æœ€ä¸Šç´šï¼ˆ"hardest"ãªã©ï¼‰
            if len(token_text) >= 5 and any(token_text.endswith(ending) for ending in superlative_endings):
                if token.pos_ in ['ADJ', 'ADV']:
                    # ofå¥ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                    has_of_phrase = any(t.text.lower() in ['of', 'among', 'in'] for t in tokens[i+1:])
                    if has_of_phrase:
                        result['comparison_type'] = 'superlative'
                        result['structure'] = token.text
                        result['consumed_tokens'].append(i)
                        self._find_superlative_phrase(tokens, i, result)
                        return True
            
            # the + -estèªå°¾ã®æ¤œå‡º
            if len(token_text) >= 5 and any(token_text.endswith(ending) for ending in superlative_endings):
                # theã®ç¢ºèª
                if i > 0 and tokens[i-1].text.lower() == 'the' and token.pos_ in ['ADJ', 'ADV']:
                    result['comparison_type'] = 'superlative'
                    result['structure'] = f"the {token.text}"
                    result['consumed_tokens'].extend([i-1, i])
                    self._find_superlative_phrase(tokens, i, result)
                    return True
            
            # the most/least + å½¢å®¹è©ãƒ‘ã‚¿ãƒ¼ãƒ³
            if token_text in ['most', 'least'] and i > 0 and tokens[i-1].text.lower() == 'the':
                if i + 1 < len(tokens) and tokens[i + 1].pos_ in ['ADJ', 'ADV']:
                    next_token = tokens[i + 1]
                    # åè©ã‚‚å«ã‚ã‚‹å ´åˆï¼ˆ"the most beautiful flower"ï¼‰
                    phrase_parts = [f"the {token.text} {next_token.text}"]
                    consumed_indices = [i-1, i, i+1]
                    
                    # ç¶šãåè©ãŒã‚ã‚Œã°è¿½åŠ 
                    if i + 2 < len(tokens) and tokens[i + 2].pos_ in ['NOUN', 'PROPN']:
                        phrase_parts[0] += f" {tokens[i + 2].text}"
                        consumed_indices.append(i + 2)
                    
                    result['comparison_type'] = 'superlative'
                    result['structure'] = phrase_parts[0]
                    result['consumed_tokens'].extend(consumed_indices)
                    self._find_superlative_phrase(tokens, i + 2 if len(consumed_indices) > 3 else i + 1, result)
                    return True
            
            # å˜ç‹¬ã®most + å‰¯è©ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ"most efficiently"ãªã©ï¼‰
            if token_text == 'most' and i + 1 < len(tokens):
                next_token = tokens[i + 1]
                if next_token.pos_ == 'ADV' and next_token.text.endswith('ly'):
                    result['comparison_type'] = 'superlative'
                    result['structure'] = f"most {next_token.text}"
                    result['consumed_tokens'].extend([i, i+1])
                    # amongå¥ã‚„inå¥ã®æ¤œç´¢
                    self._find_superlative_phrase(tokens, i + 1, result)
                    return True
            
            # æ—¢çŸ¥ã®æœ€ä¸Šç´šèªï¼ˆå³å¯†ãƒã‚§ãƒƒã‚¯ï¼‰
            if token_text in superlative_words and token.pos_ in ['ADJ', 'ADV']:
                if i > 0 and tokens[i-1].text.lower() == 'the':
                    result['comparison_type'] = 'superlative'
                    result['structure'] = f"the {token.text}"
                    result['consumed_tokens'].extend([i-1, i])
                    self._find_superlative_phrase(tokens, i, result)
                    return True
        
        return False
    
    def _find_than_phrase(self, tokens: List, start_idx: int, result: Dict):
        """thanå¥ã®æ¤œå‡ºã¨æŠ½å‡º"""
        for i in range(start_idx + 1, len(tokens)):
            if tokens[i].text.lower() == 'than':
                # thanä»¥é™ã®å¥ã‚’æŠ½å‡ºï¼ˆã‚ˆã‚ŠæŸ”è»Ÿã«ï¼‰
                than_parts = ['than']
                consumed_indices = [i]
                
                for j in range(i + 1, min(i + 10, len(tokens))):  # æœ€å¤§9èªã¾ã§
                    next_token = tokens[j]
                    if next_token.pos_ == 'PUNCT' and next_token.text in ['.', '!', '?']:
                        break
                    elif next_token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'DET', 'PRON', 'NUM', 'ADV']:
                        than_parts.append(next_token.text)
                        consumed_indices.append(j)
                    elif next_token.text.lower() in ['else', 'other', 'all', 'any', 'some']:
                        than_parts.append(next_token.text)
                        consumed_indices.append(j)
                    else:
                        # çŸ­ã„èªï¼ˆå‰ç½®è©ã€æ¥ç¶šè©ãªã©ï¼‰ã¯å«ã‚ã‚‹
                        if len(next_token.text) <= 4:
                            than_parts.append(next_token.text)
                            consumed_indices.append(j)
                        else:
                            break
                
                if len(than_parts) > 1:
                    result['than_phrase'] = ' '.join(than_parts)
                    result['consumed_tokens'].extend(consumed_indices)
                break
    
    def _find_superlative_phrase(self, tokens: List, start_idx: int, result: Dict):
        """æœ€ä¸Šç´šã®ä¿®é£¾å¥ï¼ˆamong, inå¥ãªã©ï¼‰ã®æ¤œå‡ºã¨æŠ½å‡º"""
        for i in range(start_idx + 1, len(tokens)):
            token_text = tokens[i].text.lower()
            if token_text in ['among', 'in', 'of']:
                # ä¿®é£¾å¥ã‚’æŠ½å‡º
                phrase_parts = [tokens[i].text]
                consumed_indices = [i]
                
                for j in range(i + 1, min(i + 8, len(tokens))):  # æœ€å¤§7èªã¾ã§
                    next_token = tokens[j]
                    if next_token.pos_ == 'PUNCT' and next_token.text in ['.', '!', '?']:
                        break
                    elif next_token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'DET', 'PRON', 'NUM']:
                        phrase_parts.append(next_token.text)
                        consumed_indices.append(j)
                    elif next_token.text.lower() in ['all', 'the', 'every', 'most']:
                        phrase_parts.append(next_token.text)
                        consumed_indices.append(j)
                    else:
                        if len(next_token.text) <= 4:
                            phrase_parts.append(next_token.text)
                            consumed_indices.append(j)
                        else:
                            break
                
                if len(phrase_parts) > 1:
                    if token_text == 'of':
                        result['of_phrase'] = ' '.join(phrase_parts)
                    else:
                        result['of_phrase'] = ' '.join(phrase_parts)  # among, inå¥ã‚‚of_phraseã¨ã—ã¦å‡¦ç†
                    result['consumed_tokens'].extend(consumed_indices)
                break
    
    def _find_of_phrase(self, tokens: List, start_idx: int, result: Dict):
        """ofå¥ã®æ¤œå‡ºã¨æŠ½å‡º"""
        for i in range(start_idx + 1, len(tokens)):
            if tokens[i].text.lower() == 'of':
                # ofä»¥é™ã®å¥ã‚’æŠ½å‡º
                of_parts = ['of']
                consumed_indices = [i]
                
                for j in range(i + 1, min(i + 8, len(tokens))):  # æœ€å¤§7èªã¾ã§
                    next_token = tokens[j]
                    if next_token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'DET', 'PRON', 'NUM']:
                        of_parts.append(next_token.text)
                        consumed_indices.append(j)
                    elif next_token.pos_ == 'PUNCT':
                        break
                    else:
                        if len(next_token.text) <= 3:  # çŸ­ã„èªã¯å«ã‚ã‚‹
                            of_parts.append(next_token.text)
                            consumed_indices.append(j)
                        else:
                            break
                
                if len(of_parts) > 1:
                    result['of_phrase'] = ' '.join(of_parts)
                    result['consumed_tokens'].extend(consumed_indices)
                break
    
    def _create_comparative_slots(self, comparative_info: Dict, doc) -> Dict[str, str]:
        """
        ğŸ¯ æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´šæƒ…å ±ã‹ã‚‰Rephraseã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ
        
        Args:
            comparative_info: æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´šæ¤œå‡ºçµæœ
            doc: spaCyè§£æçµæœ
            
        Returns:
            Dict: Rephraseã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ 
        """
        slots = {}
        
        if not comparative_info.get('found', False):
            return slots
        
        comparison_type = comparative_info.get('comparison_type', '')
        structure = comparative_info.get('structure', '')
        than_phrase = comparative_info.get('than_phrase', '')
        of_phrase = comparative_info.get('of_phrase', '')
        
        # æ§‹é€ ã®å“è©åˆ¤å®šï¼ˆå½¢å®¹è© vs å‰¯è©ï¼‰
        is_adverb = self._is_adverbial_comparative(structure, doc)
        
        if is_adverb:
            # å‰¯è©æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´šï¼šM2,M3ã‚¹ãƒ­ãƒƒãƒˆé…ç½®
            if structure:
                slots['M2'] = structure
                print(f"ğŸ” å‰¯è©æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´šæ§‹é€ : M2='{structure}' ({comparison_type})")
                print(f"ğŸ”§ DEBUG: Setting M2='{structure}' from structure")
            
            # thanå¥/ofå¥ã¯M3ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®
            if than_phrase:
                slots['M3'] = than_phrase
                print(f"ğŸ” thanå¥é…ç½®: M3='{than_phrase}' (Rephraseå‰¯è©ãƒ«ãƒ¼ãƒ«)")
            elif of_phrase:
                slots['M3'] = of_phrase
                print(f"ğŸ” ofå¥é…ç½®: M3='{of_phrase}' (Rephraseå‰¯è©ãƒ«ãƒ¼ãƒ«)")
                print(f"ğŸ”§ DEBUG: Setting M3='{of_phrase}' from of_phrase")
        else:
            # å½¢å®¹è©æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´šï¼šC1,M2ã‚¹ãƒ­ãƒƒãƒˆé…ç½®
            if structure:
                slots['C1'] = structure
                print(f"ğŸ” å½¢å®¹è©æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´šæ§‹é€ : C1='{structure}' ({comparison_type})")
            
            # thanå¥/ofå¥ã¯M2ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®
            if than_phrase:
                slots['M2'] = than_phrase
                print(f"ğŸ” thanå¥é…ç½®: M2='{than_phrase}' (Rephraseä¿®é£¾å¥ãƒ«ãƒ¼ãƒ«)")
            elif of_phrase:
                slots['M2'] = of_phrase
                print(f"ğŸ” ofå¥é…ç½®: M2='{of_phrase}' (Rephraseä¿®é£¾å¥ãƒ«ãƒ¼ãƒ«)")
        
        print(f"ğŸ”§ DEBUG: Final comparative slots created: {slots}")
        return slots
    
    def _is_adverbial_comparative(self, structure: str, doc) -> bool:
        """æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´šãŒå‰¯è©ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        # èªå°¾ã«ã‚ˆã‚‹åˆ¤å®š
        adverb_patterns = ['ly', 'er', 'est']
        structure_lower = structure.lower()
        
        # "more clearly", "most efficiently" ãªã©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        if 'ly' in structure_lower:
            return True
        
        # å˜èªã®å‰¯è©ãƒã‚§ãƒƒã‚¯
        for token in doc:
            if token.text.lower() in structure_lower:
                if token.pos_ == 'ADV':
                    return True
        
        # æ—¢çŸ¥ã®å‰¯è©æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´š
        adverb_comparatives = [
            'faster', 'slower', 'earlier', 'later',
            'harder', 'easier', 'clearer', 'harder',
            'more clearly', 'more efficiently', 'more frequently',
            'most clearly', 'most efficiently', 'most frequently',
            'hardest', 'fastest', 'slowest'
        ]
        
        return any(pattern in structure_lower for pattern in adverb_comparatives)
    
    # Phase 2 æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´šãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè£…çµ‚äº†
    
    def _merge_handler_results(self, base_result: Dict, handler_result: Dict, handler_name: str) -> Dict:
        """
        ãƒãƒ³ãƒ‰ãƒ©ãƒ¼çµæœã‚’ãƒ™ãƒ¼ã‚¹çµæœã«ãƒãƒ¼ã‚¸
        
        Args:
            base_result: ãƒ™ãƒ¼ã‚¹çµæœ
            handler_result: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å‡¦ç†çµæœ  
            handler_name: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å
        """
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å„ªå…ˆåº¦å®šç¾©ï¼ˆChatGPT5æ€è€ƒè¨ºæ–­ã«ã‚ˆã‚‹ã€Œå¾Œå‹ã¡ä¸Šæ›¸ãã€å¯¾ç­–ï¼‰
        handler_priority = {
            'passive_voice': 10,      # å—å‹•æ…‹ã¯æœ€é«˜å„ªå…ˆåº¦
            'comparative_superlative': 9, # æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´š
            'relative_clause': 8,     # é–¢ä¿‚ç¯€
            'auxiliary_complex': 7,   # åŠ©å‹•è©
            'basic_five_pattern': 1   # åŸºæœ¬5æ–‡å‹ã¯æœ€ä½å„ªå…ˆåº¦
        }
        
        current_priority = handler_priority.get(handler_name, 5)
        
        # ã‚¹ãƒ­ãƒƒãƒˆæƒ…å ±ãƒãƒ¼ã‚¸
        if 'slots' in handler_result:
            for slot_name, slot_data in handler_result['slots'].items():
                if slot_name not in base_result['slots']:
                    base_result['slots'][slot_name] = slot_data
                    # ChatGPT5 Step B: Slot Provenance Tracking
                    if 'slot_provenance' not in base_result:
                        base_result['slot_provenance'] = {}
                    base_result['slot_provenance'][slot_name] = {
                        'handler': handler_name,
                        'priority': current_priority,
                        'value': slot_data
                    }
                else:
                    # ç«¶åˆè§£æ±ºï¼šå„ªå…ˆåº¦ã«ã‚ˆã‚‹ä¿è­·
                    existing_value = base_result['slots'][slot_name]
                    
                    # æ—¢å­˜å€¤ã®å„ªå…ˆåº¦ãƒã‚§ãƒƒã‚¯ï¼ˆChatGPT5 Step B: Provenance-based Priorityï¼‰
                    existing_priority = 1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                    if 'slot_provenance' in base_result and slot_name in base_result['slot_provenance']:
                        existing_priority = base_result['slot_provenance'][slot_name]['priority']
                        existing_handler = base_result['slot_provenance'][slot_name]['handler']
                    else:
                        # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ—ãƒ©ã‚¤ã‚ªãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
                        if 'grammar_info' in base_result and 'handler_contributions' in base_result['grammar_info']:
                            for prev_handler, _ in base_result['grammar_info']['handler_contributions'].items():
                                if prev_handler in handler_priority:
                                    existing_priority = max(existing_priority, handler_priority[prev_handler])
                                    existing_handler = prev_handler
                    
                    # æ—¢å­˜å€¤ãŒç©ºã§æ–°å€¤ãŒæœ‰åŠ¹ãªå ´åˆã¯ä¸Šæ›¸ã
                    if not existing_value and slot_data:
                        base_result['slots'][slot_name] = slot_data
                        # ChatGPT5 Step B: Provenance update
                        if 'slot_provenance' not in base_result:
                            base_result['slot_provenance'] = {}
                        base_result['slot_provenance'][slot_name] = {
                            'handler': handler_name,
                            'priority': current_priority,
                            'value': slot_data
                        }
                    # æ–°ã—ã„ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®å„ªå…ˆåº¦ãŒé«˜ã„å ´åˆã®ã¿ä¸Šæ›¸ã
                    elif current_priority > existing_priority:
                        print(f"ğŸ”¥ ChatGPT5 Step B: Slot override - {slot_name}: '{existing_value}' ({existing_handler if 'existing_handler' in locals() else 'unknown'}) â†’ '{slot_data}' ({handler_name})")
                        base_result['slots'][slot_name] = slot_data
                        # ChatGPT5 Step B: Provenance update
                        if 'slot_provenance' not in base_result:
                            base_result['slot_provenance'] = {}
                        base_result['slot_provenance'][slot_name] = {
                            'handler': handler_name,
                            'priority': current_priority,
                            'value': slot_data
                        }
                    # å„ªå…ˆåº¦ãŒåŒã˜å ´åˆã¯æ—¢å­˜å€¤ãŒæœ‰åŠ¹ãªã‚‰ä¿æŒ
                    elif current_priority == existing_priority and existing_value:
                        pass  # æ—¢å­˜å€¤ã‚’ä¿æŒ
                    # æ—¢å­˜å€¤ãŒæœ‰åŠ¹ã§æ–°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®å„ªå…ˆåº¦ãŒä½ã„å ´åˆã¯ä¿æŒ
                    elif existing_value and current_priority <= existing_priority:
                        pass  # æ—¢å­˜å€¤ã‚’ä¿æŒï¼ˆå—å‹•æ…‹çµæœã‚’ä¿è­·ï¼‰
                    # ä¸¡æ–¹ç©ºã®å ´åˆã¯å¾Œå‹ã¡
                    elif not existing_value and not slot_data:
                        base_result['slots'][slot_name] = slot_data
        
        # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæƒ…å ±ãƒãƒ¼ã‚¸
        if 'sub_slots' in handler_result:
            for sub_slot_name, sub_slot_data in handler_result['sub_slots'].items():
                base_result['sub_slots'][sub_slot_name] = sub_slot_data
        
        # æ–‡æ³•æƒ…å ±è¨˜éŒ²
        if 'grammar_info' in handler_result:
            grammar_info = handler_result['grammar_info']
            if 'handler_contributions' not in base_result['grammar_info']:
                base_result['grammar_info']['handler_contributions'] = {}
            base_result['grammar_info']['handler_contributions'][handler_name] = grammar_info
            
            # æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³è¿½åŠ 
            if 'patterns' in grammar_info:
                if 'detected_patterns' not in base_result['grammar_info']:
                    base_result['grammar_info']['detected_patterns'] = []
                base_result['grammar_info']['detected_patterns'].extend(grammar_info['patterns'])
        
        # é–¢ä¿‚ç¯€æƒ…å ±ãƒãƒ¼ã‚¸ï¼ˆPhase 2: 2æ®µéšå‡¦ç†å¯¾å¿œï¼‰
        if 'relative_clause_info' in handler_result:
            base_result['relative_clause_info'] = handler_result['relative_clause_info']
            print(f"ğŸ”¥ é–¢ä¿‚ç¯€æƒ…å ±ãƒãƒ¼ã‚¸: {handler_result['relative_clause_info']}")
        
        return base_result

    def _unified_mapping(self, sentence: str, doc) -> Dict[str, Any]:
        """
        çµ±åˆãƒãƒƒãƒ”ãƒ³ã‚°å‡¦ç† (from Stanza Asset Migration)
        
        å…¨ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒåŒæ™‚å®Ÿè¡Œ
        å„ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã¯ç‹¬ç«‹ã—ã¦spaCyè§£æçµæœã‚’å‡¦ç†
        """
        result = {
            'sentence': sentence,
            'slots': {},
            'sub_slots': {},
            'grammar_info': {
                'detected_patterns': [],
                'handler_contributions': {},
                'control_flags': {}  # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼åˆ¶å¾¡ãƒ•ãƒ©ã‚°
            }
        }
        
        self.logger.debug(f"Unified mappingé–‹å§‹: {len(self.active_handlers)} handlers active")
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼é–“å…±æœ‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆæœŸåŒ–
        self.handler_shared_context = {
            'predefined_slots': {},        # äº‹å‰ç¢ºå®šã‚¹ãƒ­ãƒƒãƒˆ
            'remaining_elements': {},      # æ®‹ã‚Šè¦ç´ æƒ…å ±
            'handler_metadata': {}         # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼åˆ¥ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        }
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè¡Œé †åºã®åˆ¶å¾¡
        ordered_handlers = self._get_ordered_handlers()
        
        # å…¨ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®åŒæ™‚å®Ÿè¡Œï¼ˆé †åºåˆ¶å¾¡ä»˜ãï¼‰
        for handler_name in ordered_handlers:
            try:
                # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼åˆ¶å¾¡ãƒ•ãƒ©ã‚°ã‚’ãƒã‚§ãƒƒã‚¯
                control_flags = result.get('grammar_info', {}).get('control_flags', {})
                if self._should_skip_handler(handler_name, control_flags):
                    self.logger.debug(f"ğŸš« Handler ã‚¹ã‚­ãƒƒãƒ—: {handler_name} (åˆ¶å¾¡ãƒ•ãƒ©ã‚°)")
                    continue
                
                print(f"ğŸ¯ Handlerå®Ÿè¡Œ: {handler_name}")
                self.logger.debug(f"Handlerå®Ÿè¡Œ: {handler_name}")
                
                # Phase 2: æ–°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ
                handler_method = getattr(self, f'_handle_{handler_name}', None)
                if handler_method:
                    handler_result = handler_method(sentence, doc, result)
                    if handler_result:
                        result = self._merge_handler_results(result, handler_result, handler_name)
                        print(f"ğŸ” Merged result after {handler_name}: {result}")
                    continue  # æ–°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒå®Ÿè¡Œã•ã‚ŒãŸå ´åˆã€ãƒ¬ã‚¬ã‚·ãƒ¼å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
                
                # ãƒ¬ã‚¬ã‚·ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆbasic_five_patternã®ã¿ï¼‰
                if handler_name == 'basic_five_pattern':
                    # ğŸ”¥ Phase 2: å¤‰æ•°åˆæœŸåŒ–
                    analysis_sentence = sentence
                    analysis_doc = doc
                    
                    # ğŸ”¥ Phase 2: Using main sentence for basic_five_pattern: '{main_sentence}'
                    if result.get('relative_clause_info', {}).get('found'):
                        main_sentence = result['relative_clause_info']['main_sentence']
                        print(f"ğŸ”¥ Phase 2: Using main sentence for basic_five_pattern: '{main_sentence}'")
                        analysis_sentence = main_sentence
                        analysis_doc = self.nlp(main_sentence)
                    
                    # ğŸ”¥ çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ : 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã¯ S, V, O1, O2, C1 ã®ã¿å‡¦ç†
                    # å‰¯è©ï¼ˆM1, M2, M3ï¼‰ã¯å°‚ç”¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒæ‹…å½“ï¼ˆè²¬ä»»åˆ†é›¢ã®åŸå‰‡ï¼‰
                    basic_slots = self._extract_basic_five_pattern_only(analysis_sentence, analysis_doc)
                    if basic_slots:
                        for slot_name, slot_value in basic_slots.items():
                            if slot_value and slot_name in ['S', 'V', 'O1', 'O2', 'C1']:  # å‰¯è©ä»¥å¤–ã®ã¿
                                # ğŸ”¥ çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ : é«˜å„ªå…ˆåº¦ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®çµæœã‚’ä¿è­·
                                if slot_name in result['slots'] and result['slots'][slot_name]:
                                    existing_priority = 1  # basic_five_patternã®å„ªå…ˆåº¦
                                    if 'slot_provenance' in result and slot_name in result['slot_provenance']:
                                        existing_priority = result['slot_provenance'][slot_name]['priority']
                                    
                                    if existing_priority > 1:  # é«˜å„ªå…ˆåº¦ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®çµæœãŒæ—¢ã«å­˜åœ¨
                                        print(f"ğŸ”¥ 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼: Skipping slot {slot_name}='{slot_value}' (é«˜å„ªå…ˆåº¦ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ä¿è­·)")
                                        continue
                                
                                # ğŸ”¥ Phase 2: ã‚µãƒ–å¥å—å‹•æ…‹ä¿è­·ãƒ­ã‚¸ãƒƒã‚¯
                                should_skip = False
                                for sub_key, sub_value in result.get('sub_slots', {}).items():
                                    if sub_key.startswith('sub-') and sub_value and str(slot_value).lower() in str(sub_value).lower():
                                        print(f"ğŸ”¥ 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼: Skipping main slot {slot_name}='{slot_value}' (already in sub-slot {sub_key}='{sub_value}')")
                                        should_skip = True
                                        break
                                
                                if not should_skip:
                                    result['slots'][slot_name] = slot_value
                                    print(f"ğŸ”¥ 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼: Setting {slot_name}='{slot_value}'")
                    
                    # æˆåŠŸã‚«ã‚¦ãƒ³ãƒˆ
                    self.handler_success_count[handler_name] = \
                        self.handler_success_count.get(handler_name, 0) + 1
                
                elif handler_name == 'passive_voice':
                    # å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ (Phase 2å®Ÿè£…)
                    print(f"ğŸ” Executing passive_voice handler for: {sentence}")
                    passive_result = self._handle_passive_voice(sentence, doc, result)
                    print(f"ğŸ” Passive voice handler result: {passive_result}")
                    if passive_result:
                        result = self._merge_handler_results(result, passive_result, handler_name)
                        print(f"ğŸ” Merged result after passive voice: {result}")
                        self.handler_success_count[handler_name] = \
                            self.handler_success_count.get(handler_name, 0) + 1
                    else:
                        print(f"ğŸ” Passive voice handler returned None")
                
                elif handler_name == 'adverbial_modifier':
                    # å‰¯è©ãƒ»ä¿®é£¾èªãƒãƒ³ãƒ‰ãƒ©ãƒ¼ (Phase 2å®Ÿè£…)
                    print(f"ğŸ” Executing adverbial_modifier handler for: {sentence}")
                    adverb_result = self._handle_adverbial_modifier(sentence, doc, result)
                    print(f"ğŸ” Adverb handler result: {adverb_result}")
                    if adverb_result:
                        result = self._merge_handler_results(result, adverb_result, handler_name)
                        print(f"ğŸ” Merged result: {result}")
                        self.handler_success_count[handler_name] = \
                            self.handler_success_count.get(handler_name, 0) + 1
                    else:
                        print(f"ğŸ” Adverb handler returned None")
                        
            except Exception as e:
                self.logger.warning(f"Handler error ({handler_name}): {e}")
                continue
        
        # ğŸ¯ Central Controllerç”¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼æƒ…å ±ã‚’è¿½åŠ 
        winning_handler = None
        winning_priority = 0
        
        # æœ€é«˜å„ªå…ˆåº¦ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ç‰¹å®š
        if 'slot_provenance' in result:
            for slot_name, provenance_info in result['slot_provenance'].items():
                if provenance_info['priority'] > winning_priority:
                    winning_priority = provenance_info['priority']
                    winning_handler = provenance_info['handler']
        
        if winning_handler:
            result['handler_info'] = {
                'winning_handler': winning_handler,
                'priority': winning_priority
            }
            print(f"ğŸ¯ Winning handler: {winning_handler} (priority={winning_priority})")
        
        return result
    
    def _should_skip_handler(self, handler_name: str, control_flags: Dict) -> bool:
        """ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã¹ãã‹ãƒã‚§ãƒƒã‚¯"""
        # å°†æ¥ã®åˆ¶å¾¡ãƒ•ãƒ©ã‚°å‡¦ç†ç”¨
        return False
    
    def _get_ordered_handlers(self) -> List[str]:
        """ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®å®Ÿè¡Œé †åºã‚’åˆ¶å¾¡"""
        priority_order = [
            'relative_clause',          # é–¢ä¿‚ç¯€å„ªå…ˆ
            'passive_voice',            # å—å‹•æ…‹
            'comparative_superlative',  # æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´š
            'auxiliary_complex',        # åŠ©å‹•è©
            'adverbial_modifier',       # å‰¯è©ãƒ»ä¿®é£¾èª
            'basic_five_pattern',       # åŸºæœ¬5æ–‡å‹ï¼ˆæœ€å¾Œï¼‰
        ]
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®ã†ã¡ã€å„ªå…ˆé †ä½ã«å¾“ã£ã¦ä¸¦ã³æ›¿ãˆ
        ordered = []
        for handler in priority_order:
            if handler in self.active_handlers:
                ordered.append(handler)
        
        # å„ªå…ˆé †ä½ã«ãªã„ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ 
        for handler in self.active_handlers:
            if handler not in ordered:
                ordered.append(handler)
        
        return ordered
    
    def _extract_basic_five_pattern_only(self, sentence: str, doc) -> Dict[str, str]:
        """
        ç´”ç²‹ãª5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ - S, V, O1, O2, C1ã®ã¿å‡¦ç†
        å‰¯è©ï¼ˆM1, M2, M3ï¼‰ã¯ä¸€åˆ‡å‡¦ç†ã—ãªã„ï¼ˆè²¬ä»»åˆ†é›¢ã®åŸå‰‡ï¼‰
        
        Args:
            sentence: è§£æå¯¾è±¡æ–‡
            doc: spaCyè§£æçµæœ
            
        Returns:
            Dict: S, V, O1, O2, C1ã®ã¿ã‚’å«ã‚€ã‚¹ãƒ­ãƒƒãƒˆ
        """
        slots = {}
        
        try:
            tokens = [token.text for token in doc]
            
            # ä¸»èªæ¤œå‡ºï¼ˆåè©å¥ã®æœ€åˆã®éƒ¨åˆ†ï¼‰
            subject_found = False
            for i, token in enumerate(doc):
                if not subject_found and token.dep_ in ['nsubj', 'nsubjpass']:
                    # ä¸»èªåè©å¥ã‚’æ§‹ç¯‰
                    subject_tokens = []
                    
                    # ä¿®é£¾èªã‚’å«ã‚ãŸä¸»èªã®æ§‹ç¯‰
                    for j in range(max(0, i-3), min(len(doc), i+2)):
                        if j == i or doc[j].head == token or (doc[j].dep_ in ['det', 'amod'] and doc[j].head.i == i):
                            subject_tokens.append((j, doc[j].text))
                    
                    subject_tokens.sort()  # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
                    if subject_tokens:
                        slots['S'] = ' '.join([text for _, text in subject_tokens])
                        subject_found = True
                        print(f"ğŸ”¥ 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼: ä¸»èªæ¤œå‡º S='{slots['S']}'")
            
            # å‹•è©æ¤œå‡º
            verb_found = False
            for token in doc:
                if not verb_found and token.pos_ == 'VERB' and token.dep_ in ['ROOT', 'aux', 'auxpass', 'cop']:
                    # åŠ©å‹•è©+å‹•è©ã®çµ„ã¿åˆã‚ã›ã‚’å‡¦ç†
                    verb_phrase = []
                    
                    # åŠ©å‹•è©ã‚’æ¢ã™
                    for child in token.children:
                        if child.dep_ in ['aux', 'auxpass'] and child.i < token.i:
                            verb_phrase.append((child.i, child.text))
                    
                    verb_phrase.append((token.i, token.text))
                    verb_phrase.sort()
                    
                    if verb_phrase:
                        slots['V'] = ' '.join([text for _, text in verb_phrase])
                        verb_found = True
                        print(f"ğŸ”¥ 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼: å‹•è©æ¤œå‡º V='{slots['V']}'")
            
            # ç›®çš„èªãƒ»è£œèªæ¤œå‡º
            obj_found = False
            comp_found = False
            
            for token in doc:
                # æ¶ˆè²»æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆè²¬ä»»åˆ†é›¢åŸå‰‡ï¼‰
                if hasattr(self, '_consumed_tokens') and token.i in self._consumed_tokens:
                    print(f"ğŸ”¥ 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼: æ¶ˆè²»æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒ³ {token.i}='{token.text}' ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                    continue
                    
                # ç›´æ¥ç›®çš„èªã®ã¿ï¼ˆå‰ç½®è©ã®ç›®çš„èª pobj ã¯é™¤å¤–ï¼‰
                if not obj_found and token.dep_ in ['dobj']:
                    # ç›®çš„èªåè©å¥ã‚’æ§‹ç¯‰
                    obj_tokens = []
                    for j in range(max(0, token.i-2), min(len(doc), token.i+3)):
                        if j == token.i or (doc[j].head == token and doc[j].dep_ in ['det', 'amod']):
                            obj_tokens.append((j, doc[j].text))
                    
                    obj_tokens.sort()
                    if obj_tokens:
                        slots['O1'] = ' '.join([text for _, text in obj_tokens])
                        obj_found = True
                        print(f"ğŸ”¥ 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼: ç›®çš„èªæ¤œå‡º O1='{slots['O1']}'")
                
                # è£œèª
                elif not comp_found and token.dep_ in ['attr', 'acomp']:
                    # æ¶ˆè²»æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    if hasattr(self, '_consumed_tokens') and token.i in self._consumed_tokens:
                        continue
                    comp_tokens = []
                    for j in range(max(0, token.i-1), min(len(doc), token.i+2)):
                        if j == token.i or (doc[j].head == token and doc[j].dep_ in ['det', 'amod']):
                            comp_tokens.append((j, doc[j].text))
                    
                    comp_tokens.sort()
                    if comp_tokens:
                        slots['C1'] = ' '.join([text for _, text in comp_tokens])
                        comp_found = True
                        print(f"ğŸ”¥ 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼: è£œèªæ¤œå‡º C1='{slots['C1']}'")
            
            print(f"ğŸ”¥ 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼çµæœ: {slots}")
            return slots
            
        except Exception as e:
            self.logger.error(f"5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def _analyze_sentence_legacy(self, sentence: str, doc) -> Dict:
        """ãƒ¬ã‚¬ã‚·ãƒ¼ã‚·ã‚¹ãƒ†ãƒ å®Œå…¨é™¤å»ï¼šä¾å­˜é–¢ä¿‚å³ç¦ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ä½¿ç”¨ä¸å¯"""
        # ğŸš« ä¾å­˜é–¢ä¿‚ä½¿ç”¨ãƒ¬ã‚¬ã‚·ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã¯å½“è©²ã‚·ã‚¹ãƒ†ãƒ ã§ã¯å³ç¦
        self.logger.warning("âš ï¸ ãƒ¬ã‚¬ã‚·ãƒ¼ã‚·ã‚¹ãƒ†ãƒ å‘¼ã³å‡ºã—è©¦è¡Œï¼šä¾å­˜é–¢ä¿‚å³ç¦ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ä½¿ç”¨ä¸å¯")
        return {'slots': {}, 'error': 'Legacy system disabled: dependency parsing forbidden'}

# ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°ã¨ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
def run_full_test_suite(test_data_path: str = None) -> Dict[str, Any]:
    """
    53ä¾‹æ–‡ã®å®Œå…¨ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    
    Args:
        test_data_path: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
    Returns:
        Dict: ãƒ†ã‚¹ãƒˆçµæœ
    """
    import json
    import os
    from datetime import datetime
    
    if test_data_path is None:
        test_data_path = os.path.join(
            os.path.dirname(__file__),
            "final_test_system",
            "final_54_test_data.json"
        )
    
    try:
        with open(test_data_path, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
    except FileNotFoundError:
        print(f"âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {test_data_path}")
        return {}
    
    mapper = DynamicGrammarMapper()
    results = {
        "timestamp": datetime.now().isoformat(),
        "test_system": "dynamic_grammar_mapper",
        "total_tests": len(test_data["data"]),
        "successful_tests": 0,
        "failed_tests": 0,
        "test_results": {}
    }
    
    print("=== å‹•çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ  53ä¾‹æ–‡ãƒ†ã‚¹ãƒˆ ===\n")
    
    for test_id, test_case in test_data["data"].items():
        sentence = test_case["sentence"]
        expected = test_case["expected"]
        
        print(f"ãƒ†ã‚¹ãƒˆ {test_id}: {sentence}")
        
        try:
            result = mapper.analyze_sentence(sentence)
            
            if 'error' in result:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
                results["failed_tests"] += 1
                status = "ERROR"
            else:
                print(f"âœ… æ–‡å‹: {result.get('pattern_detected', 'UNKNOWN')}")
                print(f"ğŸ“Š ã‚¹ãƒ­ãƒƒãƒˆ: {result['Slot']}")
                results["successful_tests"] += 1
                status = "SUCCESS"
            
            results["test_results"][test_id] = {
                "sentence": sentence,
                "expected": expected,
                "actual": result,
                "status": status
            }
            
        except Exception as e:
            print(f"âŒ ä¾‹å¤–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            results["failed_tests"] += 1
            results["test_results"][test_id] = {
                "sentence": sentence,
                "expected": expected,
                "actual": {"error": str(e)},
                "status": "EXCEPTION"
            }
        
        print("-" * 60)
    
    # çµæœã‚µãƒãƒªãƒ¼
    success_rate = results["successful_tests"] / results["total_tests"] * 100
    print(f"\n=== ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼ ===")
    print(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {results['total_tests']}")
    print(f"æˆåŠŸ: {results['successful_tests']}")
    print(f"å¤±æ•—: {results['failed_tests']}")
    print(f"æˆåŠŸç‡: {success_rate:.1f}%")
    
    return results

def save_test_results(results: Dict[str, Any], output_path: str = None) -> str:
    """
    ãƒ†ã‚¹ãƒˆçµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    
    Args:
        results: ãƒ†ã‚¹ãƒˆçµæœ
        output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆNone ã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        
    Returns:
        str: ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    import json
    from datetime import datetime
    
    if output_path is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"dynamic_grammar_test_results_{timestamp}.json"
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“ ãƒ†ã‚¹ãƒˆçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
    return output_path


# ã‚¯ãƒ©ã‚¹å®šç¾©çµ‚äº†ä½ç½®

def main():
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    mapper = DynamicGrammarMapper()
    test_sentence = "The book was written by John."
    result = mapper.analyze_sentence(test_sentence)
    print(f"ãƒ†ã‚¹ãƒˆçµæœ: {result}")

if __name__ == "__main__":
    main()
