"""
Phase 5: è¤‡åˆæ§‹æ–‡æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ 
é–¢ä¿‚è©žæ§‹æ–‡ã€åè©žç¯€ã€ä¸å®šè©žã€å‹•åè©žã€ä»®å®šæ³•ã€å—å‹•æ…‹ã®æ¤œå‡º

å¯¾è±¡æ§‹æ–‡:
1. é–¢ä¿‚è©žæ§‹æ–‡ (åˆ¶é™çš„/éžåˆ¶é™çš„/çœç•¥)
2. åè©žç¯€æ§‹æ–‡ (that/wh-clause/åŒæ ¼ç¯€)  
3. ä¸å®šè©žæ§‹æ–‡ (ç›®çš„/çµæžœ)
4. å‹•åè©žæ§‹æ–‡
5. ä»®å®šæ³•æ§‹æ–‡
6. å—å‹•æ…‹æ§‹æ–‡
"""

import spacy
import re
from enum import Enum
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple

class ComplexConstructionType(Enum):
    # é–¢ä¿‚è©žæ§‹æ–‡ (3)
    RELATIVE_CLAUSE_RESTRICTIVE = "åˆ¶é™çš„é–¢ä¿‚è©žç¯€"
    RELATIVE_CLAUSE_NON_RESTRICTIVE = "éžåˆ¶é™çš„é–¢ä¿‚è©žç¯€"
    RELATIVE_PRONOUN_OMISSION = "é–¢ä¿‚ä»£åè©žçœç•¥"
    
    # åè©žå¥/ç¯€æ§‹æ–‡ (2)
    NOUN_CLAUSE = "åè©žç¯€"
    APPOSITIVE_CLAUSE = "åŒæ ¼ç¯€"
    
    # ä¸å®šè©žæ§‹æ–‡ (2)
    INFINITIVE_PURPOSE = "ç›®çš„ã®ä¸å®šè©ž"
    INFINITIVE_RESULT = "çµæžœã®ä¸å®šè©ž"
    
    # å‹•åè©žæ§‹æ–‡ (1)
    GERUND_CONSTRUCTION = "å‹•åè©žæ§‹æ–‡"
    
    # ä»®å®šæ³•æ§‹æ–‡ (1)
    SUBJUNCTIVE_MOOD = "ä»®å®šæ³•"
    
    # å—å‹•æ…‹æ§‹æ–‡ (1)
    PASSIVE_VOICE = "å—å‹•æ…‹"

@dataclass
class ComplexConstructionResult:
    construction_type: ComplexConstructionType
    confidence: float
    original_text: str
    analysis: str
    rephrase_slots: Dict[str, str]

class Phase5ComplexConstructions:
    def __init__(self):
        try:
            self.nlp = spacy.load("en_core_web_sm")
        except OSError:
            print("spaCyè‹±èªžãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚'python -m spacy download en_core_web_sm'ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
            self.nlp = None
            
        # é–¢ä¿‚ä»£åè©žãƒªã‚¹ãƒˆ
        self.relative_pronouns = {
            'who', 'whom', 'whose', 'which', 'that', 'where', 'when', 'why'
        }
        
        # åè©žç¯€å°Žå…¥èªž
        self.noun_clause_introducers = {
            'that', 'what', 'who', 'whom', 'whose', 'which', 'where', 'when', 
            'why', 'how', 'whether', 'if'
        }
        
        # ç›®çš„ã‚’è¡¨ã™è¡¨ç¾
        self.purpose_markers = {
            'to', 'in order to', 'so as to', 'in order that', 'so that'
        }
        
        # çµæžœã‚’è¡¨ã™è¡¨ç¾  
        self.result_markers = {
            'so as to', 'such as to', 'too', 'enough'
        }
        
        # å‹•åè©žæ§‹æ–‡ãƒžãƒ¼ã‚«ãƒ¼
        self.gerund_markers = {
            'enjoy', 'finish', 'suggest', 'avoid', 'consider', 'admit',
            'deny', 'imagine', 'practice', 'risk', 'mind', 'miss'
        }
        
        # ä»®å®šæ³•ãƒžãƒ¼ã‚«ãƒ¼
        self.subjunctive_markers = {
            'if', 'unless', 'suppose', 'supposing', 'provided', 'providing',
            'as long as', 'on condition that', 'wish', 'would rather'
        }

    def detect_construction(self, text: str) -> List[ComplexConstructionResult]:
        """è¤‡åˆæ§‹æ–‡ã‚’æ¤œå‡ºã™ã‚‹"""
        if not self.nlp:
            return []
            
        results = []
        doc = self.nlp(text)
        
        # å„æ§‹æ–‡ã‚¿ã‚¤ãƒ—ã‚’ãƒã‚§ãƒƒã‚¯
        results.extend(self._detect_relative_clauses(doc, text))
        results.extend(self._detect_noun_clauses(doc, text))
        results.extend(self._detect_infinitive_constructions(doc, text))
        results.extend(self._detect_gerund_constructions(doc, text))
        results.extend(self._detect_subjunctive_mood(doc, text))
        results.extend(self._detect_passive_voice(doc, text))
        
        return results

    def _detect_relative_clauses(self, doc, text: str) -> List[ComplexConstructionResult]:
        """é–¢ä¿‚è©žæ§‹æ–‡ã‚’æ¤œå‡ºã™ã‚‹"""
        results = []
        
        for token in doc:
            # åˆ¶é™çš„é–¢ä¿‚è©žç¯€
            if (token.text.lower() in self.relative_pronouns and 
                token.dep_ in ['nsubj', 'dobj', 'pobj']):
                
                # ã‚³ãƒ³ãƒžã§åŒºåˆ‡ã‚‰ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                is_non_restrictive = self._has_comma_separation(doc, token)
                
                if is_non_restrictive:
                    construction_type = ComplexConstructionType.RELATIVE_CLAUSE_NON_RESTRICTIVE
                    analysis = f"éžåˆ¶é™çš„é–¢ä¿‚è©žç¯€: '{token.text}'ã§å°Žã‹ã‚Œã‚‹è£œè¶³æƒ…å ±"
                else:
                    construction_type = ComplexConstructionType.RELATIVE_CLAUSE_RESTRICTIVE
                    analysis = f"åˆ¶é™çš„é–¢ä¿‚è©žç¯€: '{token.text}'ã§å°Žã‹ã‚Œã‚‹å¿…è¦ä¸å¯æ¬ ãªæƒ…å ±"
                
                rephrase_slots = {
                    'relative_pronoun': token.text,
                    'antecedent': str(token.head),
                    'relative_clause': self._extract_relative_clause(doc, token)
                }
                
                results.append(ComplexConstructionResult(
                    construction_type=construction_type,
                    confidence=0.85,
                    original_text=text,
                    analysis=analysis,
                    rephrase_slots=rephrase_slots
                ))
        
        # é–¢ä¿‚ä»£åè©žçœç•¥ã‚’ãƒã‚§ãƒƒã‚¯
        omission_result = self._detect_relative_pronoun_omission(doc, text)
        if omission_result:
            results.append(omission_result)
            
        return results

    def _detect_noun_clauses(self, doc, text: str) -> List[ComplexConstructionResult]:
        """åè©žç¯€ã‚’æ¤œå‡ºã™ã‚‹"""
        results = []
        
        for token in doc:
            if token.text.lower() in self.noun_clause_introducers:
                # thatç¯€ã®æ¤œå‡º
                if token.text.lower() == 'that' and token.dep_ == 'mark':
                    clause_type = "thatç¯€"
                    analysis = f"åè©žç¯€: '{token.text}'ã§å°Žã‹ã‚Œã‚‹å†…å®¹ç¯€"
                    
                # wh-ç¯€ã®æ¤œå‡º
                elif token.text.lower() in ['what', 'who', 'where', 'when', 'why', 'how']:
                    clause_type = "wh-ç¯€"
                    analysis = f"åè©žç¯€: '{token.text}'ã§å°Žã‹ã‚Œã‚‹ç–‘å•è©žç¯€"
                    
                # åŒæ ¼ç¯€ã®æ¤œå‡º
                elif (token.text.lower() == 'that' and 
                      self._is_appositive_clause(doc, token)):
                    construction_type = ComplexConstructionType.APPOSITIVE_CLAUSE
                    analysis = f"åŒæ ¼ç¯€: åè©žã®å†…å®¹ã‚’èª¬æ˜Žã™ã‚‹'{token.text}'ç¯€"
                    
                    results.append(ComplexConstructionResult(
                        construction_type=construction_type,
                        confidence=0.80,
                        original_text=text,
                        analysis=analysis,
                        rephrase_slots={
                            'noun': str(token.head.head) if token.head.head else '',
                            'clause_content': self._extract_clause_content(doc, token)
                        }
                    ))
                    continue
                else:
                    continue
                
                results.append(ComplexConstructionResult(
                    construction_type=ComplexConstructionType.NOUN_CLAUSE,
                    confidence=0.85,
                    original_text=text,
                    analysis=analysis,
                    rephrase_slots={
                        'clause_introducer': token.text,
                        'clause_content': self._extract_clause_content(doc, token),
                        'clause_type': clause_type
                    }
                ))
        
        return results

    def _detect_infinitive_constructions(self, doc, text: str) -> List[ComplexConstructionResult]:
        """ä¸å®šè©žæ§‹æ–‡ã‚’æ¤œå‡ºã™ã‚‹"""
        results = []
        
        for token in doc:
            if token.text.lower() == 'to' and token.pos_ == 'PART':
                # ç›®çš„ã®ä¸å®šè©ž
                if self._is_purpose_infinitive(doc, token, text):
                    results.append(ComplexConstructionResult(
                        construction_type=ComplexConstructionType.INFINITIVE_PURPOSE,
                        confidence=0.80,
                        original_text=text,
                        analysis="ç›®çš„ã®ä¸å®šè©ž: è¡Œç‚ºã®ç›®çš„ã‚’è¡¨ã™",
                        rephrase_slots={
                            'main_verb': self._find_main_verb(doc, token),
                            'purpose_infinitive': self._extract_infinitive_phrase(doc, token)
                        }
                    ))
                
                # çµæžœã®ä¸å®šè©ž
                elif self._is_result_infinitive(doc, token, text):
                    results.append(ComplexConstructionResult(
                        construction_type=ComplexConstructionType.INFINITIVE_RESULT,
                        confidence=0.75,
                        original_text=text,
                        analysis="çµæžœã®ä¸å®šè©ž: è¡Œç‚ºã®çµæžœã‚’è¡¨ã™",
                        rephrase_slots={
                            'cause': self._find_cause_element(doc, token),
                            'result_infinitive': self._extract_infinitive_phrase(doc, token)
                        }
                    ))
        
        return results

    def _detect_gerund_constructions(self, doc, text: str) -> List[ComplexConstructionResult]:
        """å‹•åè©žæ§‹æ–‡ã‚’æ¤œå‡ºã™ã‚‹"""
        results = []
        
        for token in doc:
            # å‹•åè©žã‚’å–ã‚‹å‹•è©žã®æ¤œå‡ºã‚’æ‹¡å¼µ
            if token.pos_ == 'VERB':
                for child in token.children:
                    # VBGï¼ˆå‹•åè©žï¼‰ã‚’ç›®çš„èªžã¨ã—ã¦å–ã‚‹
                    if child.tag_ == 'VBG' and child.dep_ in ['dobj', 'xcomp']:
                        # å‹•åè©žã‚’å–ã‚‹å…¸åž‹çš„å‹•è©žã‹ãƒã‚§ãƒƒã‚¯
                        if (token.lemma_ in self.gerund_markers or 
                            self._is_gerund_taking_context(token, child)):
                            
                            results.append(ComplexConstructionResult(
                                construction_type=ComplexConstructionType.GERUND_CONSTRUCTION,
                                confidence=0.85,
                                original_text=text,
                                analysis=f"å‹•åè©žæ§‹æ–‡: '{token.text}'ãŒå‹•åè©ž'{child.text}'ã‚’ç›®çš„èªžã«ã¨ã‚‹",
                                rephrase_slots={
                                    'gerund_taking_verb': token.text,
                                    'gerund': child.text,
                                    'gerund_phrase': self._extract_gerund_phrase(doc, child)
                                }
                            ))
            
            # å‹•åè©žãŒä¸»èªžã«ãªã£ã¦ã„ã‚‹ã‚±ãƒ¼ã‚¹
            elif token.tag_ == 'VBG' and token.dep_ == 'nsubj':
                results.append(ComplexConstructionResult(
                    construction_type=ComplexConstructionType.GERUND_CONSTRUCTION,
                    confidence=0.80,
                    original_text=text,
                    analysis=f"å‹•åè©žæ§‹æ–‡: '{token.text}'ãŒä¸»èªžã¨ã—ã¦æ©Ÿèƒ½",
                    rephrase_slots={
                        'gerund': token.text,
                        'gerund_phrase': self._extract_gerund_phrase(doc, token),
                        'function': 'subject'
                    }
                ))
        
        return results

    def _detect_subjunctive_mood(self, doc, text: str) -> List[ComplexConstructionResult]:
        """ä»®å®šæ³•ã‚’æ¤œå‡ºã™ã‚‹"""
        results = []
        
        # ä»®å®šæ³•ã®å…¸åž‹çš„ãƒ‘ã‚¿ãƒ¼ãƒ³
        if re.search(r'\b(if|wish|would rather|suppose)\b.*\b(were|had|would|could|should)\b', text, re.IGNORECASE):
            # ã‚ˆã‚Šè©³ç´°ãªåˆ†æž
            for token in doc:
                if token.text.lower() in self.subjunctive_markers:
                    # ä»®å®šæ³•éŽåŽ»/éŽåŽ»å®Œäº†ã®æ¤œå‡º
                    subjunctive_type = self._identify_subjunctive_type(doc, token, text)
                    
                    results.append(ComplexConstructionResult(
                        construction_type=ComplexConstructionType.SUBJUNCTIVE_MOOD,
                        confidence=0.75,
                        original_text=text,
                        analysis=f"ä»®å®šæ³•: {subjunctive_type}",
                        rephrase_slots={
                            'condition_marker': token.text,
                            'subjunctive_type': subjunctive_type,
                            'condition_clause': self._extract_condition_clause(doc, token),
                            'main_clause': self._extract_main_clause(doc, token)
                        }
                    ))
                    break
        
        return results

    def _detect_passive_voice(self, doc, text: str) -> List[ComplexConstructionResult]:
        """å—å‹•æ…‹ã‚’æ¤œå‡ºã™ã‚‹"""
        results = []
        
        for token in doc:
            # æ¨™æº–çš„ãªå—å‹•æ…‹: auxpass + VBN
            if token.dep_ == 'auxpass' and token.lemma_ == 'be':
                # å¯¾å¿œã™ã‚‹éŽåŽ»åˆ†è©žã‚’æŽ¢ã™
                for sibling in token.head.children:
                    if sibling.dep_ == 'auxpass' and sibling == token:
                        past_participle = token.head
                        if past_participle.tag_ == 'VBN':
                            # ä¸»èªžã‚’æŽ¢ã™ï¼ˆnsubjpassï¼‰
                            passive_subject = self._find_passive_subject_by_dep(doc)
                            # byå¥ã‚’æŽ¢ã™
                            by_phrase = self._find_by_phrase(doc, past_participle)
                            
                            results.append(ComplexConstructionResult(
                                construction_type=ComplexConstructionType.PASSIVE_VOICE,
                                confidence=0.95,
                                original_text=text,
                                analysis=f"å—å‹•æ…‹: beå‹•è©ž'{token.text}' + éŽåŽ»åˆ†è©ž'{past_participle.text}'",
                                rephrase_slots={
                                    'be_verb': token.text,
                                    'past_participle': past_participle.text,
                                    'subject': passive_subject,
                                    'by_phrase': by_phrase if by_phrase else 'ãªã—'
                                }
                            ))
                            break
        
        # åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³: beå‹•è©ž + éŽåŽ»åˆ†è©žã®ç›´æŽ¥æ¤œå‡º
        for token in doc:
            if (token.lemma_ == 'be' and token.pos_ in ['VERB', 'AUX'] and
                any(child.tag_ == 'VBN' for child in token.children)):
                
                past_participle = next(
                    (child for child in token.children if child.tag_ == 'VBN'), 
                    None
                )
                
                if past_participle and past_participle.dep_ != 'ROOT':
                    continue  # æ—¢ã«å‡¦ç†æ¸ˆã¿
                
                # byå¥ã®æ¤œå‡º
                by_phrase = self._find_by_phrase(doc, past_participle or token)
                
                results.append(ComplexConstructionResult(
                    construction_type=ComplexConstructionType.PASSIVE_VOICE,
                    confidence=0.90,
                    original_text=text,
                    analysis=f"å—å‹•æ…‹: beå‹•è©ž'{token.text}' + éŽåŽ»åˆ†è©ž'{past_participle.text if past_participle else 'unknown'}'",
                    rephrase_slots={
                        'be_verb': token.text,
                        'past_participle': past_participle.text if past_participle else '',
                        'subject': self._find_passive_subject(doc, token),
                        'by_phrase': by_phrase if by_phrase else 'ãªã—'
                    }
                ))
        
        # get/have + éŽåŽ»åˆ†è©žã®å—å‹•æ…‹ã‚‚æ¤œå‡º
        for token in doc:
            if token.lemma_ in ['get', 'have'] and token.pos_ == 'VERB':
                for child in token.children:
                    if child.tag_ == 'VBN' and child.dep_ in ['dobj', 'ccomp']:
                        results.append(ComplexConstructionResult(
                            construction_type=ComplexConstructionType.PASSIVE_VOICE,
                            confidence=0.75,
                            original_text=text,
                            analysis=f"å—å‹•æ…‹: {token.text}å‹•è©ž + éŽåŽ»åˆ†è©ž'{child.text}'",
                            rephrase_slots={
                                'auxiliary_verb': token.text,
                                'past_participle': child.text,
                                'subject': self._find_passive_subject(doc, token),
                                'by_phrase': self._find_by_phrase(doc, child) or 'ãªã—'
                            }
                        ))
        
        return results

    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    def _has_comma_separation(self, doc, token) -> bool:
        """é–¢ä¿‚è©žç¯€ãŒã‚³ãƒ³ãƒžã§åŒºåˆ‡ã‚‰ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        for i, t in enumerate(doc):
            if t == token:
                # å‰ã«ã‚³ãƒ³ãƒžãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if i > 0 and doc[i-1].text == ',':
                    return True
                # é–¢ä¿‚è©žç¯€ã®çµ‚ã‚ã‚Šã«ã‚³ãƒ³ãƒžãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯  
                for j in range(i+1, len(doc)):
                    if doc[j].text == ',' and doc[j].head == token.head:
                        return True
                break
        return False

    def _extract_relative_clause(self, doc, relative_pronoun) -> str:
        """é–¢ä¿‚è©žç¯€ã‚’æŠ½å‡º"""
        clause_tokens = []
        for token in doc:
            if token.head == relative_pronoun or self._is_in_relative_clause(token, relative_pronoun):
                clause_tokens.append(token)
        return ' '.join([t.text for t in sorted(clause_tokens, key=lambda x: x.i)])

    def _detect_relative_pronoun_omission(self, doc, text: str) -> Optional[ComplexConstructionResult]:
        """é–¢ä¿‚ä»£åè©žã®çœç•¥ã‚’æ¤œå‡º"""
        # å…¸åž‹çš„ãªçœç•¥ãƒ‘ã‚¿ãƒ¼ãƒ³: "The book I read" (thatçœç•¥)
        for token in doc:
            if (token.pos_ == 'NOUN' and 
                any(child.pos_ == 'VERB' and child.dep_ == 'relcl' for child in token.children)):
                
                return ComplexConstructionResult(
                    construction_type=ComplexConstructionType.RELATIVE_PRONOUN_OMISSION,
                    confidence=0.70,
                    original_text=text,
                    analysis="é–¢ä¿‚ä»£åè©žçœç•¥: thatã¾ãŸã¯whichãŒçœç•¥ã•ã‚Œã¦ã„ã‚‹",
                    rephrase_slots={
                        'antecedent': token.text,
                        'omitted_pronoun': 'that/which',
                        'relative_clause': str(next(child for child in token.children if child.dep_ == 'relcl'))
                    }
                )
        return None

    def _is_appositive_clause(self, doc, token) -> bool:
        """åŒæ ¼ç¯€ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        # ç‰¹å®šã®åè©žï¼ˆfact, idea, newsç­‰ï¼‰ã®å¾Œã®thatç¯€
        appositive_nouns = {'fact', 'idea', 'news', 'belief', 'hope', 'fear', 'rumor'}
        if token.head and token.head.lemma_.lower() in appositive_nouns:
            return True
        return False

    def _extract_clause_content(self, doc, introducer) -> str:
        """ç¯€ã®å†…å®¹ã‚’æŠ½å‡º"""
        clause_tokens = []
        for token in doc:
            if token.head == introducer or self._is_in_clause(token, introducer):
                clause_tokens.append(token)
        return ' '.join([t.text for t in sorted(clause_tokens, key=lambda x: x.i)])

    def _is_purpose_infinitive(self, doc, to_token, text: str) -> bool:
        """ç›®çš„ã®ä¸å®šè©žã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        # "in order to", "so as to"ãªã©ã®æ˜Žç¤ºçš„ãƒžãƒ¼ã‚«ãƒ¼
        text_lower = text.lower()
        if any(marker in text_lower for marker in self.purpose_markers):
            return True
        
        # æ–‡è„ˆã‹ã‚‰ç›®çš„ã‚’æŽ¨å®š
        infinitive_verb = None
        for child in to_token.children:
            if child.pos_ == 'VERB':
                infinitive_verb = child
                break
        
        if infinitive_verb:
            # å‹•ä½œã‚’è¡¨ã™å‹•è©žã®å ´åˆã¯ç›®çš„ã®å¯èƒ½æ€§ãŒé«˜ã„
            action_verbs = {'go', 'come', 'buy', 'get', 'find', 'see', 'meet', 'help'}
            if infinitive_verb.lemma_ in action_verbs:
                return True
        
        return False

    def _is_result_infinitive(self, doc, to_token, text: str) -> bool:
        """çµæžœã®ä¸å®šè©žã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        # "too...to", "so...as to", "such...as to"ãƒ‘ã‚¿ãƒ¼ãƒ³
        if re.search(r'\b(too|so|such)\b.*\bto\b', text, re.IGNORECASE):
            return True
        
        # "enough to"ãƒ‘ã‚¿ãƒ¼ãƒ³
        if re.search(r'\benough\s+to\b', text, re.IGNORECASE):
            return True
        
        return False

    def _find_main_verb(self, doc, to_token) -> str:
        """ä¸»å‹•è©žã‚’è¦‹ã¤ã‘ã‚‹"""
        for token in doc:
            if token.pos_ == 'VERB' and token.dep_ == 'ROOT':
                return token.text
        return ''

    def _extract_infinitive_phrase(self, doc, to_token) -> str:
        """ä¸å®šè©žå¥ã‚’æŠ½å‡º"""
        phrase_tokens = [to_token]
        for child in to_token.children:
            if child.pos_ == 'VERB':
                phrase_tokens.append(child)
                phrase_tokens.extend(list(child.subtree))
        return ' '.join([t.text for t in sorted(set(phrase_tokens), key=lambda x: x.i)])

    def _find_cause_element(self, doc, to_token) -> str:
        """åŽŸå› è¦ç´ ã‚’è¦‹ã¤ã‘ã‚‹"""
        for token in doc:
            if token.text.lower() in ['too', 'so', 'such', 'enough']:
                return token.text
        return ''

    def _extract_gerund_phrase(self, doc, gerund_token) -> str:
        """å‹•åè©žå¥ã‚’æŠ½å‡º"""
        phrase_tokens = [gerund_token]
        phrase_tokens.extend(list(gerund_token.subtree))
        return ' '.join([t.text for t in sorted(phrase_tokens, key=lambda x: x.i)])

    def _identify_subjunctive_type(self, doc, marker_token, text: str) -> str:
        """ä»®å®šæ³•ã®ç¨®é¡žã‚’ç‰¹å®š"""
        if 'were' in text or 'had' in text:
            if 'had' in text and re.search(r'\bhad\s+\w+ed\b', text):
                return "ä»®å®šæ³•éŽåŽ»å®Œäº†ï¼ˆéŽåŽ»ã®ä»®å®šï¼‰"
            else:
                return "ä»®å®šæ³•éŽåŽ»ï¼ˆç¾åœ¨ã®ä»®å®šï¼‰"
        elif any(modal in text for modal in ['would', 'could', 'should', 'might']):
            return "ä»®å®šæ³•ï¼ˆæ¡ä»¶æ–‡ã®å¸°çµï¼‰"
        else:
            return "ä»®å®šæ³•"

    def _extract_condition_clause(self, doc, marker_token) -> str:
        """æ¡ä»¶ç¯€ã‚’æŠ½å‡º"""
        # æ¡ä»¶ç¯€ã®æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯
        clause_tokens = []
        for token in doc:
            if (token.head == marker_token or 
                self._is_in_conditional_clause(token, marker_token)):
                clause_tokens.append(token)
        return ' '.join([t.text for t in sorted(clause_tokens, key=lambda x: x.i)])

    def _extract_main_clause(self, doc, marker_token) -> str:
        """ä¸»ç¯€ã‚’æŠ½å‡º"""
        main_clause_tokens = []
        for token in doc:
            if token.dep_ == 'ROOT' or self._is_in_main_clause(token):
                main_clause_tokens.append(token)
        return ' '.join([t.text for t in sorted(main_clause_tokens, key=lambda x: x.i)])

    def _find_by_phrase(self, doc, past_participle) -> Optional[str]:
        """byå¥ã‚’è¦‹ã¤ã‘ã‚‹"""
        for child in past_participle.children:
            if child.text.lower() == 'by':
                by_phrase_tokens = [child]
                by_phrase_tokens.extend(list(child.subtree))
                return ' '.join([t.text for t in sorted(by_phrase_tokens, key=lambda x: x.i)])
        return None

    def _find_passive_subject(self, doc, be_verb) -> str:
        """å—å‹•æ…‹ã®ä¸»èªžã‚’è¦‹ã¤ã‘ã‚‹"""
        for child in be_verb.children:
            if child.dep_ == 'nsubj':
                return child.text
        return ''

    def _is_in_relative_clause(self, token, relative_pronoun) -> bool:
        """ãƒˆãƒ¼ã‚¯ãƒ³ãŒé–¢ä¿‚è©žç¯€ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        current = token
        while current.head != current:
            if current.head == relative_pronoun:
                return True
            current = current.head
        return False

    def _is_in_clause(self, token, introducer) -> bool:
        """ãƒˆãƒ¼ã‚¯ãƒ³ãŒç¯€ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        current = token
        while current.head != current:
            if current.head == introducer:
                return True
            current = current.head
        return False

    def _is_in_conditional_clause(self, token, marker) -> bool:
        """ãƒˆãƒ¼ã‚¯ãƒ³ãŒæ¡ä»¶ç¯€ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        return self._is_in_clause(token, marker)

    def _is_in_main_clause(self, token) -> bool:
        """ãƒˆãƒ¼ã‚¯ãƒ³ãŒä¸»ç¯€ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        return token.dep_ in ['ROOT', 'ccomp', 'xcomp']

    def _is_gerund_taking_context(self, verb_token, gerund_token) -> bool:
        """å‹•åè©žã‚’å–ã‚‹æ–‡è„ˆã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        # enjoy, like, love, hate, start, begin, continueãªã©
        gerund_taking_verbs = {
            'enjoy', 'like', 'love', 'hate', 'start', 'begin', 'continue',
            'stop', 'quit', 'finish', 'complete', 'suggest', 'recommend',
            'avoid', 'consider', 'practice', 'keep', 'mind', 'miss', 'risk'
        }
        return verb_token.lemma_ in gerund_taking_verbs

    def _find_passive_subject_by_dep(self, doc) -> str:
        """ä¾å­˜é–¢ä¿‚ã‚’ä½¿ã£ã¦å—å‹•æ…‹ã®ä¸»èªžã‚’è¦‹ã¤ã‘ã‚‹"""
        for token in doc:
            if token.dep_ == 'nsubjpass':
                return token.text
        return ''

def test_phase5_constructions():
    """Phase 5æ§‹æ–‡ã®ãƒ†ã‚¹ãƒˆ"""
    detector = Phase5ComplexConstructions()
    
    test_cases = [
        # é–¢ä¿‚è©žæ§‹æ–‡
        "The book that I read was interesting.",           # åˆ¶é™çš„é–¢ä¿‚è©žç¯€
        "My brother, who lives in Tokyo, is a doctor.",    # éžåˆ¶é™çš„é–¢ä¿‚è©žç¯€  
        "The movie I watched yesterday was great.",        # é–¢ä¿‚ä»£åè©žçœç•¥
        
        # åè©žç¯€
        "I think that he is right.",                       # thatç¯€
        "I know what you mean.",                           # wh-ç¯€
        "The fact that he failed surprised us.",           # åŒæ ¼ç¯€
        
        # ä¸å®šè©žæ§‹æ–‡
        "I went to the store to buy milk.",                # ç›®çš„ã®ä¸å®šè©ž
        "He is too young to drive.",                       # çµæžœã®ä¸å®šè©ž
        
        # å‹•åè©žæ§‹æ–‡
        "I enjoy reading books.",                          # å‹•åè©žæ§‹æ–‡
        
        # ä»®å®šæ³•
        "If I were you, I would accept the offer.",        # ä»®å®šæ³•éŽåŽ»
        
        # å—å‹•æ…‹
        "The letter was written by John.",                 # å—å‹•æ…‹
    ]
    
    print("ðŸ§ª Phase 5: è¤‡åˆæ§‹æ–‡æ¤œå‡ºãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    total_tests = len(test_cases)
    successful_detections = 0
    
    for i, test_sentence in enumerate(test_cases, 1):
        print(f"\nðŸ“ ãƒ†ã‚¹ãƒˆ {i}: {test_sentence}")
        
        results = detector.detect_construction(test_sentence)
        
        if results:
            successful_detections += 1
            print("âœ… æ¤œå‡ºæˆåŠŸ:")
            for result in results:
                print(f"   ðŸŽ¯ æ§‹æ–‡: {result.construction_type.value}")
                print(f"   ðŸ“Š ä¿¡é ¼åº¦: {result.confidence:.2f}")
                print(f"   ðŸ“‹ åˆ†æž: {result.analysis}")
                print(f"   ðŸ”§ ã‚¹ãƒ­ãƒƒãƒˆ: {result.rephrase_slots}")
        else:
            print("âŒ æ¤œå‡ºå¤±æ•—")
    
    print(f"\nðŸ“Š Phase 5ãƒ†ã‚¹ãƒˆçµæžœ:")
    success_rate = (successful_detections / total_tests) * 100
    print(f"âœ… æˆåŠŸçŽ‡: {success_rate:.1f}% ({successful_detections}/{total_tests})")
    print(f"ðŸŽ¯ æ–°è¦æ§‹æ–‡: 10ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè£…")
    
    return success_rate >= 80

if __name__ == "__main__":
    test_phase5_constructions()
