#!/usr/bin/env python3
"""
Unified Stanza-Rephrase Mapper v1.0
===================================

çµ±åˆåž‹æ–‡æ³•åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³ - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ–¹å¼
- 15å€‹åˆ¥ã‚¨ãƒ³ã‚¸ãƒ³ã®çŸ¥è­˜ã‚’çµ±åˆ
- é¸æŠžå•é¡Œã‚’æŽ’é™¤ï¼ˆå…¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼åŒæ™‚å®Ÿè¡Œï¼‰
- Stanza dependency parsing â†’ Rephrase slot mapping
- spaCyè£œå®Œè§£æžï¼ˆStanzaã®èª¤è§£æžç®‡æ‰€å¯¾å¿œï¼‰

ä½œæˆæ—¥: 2025å¹´8æœˆ15æ—¥
Phase 0: åŸºç›¤æ§‹ç¯‰
"""

import stanza
from typing import Dict, List, Optional, Any, Tuple
import json
import logging
from dataclasses import dataclass
from datetime import datetime

try:
    import spacy
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False

@dataclass
class RephraseSlot:
    """Rephraseã‚¹ãƒ­ãƒƒãƒˆè¡¨ç¾"""
    slot_name: str
    content: str
    sub_slots: Dict[str, Any] = None
    confidence: float = 1.0
    source_handler: str = ""

class UnifiedStanzaRephraseMapper:
    """
    çµ±åˆåž‹Stanzaâ†’Rephraseãƒžãƒƒãƒ‘ãƒ¼
    
    æ ¸å¿ƒæ€æƒ³:
    - å…¨æ–‡æ³•ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒåŒæ™‚å®Ÿè¡Œï¼ˆé¸æŠžå•é¡ŒæŽ’é™¤ï¼‰
    - å˜ä¸€Stanzaè§£æžçµæžœã®å¤šè§’çš„åˆ†æž
    - å€‹åˆ¥ã‚¨ãƒ³ã‚¸ãƒ³ã®å®Ÿè£…çŸ¥è­˜ç¶™æ‰¿
    """
    
    def __init__(self, 
                 language='en', 
                 enable_gpu=False,
                 log_level='INFO',
                 use_spacy_hybrid=True):
        """
        çµ±åˆãƒžãƒƒãƒ‘ãƒ¼åˆæœŸåŒ–
        
        Args:
            language: å‡¦ç†è¨€èªžï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 'en'ï¼‰
            enable_gpu: GPUä½¿ç”¨ãƒ•ãƒ©ã‚°
            log_level: ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«
            use_spacy_hybrid: spaCyãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æžä½¿ç”¨ãƒ•ãƒ©ã‚°
        """
        self.language = language
        self.enable_gpu = enable_gpu
        self.use_spacy_hybrid = use_spacy_hybrid
        
        # ãƒ­ã‚°è¨­å®š
        self._setup_logging(log_level)
        
        # Stanzaãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
        self.nlp = None
        self._initialize_stanza_pipeline()
        
        # spaCyãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æžåˆæœŸåŒ–
        self.spacy_nlp = None
        if self.use_spacy_hybrid and SPACY_AVAILABLE:
            self._initialize_spacy_pipeline()
        
        # çµ±è¨ˆæƒ…å ±
        self.processing_count = 0
        self.total_processing_time = 0.0
        self.handler_success_count = {}
        
        # æ®µéšŽçš„ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ç®¡ç†ï¼ˆPhaseåˆ¥è¿½åŠ ï¼‰
        self.active_handlers = []
        
        self.logger.info("ðŸš€ Unified Stanza-Rephrase Mapper v1.0 åˆæœŸåŒ–å®Œäº†")
        if self.spacy_nlp:
            self.logger.info("ðŸ”§ spaCyãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æž æœ‰åŠ¹")
        else:
            self.logger.info("âš ï¸ spaCyãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æž ç„¡åŠ¹")
    
    def _setup_logging(self, level: str):
        """ãƒ­ã‚°è¨­å®š"""
        self.logger = logging.getLogger(f"{__name__}.UnifiedMapper")
        self.logger.setLevel(getattr(logging, level.upper()))
        
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
    
    def _initialize_stanza_pipeline(self):
        """Stanza NLPãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–"""
        try:
            self.logger.info("ðŸ”§ Stanza pipeline åˆæœŸåŒ–ä¸­...")
            
            # åŸºæœ¬çš„ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹æˆ
            processors = 'tokenize,pos,lemma,depparse'
            
            self.nlp = stanza.Pipeline(
                lang=self.language,
                processors=processors,
                download_method=None,  # äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã‚’æƒ³å®š
                use_gpu=self.enable_gpu,
                verbose=False
            )
            
            self.logger.info("âœ… Stanza pipeline åˆæœŸåŒ–æˆåŠŸ")
            
            # å‹•ä½œç¢ºèª
            test_result = self.nlp("Hello world.")
            self.logger.info(f"ðŸ§ª Pipeline å‹•ä½œç¢ºèª: {len(test_result.sentences)} sentences processed")
            
        except Exception as e:
            self.logger.error(f"âŒ Stanza pipeline åˆæœŸåŒ–å¤±æ•—: {e}")
            self.logger.error("ðŸ’¡ è§£æ±ºæ–¹æ³•: python -c 'import stanza; stanza.download(\"en\")'")
            raise RuntimeError(f"Stanza initialization failed: {e}")
    
    def _initialize_spacy_pipeline(self):
        """spaCy NLPãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æžç”¨ï¼‰"""
        try:
            self.logger.info("ðŸ”§ spaCy pipeline åˆæœŸåŒ–ä¸­...")
            
            # è‹±èªžãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
            self.spacy_nlp = spacy.load('en_core_web_sm')
            
            self.logger.info("âœ… spaCy pipeline åˆæœŸåŒ–æˆåŠŸ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ spaCy pipeline åˆæœŸåŒ–å¤±æ•—: {e}")
            self.logger.warning("  pip install spacy; python -m spacy download en_core_web_sm ã§è¨­å®šã—ã¦ãã ã•ã„")
            self.spacy_nlp = None
            self.use_spacy_hybrid = False
    
    def process(self, sentence: str) -> Dict[str, Any]:
        """
        çµ±åˆå‡¦ç†ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
        
        Args:
            sentence: å‡¦ç†å¯¾è±¡æ–‡
            
        Returns:
            Dict: Rephraseå½¢å¼å‡¦ç†çµæžœ
        """
        start_time = datetime.now()
        self.processing_count += 1
        
        try:
            self.logger.debug(f"ðŸ” Processing: {sentence}")
            
            # Phase 1: Stanzaè§£æž
            doc = self._analyze_with_stanza(sentence)
            if not doc or not doc.sentences:
                self.logger.warning(f"âš ï¸ Stanzaè§£æžå¤±æ•—: {sentence}")
                return self._create_empty_result(sentence)
            
            # Phase 1.5: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æžï¼ˆspaCyè£œå®Œï¼‰
            if self.use_spacy_hybrid and self.spacy_nlp:
                doc = self._apply_spacy_hybrid_corrections(sentence, doc)
            
            # Phase 2: çµ±åˆå‡¦ç†ï¼ˆå…¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼åŒæ™‚å®Ÿè¡Œï¼‰
            result = self._unified_mapping(sentence, doc)
            
            # Phase 3: å¾Œå‡¦ç†ãƒ»æ¤œè¨¼
            result = self._post_process_result(result, sentence)
            
            # å‡¦ç†æ™‚é–“è¨˜éŒ²
            processing_time = (datetime.now() - start_time).total_seconds()
            self.total_processing_time += processing_time
            
            result['meta'] = {
                'processing_time': processing_time,
                'sentence_id': self.processing_count,
                'active_handlers': len(self.active_handlers),
                'stanza_info': {
                    'sentences': len(doc.sentences),
                    'tokens': len(doc.sentences[0].words) if doc.sentences else 0
                }
            }
            
            self.logger.info(f"âœ… Processingå®Œäº† ({processing_time:.3f}s): {len(result.get('slots', {}))} slots detected")
            return result
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            self.logger.error(f"âŒ Processing error: {e}")
            
            return {
                'sentence': sentence,
                'slots': {},
                'error': str(e),
                'meta': {
                    'processing_time': processing_time,
                    'sentence_id': self.processing_count,
                    'error_occurred': True
                }
            }
    
    def _analyze_with_stanza(self, sentence: str):
        """Stanzaè§£æžå®Ÿè¡Œ"""
        try:
            doc = self.nlp(sentence)
            return doc
        except Exception as e:
            self.logger.error(f"âŒ Stanza analysis failed: {e}")
            return None
    
    def _apply_spacy_hybrid_corrections(self, sentence: str, stanza_doc):
        """
        spaCyãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æžè£œå®Œ
        
        Stanzaã®èª¤è§£æžã‚’æ¤œå‡ºã—ã¦spaCyã§è£œå®Œä¿®æ­£
        ç‰¹ã«whoseæ§‹æ–‡ã§ã®å‹•è©žPOSèª¤è§£æžã‚’ä¿®æ­£
        """
        try:
            # spaCyè§£æžå®Ÿè¡Œ
            spacy_doc = self.spacy_nlp(sentence)
            
            # ä¿®æ­£ãŒå¿…è¦ãªç®‡æ‰€ã‚’æ¤œå‡º
            corrections = self._detect_analysis_discrepancies(stanza_doc, spacy_doc, sentence)
            
            if corrections:
                self.logger.debug(f"ðŸ”§ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æžè£œæ­£: {len(corrections)} ç®‡æ‰€ä¿®æ­£")
                
                # Stanzaçµæžœã«è£œæ­£ã‚’é©ç”¨
                corrected_doc = self._apply_corrections_to_stanza(stanza_doc, corrections)
                return corrected_doc
            
            return stanza_doc
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ spaCyãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æžã‚¨ãƒ©ãƒ¼: {e}")
            return stanza_doc  # è£œæ­£å¤±æ•—æ™‚ã¯å…ƒã®Stanzaçµæžœã‚’è¿”ã™
    
    def _detect_analysis_discrepancies(self, stanza_doc, spacy_doc, sentence: str) -> List[Dict]:
        """
        Stanza-spaCyè§£æžçµæžœã®ç›¸é•ç‚¹ã‚’æ¤œå‡º
        
        ç‰¹ã«é‡è¦ãªä¿®æ­£ç®‡æ‰€:
        1. whoseæ§‹æ–‡ã§ã®å‹•è©žPOSèª¤è§£æž (NOUN â†’ VERB)
        2. é–¢ä¿‚ç¯€å‹•è©žã®èª¤åˆ†é¡ž
        """
        corrections = []
        
        # whoseæ§‹æ–‡ç‰¹åˆ¥å‡¦ç†
        if 'whose' in sentence.lower():
            corrections.extend(self._detect_whose_verb_misanalysis(stanza_doc, spacy_doc, sentence))
        
        return corrections
    
    def _detect_whose_verb_misanalysis(self, stanza_doc, spacy_doc, sentence: str) -> List[Dict]:
        """whoseæ§‹æ–‡ã§ã®å‹•è©žPOSèª¤è§£æžã‚’æ¤œå‡º"""
        corrections = []
        
        stanza_words = {w.text.lower(): w for w in stanza_doc.sentences[0].words}
        spacy_tokens = {t.text.lower(): t for t in spacy_doc}
        
        # 'lives', 'works', 'runs'ç­‰ã®å‹•è©žãŒåè©žã¨ã—ã¦èª¤è§£æžã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        potential_verbs = ['lives', 'works', 'runs', 'goes', 'comes', 'stays']
        
        for verb_text in potential_verbs:
            if verb_text in stanza_words and verb_text in spacy_tokens:
                stanza_word = stanza_words[verb_text]
                spacy_token = spacy_tokens[verb_text]
                
                # Stanza: NOUN, spaCyè§£æžã§ã‚‚NOUNã ãŒã€æ–‡è„ˆçš„ã«å‹•è©žã¨åˆ¤æ–­ã§ãã‚‹å ´åˆ
                if (stanza_word.upos == 'NOUN' and 
                    stanza_word.deprel == 'acl:relcl' and
                    self._is_contextually_verb(sentence, verb_text)):
                    
                    corrections.append({
                        'word_id': stanza_word.id,
                        'word_text': stanza_word.text,
                        'original_upos': stanza_word.upos,
                        'corrected_upos': 'VERB',
                        'correction_type': 'whose_verb_fix',
                        'confidence': 0.9
                    })
                    self.logger.debug(f"ðŸ”§ whoseæ§‹æ–‡å‹•è©žä¿®æ­£æ¤œå‡º: {verb_text} NOUNâ†’VERB")
        
        return corrections
    
    def _is_contextually_verb(self, sentence: str, word: str) -> bool:
        """æ–‡è„ˆçš„ã«å‹•è©žã¨åˆ¤æ–­ã§ãã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        # ç°¡å˜ãªãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹åˆ¤å®š
        # whose + [noun] + is + [adj] + [word] + here/there ãƒ‘ã‚¿ãƒ¼ãƒ³
        import re
        
        whose_pattern = rf'whose\s+\w+\s+is\s+\w+\s+{word}\s+(here|there)'
        if re.search(whose_pattern, sentence.lower()):
            return True
            
        return False
    
    def _apply_corrections_to_stanza(self, stanza_doc, corrections):
        """Stanzaè§£æžçµæžœã«è£œæ­£ã‚’é©ç”¨"""
        # æ³¨æ„: Stanzaã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¯èª­ã¿å–ã‚Šå°‚ç”¨ã®ãŸã‚ã€ç›´æŽ¥ä¿®æ­£ã¯ã§ããªã„
        # ã“ã“ã§ã¯ä¿®æ­£æƒ…å ±ã‚’è¨˜éŒ²ã—ã¦ã€å¾Œç¶šå‡¦ç†ã§åˆ©ç”¨ã™ã‚‹
        
        if not hasattr(stanza_doc, 'hybrid_corrections'):
            stanza_doc.hybrid_corrections = {}
        
        for correction in corrections:
            word_id = correction['word_id']
            stanza_doc.hybrid_corrections[word_id] = correction
            
        return stanza_doc
    
    def _unified_mapping(self, sentence: str, doc) -> Dict[str, Any]:
        """
        çµ±åˆãƒžãƒƒãƒ”ãƒ³ã‚°å‡¦ç†
        
        å…¨ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒåŒæ™‚å®Ÿè¡Œ
        å„ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã¯ç‹¬ç«‹ã—ã¦Stanzaè§£æžçµæžœã‚’å‡¦ç†
        """
        result = {
            'sentence': sentence,
            'slots': {},
            'sub_slots': {},
            'grammar_info': {
                'detected_patterns': [],
                'handler_contributions': {}
            }
        }
        
        # ãƒ¡ã‚¤ãƒ³æ–‡ï¼ˆæœ€åˆã®sentenceï¼‰ã‚’å¯¾è±¡ã¨ã™ã‚‹
        main_sentence = doc.sentences[0] if doc.sentences else None
        if not main_sentence:
            return result
        
        self.logger.debug(f"ðŸ”§ Unified mappingé–‹å§‹: {len(self.active_handlers)} handlers active")
        
        # å…¨ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®åŒæ™‚å®Ÿè¡Œ
        for handler_name in self.active_handlers:
            try:
                self.logger.debug(f"ðŸŽ¯ Handlerå®Ÿè¡Œ: {handler_name}")
                handler_method = getattr(self, f'_handle_{handler_name}')
                handler_result = handler_method(main_sentence, result.copy())
                
                # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼çµæžœã‚’ãƒžãƒ¼ã‚¸
                if handler_result:
                    result = self._merge_handler_results(result, handler_result, handler_name)
                    
                    # æˆåŠŸã‚«ã‚¦ãƒ³ãƒˆ
                    self.handler_success_count[handler_name] = \
                        self.handler_success_count.get(handler_name, 0) + 1
                        
            except Exception as e:
                self.logger.warning(f"âš ï¸ Handler error ({handler_name}): {e}")
                continue
        
        return result
    
    def _merge_handler_results(self, base_result: Dict, handler_result: Dict, handler_name: str) -> Dict:
        """
        ãƒãƒ³ãƒ‰ãƒ©ãƒ¼çµæžœã‚’ãƒ™ãƒ¼ã‚¹çµæžœã«ãƒžãƒ¼ã‚¸
        
        Args:
            base_result: ãƒ™ãƒ¼ã‚¹çµæžœ
            handler_result: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å‡¦ç†çµæžœ  
            handler_name: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å
        """
        # ã‚¹ãƒ­ãƒƒãƒˆæƒ…å ±ãƒžãƒ¼ã‚¸
        if 'slots' in handler_result:
            for slot_name, slot_data in handler_result['slots'].items():
                if slot_name not in base_result['slots']:
                    base_result['slots'][slot_name] = slot_data
                else:
                    # ç«¶åˆè§£æ±ºï¼šç©ºæ–‡å­—ã‚„ç©ºå€¤ã§æ—¢å­˜ã®æœ‰åŠ¹ãªå€¤ã‚’ä¸Šæ›¸ãã—ãªã„
                    existing_value = base_result['slots'][slot_name]
                    
                    # æ—¢å­˜å€¤ãŒç©ºã§æ–°å€¤ãŒæœ‰åŠ¹ãªå ´åˆã¯ä¸Šæ›¸ã
                    if not existing_value and slot_data:
                        base_result['slots'][slot_name] = slot_data
                    # æ—¢å­˜å€¤ãŒæœ‰åŠ¹ã§æ–°å€¤ã‚‚æœ‰åŠ¹ãªå ´åˆã¯å¾Œå‹ã¡ï¼ˆå¾“æ¥é€šã‚Šï¼‰
                    elif existing_value and slot_data:
                        base_result['slots'][slot_name] = slot_data
                    # æ—¢å­˜å€¤ãŒæœ‰åŠ¹ã§æ–°å€¤ãŒç©ºã®å ´åˆã¯ä¿æŒï¼ˆâ˜…ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆï¼‰
                    elif existing_value and not slot_data:
                        pass  # æ—¢å­˜å€¤ã‚’ä¿æŒ
                    # ä¸¡æ–¹ç©ºã®å ´åˆã¯å¾Œå‹ã¡
                    else:
                        base_result['slots'][slot_name] = slot_data
        
        # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæƒ…å ±ãƒžãƒ¼ã‚¸
        if 'sub_slots' in handler_result:
            for sub_slot_name, sub_slot_data in handler_result['sub_slots'].items():
                base_result['sub_slots'][sub_slot_name] = sub_slot_data
        
        # æ–‡æ³•æƒ…å ±è¨˜éŒ²
        if 'grammar_info' in handler_result:
            grammar_info = handler_result['grammar_info']
            base_result['grammar_info']['handler_contributions'][handler_name] = grammar_info
            
            # æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³è¿½åŠ 
            if 'patterns' in grammar_info:
                base_result['grammar_info']['detected_patterns'].extend(grammar_info['patterns'])
        
        return base_result
    
    def _post_process_result(self, result: Dict, sentence: str) -> Dict:
        """å¾Œå‡¦ç†ãƒ»çµæžœæ¤œè¨¼ï¼ˆwhoseæ§‹æ–‡ç‰¹åˆ¥å‡¦ç†è¿½åŠ ï¼‰"""
        
        # âœ… whoseæ§‹æ–‡ã®ç‰¹åˆ¥ãªå¾Œå‡¦ç†ï¼šä¸»æ–‡ãƒ»é–¢ä¿‚ç¯€ã®æ­£ã—ã„åˆ†é›¢
        if 'whose' in sentence.lower():
            result = self._post_process_whose_construction(result, sentence)
        
        # é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³é™¤åŽ»
        if 'detected_patterns' in result.get('grammar_info', {}):
            result['grammar_info']['detected_patterns'] = \
                list(set(result['grammar_info']['detected_patterns']))
        
        # ðŸ”§ REPHRASE SPECIFICATION COMPLIANCE: Sub-slots require empty main slots
        self._apply_rephrase_slot_structure_rules(result, sentence)
        
        # ã‚¹ãƒ­ãƒƒãƒˆæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆä»Šå¾Œå®Ÿè£…ï¼‰
        # TODO: rephrase_slot_validator.py ã¨ã®é€£æº
        
        return result
    
    def _post_process_whose_construction(self, result: Dict, sentence: str) -> Dict:
        """whoseæ§‹æ–‡ã®å¾Œå‡¦ç†ï¼šä¸»æ–‡ãƒ»é–¢ä¿‚ç¯€ã®æ­£ã—ã„åˆ†é›¢"""
        
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æžã§è£œæ­£ã•ã‚ŒãŸå‹•è©žï¼ˆä¸»æ–‡å‹•è©žï¼‰ã‚’æ¤œå‡º
        main_verb = None
        for word in sentence.split():
            if word.lower() in ['lives', 'works', 'runs', 'goes', 'sits', 'stands']:
                main_verb = word
                break
        
        if main_verb:
            # ä¸»æ–‡å‹•è©žã‚’Vã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®
            if 'slots' not in result:
                result['slots'] = {}
            result['slots']['V'] = main_verb
            
            # âœ… å‰¯è©žå‡¦ç†ã¯å°‚é–€ã‚¨ãƒ³ã‚¸ãƒ³ã«å§”è­² - å›ºå®šå‡¦ç†ã‚’ç„¡åŠ¹åŒ–
            # if 'here' in sentence.lower():
            #     result['slots']['M2'] = 'here'
            # elif 'there' in sentence.lower():
            #     result['slots']['M2'] = 'there'
                
            # ä¸»èªžã¯é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒè¨­å®šã—ãŸsub-sã‚’ç§»å‹•
            if result.get('sub_slots', {}).get('sub-s'):
                # sub-sã®å†…å®¹ã‹ã‚‰é–¢ä¿‚ç¯€éƒ¨åˆ†ã‚’é™¤åŽ»ã—ã¦ä¸»æ–‡ä¸»èªžã‚’ä½œã‚‹
                sub_s_content = result['sub_slots']['sub-s']  # "The man whose car"
                # "whose car"éƒ¨åˆ†ã‚’é™¤åŽ»ã—ã¦"The man"ã‚’ä¸»èªžã¨ã™ã‚‹
                main_subject = sub_s_content.split(' whose ')[0]  # "The man"
                result['slots']['S'] = main_subject
                
            # é–¢ä¿‚ç¯€ã®sub-c1ãŒä¸»æ–‡å‹•è©žã«ãªã£ã¦ã„ã‚‹å ´åˆã¯ä¿®æ­£
            if result.get('sub_slots', {}).get('sub-c1') == main_verb:
                # æœ¬æ¥ã®é–¢ä¿‚ç¯€è£œèªžã‚’æŽ¢ã™
                if 'red' in sentence.lower():
                    result['sub_slots']['sub-c1'] = 'red'
                    
            self.logger.debug(f"ðŸ”§ whoseæ§‹æ–‡å¾Œå‡¦ç†: ä¸»æ–‡V={main_verb}, S={result['slots'].get('S')}")
        
        return result
    
    def _apply_rephrase_slot_structure_rules(self, result: Dict, sentence: str) -> None:
        """
        Rephraseä»•æ§˜æº–æ‹ ï¼šè¤‡æ–‡ã§ã®æ­£ã—ã„ã‚¹ãƒ­ãƒƒãƒˆé…ç½®
        
        é‡è¦ãƒ«ãƒ¼ãƒ«ï¼šsub-slotsãŒå­˜åœ¨ã™ã‚‹å ´åˆã€å¯¾å¿œã™ã‚‹main slotsã¯ç©ºæ–‡å­—ã«ã™ã‚‹
        ä¾‹å¤–ï¼šAux, Vã‚¹ãƒ­ãƒƒãƒˆã¯ä¾‹å¤–é©ç”¨ãªã—
        
        å¯¾å¿œé–¢ä¿‚ï¼š
        - S â†â†’ sub-s (Sä½ç½®ã®å¾“å±žç¯€)
        - O1 â†â†’ sub-o1 (O1ä½ç½®ã®å¾“å±žç¯€)  
        - O2 â†â†’ sub-o2 (O2ä½ç½®ã®å¾“å±žç¯€)
        - C1 â†â†’ sub-c1 (C1ä½ç½®ã®å¾“å±žç¯€)
        - C2 â†â†’ sub-c2 (C2ä½ç½®ã®å¾“å±žç¯€)
        - M1 â†â†’ sub-m1 (M1ä½ç½®ã®å¾“å±žç¯€)
        - M2 â†â†’ sub-m2 (M2ä½ç½®ã®å¾“å±žç¯€) 
        - M3 â†â†’ sub-m3 (M3ä½ç½®ã®å¾“å±žç¯€)
        """
        slots = result.get('slots', {})
        sub_slots = result.get('sub_slots', {})
        
        # å¯¾å¿œé–¢ä¿‚ãƒžãƒƒãƒ”ãƒ³ã‚°ï¼ˆAux, Vé™¤å¤–ï¼‰
        main_to_sub_mapping = {
            'S': 'sub-s',
            'O1': 'sub-o1', 
            'O2': 'sub-o2',
            'C1': 'sub-c1',
            'C2': 'sub-c2', 
            'M1': 'sub-m1',
            'M2': 'sub-m2',
            'M3': 'sub-m3'
        }
        
        self.logger.debug(f"ðŸ—ï¸ Rephraseä»•æ§˜é©ç”¨é–‹å§‹ - Sub-slots: {list(sub_slots.keys())}")
        
        # è¤‡æ–‡åˆ¤å®šï¼†ã‚¹ãƒ­ãƒƒãƒˆç©ºæ–‡å­—åŒ–å‡¦ç†
        for main_slot, sub_slot in main_to_sub_mapping.items():
            if sub_slot in sub_slots and sub_slots[sub_slot]:
                # Sub-slotãŒå­˜åœ¨ã—å†…å®¹ãŒã‚ã‚‹å ´åˆã€å¯¾å¿œã™ã‚‹main slotã‚’ç©ºã«ã™ã‚‹
                if main_slot in slots:
                    original_value = slots[main_slot]
                    slots[main_slot] = ""  # ä½ç½®ãƒžãƒ¼ã‚«ãƒ¼ã¨ã—ã¦ç©ºæ–‡å­—è¨­å®š
                    
                    self.logger.debug(
                        f"ðŸ”„ Complex sentence rule applied: "
                        f"{main_slot}: '{original_value}' â†’ '' "
                        f"(sub-slot {sub_slot}: '{sub_slots[sub_slot]}')"
                    )
        
        # å‡¦ç†çµæžœã‚’ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°å‡ºåŠ›
        applied_rules = [
            f"{main}â†’{sub}" for main, sub in main_to_sub_mapping.items() 
            if sub in sub_slots and sub_slots[sub] and main in slots
        ]
        
        if applied_rules:
            self.logger.info(f"âœ… Rephraseè¤‡æ–‡ãƒ«ãƒ¼ãƒ«é©ç”¨: {', '.join(applied_rules)}")
        
        # âœ… whoseæ§‹æ–‡ã®ä¸»æ–‡å‰¯è©žã¯ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã«ä¿æŒï¼ˆè‡ªå‹•ç§»å‹•ç„¡åŠ¹åŒ–ï¼‰
        # ä¸»æ–‡ã®å‰¯è©žï¼ˆM1, M2, M3ï¼‰ã¯ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã«æ®‹ã™ã®ãŒæ­£ã—ã„ä»•æ§˜
        # if 'whose' in sentence.lower() and any(s in sub_slots for s in ['sub-s', 'sub-v', 'sub-c1']):
        #     # whoseæ§‹æ–‡æ¤œå‡ºæ™‚ã€ä¸»æ–‡ã®M-slotã‚’è‡ªå‹•çš„ã«sub-slotã«ç§»å‹•
        #     additional_rules = []
        #     for main_slot in ['M1', 'M2', 'M3']:
        #         if main_slot in slots and slots[main_slot]:  # å†…å®¹ãŒã‚ã‚‹å ´åˆ
        #             sub_slot = main_to_sub_mapping[main_slot]
        #             if sub_slot not in sub_slots or not sub_slots[sub_slot]:  # sub-slotãŒç©ºã®å ´åˆ
        #                 sub_slots[sub_slot] = slots[main_slot]
        #                 slots[main_slot] = ""
        #                 additional_rules.append(f"{main_slot}â†’{sub_slot}")
        #                 self.logger.debug(f"ðŸ”„ whoseæ§‹æ–‡ä¸»æ–‡å‰¯è©žç§»å‹•: {main_slot}: '{sub_slots[sub_slot]}' â†’ {sub_slot}")
        #     
        #     if additional_rules:
        #         self.logger.info(f"âœ… whoseæ§‹æ–‡ä¸»æ–‡å‰¯è©žç§»å‹•: {', '.join(additional_rules)}")
        
        else:
            self.logger.debug("ðŸ” Simple sentence detected - No main slot emptying required")
    
    def _create_empty_result(self, sentence: str) -> Dict[str, Any]:
        """ç©ºçµæžœã®ä½œæˆ"""
        return {
            'sentence': sentence,
            'slots': {},
            'sub_slots': {},
            'grammar_info': {
                'detected_patterns': [],
                'handler_contributions': {}
            },
            'meta': {
                'processing_time': 0.0,
                'sentence_id': self.processing_count,
                'empty_result': True
            }
        }
    
    # =============================================================================
    # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ç®¡ç†ï¼ˆPhaseåˆ¥æ©Ÿèƒ½è¿½åŠ ç”¨ï¼‰
    # =============================================================================
    
    def add_handler(self, handler_name: str):
        """ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ ï¼ˆPhaseåˆ¥é–‹ç™ºç”¨ï¼‰"""
        if handler_name not in self.active_handlers:
            self.active_handlers.append(handler_name)
            self.logger.info(f"âž• Handlerè¿½åŠ : {handler_name}")
        else:
            self.logger.warning(f"âš ï¸ Handler already active: {handler_name}")
    
    def remove_handler(self, handler_name: str):
        """ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å‰Šé™¤"""
        if handler_name in self.active_handlers:
            self.active_handlers.remove(handler_name)
            self.logger.info(f"âž– Handlerå‰Šé™¤: {handler_name}")
    
    def list_active_handlers(self) -> List[str]:
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ä¸€è¦§"""
        return self.active_handlers.copy()
    
    # =============================================================================
    # çµ±è¨ˆãƒ»ãƒ‡ãƒãƒƒã‚°æƒ…å ±
    # =============================================================================
    
    def get_stats(self) -> Dict[str, Any]:
        """å‡¦ç†çµ±è¨ˆæƒ…å ±å–å¾—"""
        avg_processing_time = (
            self.total_processing_time / self.processing_count 
            if self.processing_count > 0 else 0.0
        )
        
        return {
            'processing_count': self.processing_count,
            'total_processing_time': self.total_processing_time,
            'average_processing_time': avg_processing_time,
            'active_handlers': self.active_handlers.copy(),
            'handler_success_count': self.handler_success_count.copy(),
            'stanza_pipeline_status': 'active' if self.nlp else 'inactive'
        }
    
    def reset_stats(self):
        """çµ±è¨ˆæƒ…å ±ãƒªã‚»ãƒƒãƒˆ"""
        self.processing_count = 0
        self.total_processing_time = 0.0
        self.handler_success_count.clear()
        self.logger.info("ðŸ“Š Statistics reset")
    
    # =============================================================================
    # æ–‡æ³•ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè£…ï¼ˆPhase 1+: æ®µéšŽçš„è¿½åŠ ï¼‰
    # =============================================================================
    
    def _handle_relative_clause(self, sentence, base_result: Dict) -> Optional[Dict]:
        """
        é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆPhase 1å®Ÿè£…ï¼‰
        
        simple_relative_engine.py ã®æ©Ÿèƒ½ã‚’çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã«ç§»æ¤
        Stanza dependency parsing ã«ã‚ˆã‚‹ç›´æŽ¥çš„ãªé–¢ä¿‚ç¯€æ¤œå‡ºãƒ»åˆ†è§£
        
        Args:
            sentence: Stanzaè§£æžæ¸ˆã¿sentence object
            base_result: ãƒ™ãƒ¼ã‚¹çµæžœï¼ˆã‚³ãƒ”ãƒ¼ï¼‰
            
        Returns:
            Dict: é–¢ä¿‚ç¯€åˆ†è§£çµæžœ or None
        """
        try:
            self.logger.debug("ðŸ” é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè¡Œä¸­...")
            
            # é–¢ä¿‚ç¯€å­˜åœ¨ãƒã‚§ãƒƒã‚¯
            if not self._has_relative_clause(sentence):
                self.logger.debug("  é–¢ä¿‚ç¯€ãªã— - ã‚¹ã‚­ãƒƒãƒ—")
                return None
            
            self.logger.debug("  âœ… é–¢ä¿‚ç¯€æ¤œå‡º")
            return self._process_relative_clause_structure(sentence, base_result)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _has_relative_clause(self, sentence) -> bool:
        """é–¢ä¿‚ç¯€ã‚’å«ã‚€ã‹ãƒã‚§ãƒƒã‚¯"""
        # âœ… whoseæ§‹æ–‡ã®è©³ç´°å‡¦ç†
        has_acl_relcl = any(w.deprel in ['acl:relcl', 'acl'] for w in sentence.words)
        
        if has_acl_relcl and any(w.text.lower() == 'whose' for w in sentence.words):
            # whoseæ§‹æ–‡ã§acl:relclèªžãŒãƒ¡ã‚¤ãƒ³å‹•è©žå€™è£œã®å ´åˆã¯é–¢ä¿‚ç¯€ãªã—ã¨åˆ¤å®š
            acl_relcl_word = self._find_word_by_deprel(sentence, 'acl:relcl')
            if (acl_relcl_word and 
                acl_relcl_word.text.lower() in ['lives', 'works', 'runs', 'goes', 'sits', 'stands']):
                self.logger.debug(f"ðŸ”§ whoseæ§‹æ–‡: {acl_relcl_word.text}ã‚’ãƒ¡ã‚¤ãƒ³å‹•è©žã¨ã—ã¦å‡¦ç†ï¼ˆé–¢ä¿‚ç¯€ã§ã¯ãªã„ï¼‰")
                
                # ãŸã ã—ã€çœŸã®é–¢ä¿‚ç¯€ï¼ˆwhose car is redéƒ¨åˆ†ï¼‰ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯å‡¦ç†ã™ã‚‹
                # copé–¢ä¿‚ã®beå‹•è©žãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                cop_verb = None
                for word in sentence.words:
                    if word.deprel == 'cop':
                        cop_verb = word
                        break
                
                if cop_verb:
                    self.logger.debug(f"ðŸ”§ whoseæ§‹æ–‡å†…ã®çœŸã®é–¢ä¿‚ç¯€æ¤œå‡º: copå‹•è©ž {cop_verb.text}")
                    return True  # çœŸã®é–¢ä¿‚ç¯€ãŒå­˜åœ¨
                else:
                    return False  # é–¢ä¿‚ç¯€ã§ã¯ãªããƒ¡ã‚¤ãƒ³å‹•è©ž
        
        return has_acl_relcl
    
    def _process_relative_clause_structure(self, sentence, base_result: Dict) -> Dict:
        """é–¢ä¿‚ç¯€æ§‹é€ ã®åˆ†è§£å‡¦ç†"""
        
        # === 1. è¦ç´ ç‰¹å®š ===
        # âœ… whoseæ§‹æ–‡ã®çœŸã®é–¢ä¿‚ç¯€æ¤œå‡º
        rel_verb = None
        antecedent = None
        
        is_whose_construction = any(w.text.lower() == 'whose' for w in sentence.words)
        
        if is_whose_construction:
            # whoseæ§‹æ–‡ã§ã¯ã€ã¾ãšacl:relclé–¢ä¿‚ã®å®Ÿå‹•è©žã‚’æŽ¢ã™
            acl_word = self._find_word_by_deprel(sentence, 'acl:relcl')
            if acl_word and acl_word.upos == 'VERB':
                # Pattern B: å®Ÿå‹•è©žãŒé–¢ä¿‚ç¯€å‹•è©ž (ä¾‹: borrowed)
                rel_verb = acl_word
                if acl_word.head > 0:
                    antecedent = self._find_word_by_id(sentence, acl_word.head)
            else:
                # Pattern A: copå‹•è©žãŒé–¢ä¿‚ç¯€å‹•è©ž (ä¾‹: is in "car is red")  
                for word in sentence.words:
                    if word.deprel == 'cop':
                        rel_verb = word
                        # acl:relclã®headã‹ã‚‰å…ˆè¡Œè©žã‚’æŽ¢ã™
                        if acl_word and acl_word.head > 0:
                            antecedent = self._find_word_by_id(sentence, acl_word.head)
                        else:
                            # fallback: rootèªžã‚’å…ˆè¡Œè©žã¨ã™ã‚‹
                            for w in sentence.words:
                                if w.deprel == 'root':
                                    antecedent = w
                                    break
                        break
                        
            if rel_verb and antecedent:
                self.logger.debug(f"ðŸ”§ whoseæ§‹æ–‡ä¿®æ­£: é–¢ä¿‚ç¯€å‹•è©ž={rel_verb.text}, å…ˆè¡Œè©ž={antecedent.text}")
        
        # é€šå¸¸ã®é–¢ä¿‚ç¯€æ¤œå‡º
        if not rel_verb:
            rel_verb = self._find_word_by_deprel(sentence, 'acl:relcl')
            if not rel_verb:
                rel_verb = self._find_word_by_deprel(sentence, 'acl')
            if not rel_verb:
                return base_result
            
            # å…ˆè¡Œè©žï¼ˆé–¢ä¿‚ç¯€å‹•è©žã®é ­ï¼‰
            antecedent = self._find_word_by_id(sentence, rel_verb.head)
            
        if not antecedent:
            return base_result

        self.logger.debug(f"  å…ˆè¡Œè©ž: {antecedent.text}, é–¢ä¿‚å‹•è©ž: {rel_verb.text}")
        
        # === 2. é–¢ä¿‚ä»£åè©ž/é–¢ä¿‚å‰¯è©žç‰¹å®š ===
        rel_pronoun, rel_type = self._identify_relative_pronoun(sentence, rel_verb)
        
        # === 3. é–¢ä¿‚ç¯€å†…è¦ç´ ç‰¹å®š ===
        rel_subject = None
        if rel_type in ['obj', 'advmod']:  # ç›®çš„èªžãƒ»é–¢ä¿‚å‰¯è©žã®å ´åˆã®ã¿ä¸»èªžæ¤œç´¢
            rel_subject = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'nsubj')
        elif rel_type == 'poss':
            # æ‰€æœ‰æ ¼é–¢ä¿‚ä»£åè©žã®å ´åˆã¯ç‰¹åˆ¥å‡¦ç†
            # whoseæ§‹æ–‡ã§ã¯ã€æ‰€æœ‰ã•ã‚Œã‚‹åè©žä»¥å¤–ã®ç‹¬ç«‹ã—ãŸä¸»èªžã‚’æŽ¢ã™
            nsubj_word = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'nsubj')
            possessed_noun = self._find_word_by_id(sentence, rel_pronoun.head) if rel_pronoun else None
            
            # æ‰€æœ‰ã•ã‚Œã‚‹åè©žã¨ç•°ãªã‚‹ä¸»èªžãŒã‚ã‚‹å ´åˆã®ã¿rel_subjectã¨ã—ã¦èªè­˜
            if nsubj_word and possessed_noun and nsubj_word.id != possessed_noun.id:
                rel_subject = nsubj_word
        
        # æ‰€æœ‰æ ¼é–¢ä¿‚ä»£åè©žã®ç‰¹åˆ¥å‡¦ç†
        possessed_noun = None
        if rel_type == 'poss':
            possessed_noun = self._find_word_by_id(sentence, rel_pronoun.head)
        
        self.logger.debug(f"  é–¢ä¿‚ä»£åè©ž: {rel_pronoun.text if rel_pronoun else 'None'} ({rel_type})")
        if rel_subject:
            self.logger.debug(f"  é–¢ä¿‚ç¯€ä¸»èªž: {rel_subject.text}")
        if possessed_noun:
            self.logger.debug(f"  æ‰€æœ‰ã•ã‚Œã‚‹åè©ž: {possessed_noun.text}")
        
        # === 4. å…ˆè¡Œè©žå¥æ§‹ç¯‰ ===
        noun_phrase = self._build_antecedent_phrase(sentence, antecedent, rel_pronoun, possessed_noun)
        self.logger.debug(f"  æ§‹ç¯‰å…ˆè¡Œè©žå¥: '{noun_phrase}'")
        
        # === 5. Rephraseã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ ===
        result = base_result.copy()
        
        # âœ… whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†: ãƒ¡ã‚¤ãƒ³å‹•è©žå‡¦ç†ã‚’å¦¨å®³ã—ãªã„
        if is_whose_construction and rel_verb and rel_verb.deprel == 'cop':
            # é–¢ä¿‚ç¯€ã‚¹ãƒ­ãƒƒãƒˆã®ã¿ç”Ÿæˆã—ã€ãƒ¡ã‚¤ãƒ³æ–‡ã¯5æ–‡åž‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã«ä»»ã›ã‚‹
            rephrase_slots = self._generate_whose_relative_clause_slots(
                antecedent, rel_verb, sentence
            )
            
            # çµæžœãƒžãƒ¼ã‚¸ï¼ˆãƒ¡ã‚¤ãƒ³æ–‡ã‚¹ãƒ­ãƒƒãƒˆã¯ä¿æŒï¼‰
            if 'slots' not in result:
                result['slots'] = {}
            if 'sub_slots' not in result:
                result['sub_slots'] = {}
            
            # é–¢ä¿‚ç¯€ã®sub-slotsã®ã¿ãƒžãƒ¼ã‚¸ï¼ˆãƒ¡ã‚¤ãƒ³æ–‡ã‚¹ãƒ­ãƒƒãƒˆã¯å¤‰æ›´ã—ãªã„ï¼‰
            result['sub_slots'].update(rephrase_slots.get('sub_slots', {}))
            
            self.logger.debug(f"ðŸ”§ whoseæ§‹æ–‡: ãƒ¡ã‚¤ãƒ³æ–‡ã‚¹ãƒ­ãƒƒãƒˆä¿æŒ, é–¢ä¿‚ç¯€ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆè¿½åŠ ")
            
        else:
            # é€šå¸¸ã®é–¢ä¿‚ç¯€å‡¦ç†
            rephrase_slots = self._generate_relative_clause_slots(
                rel_type, noun_phrase, rel_subject, rel_verb, sentence
            )
            
            # çµæžœãƒžãƒ¼ã‚¸
            if 'slots' not in result:
                result['slots'] = {}
            if 'sub_slots' not in result:
                result['sub_slots'] = {}
            
            # é€šå¸¸ã®ãƒžãƒ¼ã‚¸
            result['slots'].update(rephrase_slots.get('slots', {}))
            result['sub_slots'].update(rephrase_slots.get('sub_slots', {}))
        
        # æ–‡æ³•æƒ…å ±è¨˜éŒ²
        result['grammar_info'] = {
            'patterns': ['relative_clause'],
            'rel_type': rel_type if not is_whose_construction else 'poss',
            'antecedent': antecedent.text,
            'rel_pronoun': 'whose' if is_whose_construction else (rel_pronoun.text if rel_pronoun else None),
            'rel_verb': rel_verb.text
        }
        
        self.logger.debug(f"  âœ… é–¢ä¿‚ç¯€å‡¦ç†å®Œäº†: {len(result.get('slots', {}))} main slots, {len(result.get('sub_slots', {}))} sub slots")
        return result
    
    def _identify_relative_pronoun(self, sentence, rel_verb) -> Tuple[Optional[Any], str]:
        """é–¢ä¿‚ä»£åè©ž/é–¢ä¿‚å‰¯è©žã®ç‰¹å®šã¨åˆ†é¡žï¼ˆçœç•¥æ–‡å¯¾å¿œå¼·åŒ–ãƒ»å—å‹•æ…‹è€ƒæ…®ï¼‰"""
        
        # 1. é–¢ä¿‚å‰¯è©žæ¤œå‡ºï¼ˆæœ€å„ªå…ˆï¼‰
        advmod_word = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'advmod')
        if advmod_word and advmod_word.text.lower() in ['where', 'when', 'why', 'how']:
            return advmod_word, 'advmod'
        
        # 2. æ‰€æœ‰æ ¼é–¢ä¿‚ä»£åè©žæ¤œå‡º
        for word in sentence.words:
            if word.text.lower() == 'whose' and word.deprel == 'nmod:poss':
                return word, 'poss'
        
        # 3. æ˜Žç¤ºçš„é–¢ä¿‚ä»£åè©žæ¤œå‡º
        # ç›®çš„èªžé–¢ä¿‚ä»£åè©ž
        obj_pronoun = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'obj')
        if obj_pronoun and obj_pronoun.text.lower() in ['who', 'whom', 'which', 'that']:
            return obj_pronoun, 'obj'
        
        # ä¸»èªžé–¢ä¿‚ä»£åè©žï¼ˆå—å‹•æ…‹ãƒã‚§ãƒƒã‚¯è¿½åŠ ï¼‰  
        subj_pronoun = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'nsubj')
        if subj_pronoun and subj_pronoun.text.lower() in ['who', 'which', 'that']:
            # å—å‹•æ…‹ã®å ´åˆã¯ä¸»èªžé–¢ä¿‚ä»£åè©žã¨ã—ã¦å‡¦ç†
            return subj_pronoun, 'nsubj'
            
        # å—å‹•æ…‹ä¸»èªžé–¢ä¿‚ä»£åè©ž
        pass_subj_pronoun = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'nsubj:pass')
        if pass_subj_pronoun and pass_subj_pronoun.text.lower() in ['who', 'which', 'that']:
            return pass_subj_pronoun, 'nsubj:pass'
        
        # 4. çœç•¥é–¢ä¿‚ä»£åè©žã®æŽ¨å®šï¼ˆå—å‹•æ…‹æ§‹é€ æ”¹å–„ï¼‰
        inferred_type = self._infer_omitted_relative_pronoun(sentence, rel_verb)
        if inferred_type:
            # ä»®æƒ³çš„ãªé–¢ä¿‚ä»£åè©žã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
            virtual_pronoun = self._create_virtual_relative_pronoun(sentence, rel_verb, inferred_type)
            return virtual_pronoun, inferred_type
        
        return None, 'unknown'
    
    def _infer_omitted_relative_pronoun(self, sentence, rel_verb) -> Optional[str]:
        """çœç•¥ã•ã‚ŒãŸé–¢ä¿‚ä»£åè©žã®æŽ¨å®šï¼ˆå—å‹•æ…‹æ§‹é€ æ”¹å–„ï¼‰"""
        
        # é–¢ä¿‚ç¯€å‹•è©žã®ä¾å­˜æ§‹é€ ã‚’åˆ†æž
        rel_verb_deps = []
        for word in sentence.words:
            if word.head == rel_verb.id:
                rel_verb_deps.append(word.deprel)
        
        self.logger.debug(f"    é–¢ä¿‚å‹•è©ž '{rel_verb.text}' ã®ä¾å­˜èªž: {rel_verb_deps}")
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: å—å‹•æ…‹é–¢ä¿‚ç¯€ã®æ¤œå‡ºï¼ˆæ”¹å–„ï¼‰
        has_nsubj_pass = 'nsubj:pass' in rel_verb_deps or 'nsubjpass' in rel_verb_deps
        has_aux_pass = any(word.deprel in ['aux:pass', 'auxpass'] and word.head == rel_verb.id 
                          for word in sentence.words)
        
        if has_nsubj_pass or has_aux_pass:
            # å—å‹•æ…‹é–¢ä¿‚ç¯€ï¼šå…ˆè¡Œè©žãŒå—å‹•æ…‹ã®ä¸»èªž
            self.logger.debug(f"    æŽ¨å®š: å—å‹•æ…‹ä¸»èªžé–¢ä¿‚ä»£åè©ž")
            return 'nsubj:pass'  # å—å‹•æ…‹ä¸»èªžã¨ã—ã¦æ‰±ã†
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: èƒ½å‹•æ…‹ã§ç›®çš„èªžãŒãªã„å ´åˆ
        has_nsubj = 'nsubj' in rel_verb_deps
        has_obj = 'obj' in rel_verb_deps or 'dobj' in rel_verb_deps
        
        if has_nsubj and not has_obj:
            # èƒ½å‹•æ…‹ã§ç›®çš„èªžãŒãªã„å ´åˆã€å…ˆè¡Œè©žãŒç›®çš„èªžã®å¯èƒ½æ€§
            self.logger.debug(f"    æŽ¨å®š: çœç•¥ç›®çš„èªžé–¢ä¿‚ä»£åè©žï¼ˆèƒ½å‹•æ…‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰")  
            return 'obj_omitted'
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ä¸»èªžãŒãªãã€é–¢ä¿‚ç¯€ãŒèƒ½å‹•æ…‹ã®å ´åˆ
        if not has_nsubj and not has_nsubj_pass:
            self.logger.debug(f"    æŽ¨å®š: çœç•¥ä¸»èªžé–¢ä¿‚ä»£åè©ž")
            return 'nsubj_omitted'
        
        return None
    
    def _create_virtual_relative_pronoun(self, sentence, rel_verb, inferred_type):
        """ä»®æƒ³çš„ãªé–¢ä¿‚ä»£åè©žã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ"""
        
        # é–¢ä¿‚ç¯€ã®å…ˆè¡Œè©žã‚’å–å¾—
        antecedent = self._find_word_by_id(sentence, rel_verb.head)
        
        # ä»®æƒ³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆè¾žæ›¸å½¢å¼ã§ç°¡æ˜“å®Ÿè£…ï¼‰
        virtual_pronoun = type('VirtualWord', (), {
            'text': '[omitted]',  # çœç•¥ãƒžãƒ¼ã‚«ãƒ¼
            'id': rel_verb.id - 0.5,  # ä»®æƒ³IDï¼ˆé–¢ä¿‚å‹•è©žã®ç›´å‰ï¼‰
            'head': rel_verb.head,
            'deprel': inferred_type.replace('_omitted', ''),
            'lemma': '[omitted]'
        })()
        
        self.logger.debug(f"    ä»®æƒ³é–¢ä¿‚ä»£åè©žä½œæˆ: type={inferred_type}, text=[omitted]")
        return virtual_pronoun
    
    def _build_antecedent_phrase(self, sentence, antecedent, rel_pronoun, possessed_noun=None) -> str:
        """å…ˆè¡Œè©žå¥æ§‹ç¯‰ï¼ˆä¿®é£¾èªžå«ã‚€ï¼‰- é–¢ä¿‚ç¯€ã®å‹•è©žéƒ¨åˆ†ã¯é™¤å¤–"""
        if not antecedent:
            return rel_pronoun.text if rel_pronoun else ""
        
        # å…ˆè¡Œè©žã®ä¿®é£¾èªžåŽé›†
        modifiers = []
        for word in sentence.words:
            if word.head == antecedent.id and word.deprel in ['det', 'amod', 'compound']:
                modifiers.append(word)
        
        # åŸºæœ¬æ§‹æˆï¼šä¿®é£¾èªž + å…ˆè¡Œè©ž + é–¢ä¿‚ä»£åè©ž
        phrase_words = modifiers + [antecedent]
        
        # é–¢ä¿‚ä»£åè©žã‚’è¿½åŠ ï¼ˆå‹•è©žéƒ¨åˆ†ã¯é™¤å¤–ï¼‰
        if rel_pronoun:
            phrase_words.append(rel_pronoun)
        
        # æ‰€æœ‰æ ¼ã®ç‰¹åˆ¥å‡¦ç†ï¼ˆæ‰€æœ‰ã•ã‚Œã‚‹åè©žã®ã¿ï¼‰
        if possessed_noun and rel_pronoun:
            if possessed_noun not in phrase_words:
                phrase_words.append(possessed_noun)
        
        # IDé †ã‚½ãƒ¼ãƒˆï¼ˆèªžé †ä¿æŒï¼‰
        phrase_words.sort(key=lambda w: w.id)
        return ' '.join(w.text for w in phrase_words)
    
    def _generate_relative_clause_slots(self, rel_type: str, noun_phrase: str, rel_subject, rel_verb, sentence) -> Dict:
        """é–¢ä¿‚ç¯€ã‚¿ã‚¤ãƒ—åˆ¥ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆï¼ˆå—å‹•æ…‹å¯¾å¿œæ”¹å–„ï¼‰"""
        
        slots = {}
        sub_slots = {}
        
        # å—å‹•æ…‹è£œåŠ©å‹•è©žã®æ¤œå‡º
        aux_word = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'aux:pass')
        if not aux_word:
            aux_word = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'aux')
        
        # âœ… é–¢ä¿‚ç¯€å†…ã®å‰¯è©žã‚’æ¤œå‡ºã—ã¦sub-m2ã«é…ç½®
        adverb_word = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'advmod')
        if adverb_word:
            # é–¢ä¿‚å‰¯è©žã¯é™¤å¤–ï¼ˆwhere, when, why, howã¯é–¢ä¿‚å‰¯è©žã¨ã—ã¦åˆ¥é€”å‡¦ç†ï¼‰
            if adverb_word.text.lower() not in ['where', 'when', 'why', 'how']:
                sub_slots["sub-m2"] = adverb_word.text
                self.logger.debug(f"ðŸ”§ é–¢ä¿‚ç¯€å†…å‰¯è©žæ¤œå‡º: sub-m2 = '{adverb_word.text}'")
        
        # âœ… é–¢ä¿‚ç¯€å†…ã®å‰ç½®è©žå¥ãƒ»å‰¯è©žå¥ã‚’æ¤œå‡ºã—ã¦sub-m3ã«é…ç½®
        obl_word = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'obl')
        if obl_word:
            sub_slots["sub-m3"] = obl_word.text
            self.logger.debug(f"ðŸ”§ é–¢ä¿‚ç¯€å†…å‰¯è©žå¥æ¤œå‡º: sub-m3 = '{obl_word.text}'")
        
        if rel_type == 'obj':
            # ç›®çš„èªžé–¢ä¿‚ä»£åè©ž: "The book that he bought"
            # slots["O1"] = ""  # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã¯5æ–‡åž‹ã‚¨ãƒ³ã‚¸ãƒ³ã«ä»»ã›ã‚‹
            sub_slots["sub-o1"] = noun_phrase
            if rel_subject:
                sub_slots["sub-s"] = rel_subject.text
            sub_slots["sub-v"] = rel_verb.text
            
        elif rel_type == 'nsubj':
            # ä¸»èªžé–¢ä¿‚ä»£åè©ž: "The man who runs"
            # slots["S"] = ""  # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã¯5æ–‡åž‹ã‚¨ãƒ³ã‚¸ãƒ³ã«ä»»ã›ã‚‹
            sub_slots["sub-s"] = noun_phrase
            sub_slots["sub-v"] = rel_verb.text
            
        elif rel_type == 'nsubj:pass':
            # å—å‹•æ…‹ä¸»èªžé–¢ä¿‚ä»£åè©ž: "The car which was crashed"
            # slots["S"] = ""  # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã¯5æ–‡åž‹ã‚¨ãƒ³ã‚¸ãƒ³ã«ä»»ã›ã‚‹
            sub_slots["sub-s"] = noun_phrase
            if aux_word:
                sub_slots["sub-aux"] = aux_word.text
            sub_slots["sub-v"] = rel_verb.text
            
        elif rel_type == 'poss':
            # æ‰€æœ‰æ ¼é–¢ä¿‚ä»£åè©ž: whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†
            
            # âœ… ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æžè£œæ­£ãŒã‚ã‚‹å ´åˆã®ç‰¹åˆ¥å‡¦ç†
            if hasattr(sentence, 'hybrid_corrections'):
                # whoseæ§‹æ–‡ã§å‹•è©žãŒè£œæ­£ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ä¸»æ–‡ãƒ»é–¢ä¿‚ç¯€æ§‹é€ ã‚’æ­£ã—ãåˆ†é›¢
                for word_id, correction in sentence.hybrid_corrections.items():
                    if correction['correction_type'] == 'whose_verb_fix':
                        # è£œæ­£ã•ã‚ŒãŸå‹•è©žï¼ˆä¾‹ï¼šlivesï¼‰ã¯ä¸»æ–‡å‹•è©žãªã®ã§ã€é–¢ä¿‚ç¯€ã®å‡¦ç†ã‹ã‚‰é™¤å¤–
                        main_verb_word = self._find_word_by_id(sentence, word_id)
                        if main_verb_word:
                            self.logger.debug(f"ðŸ”§ whoseæ§‹æ–‡ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è£œæ­£: {main_verb_word.text}ã¯ä¸»æ–‡å‹•è©žã¨ã—ã¦å‡¦ç†")
                            # ã“ã®å ´åˆã€é–¢ä¿‚ç¯€ã¯"car is red"ã®éƒ¨åˆ†
                            # rel_verbã¯copula "is"
                            sub_slots["sub-s"] = noun_phrase  # "The man whose car"
                            sub_slots["sub-v"] = rel_verb.text  # "is"
                            
                            # è£œèªžã‚’æ¤œå‡ºï¼ˆ"red"ï¼‰
                            complement = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'amod')
                            if complement:
                                sub_slots["sub-c1"] = complement.text
                            
                            # ãƒ¡ã‚¤ãƒ³æ–‡ã¯åˆ¥é€”åŸºæœ¬5æ–‡åž‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒå‡¦ç†ã™ã‚‹
                            return {"slots": slots, "sub_slots": sub_slots}
            
            # é€šå¸¸ã®whoseæ§‹æ–‡å‡¦ç†
            if rel_subject:
                # åˆ¥ã®ä¸»èªžãŒã‚ã‚‹å ´åˆ: "The student whose book I borrowed"
                # â†’ ç›®çš„èªžé–¢ä¿‚ä»£åè©žã¨ã—ã¦å‡¦ç†
                sub_slots["sub-o1"] = noun_phrase
                sub_slots["sub-s"] = rel_subject.text
            else:
                # åˆ¥ã®ä¸»èªžãŒãªã„å ´åˆ: "The woman whose dog barks"  
                # â†’ ä¸»èªžé–¢ä¿‚ä»£åè©žã¨ã—ã¦å‡¦ç†
                sub_slots["sub-s"] = noun_phrase
            
            # é–¢ä¿‚ç¯€å†…ã®å‹•è©žãƒ»è£œèªžã‚’æ­£ã—ãæŠ½å‡º
            if aux_word:
                sub_slots["sub-aux"] = aux_word.text
            sub_slots["sub-v"] = rel_verb.text
            
            # whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†ï¼šStanzaã®èª¤è§£æžå¯¾å¿œ
            if any(w.text.lower() == 'whose' for w in sentence.words):
                # acl:relclã¨ã—ã¦è§£æžã•ã‚ŒãŸlivesï¼ˆid=7ï¼‰ã®ä¾å­˜èªžã‹ã‚‰redã‚’æŽ¢ã™
                acl_relcl_word = self._find_word_by_deprel(sentence, 'acl:relcl')
                if acl_relcl_word:
                    complement = self._find_word_by_head_and_deprel(sentence, acl_relcl_word.id, 'amod')
                    if complement:
                        sub_slots["sub-c1"] = complement.text
            else:
                # é€šå¸¸ã®è£œèªžæ¤œå‡º
                complement = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'acomp')  # å½¢å®¹è©žè£œèªž
                if not complement:
                    complement = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'attr')  # å±žæ€§è£œèªž
                if not complement:
                    complement = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'nmod')  # åè©žä¿®é£¾
                if complement:
                    sub_slots["sub-c1"] = complement.text
            
        elif rel_type == 'advmod':
            # é–¢ä¿‚å‰¯è©ž: "The place where he lives"
            # slots["M3"] = ""  # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã¯5æ–‡åž‹ã‚¨ãƒ³ã‚¸ãƒ³ã«ä»»ã›ã‚‹
            sub_slots["sub-m3"] = noun_phrase
            if rel_subject:
                sub_slots["sub-s"] = rel_subject.text
            sub_slots["sub-v"] = rel_verb.text
            
        # çœç•¥é–¢ä¿‚ä»£åè©žã®å‡¦ç†
        elif rel_type == 'obj_omitted':
            # çœç•¥ç›®çš„èªžé–¢ä¿‚ä»£åè©ž: "The book I read"
            # slots["O1"] = ""  # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã¯5æ–‡åž‹ã‚¨ãƒ³ã‚¸ãƒ³ã«ä»»ã›ã‚‹
            sub_slots["sub-o1"] = noun_phrase
            sub_slots["sub-v"] = rel_verb.text
            
        elif rel_type == 'nsubj_omitted':  
            # çœç•¥ä¸»èªžé–¢ä¿‚ä»£åè©ž: "The person standing there"
            # slots["O1"] = ""  # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã¯5æ–‡åž‹ã‚¨ãƒ³ã‚¸ãƒ³ã«ä»»ã›ã‚‹
            sub_slots["sub-o1"] = noun_phrase
            sub_slots["sub-v"] = rel_verb.text
            
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆç›®çš„èªžæ‰±ã„ï¼‰
            # slots["O1"] = ""  # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã¯5æ–‡åž‹ã‚¨ãƒ³ã‚¸ãƒ³ã«ä»»ã›ã‚‹
            sub_slots["sub-o1"] = noun_phrase
            if rel_subject:
                sub_slots["sub-s"] = rel_subject.text
            sub_slots["sub-v"] = rel_verb.text
        
        return {"slots": slots, "sub_slots": sub_slots}
    
    def _generate_whose_relative_clause_slots(self, antecedent, cop_verb, sentence) -> Dict:
        """whoseæ§‹æ–‡å°‚ç”¨ã®é–¢ä¿‚ç¯€ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆï¼ˆãƒ¡ã‚¤ãƒ³æ–‡ã‚’å¦¨å®³ã—ãªã„ï¼‰"""
        
        slots = {}  # ãƒ¡ã‚¤ãƒ³æ–‡ã‚¹ãƒ­ãƒƒãƒˆã¯å¤‰æ›´ã—ãªã„
        sub_slots = {}
        
        # whoseæ§‹æ–‡ã®é–¢ä¿‚ç¯€: "whose car is red"
        # æ‰€æœ‰æ ¼é–¢ä¿‚ä»£åè©žã‚’å«ã‚€å…ˆè¡Œè©žå¥ã‚’æ§‹ç¯‰
        whose_word = None
        car_word = None
        
        for word in sentence.words:
            if word.text.lower() == 'whose':
                whose_word = word
                # whoseãŒä¾å­˜ã™ã‚‹èªžï¼ˆcarï¼‰ã‚’å–å¾—
                car_word = self._find_word_by_id(sentence, whose_word.head)
                break
        
        if whose_word and car_word:
            # "The man whose car"ã®æ§‹ç¯‰
            man_phrase = self._build_phrase_with_modifiers(sentence, antecedent)
            whose_car_phrase = f"{man_phrase} {whose_word.text} {car_word.text}"
            
            sub_slots["sub-s"] = whose_car_phrase
            sub_slots["sub-v"] = cop_verb.text  # "is"
            
            # è£œèªžï¼ˆredï¼‰ã‚’å–å¾—
            complement = self._find_word_by_id(sentence, cop_verb.head)
            if complement:
                sub_slots["sub-c1"] = complement.text
            
            self.logger.debug(f"ðŸ”§ whoseé–¢ä¿‚ç¯€ã‚¹ãƒ­ãƒƒãƒˆ: sub-s='{whose_car_phrase}', sub-v='{cop_verb.text}', sub-c1='{complement.text if complement else ''}'")
        
        return {"slots": slots, "sub_slots": sub_slots}
    
    def _process_main_clause_after_relative(self, sentence, antecedent, rel_verb, noun_phrase) -> Optional[Dict]:
        """é–¢ä¿‚ç¯€å‡¦ç†å¾Œã®ä¸»æ–‡éƒ¨åˆ†ã‚’5æ–‡åž‹ã§å‡¦ç†"""
        
        # ä¸»æ–‡ã®å‹•è©žï¼ˆROOTèªžï¼‰ã‚’ç‰¹å®š
        main_verb = self._find_root_word(sentence)
        if not main_verb:
            self.logger.debug("  âš ï¸ ä¸»æ–‡å‹•è©žãªã—")
            return None
            
        if main_verb.id == rel_verb.id:
            self.logger.debug(f"  âš ï¸ é–¢ä¿‚ç¯€å‹•è©žãŒROOT - ä¸»æ–‡ãªã— (main_verb={main_verb.text}, rel_verb={rel_verb.text})")
            return None
        
        self.logger.debug(f"  ðŸ” ä¸»æ–‡å‹•è©žæ¤œå‡º: {main_verb.text} (id: {main_verb.id}, POS: {main_verb.upos})")
        
        # ä¾å­˜é–¢ä¿‚ãƒžãƒƒãƒ—æ§‹ç¯‰ï¼ˆé–¢ä¿‚ç¯€ã‚’é™¤å¤–ï¼‰
        dep_relations = {}
        excluded_words = []
        
        for word in sentence.words:
            # é–¢ä¿‚ç¯€å†…ã®èªžã‚’ã‚¹ã‚­ãƒƒãƒ—
            if self._is_word_in_relative_clause(word, rel_verb):
                excluded_words.append(word.text)
                continue
                
            if word.deprel not in dep_relations:
                dep_relations[word.deprel] = []
            dep_relations[word.deprel].append(word)
        
        self.logger.debug(f"  ðŸš« é™¤å¤–èªž: {excluded_words}")
        self.logger.debug(f"  ðŸ“ ä¸»æ–‡ä¾å­˜é–¢ä¿‚: {list(dep_relations.keys())}")
        
        # åŸºæœ¬5æ–‡åž‹ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        pattern_result = self._detect_basic_five_pattern(main_verb, dep_relations)
        if not pattern_result:
            self.logger.debug("  âŒ ä¸»æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå¤±æ•—")
            return None
        
        self.logger.debug(f"  ðŸŽ¯ ä¸»æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º: {pattern_result['pattern']}")
        
        # ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆï¼ˆSã‚¹ãƒ­ãƒƒãƒˆã¯ç©ºã«ã—ã¦æ§‹é€ ã‚’ç¶­æŒï¼‰
        five_pattern_slots = self._generate_basic_five_slots(
            pattern_result['pattern'], pattern_result['mapping'], dep_relations, sentence
        )
        
        # é–¢ä¿‚ç¯€ã‚’å«ã‚€ä¸»èªžã¯ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«ã‚ã‚‹ãŸã‚ã€ä¸Šä½Sã‚¹ãƒ­ãƒƒãƒˆã¯Noneã¾ãŸã¯ç©º
        if 'slots' in five_pattern_slots and 'S' in five_pattern_slots['slots']:
            five_pattern_slots['slots']['S'] = ""  # é–¢ä¿‚ç¯€ãŒã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«å«ã¾ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã™
            
        self.logger.debug(f"  âœ… ä¸»æ–‡å‡¦ç†å®Œäº†: ãƒ‘ã‚¿ãƒ¼ãƒ³={pattern_result['pattern']}")
        return five_pattern_slots
    
    def _is_word_in_relative_clause(self, word, rel_verb) -> bool:
        """èªžãŒé–¢ä¿‚ç¯€å†…ã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        
        # é–¢ä¿‚ç¯€å‹•è©žè‡ªèº«
        if word.id == rel_verb.id:
            return True
            
        # é–¢ä¿‚ç¯€å‹•è©žã®ç›´æŽ¥ä¾å­˜èªžï¼ˆå…¨ç¨®é¡žï¼‰
        if word.head == rel_verb.id:
            return True
            
        # é–¢ä¿‚ä»£åè©žï¼ˆé–¢ä¿‚ç¯€å‹•è©žã«ä¾å­˜ã™ã‚‹nsubj/objç­‰ï¼‰
        if word.deprel in ['nsubj', 'obj', 'advmod', 'obl', 'aux', 'aux:pass', 'acomp', 'attr', 'nmod'] and word.head == rel_verb.id:
            return True
        
        # é–¢ä¿‚ç¯€ã‚’ä¿®é£¾ã™ã‚‹acl:relclã®ä¾å­˜èªž
        if word.deprel == 'acl:relcl':
            return True
            
        return False
    
    def _get_all_dependents(self, head_word) -> List:
        """æŒ‡å®šèªžã®ã™ã¹ã¦ã®ä¾å­˜èªžã‚’å–å¾—"""
        # ã“ã®å®Ÿè£…ã§ã¯ã€sentenceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ç›´æŽ¥ã‚¢ã‚¯ã‚»ã‚¹ã§ããªã„ãŸã‚
        # ç°¡æ˜“å®Ÿè£…ã¨ã—ã¦ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™
        # å®Ÿéš›ã®ä½¿ç”¨ã§ã¯ã€sentence.wordsã‚’é€šã˜ã¦ä¾å­˜èªžã‚’æ¤œç´¢ã™ã‚‹
        return []
    
    # === Stanzaè§£æžãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ ===
    
    def _find_word_by_deprel(self, sentence, deprel: str):
        """ä¾å­˜é–¢ä¿‚ã§èªžã‚’æ¤œç´¢"""
        return next((w for w in sentence.words if w.deprel == deprel), None)
    
    def _find_word_by_id(self, sentence, word_id: int):
        """IDã§èªžã‚’æ¤œç´¢"""
        return next((w for w in sentence.words if w.id == word_id), None)
    
    def _find_word_by_head_and_deprel(self, sentence, head_id: int, deprel: str):
        """é ­IDã¨ä¾å­˜é–¢ä¿‚ã§èªžã‚’æ¤œç´¢"""
        return next((w for w in sentence.words if w.head == head_id and w.deprel == deprel), None)
    
    def _find_main_verb(self, sentence):
        """ä¸»æ–‡ã®å‹•è©žã‚’æ¤œç´¢ï¼ˆé–¢ä¿‚ç¯€ã‚’é™¤å¤–ãƒ»ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æžå¯¾å¿œï¼‰"""
        
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æžã®è£œæ­£æƒ…å ±ã‚’ãƒã‚§ãƒƒã‚¯
        if hasattr(sentence, 'hybrid_corrections'):
            for word in sentence.words:
                if word.id in sentence.hybrid_corrections:
                    correction = sentence.hybrid_corrections[word.id]
                    if correction['correction_type'] == 'whose_verb_fix':
                        # è£œæ­£ã•ã‚ŒãŸå‹•è©žã‚’ä¸»æ–‡å‹•è©žã¨ã—ã¦è¿”ã™
                        self.logger.debug(f"ðŸ”§ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æž: ä¸»æ–‡å‹•è©žã¨ã—ã¦ {word.text} ã‚’ä½¿ç”¨ (è£œæ­£æ¸ˆã¿)")
                        return word
        
        # whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†ï¼šStanzaãŒlivesã‚’èª¤è§£æžã™ã‚‹å ´åˆã®å¯¾å¿œ
        if any(w.text.lower() == 'whose' for w in sentence.words):
            # acl:relclé–¢ä¿‚ã«ã‚ã‚‹èªžã‚’ç¢ºèª
            acl_relcl_word = self._find_word_by_deprel(sentence, 'acl:relcl')
            if (acl_relcl_word and 
                acl_relcl_word.text.lower() in ['lives', 'works', 'runs', 'goes'] and
                acl_relcl_word.lemma in ['live', 'work', 'run', 'go']):
                # ã“ã‚Œã¯å‹•è©žã¨ã—ã¦è§£é‡ˆã™ã¹ã
                self.logger.debug(f"ðŸ”§ whoseæ§‹æ–‡: ä¸»æ–‡å‹•è©žã¨ã—ã¦ {acl_relcl_word.text} ã‚’ä½¿ç”¨")
                return acl_relcl_word
        
        # é€šå¸¸ã®å ´åˆï¼šrootã‚’æ¤œç´¢
        root_word = None
        for word in sentence.words:
            if word.head == 0:  # root
                root_word = word
                break
        
        if not root_word:
            return None
            
        # rootãŒå½¢å®¹è©žã®å ´åˆã€copå‹•è©žã‚’ä¸»å‹•è©žã¨ã™ã‚‹ï¼ˆ"The man is strong"æ§‹é€ ï¼‰
        if root_word.upos == 'ADJ':
            cop_verb = self._find_word_by_head_and_deprel(sentence, root_word.id, 'cop')
            if cop_verb:
                return cop_verb
        
        return root_word
    
    def _build_full_subject_with_relative_clause(self, sentence, antecedent, rel_verb):
        """é–¢ä¿‚ç¯€ã‚’å«ã‚€å®Œå…¨ãªä¸»èªžå¥ã‚’æ§‹ç¯‰"""
        # å…ˆè¡Œè©žã‹ã‚‰é–‹å§‹
        subject_phrase = antecedent.text
        
        # å…ˆè¡Œè©žã®ä¿®é£¾èªžã‚’è¿½åŠ 
        modifiers = []
        for word in sentence.words:
            if word.head == antecedent.id and word.id != rel_verb.id:
                if word.deprel in ['det', 'amod', 'compound']:
                    modifiers.append((word.id, word.text))
        
        # ä¿®é£¾èªžã‚’ä½ç½®é †ã§ã‚½ãƒ¼ãƒˆ
        modifiers.sort(key=lambda x: x[0])
        
        # å®Œå…¨ãªä¸»èªžå¥ã‚’æ§‹ç¯‰
        if modifiers:
            modifier_text = ' '.join([m[1] for m in modifiers])
            subject_phrase = f"{modifier_text} {subject_phrase}"
        
        return subject_phrase
    
    def _is_whose_construction(self, sentence, rel_verb):
        """whoseæ§‹æ–‡ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        # whoseãŒå­˜åœ¨ã—ã€ã‹ã¤rel_verbã®ä¾å­˜èªžã«copãŒã‚ã‚‹å ´åˆ
        has_whose = any(w.text.lower() == 'whose' for w in sentence.words)
        has_cop_child = any(w.head == rel_verb.id and w.deprel == 'cop' for w in sentence.words)
        return has_whose and has_cop_child
    
    def _find_cop_verb_in_whose_clause(self, sentence, rel_verb):
        """whoseæ§‹æ–‡ã§ã®å®Ÿéš›ã®é–¢ä¿‚ç¯€å‹•è©žï¼ˆcopï¼‰ã‚’æ¤œç´¢"""
        # rel_verbã®ä¾å­˜èªžã§copã®ã‚‚ã®ã‚’æŽ¢ã™
        cop_verb = next((w for w in sentence.words if w.head == rel_verb.id and w.deprel == 'cop'), None)
        return cop_verb
    
    def _find_whose_antecedent(self, sentence):
        """whoseæ§‹æ–‡ã®å…ˆè¡Œè©žã‚’æ¤œç´¢"""
        # rootä¸»èªžã‚’å–å¾—ï¼ˆé€šå¸¸ã¯å…ˆè¡Œè©žï¼‰
        for word in sentence.words:
            if word.head == 0 and word.deprel == 'root':
                return word
        return None
    
    def _handle_basic_five_pattern(self, sentence, base_result: Dict) -> Optional[Dict]:
        """
        åŸºæœ¬5æ–‡åž‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆPhase 1å®Ÿè£…ï¼‰
        
        basic_five_pattern_engine.py ã®æ©Ÿèƒ½ã‚’çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã«ç§»æ¤
        Stanza dependency parsing ã«ã‚ˆã‚‹åŸºæœ¬æ–‡åž‹æ¤œå‡ºãƒ»åˆ†è§£
        
        Args:
            sentence: Stanza sentence object
            base_result: åŸºæœ¬çµæžœè¾žæ›¸
            
        Returns:
            Optional[Dict]: 5æ–‡åž‹å‡¦ç†çµæžœ or None
        """
        try:
            self.logger.debug("ðŸ” 5æ–‡åž‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè¡Œä¸­...")
            
            # ä»–ã®ã‚¨ãƒ³ã‚¸ãƒ³ãŒä¸»æ–‡å‹•è©žï¼ˆVï¼‰ã‚’æ—¢ã«å‡¦ç†æ¸ˆã¿ã®å ´åˆã®ã¿ã‚¹ã‚­ãƒƒãƒ—
            # sub-vã¯é–¢ä¿‚ç¯€å‹•è©žãªã®ã§ä¸»æ–‡å‡¦ç†ã«ã¯å½±éŸ¿ã—ãªã„
            if base_result.get('slots', {}).get('V'):
                self.logger.debug("  ä¸»æ–‡å‹•è©ž(V)ãŒå‡¦ç†æ¸ˆã¿ - ã‚¹ã‚­ãƒƒãƒ—")
                return None
            
            return self._process_basic_five_pattern_structure(sentence, base_result)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ 5æ–‡åž‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _process_basic_five_pattern_structure(self, sentence, base_result: Dict) -> Dict:
        """åŸºæœ¬5æ–‡åž‹æ§‹é€ ã®åˆ†è§£å‡¦ç†ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æžå¯¾å¿œï¼‰"""
        
        # âœ… ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æžè£œæ­£æƒ…å ±ã‚’å„ªå…ˆçš„ã«åˆ©ç”¨
        root_word = None
        is_whose_construction = any(w.text.lower() == 'whose' for w in sentence.words)
        
        if hasattr(sentence, 'hybrid_corrections'):
            # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æžã§VERBã¨ã—ã¦è£œæ­£ã•ã‚ŒãŸèªžã‚’ä¸»æ–‡å‹•è©žã¨ã—ã¦æŽ¡ç”¨
            for word_id, correction in sentence.hybrid_corrections.items():
                if correction['correction_type'] == 'whose_verb_fix':
                    root_word = self._find_word_by_id(sentence, word_id)
                    if root_word:
                        self.logger.debug(f"ðŸ”§ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æž: {root_word.text} ã‚’ãƒ¡ã‚¤ãƒ³å‹•è©žã¨ã—ã¦ä½¿ç”¨")
                        break
        
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æžãŒãªã„å ´åˆã®å¾“æ¥å‡¦ç†        
        if not root_word and is_whose_construction:
            # acl:relclé–¢ä¿‚ã«ã‚ã‚‹èªžã‚’ç¢ºèª
            acl_relcl_word = self._find_word_by_deprel(sentence, 'acl:relcl')
            if (acl_relcl_word and 
                acl_relcl_word.text.lower() in ['lives', 'works', 'runs', 'goes', 'sits', 'stands']):
                # ã“ã‚Œã¯å®Ÿéš›ã®ãƒ¡ã‚¤ãƒ³å‹•è©žã¨ã—ã¦è§£é‡ˆã™ã¹ã
                root_word = acl_relcl_word
                self.logger.debug(f"ðŸ”§ whoseæ§‹æ–‡æ¤œå‡º: ãƒ¡ã‚¤ãƒ³å‹•è©žã‚’ {acl_relcl_word.text} ã«ä¿®æ­£")
        
        # é€šå¸¸ã®å ´åˆï¼šROOTèªžæ¤œå‡º
        if not root_word:
            root_word = self._find_root_word(sentence)
            if not root_word:
                return base_result

        # ä¾å­˜é–¢ä¿‚ãƒžãƒƒãƒ—æ§‹ç¯‰
        dep_relations = {}
        for word in sentence.words:
            if word.deprel not in dep_relations:
                dep_relations[word.deprel] = []
            dep_relations[word.deprel].append(word)
        
        # âœ… whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†ï¼šãƒ¡ã‚¤ãƒ³æ–‡ã®ä¾å­˜é–¢ä¿‚ãƒžãƒƒãƒ—ã‚’æ­£ã—ãæ§‹ç¯‰
        if is_whose_construction and root_word:
            # ãƒ¡ã‚¤ãƒ³å‹•è©žã®ç›´æŽ¥ä¾å­˜èªžã‚’ä¾å­˜é–¢ä¿‚ãƒžãƒƒãƒ—ã«è¿½åŠ 
            for word in sentence.words:
                if word.head == root_word.id:
                    if word.deprel not in dep_relations:
                        dep_relations[word.deprel] = []
                    dep_relations[word.deprel].append(word)
                    
            # ROOTèªžï¼ˆå…ˆè¡Œè©žï¼‰ã‚’ä¸»èªžã¨ã—ã¦è¿½åŠ 
            if 'nsubj' not in dep_relations:
                dep_relations['nsubj'] = []
            root_word_from_stanza = self._find_root_word(sentence)
            if root_word_from_stanza:
                dep_relations['nsubj'].append(root_word_from_stanza)
                
            self.logger.debug(f"ðŸ”§ whoseæ§‹æ–‡: ä¾å­˜é–¢ä¿‚å†æ§‹ç¯‰å®Œäº†, ãƒ¡ã‚¤ãƒ³å‹•è©ž={root_word.text}")

        # åŸºæœ¬5æ–‡åž‹ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        pattern_result = self._detect_basic_five_pattern(root_word, dep_relations)
        if not pattern_result:
            return base_result
        
        # ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ
        result = base_result.copy()
        if 'slots' not in result:
            result['slots'] = {}
        if 'sub_slots' not in result:
            result['sub_slots'] = {}
        
        five_pattern_slots = self._generate_basic_five_slots(
            pattern_result['pattern'], pattern_result['mapping'], dep_relations, sentence
        )
        
        result['slots'].update(five_pattern_slots.get('slots', {}))
        result['sub_slots'].update(five_pattern_slots.get('sub_slots', {}))
        
        # æ–‡æ³•æƒ…å ±è¨˜éŒ²ï¼ˆ_merge_handler_resultsã¨äº’æ›æ€§ã®ã‚ã‚‹å½¢å¼ï¼‰
        result['grammar_info'] = {
            'detected_patterns': ['basic_five_pattern'],
            'handler_contributions': {
                'basic_five_pattern': {
                    'pattern': pattern_result['pattern'],
                    'confidence': pattern_result.get('confidence', 0.8)
                }
            }
        }
        
        self.logger.debug(f"  âœ… 5æ–‡åž‹å‡¦ç†å®Œäº†: ãƒ‘ã‚¿ãƒ¼ãƒ³={pattern_result['pattern']}")
        return result
    
    def _find_root_word(self, sentence):
        """ROOTèªžã‚’æ¤œç´¢"""
        return next((w for w in sentence.words if w.head == 0), None)
    
    def _detect_basic_five_pattern(self, root_word, dep_relations):
        """åŸºæœ¬5æ–‡åž‹ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º"""
        
        # åŸºæœ¬5æ–‡åž‹ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©ï¼ˆè©³ç´°â†’å˜ç´”ã®é †åºã§æ¤œå‡ºï¼‰
        patterns = {
            "SVOO": {
                "required": ["nsubj", "obj", "iobj"],
                "optional": [],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "root": "V", "iobj": "O1", "obj": "O2"}
            },
            "SVOC": {
                "required": ["nsubj", "obj", "xcomp"],
                "optional": [],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "root": "V", "obj": "O1", "xcomp": "C2"}
            },
            "SVO": {
                "required": ["nsubj", "obj"],
                "optional": [],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "root": "V", "obj": "O1"}
            },
            "SVC": {
                "required": ["nsubj", "cop"],
                "optional": [],
                "root_pos": ["ADJ", "NOUN"],
                "mapping": {"nsubj": "S", "cop": "V", "root": "C1"}
            },
            "SVC_PRON": {
                "required": ["nsubj", "cop"],
                "optional": [],
                "root_pos": ["PRON"],
                "mapping": {"nsubj": "S", "cop": "V", "root": "C1"}
            },
            "SVC_ALT": {
                "required": ["nsubj", "xcomp"],
                "optional": [],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "root": "V", "xcomp": "C1"}
            },
            "SV": {
                "required": ["nsubj"],
                "optional": [],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "root": "V"}
            }
        }
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒžãƒƒãƒãƒ³ã‚°
        for pattern_name, pattern_info in patterns.items():
            if self._matches_five_pattern(pattern_info, dep_relations, root_word):
                return {
                    'pattern': pattern_name,
                    'mapping': pattern_info['mapping'],
                    'confidence': 0.9
                }
        
        return None
    
    def _matches_five_pattern(self, pattern_info, dep_relations, root_word):
        """5æ–‡åž‹ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒžãƒƒãƒãƒ³ã‚°"""
        # å¿…è¦ãªä¾å­˜é–¢ä¿‚ã®ç¢ºèª
        for rel in pattern_info['required']:
            if rel not in dep_relations:
                return False
        
        # ROOTèªžã®å“è©žãƒã‚§ãƒƒã‚¯
        if root_word.upos not in pattern_info['root_pos']:
            return False
        
        return True
    
    def _build_phrase_with_modifiers(self, sentence, main_word):
        """
        ä¿®é£¾èªžå¥ã‚’å«ã‚€å®Œå…¨ãªå¥ã‚’æ§‹ç¯‰
        
        å¯¾å¿œä¿®é£¾èªžã‚¿ã‚¤ãƒ—ï¼š
        - det: é™å®šè©ž (a, an, the, my, your, his, her, its, our, their)
        - amod: å½¢å®¹è©žä¿®é£¾èªž (red, beautiful, smart, old)
        - nummod: æ•°è©žä¿®é£¾èªž (one, two, first, second)  
        - nmod:poss: æ‰€æœ‰æ ¼ä¿®é£¾èªž (John's, Mary's, my, your)
        - compound: è¤‡åˆåè©ž (car door, school bus)
        """
        if not main_word:
            return ""
        
        # ä¿®é£¾èªžåŽé›†
        modifiers = []
        for word in sentence.words:
            if word.head == main_word.id:
                if word.deprel in ['det', 'amod', 'nummod', 'nmod:poss', 'compound']:
                    modifiers.append(word)
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ 
        if modifiers:
            self.logger.debug(f"ðŸ”§ ä¿®é£¾èªžæ¤œå‡º [{main_word.text}]: {[(m.text, m.deprel) for m in modifiers]}")
        
        # ä¿®é£¾èªžã‚’IDé †ã§ã‚½ãƒ¼ãƒˆï¼ˆèªžé †ä¿æŒï¼‰
        modifiers.sort(key=lambda w: w.id)
        
        # å¥æ§‹ç¯‰: ä¿®é£¾èªž + ãƒ¡ã‚¤ãƒ³èªž
        phrase_words = modifiers + [main_word]
        phrase_words.sort(key=lambda w: w.id)  # æœ€çµ‚çš„ãªèªžé †ç¢ºä¿
        
        result = ' '.join(word.text for word in phrase_words)
        self.logger.debug(f"ðŸ”§ å¥æ§‹ç¯‰å®Œäº†: '{result}'")
        
        return result
    
    def _generate_basic_five_slots(self, pattern, mapping, dep_relations, sentence):
        """åŸºæœ¬5æ–‡åž‹ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆï¼ˆä¿®é£¾èªžå¥å¯¾å¿œå¼·åŒ–ï¼‰"""
        slots = {}
        sub_slots = {}
        
        # ãƒžãƒƒãƒ”ãƒ³ã‚°ã«å¾“ã£ã¦ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ
        for dep_rel, slot in mapping.items():
            if dep_rel == "root":
                # ROOTèªžã®å‡¦ç†ï¼ˆå‹•è©žã¯é€šå¸¸ä¿®é£¾èªžãªã—ãªã®ã§å˜èªžã®ã¿ï¼‰
                root_word = self._find_root_word(sentence)
                if root_word:
                    slots[slot] = root_word.text
            elif dep_rel in dep_relations:
                # ä¾å­˜é–¢ä¿‚èªžã®å‡¦ç†ï¼ˆä¿®é£¾èªžå¥ã‚’å«ã‚€å®Œå…¨ãªå¥ã‚’æ§‹ç¯‰ï¼‰
                words = dep_relations[dep_rel]
                if words:
                    # ãƒ¡ã‚¤ãƒ³ã®èªž
                    main_word = words[0]
                    # ä¿®é£¾èªžå¥ã‚’æ§‹ç¯‰
                    phrase = self._build_phrase_with_modifiers(sentence, main_word)
                    slots[slot] = phrase
        
        # âœ… è¿½åŠ å‡¦ç†ï¼šROOTãƒ¯ãƒ¼ãƒ‰ã«ã‚‚ä¿®é£¾èªžå¥å‡¦ç†ã‚’é©ç”¨ï¼ˆå‹•è©žä»¥å¤–ã®å ´åˆï¼‰
        # ä¾‹: "The woman is my neighbor" ã§neighborãŒROOTã®å ´åˆ
        root_word = self._find_root_word(sentence)
        if root_word and root_word.pos in ['NOUN', 'PRON', 'ADJ']:
            # åè©žãƒ»ä»£åè©žãƒ»å½¢å®¹è©žãŒROOTã®å ´åˆã€ä¿®é£¾èªžå¥ã‚’æ§‹ç¯‰
            root_phrase = self._build_phrase_with_modifiers(sentence, root_word)
            
            # ROOTãƒ¯ãƒ¼ãƒ‰å¯¾å¿œã®ã‚¹ãƒ­ãƒƒãƒˆã‚’æ›´æ–°
            for dep_rel, slot in mapping.items():
                if dep_rel == "root" and slot in slots:
                    if slots[slot] == root_word.text:  # å˜èªžã®ã¿ã®å ´åˆ
                        slots[slot] = root_phrase  # ä¿®é£¾èªžå¥ã«æ›´æ–°
                        self.logger.debug(f"ðŸ”§ ROOTèªžä¿®é£¾èªžå¥é©ç”¨: {slot} = '{root_phrase}'")
        
        # ä¿®é£¾èªžã®å‡¦ç†ï¼ˆåŸºæœ¬çš„ãªã‚‚ã®ã®ã¿ï¼‰
        # é–¢ä¿‚å‰¯è©žã¯é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã«ä»»ã›ã‚‹ãŸã‚é™¤å¤–
        relative_adverbs = ['where', 'when', 'why', 'how']
        
        # âœ… é–¢ä¿‚ç¯€å†…ã®èªžã‚’äº‹å‰ã«ç‰¹å®šã—ã¦é™¤å¤–
        rel_verb_candidates = [w for w in sentence.words if w.deprel in ['acl:relcl', 'acl']]
        excluded_word_ids = set()
        for rel_verb_cand in rel_verb_candidates:
            # é–¢ä¿‚ç¯€å‹•è©žã¨ãã®ä¾å­˜èªžã‚’ã™ã¹ã¦é™¤å¤–
            excluded_word_ids.add(rel_verb_cand.id)
            for word in sentence.words:
                if word.head == rel_verb_cand.id:
                    excluded_word_ids.add(word.id)
        
        for word in sentence.words:
            # é–¢ä¿‚ç¯€å†…ã®èªžã‚’ã‚¹ã‚­ãƒƒãƒ—
            if word.id in excluded_word_ids:
                continue
                
            # âœ… å‰¯è©žå‡¦ç†ã¯å°‚é–€ã‚¨ãƒ³ã‚¸ãƒ³ã«å§”è­² - åŸºæœ¬5æ–‡åž‹ã§ã¯å‡¦ç†ã—ãªã„
            # if word.deprel == 'advmod' and 'M2' not in slots:
            #     if word.text.lower() not in relative_adverbs:
            #         slots['M2'] = word.text  # é€šå¸¸ã®å‰¯è©žä¿®é£¾èªžã®ã¿
            #     else:
            #         self.logger.debug(f"ðŸ” é–¢ä¿‚å‰¯è©žé™¤å¤–: {word.text} (é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã«å§”è­²)")
            # elif word.deprel == 'obl' and 'M3' not in slots:
            #     slots['M3'] = word.text  # å‰ç½®è©žå¥ç­‰
        
        return {'slots': slots, 'sub_slots': sub_slots}

    def _handle_adverbial_modifier(self, sentence, base_result: Dict) -> Optional[Dict]:
        """
        å‰¯è©žã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆPhase 4å®Ÿè£…ï¼‰
        
        migration_source/prepositional_phrase_engine.py ã®åˆ†é¡žã‚·ã‚¹ãƒ†ãƒ ã‚’å‚è€ƒã«
        çµ±ä¸€ã•ã‚ŒãŸå‰¯è©žå‡¦ç†ã‚’å®Ÿè£…
        
        Args:
            sentence: Stanza sentence ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            base_result: åŸºæœ¬è§£æžçµæžœï¼ˆé‡è¤‡é˜²æ­¢ç”¨ï¼‰
            
        Returns:
            Optional[Dict]: å‰¯è©žå‡¦ç†çµæžœã€ã¾ãŸã¯ None
        """
        from enum import Enum
        
        class AdverbialType(Enum):
            """å‰¯è©žã®æ„å‘³åˆ†é¡ž"""
            TIME = "time"           # æ™‚é–“å‰¯è©ž â†’ M1
            FREQUENCY = "frequency" # é »åº¦å‰¯è©ž â†’ M2
            MANNER = "manner"       # æ§˜æ…‹å‰¯è©ž â†’ M2/M3
            LOCATION = "location"   # å ´æ‰€å‰¯è©ž â†’ M2/M3
            DEGREE = "degree"       # ç¨‹åº¦å‰¯è©ž â†’ M2
        
        self.logger.debug("ðŸ” å‰¯è©žãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè¡Œä¸­...")
        
        # === æ—¢å­˜ã‚¹ãƒ­ãƒƒãƒˆç¢ºèªï¼ˆé‡è¤‡é˜²æ­¢ï¼‰ ===
        existing_slots = {}
        if base_result and 'slots' in base_result:
            existing_slots = base_result['slots']
        
        # æ—¢ã«å‰²ã‚Šå½“ã¦æ¸ˆã¿ã®å‰¯è©žæ–‡å­—åˆ—ã‚’ç‰¹å®š
        existing_adverbs = set()
        for slot_key, slot_value in existing_slots.items():
            if slot_key.startswith('M') and slot_value:
                # ã‚¹ãƒ­ãƒƒãƒˆå€¤ã‚’å˜èªžã«åˆ†è§£ã—ã¦å‰¯è©žã‚’ç‰¹å®š
                words = slot_value.split()
                existing_adverbs.update(words)
        
        # === 1. å‰¯è©žæ¤œå‡ºï¼ˆæ‹¡å¼µå¯¾å¿œï¼‰ ===
        adverbial_modifiers = []
        sentence_length = len(sentence.words)
        
        for word in sentence.words:
            # æ‹¡å¼µå‰¯è©žä¾å­˜é–¢ä¿‚å¯¾å¿œï¼ˆYesterdayæ¤œå‡ºãƒã‚°ä¿®æ­£æ¸ˆã¿ï¼‰
            if word.deprel in ['advmod', 'obl', 'obl:unmarked', 'obl:tmod', 'obl:npmod', 'nmod:unmarked', 'nmod:tmod']:
                # æ—¢ã«å‡¦ç†æ¸ˆã¿ã®å‰¯è©žã‚’ã‚¹ã‚­ãƒƒãƒ—
                if word.text in existing_adverbs:
                    self.logger.debug(f"âš ï¸ æ—¢å­˜ã‚¹ãƒ­ãƒƒãƒˆã«å‰²ã‚Šå½“ã¦æ¸ˆã¿ - ã‚¹ã‚­ãƒƒãƒ—: {word.text}")
                    continue
                    
                # é–¢ä¿‚å‰¯è©žã¯é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã«ä»»ã›ã‚‹
                if word.text.lower() in ['where', 'when', 'why', 'how']:
                    continue
                
                # å‰¯è©žåˆ†é¡ž
                adv_type = self._classify_adverb(word, sentence)
                
                # ä½ç½®è¨ˆç®—ï¼ˆ1ãƒ™ãƒ¼ã‚¹ï¼‰
                position_ratio = word.id / sentence_length
                
                adverbial_modifiers.append({
                    'word': word,
                    'type': adv_type,
                    'position': word.id,
                    'position_ratio': position_ratio,
                    'text': word.text
                })
        
        if not adverbial_modifiers:
            self.logger.debug("âŒ å‰¯è©žãªã— - ã‚¹ã‚­ãƒƒãƒ—")
            return None
        
        # === 2. ä½ç½®ãƒ™ãƒ¼ã‚¹é…ç½® ===
        slots = {}
        sub_slots = {}
        
        # ä½ç½®é †ã§ã‚½ãƒ¼ãƒˆï¼ˆå‰ã‹ã‚‰å¾Œã‚ã¸ï¼‰
        adverbial_modifiers.sort(key=lambda x: x['position'])
        
        for adv_info in adverbial_modifiers:
            word = adv_info['word']
            adv_type = adv_info['type']
            position_ratio = adv_info['position_ratio']
            word_text = word.text
            
            # è¤‡åˆå‰¯è©žå¥ã®æ§‹ç¯‰
            phrase = self._build_adverbial_phrase(sentence, word)
            if phrase != word_text:
                word_text = phrase
            
            # ä½ç½®ãƒ™ãƒ¼ã‚¹é…ç½®åˆ¤å®š
            target_slot = self._determine_adverb_slot(adv_type, position_ratio)
            
            # ã‚¹ãƒ­ãƒƒãƒˆé…ç½®ï¼ˆé‡è¤‡å›žé¿ï¼‰
            if target_slot == 'M1' and 'M1' not in slots:
                slots['M1'] = word_text
                self.logger.debug(f"ðŸ”§ M1é…ç½®({adv_type.value}): {word_text} (ä½ç½®: {position_ratio:.2f})")
            elif target_slot == 'M2' and 'M2' not in slots:
                slots['M2'] = word_text
                self.logger.debug(f"ðŸ”§ M2é…ç½®({adv_type.value}): {word_text} (ä½ç½®: {position_ratio:.2f})")
            elif target_slot == 'M3' and 'M3' not in slots:
                slots['M3'] = word_text
                self.logger.debug(f"ðŸ”§ M3é…ç½®({adv_type.value}): {word_text} (ä½ç½®: {position_ratio:.2f})")
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é…ç½®
                for fallback_slot in ['M1', 'M2', 'M3']:
                    if fallback_slot not in slots:
                        slots[fallback_slot] = word_text
                        self.logger.debug(f"ðŸ”§ {fallback_slot}ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é…ç½®: {word_text}")
                        break
        
        if slots:
            self.logger.debug(f"  âœ… å‰¯è©žå‡¦ç†å®Œäº†: {len(slots)} slots detected")
            return {'slots': slots, 'sub_slots': sub_slots}
        else:
            return None
    
    def _classify_adverb(self, word, sentence):
        """å‰¯è©žã®æ„å‘³åˆ†é¡ž"""
        from enum import Enum
        
        class AdverbialType(Enum):
            TIME = "time"
            FREQUENCY = "frequency"
            MANNER = "manner"
            LOCATION = "location"
            DEGREE = "degree"
        
        word_lower = word.text.lower()
        
        # æ™‚é–“å‰¯è©ž
        time_adverbs = {
            'yesterday', 'today', 'tomorrow', 'now', 'then', 'recently', 
            'currently', 'formerly', 'previously', 'eventually', 'finally',
            'earlier', 'later', 'soon', 'immediately', 'already', 'still',
            'ago', 'before', 'after', 'during', 'meanwhile'
        }
        
        # é »åº¦å‰¯è©ž
        frequency_adverbs = {
            'always', 'usually', 'often', 'sometimes', 'rarely', 'never',
            'frequently', 'occasionally', 'seldom', 'constantly', 'repeatedly',
            'once', 'twice', 'again', 'daily', 'weekly', 'monthly'
        }
        
        # æ§˜æ…‹å‰¯è©ž
        manner_adverbs = {
            'carefully', 'quickly', 'slowly', 'quietly', 'loudly', 'gently',
            'suddenly', 'gradually', 'easily', 'hardly', 'clearly', 'properly',
            'correctly', 'incorrectly', 'well', 'badly', 'perfectly', 'seriously'
        }
        
        # å ´æ‰€å‰¯è©ž
        location_adverbs = {
            'here', 'there', 'everywhere', 'nowhere', 'somewhere', 'anywhere',
            'upstairs', 'downstairs', 'outside', 'inside', 'nearby', 'far',
            'home', 'abroad', 'locally', 'globally'
        }
        
        # ç¨‹åº¦å‰¯è©ž
        degree_adverbs = {
            'very', 'quite', 'rather', 'extremely', 'completely', 'totally',
            'partially', 'slightly', 'barely', 'almost', 'entirely', 'mostly',
            'too', 'enough', 'highly', 'deeply'
        }
        
        # åˆ†é¡žå®Ÿè¡Œ
        if word_lower in time_adverbs:
            return AdverbialType.TIME
        elif word_lower in frequency_adverbs:
            return AdverbialType.FREQUENCY
        elif word_lower in manner_adverbs:
            return AdverbialType.MANNER
        elif word_lower in location_adverbs:
            return AdverbialType.LOCATION
        elif word_lower in degree_adverbs:
            return AdverbialType.DEGREE
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆå“è©žãƒ™ãƒ¼ã‚¹åˆ¤å®šï¼‰
            if word.upos == 'ADV':
                return AdverbialType.MANNER  # å‰¯è©žã¯æ§˜æ…‹ã¨ã—ã¦æ‰±ã†
            else:
                return AdverbialType.LOCATION  # åè©žå¥ã¯å ´æ‰€ã¨ã—ã¦æ‰±ã†
    
    def _determine_adverb_slot(self, adv_type, position_ratio) -> str:
        """å‰¯è©žã‚¿ã‚¤ãƒ—ã¨ä½ç½®ã«åŸºã¥ãã‚¹ãƒ­ãƒƒãƒˆæ±ºå®š"""
        from enum import Enum
        
        class AdverbialType(Enum):
            TIME = "time"
            FREQUENCY = "frequency"
            MANNER = "manner"
            LOCATION = "location"
            DEGREE = "degree"
        
        # æ™‚é–“å‰¯è©žã¯å¸¸ã«M1
        if adv_type == AdverbialType.TIME:
            return 'M1'
        
        # ä½ç½®ãƒ™ãƒ¼ã‚¹åˆ¤å®š
        if position_ratio <= 0.3:  # æ–‡é ­30%
            return 'M1'
        elif position_ratio >= 0.7:  # æ–‡æœ«30% 
            return 'M3'
        else:  # æ–‡ä¸­40%
            return 'M2'
    
    def _build_adverbial_phrase(self, sentence, main_word):
        """å‰¯è©žå¥ã®æ§‹ç¯‰ï¼ˆå‰ç½®è©žå¥å¯¾å¿œï¼‰"""
        # ä¿®é£¾èªžåŽé›†
        modifiers = []
        for word in sentence.words:
            if word.head == main_word.id:
                if word.deprel in ['det', 'amod', 'case', 'compound']:
                    modifiers.append(word)
        
        if not modifiers:
            return main_word.text
        
        # ä¿®é£¾èªžã‚’IDé †ã§ã‚½ãƒ¼ãƒˆï¼ˆèªžé †ä¿æŒï¼‰
        modifiers.sort(key=lambda w: w.id)
        
        # å¥æ§‹ç¯‰: ä¿®é£¾èªž + ãƒ¡ã‚¤ãƒ³èªž
        phrase_words = modifiers + [main_word]
        phrase_words.sort(key=lambda w: w.id)
        
        return ' '.join(word.text for word in phrase_words)

    def _handle_passive_voice(self, sentence, base_result: Dict) -> Optional[Dict]:
        """
        å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆPhase 2å®Ÿè£…ï¼‰
        
        passive_voice_engine.py ã®æ©Ÿèƒ½ã‚’çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã«ç§»æ¤
        Stanza dependency parsing ã«ã‚ˆã‚‹å—å‹•æ…‹æ¤œå‡ºãƒ»åˆ†è§£
        
        Args:
            sentence: Stanzaè§£æžæ¸ˆã¿sentence object
            base_result: ãƒ™ãƒ¼ã‚¹çµæžœï¼ˆã‚³ãƒ”ãƒ¼ï¼‰
            
        Returns:
            Dict: å—å‹•æ…‹åˆ†è§£çµæžœ or None
        """
        try:
            self.logger.debug("ðŸ” å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè¡Œä¸­...")
            
            # å—å‹•æ…‹æ§‹é€ åˆ†æž
            passive_info = self._analyze_passive_structure(sentence)
            if not passive_info:
                self.logger.debug("  å—å‹•æ…‹ãªã— - ã‚¹ã‚­ãƒƒãƒ—")
                return None
                
            self.logger.debug("  âœ… å—å‹•æ…‹æ¤œå‡º")
            return self._process_passive_construction(sentence, passive_info, base_result)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _analyze_passive_structure(self, sentence) -> Optional[Dict]:
        """å—å‹•æ…‹æ§‹é€ ã®åˆ†æž"""
        passive_features = {
            'auxiliary': None,      # beå‹•è©ž
            'main_verb': None,      # éŽåŽ»åˆ†è©ž
            'subject': None,        # ä¸»èªž
            'agent': None,          # byå¥å‹•ä½œä¸»
            'agent_phrase': None,   # byå¥å…¨ä½“
            'type': None            # å—å‹•æ…‹ã®ç¨®é¡ž
        }
        
        # å…¸åž‹çš„ãªéŽåŽ»åˆ†è©žãƒªã‚¹ãƒˆ
        common_past_participles = {
            'written', 'bought', 'sold', 'made', 'taken', 'given', 'seen', 'done',
            'broken', 'stolen', 'found', 'lost', 'taught', 'caught', 'brought',
            'eaten', 'driven', 'shown', 'known', 'grown', 'thrown', 'chosen'
        }
        
        # æ§‹é€ è¦ç´ ã®æ¤œå‡º
        for word in sentence.words:
            # å—å‹•æ…‹ä¸»èªžæ¤œå‡º
            if word.deprel == 'nsubj:pass':
                passive_features['subject'] = word
            elif word.deprel == 'nsubjpass':  # æ—§ç‰ˆStanzaå¯¾å¿œ
                passive_features['subject'] = word
            elif word.deprel == 'nsubj':  # å½¢å®¹è©žå—å‹•æ…‹ã®å ´åˆ
                if not passive_features['subject']:  # ã¾ã è¦‹ã¤ã‹ã£ã¦ã„ãªã„å ´åˆã®ã¿
                    passive_features['subject'] = word
                    
            # å—å‹•æ…‹è£œåŠ©å‹•è©žæ¤œå‡º
            elif word.deprel == 'aux:pass':
                passive_features['auxiliary'] = word
            elif word.deprel == 'auxpass':  # æ—§ç‰ˆStanzaå¯¾å¿œ
                passive_features['auxiliary'] = word
            elif word.deprel == 'cop' and word.lemma == 'be':
                passive_features['auxiliary'] = word
                
            # ä¸»å‹•è©žæ¤œå‡ºï¼ˆéŽåŽ»åˆ†è©žï¼‰
            elif word.deprel == 'root':
                if word.upos == 'VERB' and word.xpos == 'VBN':  # éŽåŽ»åˆ†è©ž
                    passive_features['main_verb'] = word
                elif word.upos == 'ADJ' and word.text.lower() in common_past_participles:
                    passive_features['main_verb'] = word
                    
            # byå¥å‹•ä½œä¸»æ¤œå‡º
            elif word.deprel == 'obl:agent':
                passive_features['agent'] = word
                passive_features['agent_phrase'] = self._build_agent_phrase(sentence, word)
            elif word.deprel == 'agent':  # æ—§ç‰ˆå¯¾å¿œ
                passive_features['agent'] = word
                passive_features['agent_phrase'] = self._build_agent_phrase(sentence, word)
        
        # å—å‹•æ…‹åˆ¤å®š
        if (passive_features['auxiliary'] and 
            passive_features['main_verb'] and 
            passive_features['subject']):
            
            passive_features['type'] = 'agent_passive' if passive_features['agent'] else 'simple_passive'
            
            self.logger.debug(f"  ä¸»èªž: {passive_features['subject'].text}")
            self.logger.debug(f"  è£œåŠ©å‹•è©ž: {passive_features['auxiliary'].text}")
            self.logger.debug(f"  ä¸»å‹•è©ž: {passive_features['main_verb'].text}")
            self.logger.debug(f"  å‹•ä½œä¸»: {passive_features['agent'].text if passive_features['agent'] else 'ãªã—'}")
            self.logger.debug(f"  ç¨®é¡ž: {passive_features['type']}")
            
            return passive_features
        
        return None
    
    def _process_passive_construction(self, sentence, passive_info: Dict, base_result: Dict) -> Dict:
        """å—å‹•æ…‹æ§‹æ–‡ã®å‡¦ç†"""
        result = base_result.copy()
        
        auxiliary = passive_info['auxiliary']
        main_verb = passive_info['main_verb']
        subject = passive_info['subject']
        agent_phrase = passive_info['agent_phrase']
        passive_type = passive_info['type']
        
        self.logger.debug(f"  å—å‹•æ…‹å‡¦ç†: {passive_type}")
        
        # ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ
        rephrase_slots = self._generate_passive_voice_slots(
            passive_type, subject, auxiliary, main_verb, agent_phrase, passive_info['agent'], sentence
        )
        
        # çµæžœãƒžãƒ¼ã‚¸
        if 'slots' not in result:
            result['slots'] = {}
        if 'sub_slots' not in result:
            result['sub_slots'] = {}
        
        result['slots'].update(rephrase_slots.get('slots', {}))
        result['sub_slots'].update(rephrase_slots.get('sub_slots', {}))
        
        # æ–‡æ³•æƒ…å ±è¨˜éŒ²
        result['grammar_info'] = {
            'patterns': ['passive_voice'],
            'passive_type': passive_type,
            'subject': subject.text,
            'auxiliary': auxiliary.text,
            'main_verb': main_verb.text,
            'agent': passive_info['agent'].text if passive_info['agent'] else None
        }
        
        self.logger.debug(f"  âœ… å—å‹•æ…‹å‡¦ç†å®Œäº†: {len(rephrase_slots.get('slots', {}))} main slots, {len(rephrase_slots.get('sub_slots', {}))} sub slots")
        return result
    
    def _generate_passive_voice_slots(self, passive_type: str, subject, auxiliary, main_verb, 
                                     agent_phrase: str, agent, sentence) -> Dict:
        """å—å‹•æ…‹ã‚¿ã‚¤ãƒ—åˆ¥ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ"""
        
        slots = {}
        sub_slots = {}
        
        # åŸºæœ¬ã‚¹ãƒ­ãƒƒãƒˆï¼ˆå…±é€šï¼‰
        slots['S'] = self._build_subject_phrase(sentence, subject)
        slots['Aux'] = auxiliary.text
        slots['V'] = main_verb.text
        
        # byå¥ä»˜ãå—å‹•æ…‹ã®å ´åˆ
        if passive_type == 'agent_passive' and agent_phrase:
            slots['M1'] = agent_phrase  # byå¥å…¨ä½“
            if agent:
                sub_slots['sub-m1'] = agent.text  # å‹•ä½œä¸»ã®ã¿
        
        return {'slots': slots, 'sub_slots': sub_slots}
    
    def _build_agent_phrase(self, sentence, agent_word) -> str:
        """byå¥å…¨ä½“ã®æ§‹ç¯‰"""
        if not agent_word:
            return None
        
        # byå‰ç½®è©žã‚’æŽ¢ã™
        by_preposition = None
        for word in sentence.words:
            if word.text.lower() == 'by' and word.deprel == 'case' and word.head == agent_word.id:
                by_preposition = word
                break
        
        if by_preposition:
            # by + å‹•ä½œä¸» + ä¿®é£¾èªž
            phrase_words = [by_preposition, agent_word]
            
            # å‹•ä½œä¸»ã®ä¿®é£¾èªžã‚’è¿½åŠ 
            for word in sentence.words:
                if word.head == agent_word.id and word.deprel in ['det', 'amod', 'nmod']:
                    phrase_words.append(word)
            
            # IDé †ã‚½ãƒ¼ãƒˆï¼ˆèªžé †ä¿æŒï¼‰
            phrase_words.sort(key=lambda w: w.id)
            return ' '.join(w.text for w in phrase_words)
        
        return f"by {agent_word.text}"
    
    def _build_subject_phrase(self, sentence, subject) -> str:
        """ä¸»èªžå¥ã®æ§‹ç¯‰ï¼ˆä¿®é£¾èªžå«ã‚€ï¼‰"""
        if not subject:
            return ""
            
        subject_words = [subject]
        
        # ä¸»èªžã®ä¿®é£¾èªžã‚’åŽé›†
        for word in sentence.words:
            if word.head == subject.id and word.deprel in ['det', 'amod', 'compound', 'nmod']:
                subject_words.append(word)
        
        # IDé †ã‚½ãƒ¼ãƒˆï¼ˆèªžé †ä¿æŒï¼‰
        subject_words.sort(key=lambda w: w.id)
        return ' '.join(w.text for w in subject_words)

# =============================================================================
# Phase 0 ãƒ†ã‚¹ãƒˆç”¨ åŸºæœ¬ãƒ†ã‚¹ãƒˆãƒãƒ¼ãƒã‚¹
# =============================================================================

def test_phase0_basic():
    """Phase 0 åŸºæœ¬å‹•ä½œç¢ºèªãƒ†ã‚¹ãƒˆ"""
    print("ðŸ§ª Phase 0 åŸºæœ¬ãƒ†ã‚¹ãƒˆé–‹å§‹...")
    
    try:
        # åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ
        mapper = UnifiedStanzaRephraseMapper(log_level='DEBUG')
        print("âœ… åˆæœŸåŒ–æˆåŠŸ")
        
        # åŸºæœ¬å‡¦ç†ãƒ†ã‚¹ãƒˆ
        test_sentence = "The car is red."
        result = mapper.process(test_sentence)
        
        print(f"âœ… åŸºæœ¬å‡¦ç†æˆåŠŸ: {result['sentence']}")
        print(f"ðŸ“Š å‡¦ç†æ™‚é–“: {result['meta']['processing_time']:.3f}s")
        print(f"ðŸ”§ Stanzaæƒ…å ±: {result['meta']['stanza_info']}")
        
        # çµ±è¨ˆç¢ºèª
        stats = mapper.get_stats()
        print(f"ðŸ“ˆ å‡¦ç†çµ±è¨ˆ: {stats}")
        
        print("ðŸŽ‰ Phase 0 åŸºæœ¬ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ Phase 0 ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return False

# =============================================================================
# Phase 1 ãƒ†ã‚¹ãƒˆç”¨ é–¢ä¿‚ç¯€ãƒ†ã‚¹ãƒˆãƒãƒ¼ãƒã‚¹
# =============================================================================

def test_phase2_passive_voice():
    """Phase 2 å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆ"""
    print("ðŸ§ª Phase 2 å—å‹•æ…‹ãƒ†ã‚¹ãƒˆé–‹å§‹...")
    
    try:
        # åˆæœŸåŒ–
        mapper = UnifiedStanzaRephraseMapper(log_level='DEBUG')
        
        # Phase 1 & 2 ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¿½åŠ 
        mapper.add_handler('relative_clause')
        mapper.add_handler('passive_voice')
        print("âœ… é–¢ä¿‚ç¯€ + å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¿½åŠ å®Œäº†")
        
        # é‡è¦ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
        test_cases = [
            ("The car was bought.", "å˜ç´”å—å‹•æ…‹"),
            ("The car was bought by him.", "byå¥ä»˜ãå—å‹•æ…‹"),
            ("The book which was read was interesting.", "é–¢ä¿‚ç¯€+å—å‹•æ…‹è¤‡åˆ"),
            ("The letter was written by her.", "å—å‹•æ…‹åŸºæœ¬å½¢")
        ]
        
        success_count = 0
        for i, (test_sentence, pattern_type) in enumerate(test_cases, 1):
            print(f"\nðŸ“– ãƒ†ã‚¹ãƒˆ{i}: '{test_sentence}' ({pattern_type})")
            print("-" * 60)
            
            try:
                result = mapper.process(test_sentence)
                
                print("ðŸ“Š å‡¦ç†çµæžœ:")
                print(f"  ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ­ãƒƒãƒˆ: {result.get('slots', {})}")
                print(f"  ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ: {result.get('sub_slots', {})}")
                print(f"  æ–‡æ³•æƒ…å ±: {result.get('grammar_info', {})}")
                print(f"  å‡¦ç†æ™‚é–“: {result['meta']['processing_time']:.3f}s")
                
                # å—å‹•æ…‹ãƒã‚§ãƒƒã‚¯
                slots = result.get('slots', {})
                if 'Aux' in slots and 'V' in slots:
                    print(f"\nðŸŽ¯ å—å‹•æ…‹ãƒã‚§ãƒƒã‚¯:")
                    print(f"  S: '{slots.get('S', '')}'")
                    print(f"  Aux: '{slots.get('Aux', '')}'")  
                    print(f"  V: '{slots.get('V', '')}'")
                    if 'M1' in slots:
                        print(f"  M1 (byå¥): '{slots.get('M1', '')}'")
                    
                    print("  âœ… å—å‹•æ…‹æ§‹é€ æ¤œå‡ºæˆåŠŸï¼")
                    success_count += 1
                else:
                    print("  âŒ å—å‹•æ…‹æ§‹é€ æœªæ¤œå‡º")
                    
            except Exception as e:
                print(f"âŒ ãƒ†ã‚¹ãƒˆ{i}ã‚¨ãƒ©ãƒ¼: {e}")
        
        # çµ±è¨ˆç¢ºèª
        stats = mapper.get_stats()
        print(f"\nðŸ“ˆ Phase 2 çµ±è¨ˆ:")
        print(f"  å‡¦ç†æ•°: {stats['processing_count']}")
        print(f"  å¹³å‡å‡¦ç†æ™‚é–“: {stats['average_processing_time']:.3f}s")
        print(f"  ãƒãƒ³ãƒ‰ãƒ©ãƒ¼æˆåŠŸæ•°: {stats['handler_success_count']}")
        
        print(f"\nðŸŽ‰ Phase 2 ãƒ†ã‚¹ãƒˆå®Œäº†! æˆåŠŸ: {success_count}/{len(test_cases)}")
        return success_count == len(test_cases)
        
    except Exception as e:
        print(f"âŒ Phase 2 ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return False

def test_phase1_relative_clause():
    """Phase 1 é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆ"""
    print("ðŸ§ª Phase 1 é–¢ä¿‚ç¯€ãƒ†ã‚¹ãƒˆé–‹å§‹...")
    
    try:
        # åˆæœŸåŒ–
        mapper = UnifiedStanzaRephraseMapper(log_level='DEBUG')
        
        # Phase 1 ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¿½åŠ 
        mapper.add_handler('relative_clause')
        print("âœ… é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¿½åŠ å®Œäº†")
        
        # é‡è¦ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ï¼ˆçœç•¥é–¢ä¿‚ä»£åè©žå¯¾å¿œå¼·åŒ–ï¼‰
        test_cases = [
            ("The car which we saw was red.", "ç›®çš„èªžé–¢ä¿‚ä»£åè©ž"),
            ("The man who runs fast is strong.", "ä¸»èªžé–¢ä¿‚ä»£åè©ž"), 
            ("The man whose car is red lives here.", "æ‰€æœ‰æ ¼é–¢ä¿‚ä»£åè©ž"),
            ("The place where he lives is nice.", "é–¢ä¿‚å‰¯è©žwhere"),
            ("The book I read was interesting.", "çœç•¥ç›®çš„èªžé–¢ä¿‚ä»£åè©žï¼ˆèƒ½å‹•æ…‹ï¼‰"),
            ("The book that was written is famous.", "çœç•¥ç›®çš„èªžé–¢ä¿‚ä»£åè©žï¼ˆå—å‹•æ…‹ï¼‰"),
            ("The person standing there is my friend.", "çœç•¥ä¸»èªžé–¢ä¿‚ä»£åè©žï¼ˆç¾åœ¨åˆ†è©žï¼‰")
        ]
        
        success_count = 0
        for i, (test_sentence, pattern_type) in enumerate(test_cases, 1):
            print(f"\nðŸ“– ãƒ†ã‚¹ãƒˆ{i}: '{test_sentence}' ({pattern_type})")
            print("-" * 60)
            
            try:
                result = mapper.process(test_sentence)
                
                print("ðŸ“Š å‡¦ç†çµæžœ:")
                print(f"  ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ­ãƒƒãƒˆ: {result.get('slots', {})}")
                print(f"  ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ: {result.get('sub_slots', {})}")
                print(f"  æ–‡æ³•æƒ…å ±: {result.get('grammar_info', {})}")
                print(f"  å‡¦ç†æ™‚é–“: {result['meta']['processing_time']:.3f}s")
                
                # ç¬¬1ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®ç‰¹åˆ¥ãƒã‚§ãƒƒã‚¯
                if i == 1:  # "The car which we saw was red."
                    slots = result.get('slots', {})
                    sub_slots = result.get('sub_slots', {})
                    
                    print(f"\nðŸŽ¯ é‡è¦ãƒã‚§ãƒƒã‚¯:")
                    expected_sub_o1 = "The car which we saw"
                    actual_sub_o1 = sub_slots.get('sub-o1', '')
                    print(f"  æœŸå¾… sub-o1: '{expected_sub_o1}'")
                    print(f"  å®Ÿéš› sub-o1: '{actual_sub_o1}'")
                    
                    if expected_sub_o1.lower() in actual_sub_o1.lower():
                        print("  âœ… åŸºæœ¬è¦æ±‚é”æˆï¼")
                        success_count += 1
                    else:
                        print("  âŒ åŸºæœ¬è¦æ±‚æœªé”æˆ")
                else:
                    success_count += 1
                    
            except Exception as e:
                print(f"âŒ ãƒ†ã‚¹ãƒˆ{i}ã‚¨ãƒ©ãƒ¼: {e}")
        
        # çµ±è¨ˆç¢ºèª
        stats = mapper.get_stats()
        print(f"\nðŸ“ˆ Phase 1 çµ±è¨ˆ:")
        print(f"  å‡¦ç†æ•°: {stats['processing_count']}")
        print(f"  å¹³å‡å‡¦ç†æ™‚é–“: {stats['average_processing_time']:.3f}s")
        print(f"  ãƒãƒ³ãƒ‰ãƒ©ãƒ¼æˆåŠŸæ•°: {stats['handler_success_count']}")
        
        print(f"\nðŸŽ‰ Phase 1 ãƒ†ã‚¹ãƒˆå®Œäº†! æˆåŠŸ: {success_count}/{len(test_cases)}")
        return success_count == len(test_cases)
        
    except Exception as e:
        print(f"âŒ Phase 1 ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return False

if __name__ == "__main__":
    # Phase 0 åŸºæœ¬ãƒ†ã‚¹ãƒˆ
    if test_phase0_basic():
        print("\n" + "="*60)
        # Phase 1 é–¢ä¿‚ç¯€ãƒ†ã‚¹ãƒˆ  
        if test_phase1_relative_clause():
            print("\n" + "="*60)
            # Phase 2 å—å‹•æ…‹ãƒ†ã‚¹ãƒˆ
            test_phase2_passive_voice()
        else:
            print("âŒ Phase 1å¤±æ•—ã®ãŸã‚ Phase 2ã‚’ã‚¹ã‚­ãƒƒãƒ—")
    else:
        print("âŒ Phase 0å¤±æ•—ã®ãŸã‚ Phase 1,2ã‚’ã‚¹ã‚­ãƒƒãƒ—")
