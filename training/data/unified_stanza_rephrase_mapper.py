#!/usr/bin/env python3
"""
Unified Stanza-Rephrase Mapper v1.0
===================================

çµ±åˆå‹æ–‡æ³•åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³ - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ–¹å¼
- 15å€‹åˆ¥ã‚¨ãƒ³ã‚¸ãƒ³ã®çŸ¥è­˜ã‚’çµ±åˆ
- é¸æŠå•é¡Œã‚’æ’é™¤ï¼ˆå…¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼åŒæ™‚å®Ÿè¡Œï¼‰
- Stanza dependency parsing â†’ Rephrase slot mapping
- spaCyè£œå®Œè§£æï¼ˆStanzaã®èª¤è§£æç®‡æ‰€å¯¾å¿œï¼‰

ä½œæˆæ—¥: 2025å¹´8æœˆ15æ—¥
Phase 0: åŸºç›¤æ§‹ç¯‰
"""

import stanza
from typing import Dict, List, Optional, Any, Tuple
import json
import logging
from dataclasses import dataclass
from datetime import datetime

try:
    import spacy
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False

@dataclass
class RephraseSlot:
    """Rephraseã‚¹ãƒ­ãƒƒãƒˆè¡¨ç¾"""
    slot_name: str
    content: str
    sub_slots: Dict[str, Any] = None
    confidence: float = 1.0
    source_handler: str = ""

class UnifiedStanzaRephraseMapper:
    """
    çµ±åˆå‹Stanzaâ†’Rephraseãƒãƒƒãƒ‘ãƒ¼
    
    æ ¸å¿ƒæ€æƒ³:
    - å…¨æ–‡æ³•ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒåŒæ™‚å®Ÿè¡Œï¼ˆé¸æŠå•é¡Œæ’é™¤ï¼‰
    - å˜ä¸€Stanzaè§£æçµæœã®å¤šè§’çš„åˆ†æ
    - å€‹åˆ¥ã‚¨ãƒ³ã‚¸ãƒ³ã®å®Ÿè£…çŸ¥è­˜ç¶™æ‰¿
    """
    
    def __init__(self, 
                 language='en', 
                 enable_gpu=False,
                 log_level='DEBUG',
                 use_spacy_hybrid=True):
        """
        çµ±åˆãƒãƒƒãƒ‘ãƒ¼åˆæœŸåŒ–
        
        Args:
            language: å‡¦ç†è¨€èªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 'en'ï¼‰
            enable_gpu: GPUä½¿ç”¨ãƒ•ãƒ©ã‚°
            log_level: ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«
            use_spacy_hybrid: spaCyãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æä½¿ç”¨ãƒ•ãƒ©ã‚°
        """
        self.language = language
        self.enable_gpu = enable_gpu
        self.use_spacy_hybrid = use_spacy_hybrid
        
        # ãƒ­ã‚°è¨­å®š
        self._setup_logging(log_level)
        
        # Stanzaãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
        self.nlp = None
        self._initialize_stanza_pipeline()
        
        # spaCyãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æåˆæœŸåŒ–
        self.spacy_nlp = None
        if self.use_spacy_hybrid and SPACY_AVAILABLE:
            self._initialize_spacy_pipeline()
        
        # çµ±è¨ˆæƒ…å ±
        self.processing_count = 0
        self.total_processing_time = 0.0
        self.handler_success_count = {}
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼é–“æƒ…å ±å…±æœ‰ã‚·ã‚¹ãƒ†ãƒ 
        self.handler_shared_context = {}
        
        # æ®µéšçš„ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ç®¡ç†ï¼ˆPhaseåˆ¥è¿½åŠ ï¼‰
        self.active_handlers = []
        
        # åŸºæœ¬ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®åˆæœŸåŒ–
        self._initialize_basic_handlers()
        
        self.logger.info("Unified Stanza-Rephrase Mapper v1.0 åˆæœŸåŒ–å®Œäº†")
        if self.spacy_nlp:
            self.logger.info("spaCyãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æ æœ‰åŠ¹")
        else:
            self.logger.info("spaCyãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æ ç„¡åŠ¹")
    
    def _setup_logging(self, level: str):
        """ãƒ­ã‚°è¨­å®š"""
        self.logger = logging.getLogger(f"{__name__}.UnifiedMapper")
        self.logger.setLevel(getattr(logging, level.upper()))
        
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
    
    def _initialize_stanza_pipeline(self):
        """Stanza NLPãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–"""
        try:
            self.logger.info("Stanza pipeline åˆæœŸåŒ–ä¸­...")
            
            # åŸºæœ¬çš„ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹æˆ
            processors = 'tokenize,pos,lemma,depparse'
            
            self.nlp = stanza.Pipeline(
                lang=self.language,
                processors=processors,
                download_method=None,  # äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã‚’æƒ³å®š
                use_gpu=self.enable_gpu,
                verbose=False
            )
            
            self.logger.info("Stanza pipeline åˆæœŸåŒ–æˆåŠŸ")
            
            # å‹•ä½œç¢ºèª
            test_result = self.nlp("Hello world.")
            self.logger.info(f"Pipeline å‹•ä½œç¢ºèª: {len(test_result.sentences)} sentences processed")
            
        except Exception as e:
            self.logger.error(f"Stanza pipeline åˆæœŸåŒ–å¤±æ•—: {e}")
            self.logger.error("è§£æ±ºæ–¹æ³•: python -c 'import stanza; stanza.download(\"en\")'")
            raise RuntimeError(f"Stanza initialization failed: {e}")
    
    def _initialize_spacy_pipeline(self):
        """spaCy NLPãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æç”¨ï¼‰"""
        try:
            self.logger.info("spaCy pipeline åˆæœŸåŒ–ä¸­...")
            
            # è‹±èªãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
            self.spacy_nlp = spacy.load('en_core_web_sm')
            
            self.logger.info("spaCy pipeline åˆæœŸåŒ–æˆåŠŸ")
            
        except Exception as e:
            self.logger.warning(f"spaCy pipeline åˆæœŸåŒ–å¤±æ•—: {e}")
            self.logger.warning("  pip install spacy; python -m spacy download en_core_web_sm ã§è¨­å®šã—ã¦ãã ã•ã„")
            self.spacy_nlp = None
            self.use_spacy_hybrid = False
    
    def _initialize_basic_handlers(self):
        """åŸºæœ¬ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®åˆæœŸåŒ–"""
        basic_handlers = [
            'basic_five_pattern',     # åŸºæœ¬5æ–‡å‹
            'relative_clause',        # é–¢ä¿‚ç¯€
            'passive_voice',          # å—å‹•æ…‹  
            'participle_construction', # åˆ†è©æ§‹æ–‡ï¼ˆå‰¯è©å‡¦ç†ã‚ˆã‚Šå…ˆï¼‰
            'adverbial_modifier',     # å‰¯è©å¥ï¼ˆå‰ç½®è©å¥å«ã‚€ï¼‰
            'auxiliary_complex',      # åŠ©å‹•è©
            'conjunction',            # æ¥ç¶šè©ï¼ˆ"as if"ç­‰ï¼‰
        ]
        
        for handler in basic_handlers:
            self.add_handler(handler)
        
        self.logger.info(f"åŸºæœ¬ãƒãƒ³ãƒ‰ãƒ©ãƒ¼åˆæœŸåŒ–å®Œäº†: {len(self.active_handlers)}å€‹")
    
    def process(self, sentence: str) -> Dict[str, Any]:
        """
        çµ±åˆå‡¦ç†ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
        
        Args:
            sentence: å‡¦ç†å¯¾è±¡æ–‡
            
        Returns:
            Dict: Rephraseå½¢å¼å‡¦ç†çµæœ
        """
        start_time = datetime.now()
        self.processing_count += 1
        
        try:
            self.logger.debug(f"Processing: {sentence}")
            
            # Stanzaè§£æ
            doc = self._analyze_with_stanza(sentence)
            if not doc or not doc.sentences:
                self.logger.warning(f"Stanzaè§£æå¤±æ•—: {sentence}")
                return self._create_empty_result(sentence)
            
            # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æï¼ˆspaCyè£œå®Œï¼‰
            if self.use_spacy_hybrid and self.spacy_nlp:
                doc = self._apply_spacy_hybrid_corrections(sentence, doc)
            
            # äººé–“æ–‡æ³•èªè­˜ã«ã‚ˆã‚‹å‰å‡¦ç†ï¼ˆstanzaèª¤åˆ¤å®šä¿®æ­£ï¼‰
            doc = self._apply_human_grammar_patterns(sentence, doc)
            
            # Phase 2: çµ±åˆå‡¦ç†ï¼ˆå…¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼åŒæ™‚å®Ÿè¡Œï¼‰
            result = self._unified_mapping(sentence, doc)
            
            # Phase 3: å¾Œå‡¦ç†ãƒ»æ¤œè¨¼
            result = self._post_process_result(result, sentence)
            
            # å‡¦ç†æ™‚é–“è¨˜éŒ²
            processing_time = (datetime.now() - start_time).total_seconds()
            self.total_processing_time += processing_time
            
            result['meta'] = {
                'processing_time': processing_time,
                'sentence_id': self.processing_count,
                'active_handlers': len(self.active_handlers),
                'stanza_info': {
                    'sentences': len(doc.sentences),
                    'tokens': len(doc.sentences[0].words) if doc.sentences else 0
                },
                'slot_positions': result.get('slot_positions', {})  # ãƒ‡ãƒãƒƒã‚°ç”¨
            }
            
            self.logger.info(f"Processingå®Œäº† ({processing_time:.3f}s): {len(result.get('slots', {}))} slots detected")
            return result
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            self.logger.error(f"Processing error: {e}")
            
            return {
                'sentence': sentence,
                'slots': {},
                'error': str(e),
                'meta': {
                    'processing_time': processing_time,
                    'sentence_id': self.processing_count,
                    'error_occurred': True
                }
            }
    
    def _analyze_with_stanza(self, sentence: str):
        """Stanzaè§£æå®Ÿè¡Œ"""
        try:
            doc = self.nlp(sentence)
            return doc
        except Exception as e:
            self.logger.error(f"Stanza analysis failed: {e}")
            return None
    
    def _apply_spacy_hybrid_corrections(self, sentence: str, stanza_doc):
        """
        spaCyãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æè£œå®Œ
        
        Stanzaã®èª¤è§£æã‚’æ¤œå‡ºã—ã¦spaCyã§è£œå®Œä¿®æ­£
        ç‰¹ã«whoseæ§‹æ–‡ã§ã®å‹•è©POSèª¤è§£æã‚’ä¿®æ­£
        """
        try:
            # spaCyè§£æå®Ÿè¡Œ
            spacy_doc = self.spacy_nlp(sentence)
            
            # ä¿®æ­£ãŒå¿…è¦ãªç®‡æ‰€ã‚’æ¤œå‡º
            corrections = self._detect_analysis_discrepancies(stanza_doc, spacy_doc, sentence)
            
            if corrections:
                self.logger.debug(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æè£œæ­£: {len(corrections)} ç®‡æ‰€ä¿®æ­£")
                
                # Stanzaçµæœã«è£œæ­£ã‚’é©ç”¨
                corrected_doc = self._apply_corrections_to_stanza(stanza_doc, corrections)
                return corrected_doc
            
            return stanza_doc
            
        except Exception as e:
            self.logger.warning(f"spaCyãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return stanza_doc  # è£œæ­£å¤±æ•—æ™‚ã¯å…ƒã®Stanzaçµæœã‚’è¿”ã™
    
    def _apply_human_grammar_patterns(self, sentence: str, doc):
        """
        äººé–“ã®æ–‡æ³•èªè­˜ãƒ—ãƒ­ã‚»ã‚¹ã«ã‚ˆã‚‹stanzaèª¤åˆ¤å®šä¿®æ­£
        
        äººé–“ãŒç„¡æ„è­˜ã«è¡Œã†æ–‡æ³•ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã‚’ã‚³ãƒ¼ãƒ‰åŒ–ã—ã€
        stanza/spaCyã®çµ±è¨ˆçš„åˆ¤å®šã‚’æ–‡æ³•çš„åˆ¤å®šã§ä¿®æ­£ã™ã‚‹
        """
        try:
            self.logger.debug("ğŸ§  äººé–“æ–‡æ³•èªè­˜é–‹å§‹")
            
            # beå‹•è© + éå»åˆ†è© = å—å‹•æ…‹ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
            corrected_doc = self._correct_passive_voice_pattern(doc, sentence)
            
            # whoseæ§‹æ–‡ + å‹•è©/åè©åŒå½¢èªãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
            corrected_doc = self._correct_whose_ambiguous_verb_pattern(corrected_doc, sentence)
            
            self.logger.debug("ğŸ§  äººé–“æ–‡æ³•èªè­˜å®Œäº†")
            return corrected_doc
            
        except Exception as e:
            self.logger.warning(f"äººé–“æ–‡æ³•èªè­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return doc  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®docã‚’è¿”ã™
    
    def _correct_passive_voice_pattern(self, doc, sentence):
        """
        beå‹•è© + éå»åˆ†è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å—å‹•æ…‹åˆ¤å®šãƒ»ä¿®æ­£
        
        äººé–“ã®èªè­˜: "was unexpected" â†’ be + pp â†’ å—å‹•æ…‹
        stanzaèª¤åˆ¤å®š: unexpected(root) + was(cop) â†’ è£œèªæ§‹æ–‡
        """
        if not doc.sentences:
            return doc
            
        sent = doc.sentences[0]
        words = sent.words
        
        # å„å˜èªã«æ–‡è„ˆæƒ…å ±ã‚’è¿½åŠ ï¼ˆä½ç½®ãƒ™ãƒ¼ã‚¹åˆ¤å®šç”¨ï¼‰
        for word in words:
            word._sentence_words = words
        
        # æ§‹é€ çš„ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜: beå‹•è© + éå»åˆ†è©
        passive_pattern = self._detect_passive_voice_structural_pattern(words)
        
        if passive_pattern['found']:
            be_verb = passive_pattern['be_verb']
            past_participle = passive_pattern['past_participle']
            
            # stanzaãŒéå»åˆ†è©ã‚’rootã¨ã—ã¦èª¤åˆ¤å®šã—ã¦ã„ã‚‹å ´åˆ
            if past_participle.deprel == 'root' and be_verb.deprel == 'cop':
                self.logger.info(
                    f"äººé–“æ–‡æ³•ä¿®æ­£: '{be_verb.text} {past_participle.text}' "
                    f"â†’ å—å‹•æ…‹ (stanza: {past_participle.text}=root, {be_verb.text}=cop)"
                )
                
                # ä¿®æ­£æƒ…å ±ã‚’è¨˜éŒ²
                if not hasattr(doc, '_human_grammar_corrections'):
                    doc._human_grammar_corrections = []
                
                doc._human_grammar_corrections.append({
                    'type': 'passive_voice',
                    'be_verb': be_verb,
                    'past_participle': past_participle,
                    'correction': f"Convert '{be_verb.text} {past_participle.text}' to passive voice",
                    'confidence': passive_pattern['confidence']
                })
        
        return doc
    
    def _detect_passive_voice_structural_pattern(self, words):
        """æ§‹é€ çš„å—å‹•æ…‹ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º"""
        result = {'found': False, 'be_verb': None, 'past_participle': None, 'confidence': 0.0}
        
        for i in range(len(words) - 1):
            current = words[i]
            next_word = words[i + 1]
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: beå‹•è© + ç›´å¾Œã®èª
            if self._is_be_verb(current):
                confidence = 0.0
                
                # ç›´å¾ŒãŒæ˜ç¢ºãªéå»åˆ†è©
                if self._is_past_participle(next_word):
                    confidence = 0.9
                    
                    # é«˜ä¿¡é ¼åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³
                    if next_word.xpos == 'VBN':
                        confidence = 0.95
                    elif next_word.upos == 'ADJ' and self._has_past_participle_morphology(next_word):
                        confidence = 0.8
                    
                    if confidence > result['confidence']:
                        result.update({
                            'found': True,
                            'be_verb': current,
                            'past_participle': next_word,
                            'confidence': confidence
                        })
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: beå‹•è© + å‰¯è© + éå»åˆ†è©ï¼ˆå°†æ¥ã®æ‹¡å¼µç”¨ï¼‰
            if (i < len(words) - 2 and 
                self._is_be_verb(current) and 
                words[i + 1].upos == 'ADV' and
                self._is_past_participle(words[i + 2])):
                
                confidence = 0.85
                if confidence > result['confidence']:
                    result.update({
                        'found': True,
                        'be_verb': current,
                        'past_participle': words[i + 2],
                        'confidence': confidence
                    })
        
        return result
    
    def _is_be_verb(self, word):
        """beå‹•è©åˆ¤å®šï¼ˆæ±ç”¨çš„ãƒ»lemmaãƒ™ãƒ¼ã‚¹ï¼‰"""
        return (word.upos == 'AUX' and word.lemma.lower() == 'be')
    
    def _is_past_participle(self, word):
        """éå»åˆ†è©åˆ¤å®šï¼ˆæ±ç”¨çš„ãƒ»å½¢æ…‹è«–çš„åˆ†æé‡è¦–ï¼‰"""
        # 1. stanzaã®å½¢æ…‹è«–çš„åˆ¤å®šã‚’æœ€å„ªå…ˆ
        if word.xpos == 'VBN':  # Past participle
            return True
            
        # 2. beå‹•è©ç›´å¾Œã®å½¢å®¹è©çš„èªã®æ–‡è„ˆçš„åˆ¤å®š
        if word.upos == 'ADJ':
            return self._contextual_past_participle_check(word)
        
        return False
    
    def _contextual_past_participle_check(self, word):
        """æ–‡è„ˆçš„éå»åˆ†è©åˆ¤å®šï¼ˆbeå‹•è©ç›´å¾Œã®å½¢å®¹è©ï¼‰"""
        # beå‹•è©ç›´å¾Œã§å½¢å®¹è©ã‚¿ã‚° â†’ å—å‹•æ…‹ã®å¯èƒ½æ€§
        if self._follows_be_verb_directly(word):
            # å½¢æ…‹è«–çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
            return self._has_past_participle_morphology(word)
        return False
    
    def _follows_be_verb_directly(self, word):
        """ç›´å‰ã«beå‹•è©ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        # word.headã§beå‹•è©ã‚’ãƒã‚§ãƒƒã‚¯ã€ã¾ãŸã¯position-based check
        if hasattr(word, '_sentence_words'):
            words = word._sentence_words
            word_pos = next((i for i, w in enumerate(words) if w.id == word.id), -1)
            if word_pos > 0:
                prev_word = words[word_pos - 1]
                return self._is_be_verb(prev_word)
        return False
    
    def _has_past_participle_morphology(self, word):
        """å½¢æ…‹è«–çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯ï¼ˆèªå°¾åˆ†æï¼‰"""
        text = word.text.lower()
        
        # è¦å‰‡å‹•è©ã®-edèªå°¾ï¼ˆæœ€ä½4æ–‡å­—ä»¥ä¸Šï¼‰
        if text.endswith('ed') and len(text) > 3:
            # ãŸã ã—ç´”ç²‹ãªå½¢å®¹è©ï¼ˆkindred, sacredç­‰ï¼‰ã‚’é™¤å¤–
            if not self._is_pure_adjective_ending(text):
                return True
        
        # -enèªå°¾ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆbroken, chosenç­‰ï¼‰
        if text.endswith('en') and len(text) > 3:
            # listen, kittenç­‰ã®åè©ãƒ»å‹•è©ã‚’é™¤å¤–
            if not text.endswith(('tten', 'sten', 'chen', 'len')):
                return True
        
        # ç‰¹å¾´çš„ãªéå»åˆ†è©èªå°¾
        past_participle_endings = ['ated', 'ized', 'ified', 'ected', 'ested']
        if any(text.endswith(ending) for ending in past_participle_endings):
            return True
        
        return False
    
    def _is_pure_adjective_ending(self, text):
        """ç´”ç²‹ãªå½¢å®¹è©èªå°¾ï¼ˆéå»åˆ†è©ã§ã¯ãªã„ï¼‰"""
        pure_adjective_patterns = ['red', 'ded', 'eed', 'ted']
        return any(text.endswith(pattern) for pattern in pure_adjective_patterns)
    
    def _correct_whose_ambiguous_verb_pattern(self, doc, sentence: str):
        """
        whoseæ§‹æ–‡ã§ã®å‹•è©/åè©åŒå½¢èªã®äººé–“æ–‡æ³•çš„åˆ¤å®š
        
        äººé–“ã®èªè­˜: "whose car is red lives here" 
        â†’ whose [åè©] [beå‹•è©] [å½¢å®¹è©] [å‹•è©] [å ´æ‰€] 
        â†’ [å‹•è©]ã¯ç¢ºå®Ÿã«å‹•è©ã¨ã—ã¦æ‰±ã†
        
        stanzaèª¤åˆ¤å®š: lives(NOUN, acl:relcl) â†’ åè©ã¨ã—ã¦é–¢ä¿‚ç¯€ä¿®é£¾
        äººé–“ä¿®æ­£: lives(VERB, root) â†’ å‹•è©ã¨ã—ã¦ãƒ¡ã‚¤ãƒ³å‹•è©
        """
        if not doc.sentences or 'whose' not in sentence.lower():
            return doc
        
        sent = doc.sentences[0]
        words = sent.words
        
        # whoseæ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜
        whose_pattern = self._detect_whose_ambiguous_verb_pattern(words, sentence)
        
        if whose_pattern['found']:
            ambiguous_word = whose_pattern['ambiguous_verb']
            self.logger.debug(f"ğŸ§  whoseæ§‹æ–‡å‹•è©ä¿®æ­£: {ambiguous_word.text} NOUNâ†’VERB (äººé–“æ–‡æ³•èªè­˜)")
            
            # ä¿®æ­£æƒ…å ±ã‚’è¨˜éŒ²ï¼ˆstanzaãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¯ä¸å¤‰ã®ãŸã‚ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§ç®¡ç†ï¼‰
            if not hasattr(doc, 'human_grammar_corrections'):
                doc.human_grammar_corrections = {}
            
            doc.human_grammar_corrections[ambiguous_word.id] = {
                'word_text': ambiguous_word.text,
                'original_upos': ambiguous_word.upos,
                'corrected_upos': 'VERB',
                'original_deprel': ambiguous_word.deprel,
                'corrected_deprel': 'root',
                'correction_type': 'whose_ambiguous_verb',
                'confidence': 0.95
            }
        
        return doc
    
    def _detect_whose_ambiguous_verb_pattern(self, words, sentence: str):
        """whoseæ§‹æ–‡ã§ã®å‹•è©/åè©åŒå½¢èªãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º"""
        result = {'found': False, 'ambiguous_verb': None, 'confidence': 0.0}
        
        # å‹•è©/åè©åŒå½¢èªãƒªã‚¹ãƒˆ
        ambiguous_verbs = ['lives', 'works', 'runs', 'goes', 'comes', 'stays', 'plays', 'looks']
        
        # whoseæ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³: whose + åè© + (ä¿®é£¾èª) + åŒå½¢èª + å ´æ‰€/æ™‚é–“è¡¨ç¾
        import re
        
        for verb_text in ambiguous_verbs:
            if verb_text in sentence.lower():
                # ãƒ‘ã‚¿ãƒ¼ãƒ³1: whose [åè©] is [å½¢å®¹è©] [å‹•è©] (here|there|in...)
                pattern1 = rf'whose\s+\w+\s+is\s+\w+\s+{verb_text}\s+(here|there|in\s+\w+)'
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³2: whose [åè©] [ä¿®é£¾èª]* [å‹•è©] (å ´æ‰€è¡¨ç¾)
                pattern2 = rf'whose\s+\w+.*?\s+{verb_text}\s+(here|there|in|at|on)\s+\w+'
                
                if re.search(pattern1, sentence.lower()) or re.search(pattern2, sentence.lower()):
                    # è©²å½“ã™ã‚‹èªã‚’æ¢ã™
                    for word in words:
                        if (word.text.lower() == verb_text and 
                            word.upos == 'NOUN' and 
                            word.deprel == 'acl:relcl'):
                            
                            result.update({
                                'found': True,
                                'ambiguous_verb': word,
                                'confidence': 0.95
                            })
                            break
        
        return result

    def _detect_analysis_discrepancies(self, stanza_doc, spacy_doc, sentence: str) -> List[Dict]:
        """
        Stanza-spaCyè§£æçµæœã®ç›¸é•ç‚¹ã‚’æ¤œå‡º
        
        ç‰¹ã«é‡è¦ãªä¿®æ­£ç®‡æ‰€:
        1. whoseæ§‹æ–‡ã§ã®å‹•è©POSèª¤è§£æ (NOUN â†’ VERB)
        2. é–¢ä¿‚ç¯€å‹•è©ã®èª¤åˆ†é¡
        """
        corrections = []
        
        # whoseæ§‹æ–‡ç‰¹åˆ¥å‡¦ç†
        if 'whose' in sentence.lower():
            corrections.extend(self._detect_whose_verb_misanalysis(stanza_doc, spacy_doc, sentence))
        
        return corrections
    
    def _detect_whose_verb_misanalysis(self, stanza_doc, spacy_doc, sentence: str) -> List[Dict]:
        """whoseæ§‹æ–‡ã§ã®å‹•è©POSèª¤è§£æã‚’æ¤œå‡º"""
        corrections = []
        
        stanza_words = {w.text.lower(): w for w in stanza_doc.sentences[0].words}
        spacy_tokens = {t.text.lower(): t for t in spacy_doc}
        
        # 'lives', 'works', 'runs'ç­‰ã®å‹•è©ãŒåè©ã¨ã—ã¦èª¤è§£æã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        potential_verbs = ['lives', 'works', 'runs', 'goes', 'comes', 'stays']
        
        for verb_text in potential_verbs:
            if verb_text in stanza_words and verb_text in spacy_tokens:
                stanza_word = stanza_words[verb_text]
                spacy_token = spacy_tokens[verb_text]
                
                # Stanza: NOUN, spaCyè§£æã§ã‚‚NOUNã ãŒã€æ–‡è„ˆçš„ã«å‹•è©ã¨åˆ¤æ–­ã§ãã‚‹å ´åˆ
                if (stanza_word.upos == 'NOUN' and 
                    stanza_word.deprel == 'acl:relcl' and
                    self._is_contextually_verb(sentence, verb_text)):
                    
                    corrections.append({
                        'word_id': stanza_word.id,
                        'word_text': stanza_word.text,
                        'original_upos': stanza_word.upos,
                        'corrected_upos': 'VERB',
                        'correction_type': 'whose_verb_fix',
                        'confidence': 0.9
                    })
                    self.logger.debug(f"whoseæ§‹æ–‡å‹•è©ä¿®æ­£æ¤œå‡º: {verb_text} NOUNâ†’VERB")
        
        return corrections
    
    def _is_contextually_verb(self, sentence: str, word: str) -> bool:
        """æ–‡è„ˆçš„ã«å‹•è©ã¨åˆ¤æ–­ã§ãã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        # æ‹¡å¼µã•ã‚ŒãŸãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹åˆ¤å®š
        import re
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: whose + [noun] + is + [adj] + [word] + å ´æ‰€è¡¨ç¾
        whose_pattern1 = rf'whose\s+\w+\s+is\s+\w+\s+{word}\s+(here|there|in\s+\w+)'
        if re.search(whose_pattern1, sentence.lower()):
            return True
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: whose + [noun] + beå‹•è© + [å½¢å®¹è©] + [word] + å‰ç½®è©å¥ï¼ˆã‚ˆã‚Šä¸€èˆ¬çš„ï¼‰
        whose_pattern2 = rf'whose\s+\w+\s+(is|are|was|were)\s+\w+\s+{word}\s+(in|at|on|with|for)\s+\w+'
        if re.search(whose_pattern2, sentence.lower()):
            return True
            
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: whoseæ§‹æ–‡ã§èªé †çš„ã«å‹•è©ä½ç½®ã«ã‚ã‚‹å ´åˆ
        # whose + [åè©] + [ä¿®é£¾èª] + [word] + [å ´æ‰€/æ™‚é–“è¡¨ç¾]
        whose_pattern3 = rf'whose\s+\w+.*?\s+{word}\s+(in|at|on|during|after|before)\s+\w+'
        if re.search(whose_pattern3, sentence.lower()):
            return True
            
        return False
    
    def _apply_corrections_to_stanza(self, stanza_doc, corrections):
        """Stanzaè§£æçµæœã«è£œæ­£ã‚’é©ç”¨"""
        # æ³¨æ„: Stanzaã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¯èª­ã¿å–ã‚Šå°‚ç”¨ã®ãŸã‚ã€ç›´æ¥ä¿®æ­£ã¯ã§ããªã„
        # ã“ã“ã§ã¯ä¿®æ­£æƒ…å ±ã‚’è¨˜éŒ²ã—ã¦ã€å¾Œç¶šå‡¦ç†ã§åˆ©ç”¨ã™ã‚‹
        
        if not hasattr(stanza_doc, 'hybrid_corrections'):
            stanza_doc.hybrid_corrections = {}
        
        for correction in corrections:
            word_id = correction['word_id']
            stanza_doc.hybrid_corrections[word_id] = correction
            
        return stanza_doc
    
    def _unified_mapping(self, sentence: str, doc) -> Dict[str, Any]:
        """
        çµ±åˆãƒãƒƒãƒ”ãƒ³ã‚°å‡¦ç†
        
        å…¨ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒåŒæ™‚å®Ÿè¡Œ
        å„ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã¯ç‹¬ç«‹ã—ã¦Stanzaè§£æçµæœã‚’å‡¦ç†
        """
        result = {
            'sentence': sentence,
            'slots': {},
            'sub_slots': {},
            'grammar_info': {
                'detected_patterns': [],
                'handler_contributions': {},
                'control_flags': {}  # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼åˆ¶å¾¡ãƒ•ãƒ©ã‚°
            }
        }
        
        # ãƒ¡ã‚¤ãƒ³æ–‡ï¼ˆæœ€åˆã®sentenceï¼‰ã‚’å¯¾è±¡ã¨ã™ã‚‹
        main_sentence = doc.sentences[0] if doc.sentences else None
        if not main_sentence:
            return result
        
        # äººé–“æ–‡æ³•èªè­˜ã®çµæœã‚’å‡¦ç†
        if hasattr(doc, '_human_grammar_corrections'):
            self._apply_human_grammar_corrections(doc._human_grammar_corrections, result)
        
        self.logger.debug(f"Unified mappingé–‹å§‹: {len(self.active_handlers)} handlers active")
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼é–“å…±æœ‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆæœŸåŒ–
        self.handler_shared_context = {
            'predefined_slots': {},        # äº‹å‰ç¢ºå®šã‚¹ãƒ­ãƒƒãƒˆ
            'remaining_elements': {},      # æ®‹ã‚Šè¦ç´ æƒ…å ±
            'handler_metadata': {}         # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼åˆ¥ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        }
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè¡Œé †åºã®åˆ¶å¾¡ï¼ˆåˆ†è©æ§‹æ–‡ã‚’æœ€å„ªå…ˆï¼‰
        ordered_handlers = self._get_ordered_handlers()
        
        # å…¨ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®åŒæ™‚å®Ÿè¡Œï¼ˆé †åºåˆ¶å¾¡ä»˜ãï¼‰
        for handler_name in ordered_handlers:
            try:
                # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼åˆ¶å¾¡ãƒ•ãƒ©ã‚°ã‚’ãƒã‚§ãƒƒã‚¯
                control_flags = result.get('grammar_info', {}).get('control_flags', {})
                if self._should_skip_handler(handler_name, control_flags):
                    self.logger.debug(f"ğŸš« Handler ã‚¹ã‚­ãƒƒãƒ—: {handler_name} (åˆ¶å¾¡ãƒ•ãƒ©ã‚°)")
                    continue
                
                self.logger.debug(f"Handlerå®Ÿè¡Œ: {handler_name}")
                handler_method = getattr(self, f'_handle_{handler_name}')
                
                # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã«å…±æœ‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¸¡ã™
                handler_result = handler_method(main_sentence, result, self.handler_shared_context)
                
                # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼çµæœã‚’ãƒãƒ¼ã‚¸
                if handler_result:
                    result = self._merge_handler_results(result, handler_result, handler_name)
                    
                    # æˆåŠŸã‚«ã‚¦ãƒ³ãƒˆ
                    self.handler_success_count[handler_name] = \
                        self.handler_success_count.get(handler_name, 0) + 1
                        
            except Exception as e:
                self.logger.warning(f"Handler error ({handler_name}): {e}")
                continue
        
        return result
    
    def _should_skip_handler(self, handler_name: str, control_flags: Dict) -> bool:
        """ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã¹ãã‹ãƒã‚§ãƒƒã‚¯"""
        # åˆ†è©æ§‹æ–‡ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã€é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—
        if handler_name == 'relative_clause' and control_flags.get('participle_detected', False):
            return True
        
        return False
    
    def _get_ordered_handlers(self) -> List[str]:
        """ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®å®Ÿè¡Œé †åºã‚’åˆ¶å¾¡ï¼ˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼é–“æƒ…å ±å…±æœ‰å¯¾å¿œï¼‰"""
        
        # å„ªå…ˆé †ä½ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼é–“æƒ…å ±å…±æœ‰ã®ãŸã‚ã€é–¢ä¿‚ç¯€ã‚’5æ–‡å‹ã‚ˆã‚Šå‰ã«å®Ÿè¡Œï¼‰
        priority_order = [
            'participle_construction',  # æœ€å„ªå…ˆï¼šåˆ†è©æ§‹æ–‡ãŒåˆ¶å¾¡ãƒ•ãƒ©ã‚°ã‚’è¨­å®š
            'relative_clause',          # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆå æœ‰æƒ…å ±ã‚’æä¾›
            'basic_five_pattern',       # å æœ‰æ¸ˆã¿ã‚¹ãƒ­ãƒƒãƒˆæƒ…å ±ã‚’å—ã‘å–ã‚‹
            'passive_voice',
            'adverbial_modifier',
            'auxiliary_complex',
            'conjunction'
        ]
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å„ªå…ˆé †ä½é †ã«ä¸¦ã¹æ›¿ãˆ
        ordered = []
        
        # 1. å„ªå…ˆé †ä½ãƒ†ãƒ¼ãƒ–ãƒ«ã«å¾“ã£ã¦é †åºä»˜ã‘
        for handler in priority_order:
            if handler in self.active_handlers:
                ordered.append(handler)
        
        # 2. ãƒ†ãƒ¼ãƒ–ãƒ«ã«ãªã„æ–°ã—ã„ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã¯æœ€å¾Œã«è¿½åŠ 
        for handler in self.active_handlers:
            if handler not in ordered:
                ordered.append(handler)
        
        self.logger.debug(f"ğŸ“‹ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè¡Œé †åº: {ordered}")
        return ordered
    
    def _apply_human_grammar_corrections(self, corrections, result):
        """
        äººé–“æ–‡æ³•èªè­˜ã«ã‚ˆã‚‹ä¿®æ­£ã‚’resultã«é©ç”¨
        
        Args:
            corrections: äººé–“æ–‡æ³•èªè­˜ã«ã‚ˆã‚‹ä¿®æ­£ãƒªã‚¹ãƒˆ
            result: å‡¦ç†çµæœè¾æ›¸
        """
        for correction in corrections:
            if correction['type'] == 'passive_voice':
                # å—å‹•æ…‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä¿®æ­£
                be_verb = correction['be_verb']
                past_participle = correction['past_participle']
                
                # ã‚¹ãƒ­ãƒƒãƒˆã‚’ç›´æ¥è¨­å®šï¼ˆstanzaèª¤åˆ¤å®šã‚’ä¸Šæ›¸ãï¼‰
                result['slots']['V'] = past_participle.text
                result['slots']['Aux'] = be_verb.text
                
                # æ–‡æ³•æƒ…å ±ã¨ã—ã¦è¨˜éŒ²
                result['grammar_info']['detected_patterns'].append('human_corrected_passive_voice')
                result['grammar_info']['human_corrections'] = result['grammar_info'].get('human_corrections', [])
                result['grammar_info']['human_corrections'].append({
                    'type': 'passive_voice',
                    'original_stanza': f"{past_participle.text}(root), {be_verb.text}(cop)",
                    'corrected_to': f"{past_participle.text}(V), {be_verb.text}(Aux)",
                    'reason': 'Human grammar pattern: be + past_participle = passive_voice'
                })
                
                self.logger.info(
                    f"äººé–“æ–‡æ³•ä¿®æ­£é©ç”¨: V='{past_participle.text}', Aux='{be_verb.text}' "
                    f"(stanzaèª¤åˆ¤å®šä¿®æ­£)"
                )
    
    def _merge_handler_results(self, base_result: Dict, handler_result: Dict, handler_name: str) -> Dict:
        """
        ãƒãƒ³ãƒ‰ãƒ©ãƒ¼çµæœã‚’ãƒ™ãƒ¼ã‚¹çµæœã«ãƒãƒ¼ã‚¸
        
        Args:
            base_result: ãƒ™ãƒ¼ã‚¹çµæœ
            handler_result: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å‡¦ç†çµæœ  
            handler_name: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å
        """
        # ã‚¹ãƒ­ãƒƒãƒˆæƒ…å ±ãƒãƒ¼ã‚¸
        if 'slots' in handler_result:
            for slot_name, slot_data in handler_result['slots'].items():
                if slot_name not in base_result['slots']:
                    base_result['slots'][slot_name] = slot_data
                else:
                    # ç«¶åˆè§£æ±ºï¼šç©ºæ–‡å­—ã‚„ç©ºå€¤ã§æ—¢å­˜ã®æœ‰åŠ¹ãªå€¤ã‚’ä¸Šæ›¸ãã—ãªã„
                    existing_value = base_result['slots'][slot_name]
                    
                    # åˆ†è©æ§‹æ–‡ä¿è­·ï¼šåˆ†è©æ§‹æ–‡ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒè¨­å®šã—ãŸç©ºæ–‡å­—ã‚’ä¿è­·
                    control_flags = base_result.get('grammar_info', {}).get('control_flags', {})
                    participle_detected = control_flags.get('participle_detected', False)
                    modified_slot = control_flags.get('modified_slot')
                    
                    if (participle_detected and slot_name == modified_slot and 
                        existing_value == "" and handler_name != 'participle_construction'):
                        # åˆ†è©æ§‹æ–‡ã§ç©ºæ–‡å­—åŒ–ã•ã‚ŒãŸã‚¹ãƒ­ãƒƒãƒˆã¯ä»–ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã§ä¸Šæ›¸ãç¦æ­¢
                        self.logger.debug(f"åˆ†è©æ§‹æ–‡ä¿è­·: {slot_name} ç©ºæ–‡å­—ä¿æŒ (by participle_construction)")
                        pass  # ç©ºæ–‡å­—ã‚’ä¿æŒ
                    # â˜… Mã‚¹ãƒ­ãƒƒãƒˆä¿è­·ï¼šå‰¯è©ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã§è¨­å®šã•ã‚ŒãŸMã‚¹ãƒ­ãƒƒãƒˆã‚’ä¿è­·
                    elif slot_name.startswith('M') and existing_value and handler_name != 'adverbial_modifier':
                        # å‰¯è©ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ä»¥å¤–ã¯Mã‚¹ãƒ­ãƒƒãƒˆã‚’ä¸Šæ›¸ãã§ããªã„
                        pass  # æ—¢å­˜å€¤ã‚’ä¿æŒ
                    # æ—¢å­˜å€¤ãŒç©ºã§æ–°å€¤ãŒæœ‰åŠ¹ãªå ´åˆã¯ä¸Šæ›¸ã
                    elif not existing_value and slot_data:
                        base_result['slots'][slot_name] = slot_data
                    # æ—¢å­˜å€¤ãŒæœ‰åŠ¹ã§æ–°å€¤ã‚‚æœ‰åŠ¹ãªå ´åˆã¯å¾Œå‹ã¡ï¼ˆå¾“æ¥é€šã‚Šï¼‰
                    elif existing_value and slot_data:
                        base_result['slots'][slot_name] = slot_data
                    # æ—¢å­˜å€¤ãŒæœ‰åŠ¹ã§æ–°å€¤ãŒç©ºã®å ´åˆã¯ä¿æŒï¼ˆâ˜…ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆï¼‰
                    elif existing_value and not slot_data:
                        pass  # æ—¢å­˜å€¤ã‚’ä¿æŒ
                    # ä¸¡æ–¹ç©ºã®å ´åˆã¯å¾Œå‹ã¡
                    else:
                        base_result['slots'][slot_name] = slot_data
        
        # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæƒ…å ±ãƒãƒ¼ã‚¸
        if 'sub_slots' in handler_result:
            for sub_slot_name, sub_slot_data in handler_result['sub_slots'].items():
                # æ—¢å­˜ã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå€¤ã‚’ãƒã‚§ãƒƒã‚¯
                existing_sub_value = base_result['sub_slots'].get(sub_slot_name)
                
                # åˆ†è©æ§‹æ–‡ãŒè¨­å®šã—ãŸsub-auxã‚’ä¿è­·
                if (sub_slot_name == 'sub-aux' and existing_sub_value and 
                    ('being' in existing_sub_value or len(existing_sub_value.split()) > 1) and
                    handler_name != 'participle_construction'):
                    self.logger.debug(f"åˆ†è©æ§‹æ–‡sub-auxä¿è­·: '{existing_sub_value}' from {handler_name}")
                    continue  # æ—¢å­˜å€¤ã‚’ä¿æŒ
                
                # é€šå¸¸ã®ãƒãƒ¼ã‚¸
                base_result['sub_slots'][sub_slot_name] = sub_slot_data
        
        # ğŸ”¥ ä½ç½®æƒ…å ±ãƒãƒ¼ã‚¸ï¼ˆé‡è¦ï¼ï¼‰
        if 'slot_positions' in handler_result:
            if 'slot_positions' not in base_result:
                base_result['slot_positions'] = {}
            base_result['slot_positions'].update(handler_result['slot_positions'])
            self.logger.debug(f"ğŸ“ ä½ç½®æƒ…å ±ãƒãƒ¼ã‚¸ from {handler_name}: {handler_result['slot_positions']}")
        
        # æ–‡æ³•æƒ…å ±è¨˜éŒ²
        if 'grammar_info' in handler_result:
            grammar_info = handler_result['grammar_info']
            base_result['grammar_info']['handler_contributions'][handler_name] = grammar_info
            
            # æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³è¿½åŠ 
            if 'patterns' in grammar_info:
                base_result['grammar_info']['detected_patterns'].extend(grammar_info['patterns'])
        
        return base_result
    
    def _post_process_result(self, result: Dict, sentence: str) -> Dict:
        """å¾Œå‡¦ç†ãƒ»çµæœæ¤œè¨¼ï¼ˆwhoseæ§‹æ–‡ç‰¹åˆ¥å‡¦ç†è¿½åŠ ï¼‰"""
        
        # âœ… whoseæ§‹æ–‡ã®ç‰¹åˆ¥ãªå¾Œå‡¦ç†ï¼šä¸»æ–‡ãƒ»é–¢ä¿‚ç¯€ã®æ­£ã—ã„åˆ†é›¢
        if 'whose' in sentence.lower():
            result = self._post_process_whose_construction(result, sentence)
        
        # é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³é™¤å»
        if 'detected_patterns' in result.get('grammar_info', {}):
            result['grammar_info']['detected_patterns'] = \
                list(set(result['grammar_info']['detected_patterns']))
        
        # ğŸ”§ REPHRASE SPECIFICATION COMPLIANCE: Sub-slots require empty main slots
        self._apply_rephrase_slot_structure_rules(result)
        
        # ã‚¹ãƒ­ãƒƒãƒˆæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆä»Šå¾Œå®Ÿè£…ï¼‰
        # TODO: rephrase_slot_validator.py ã¨ã®é€£æº
        
        return result
    
    def _post_process_whose_construction(self, result: Dict, sentence: str) -> Dict:
        """whoseæ§‹æ–‡ã®å¾Œå‡¦ç†ï¼šä¸»æ–‡ãƒ»é–¢ä¿‚ç¯€ã®æ­£ã—ã„åˆ†é›¢"""
        
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æã§è£œæ­£ã•ã‚ŒãŸå‹•è©ï¼ˆä¸»æ–‡å‹•è©ï¼‰ã‚’æ¤œå‡º
        main_verb = None
        for word in sentence.split():
            if word.lower() in ['lives', 'works', 'runs', 'goes', 'sits', 'stands']:
                main_verb = word
                break
        
        if main_verb:
            # ä¸»æ–‡å‹•è©ã‚’Vã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®
            if 'slots' not in result:
                result['slots'] = {}
            result['slots']['V'] = main_verb
            
            # âœ… å‰¯è©å‡¦ç†ã¯å°‚é–€ã‚¨ãƒ³ã‚¸ãƒ³ã«å§”è­² - å›ºå®šå‡¦ç†ã‚’ç„¡åŠ¹åŒ–
            # if 'here' in sentence.lower():
            #     result['slots']['M2'] = 'here'
            # elif 'there' in sentence.lower():
            #     result['slots']['M2'] = 'there'
                
            # ä¸»èªã¯é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒè¨­å®šã—ãŸsub-sã‚’ç§»å‹•
            if result.get('sub_slots', {}).get('sub-s'):
                # sub-sã®å†…å®¹ã‹ã‚‰é–¢ä¿‚ç¯€éƒ¨åˆ†ã‚’é™¤å»ã—ã¦ä¸»æ–‡ä¸»èªã‚’ä½œã‚‹
                sub_s_content = result['sub_slots']['sub-s']  # "The man whose car"
                # "whose car"éƒ¨åˆ†ã‚’é™¤å»ã—ã¦"The man"ã‚’ä¸»èªã¨ã™ã‚‹
                main_subject = sub_s_content.split(' whose ')[0]  # "The man"
                result['slots']['S'] = main_subject
                
            # é–¢ä¿‚ç¯€ã®sub-c1ãŒä¸»æ–‡å‹•è©ã«ãªã£ã¦ã„ã‚‹å ´åˆã¯ä¿®æ­£
            if result.get('sub_slots', {}).get('sub-c1') == main_verb:
                # æœ¬æ¥ã®é–¢ä¿‚ç¯€è£œèªã‚’æ¢ã™
                if 'red' in sentence.lower():
                    result['sub_slots']['sub-c1'] = 'red'
            
            # åŠ©å‹•è©ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒæ¤œå‡ºã—ãŸä¸»æ–‡å‹•è©ã‚’é©ç”¨
            detected_patterns = result.get('grammar_info', {}).get('detected_patterns', [])
            if 'passive_voice' in detected_patterns:
                passive_info = result.get('grammar_info', {}).get('handler_contributions', {}).get('passive_voice', {})
                if passive_info and 'main_verb' in passive_info:
                    main_verb_from_passive = passive_info['main_verb']
                    result['slots']['V'] = main_verb_from_passive
                    self.logger.debug(f"ğŸ”§ whoseæ§‹æ–‡: å—å‹•æ…‹å‹•è©ä¿®æ­£ V='{main_verb_from_passive}'")
                    
            self.logger.debug(f"ğŸ”§ whoseæ§‹æ–‡å¾Œå‡¦ç†: ä¸»æ–‡V={result['slots'].get('V')}, S={result['slots'].get('S')}")
        
        return result
    
    def _apply_rephrase_slot_structure_rules(self, result: Dict) -> None:
        """
        Rephraseä»•æ§˜æº–æ‹ ï¼šè¤‡æ–‡ã§ã®æ­£ã—ã„ã‚¹ãƒ­ãƒƒãƒˆé…ç½®
        
        é‡è¦ãƒ«ãƒ¼ãƒ«ï¼šsub-slotsãŒå­˜åœ¨ã™ã‚‹å ´åˆã€å¯¾å¿œã™ã‚‹main slotsã¯ç©ºæ–‡å­—ã«ã™ã‚‹
        ä¾‹å¤–ï¼šAux, Vã‚¹ãƒ­ãƒƒãƒˆã¯ä¾‹å¤–é©ç”¨ãªã—ã€æ¥ç¶šè©æ§‹æ–‡ã§ã¯ä¸»ç¯€è¦ç´ ä¿æŒ
        
        å¯¾å¿œé–¢ä¿‚ï¼š
        - S â†â†’ sub-s (Sä½ç½®ã®å¾“å±ç¯€)
        - O1 â†â†’ sub-o1 (O1ä½ç½®ã®å¾“å±ç¯€)  
        - O2 â†â†’ sub-o2 (O2ä½ç½®ã®å¾“å±ç¯€)
        - C1 â†â†’ sub-c1 (C1ä½ç½®ã®å¾“å±ç¯€)
        - C2 â†â†’ sub-c2 (C2ä½ç½®ã®å¾“å±ç¯€)
        - M1 â†â†’ sub-m1 (M1ä½ç½®ã®å¾“å±ç¯€)
        - M2 â†â†’ sub-m2 (M2ä½ç½®ã®å¾“å±ç¯€) 
        - M3 â†â†’ sub-m3 (M3ä½ç½®ã®å¾“å±ç¯€)
        """
        slots = result.get('slots', {})
        sub_slots = result.get('sub_slots', {})
        
        # æ¥ç¶šè©æ§‹æ–‡ã§ã¯ä¸»ç¯€è¦ç´ ã‚’ä¿æŒ
        grammar_info = result.get('grammar_info', {})
        handler_contributions = grammar_info.get('handler_contributions', {})
        is_conjunction = 'conjunction' in handler_contributions
        
        if is_conjunction:
            self.logger.debug("ğŸ”— æ¥ç¶šè©æ§‹æ–‡æ¤œå‡º: ä¸»ç¯€è¦ç´ ä¿æŒ")
            return
        
        # ä½ç½®æƒ…å ±ãƒ™ãƒ¼ã‚¹ã®å‡¦ç†ã«ç§»è¡Œï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†ï¼‰
        # å„ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆãŒè‡ªåˆ†ã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç¾¤ã®ã¿ã«åŸºã¥ã„ã¦ç©ºæ–‡å­—åŒ–ã•ã‚Œã‚‹
        
        # åˆ†è©æ§‹æ–‡åˆ¶å¾¡ãƒ•ãƒ©ã‚°ã‚’ãƒã‚§ãƒƒã‚¯
        grammar_info = result.get('grammar_info', {})
        control_flags = grammar_info.get('control_flags', {})
        participle_detected = control_flags.get('participle_detected', False)
        
        # ä½ç½®æƒ…å ±ãƒ™ãƒ¼ã‚¹ã®å‡¦ç†ã«ç§»è¡Œ
        slot_positions = result.get('slot_positions', {})
        
        self.logger.debug(f"ğŸ—ï¸ Rephraseä»•æ§˜é©ç”¨é–‹å§‹ - Sub-slots: {list(sub_slots.keys())}, ä½ç½®æƒ…å ±: {slot_positions}, åˆ†è©æ§‹æ–‡: {participle_detected}")
        
        # ğŸ”¥ ä½ç½®æƒ…å ±ãƒ™ãƒ¼ã‚¹ã®ç©ºæ–‡å­—åŒ–å‡¦ç†
        # å„ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã«ã¤ã„ã¦ã€ãã®ã‚¹ãƒ­ãƒƒãƒˆã«å±ã™ã‚‹ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        upper_slots = ['S', 'O1', 'O2', 'C1', 'C2', 'M1', 'M2', 'M3']  # Auxã¨Vã¯é™¤å¤–
        
        for upper_slot in upper_slots:
            # ã“ã®ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã«å±ã™ã‚‹ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã‚’æ¤œç´¢
            belonging_sub_slots = [
                sub_slot for sub_slot, position in slot_positions.items() 
                if position == upper_slot and sub_slot in sub_slots and sub_slots[sub_slot] and sub_slots[sub_slot].strip()
            ]
            
            if belonging_sub_slots:
                # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã€ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã‚’ç©ºæ–‡å­—ã«ã™ã‚‹
                if upper_slot in slots and slots[upper_slot] and slots[upper_slot].strip():
                    # æ—¢å­˜ã®å€¤ãŒã‚ã‚‹å ´åˆã¯ç©ºæ–‡å­—ã«å¤‰æ›´
                    original_value = slots[upper_slot]
                    slots[upper_slot] = ""
                    self.logger.debug(
                        f"ğŸ”„ ä½ç½®æƒ…å ±ãƒ™ãƒ¼ã‚¹ç©ºæ–‡å­—åŒ–: {upper_slot}: '{original_value}' â†’ '' "
                        f"(å±ã™ã‚‹ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ: {', '.join(belonging_sub_slots)})"
                    )
                elif upper_slot not in slots or not slots[upper_slot]:
                    # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆãŒå­˜åœ¨ã—ãªã„ã‹ç©ºã®å ´åˆã‚‚æ˜ç¤ºçš„ã«ç©ºæ–‡å­—ã‚’è¨­å®š
                    slots[upper_slot] = ""
                    self.logger.debug(
                        f"ğŸ”„ ä½ç½®æƒ…å ±ãƒ™ãƒ¼ã‚¹ç©ºæ–‡å­—åŒ–: {upper_slot}: æœªè¨­å®š â†’ '' "
                        f"(å±ã™ã‚‹ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ: {', '.join(belonging_sub_slots)})"
                    )
        
        # å‰¯è©é‡è¤‡ãƒã‚§ãƒƒã‚¯ã¨å‰Šé™¤
        self._remove_adverb_duplicates(slots, sub_slots)
        
        # å‡¦ç†çµæœã‚’ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°å‡ºåŠ›
        position_based_rules = []
        slot_positions = result.get('slot_positions', {})
        for upper_slot in ['S', 'O1', 'O2', 'C1', 'C2', 'M1', 'M2', 'M3']:
            belonging_sub_slots = [
                sub_slot for sub_slot, position in slot_positions.items() 
                if position == upper_slot and sub_slot in sub_slots and sub_slots[sub_slot]
            ]
            if belonging_sub_slots and upper_slot in slots and slots[upper_slot] == "":
                position_based_rules.append(f"{upper_slot}â†{', '.join(belonging_sub_slots)}")
        
        if position_based_rules:
            self.logger.info(f"âœ… ä½ç½®æƒ…å ±ãƒ™ãƒ¼ã‚¹ç©ºæ–‡å­—åŒ–: {', '.join(position_based_rules)}")
        else:
            self.logger.debug("ğŸ” Simple sentence detected - No main slot emptying required")
    
    def _remove_adverb_duplicates(self, slots: Dict, sub_slots: Dict):
        """ä¸»ç¯€ã¨é–¢ä¿‚ç¯€ã®å‰¯è©é‡è¤‡ã‚’é™¤å»ï¼ˆé–¢ä¿‚ç¯€å†…é‡è¤‡ã‚‚å¯¾å¿œï¼‰"""
        
        # === 1. é–¢ä¿‚ç¯€å†…é‡è¤‡é™¤å»ï¼ˆæœ€é‡è¦ï¼‰===
        sub_adverbs = {k: v for k, v in sub_slots.items() if k.startswith('sub-m') and v}
        
        if len(sub_adverbs) > 1:
            # é–¢ä¿‚ç¯€å†…ã§åŒã˜å‰¯è©ãŒè¤‡æ•°ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®ã•ã‚Œã¦ã„ã‚‹å ´åˆ
            seen_adverbs = {}
            slots_to_clear = []
            
            for sub_slot, sub_value in sub_adverbs.items():
                adverb_text = sub_value.strip()
                if adverb_text in seen_adverbs:
                    # é‡è¤‡æ¤œå‡º: ã‚ˆã‚Šå„ªå…ˆåº¦ã®ä½ã„ã‚¹ãƒ­ãƒƒãƒˆã‚’å‰Šé™¤
                    existing_slot = seen_adverbs[adverb_text]
                    
                    # å„ªå…ˆåº¦: sub-m2 > sub-m1 > sub-m3ï¼ˆRephraseä»•æ§˜æº–æ‹ ï¼‰
                    priority_order = {'sub-m2': 3, 'sub-m1': 2, 'sub-m3': 1}
                    
                    if priority_order.get(sub_slot, 0) > priority_order.get(existing_slot, 0):
                        # æ–°ã‚¹ãƒ­ãƒƒãƒˆã®æ–¹ãŒå„ªå…ˆåº¦é«˜â†’æ—¢å­˜ã‚’å‰Šé™¤
                        slots_to_clear.append(existing_slot)
                        seen_adverbs[adverb_text] = sub_slot
                        self.logger.debug(f"ğŸ”„ é–¢ä¿‚ç¯€å†…é‡è¤‡å‰Šé™¤: {existing_slot}='{adverb_text}' â†’ '' ({sub_slot}='{adverb_text}' ã‚’å„ªå…ˆ)")
                    else:
                        # æ—¢å­˜ã‚¹ãƒ­ãƒƒãƒˆã®æ–¹ãŒå„ªå…ˆåº¦é«˜â†’æ–°ã‚¹ãƒ­ãƒƒãƒˆã‚’å‰Šé™¤
                        slots_to_clear.append(sub_slot)
                        self.logger.debug(f"ğŸ”„ é–¢ä¿‚ç¯€å†…é‡è¤‡å‰Šé™¤: {sub_slot}='{adverb_text}' â†’ '' ({existing_slot}='{adverb_text}' ã‚’å„ªå…ˆ)")
                else:
                    seen_adverbs[adverb_text] = sub_slot
            
            # é‡è¤‡ã‚¹ãƒ­ãƒƒãƒˆã‚’ã‚¯ãƒªã‚¢
            for slot_to_clear in slots_to_clear:
                sub_slots[slot_to_clear] = ""
        
        # === ç©ºæ–‡å­—åˆ—ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå‰Šé™¤ (Case 52å¯¾å¿œ) ===
        empty_sub_slots = [k for k, v in sub_slots.items() if v == ""]
        for empty_slot in empty_sub_slots:
            del sub_slots[empty_slot]
            self.logger.debug(f"ğŸ—‘ï¸ ç©ºæ–‡å­—åˆ—ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå‰Šé™¤: {empty_slot}")
        
        # === 2. ä¸»ç¯€â†”é–¢ä¿‚ç¯€é–“é‡è¤‡é™¤å»ï¼ˆå¾“æ¥æ©Ÿèƒ½ï¼‰===
        main_adverbs = {k: v for k, v in slots.items() if k.startswith('M') and v}
        remaining_sub_adverbs = {k: v for k, v in sub_slots.items() if k.startswith('sub-m') and v}
        
        if not main_adverbs or not remaining_sub_adverbs:
            return
        
        # é‡è¤‡å‰¯è©ã®æ¤œå‡ºã¨å‰Šé™¤
        for main_slot, main_value in list(main_adverbs.items()):
            for sub_slot, sub_value in remaining_sub_adverbs.items():
                # åŒã˜å‰¯è©ãŒä¸»ç¯€ã¨é–¢ä¿‚ç¯€ã«å­˜åœ¨ã™ã‚‹å ´åˆ
                if main_value.strip() == sub_value.strip() and main_value.strip():  # ç©ºæ–‡å­—ãƒã‚§ãƒƒã‚¯è¿½åŠ 
                    # é–¢ä¿‚ç¯€ã‚’å„ªå…ˆã—ã€ä¸»ç¯€ã‹ã‚‰å‰Šé™¤
                    slots[main_slot] = ""
                    self.logger.debug(f"ğŸ”„ ä¸»ç¯€â†”é–¢ä¿‚ç¯€é‡è¤‡å‰Šé™¤: {main_slot}='{main_value}' â†’ '' (sub-slot {sub_slot}='{sub_value}' ã‚’å„ªå…ˆ)")
                    break
    
    def _create_empty_result(self, sentence: str) -> Dict[str, Any]:
        """ç©ºçµæœã®ä½œæˆ"""
        return {
            'sentence': sentence,
            'slots': {},
            'sub_slots': {},
            'grammar_info': {
                'detected_patterns': [],
                'handler_contributions': {}
            },
            'meta': {
                'processing_time': 0.0,
                'sentence_id': self.processing_count,
                'empty_result': True
            }
        }
    
    # =============================================================================
    # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ç®¡ç†ï¼ˆPhaseåˆ¥æ©Ÿèƒ½è¿½åŠ ç”¨ï¼‰
    # =============================================================================
    
    def add_handler(self, handler_name: str):
        """ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ ï¼ˆPhaseåˆ¥é–‹ç™ºç”¨ï¼‰"""
        if handler_name not in self.active_handlers:
            self.active_handlers.append(handler_name)
            self.logger.info(f"Handlerè¿½åŠ : {handler_name}")
        else:
            self.logger.warning(f"âš ï¸ Handler already active: {handler_name}")
    
    def remove_handler(self, handler_name: str):
        """ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å‰Šé™¤"""
        if handler_name in self.active_handlers:
            self.active_handlers.remove(handler_name)
            self.logger.info(f"â– Handlerå‰Šé™¤: {handler_name}")
    
    def list_active_handlers(self) -> List[str]:
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ä¸€è¦§"""
        return self.active_handlers.copy()
    
    # =============================================================================
    # çµ±è¨ˆãƒ»ãƒ‡ãƒãƒƒã‚°æƒ…å ±
    # =============================================================================
    
    def get_stats(self) -> Dict[str, Any]:
        """å‡¦ç†çµ±è¨ˆæƒ…å ±å–å¾—"""
        avg_processing_time = (
            self.total_processing_time / self.processing_count 
            if self.processing_count > 0 else 0.0
        )
        
        return {
            'processing_count': self.processing_count,
            'total_processing_time': self.total_processing_time,
            'average_processing_time': avg_processing_time,
            'active_handlers': self.active_handlers.copy(),
            'handler_success_count': self.handler_success_count.copy(),
            'stanza_pipeline_status': 'active' if self.nlp else 'inactive'
        }
    
    def reset_stats(self):
        """çµ±è¨ˆæƒ…å ±ãƒªã‚»ãƒƒãƒˆ"""
        self.processing_count = 0
        self.total_processing_time = 0.0
        self.handler_success_count.clear()
        self.logger.info("ğŸ“Š Statistics reset")
    
    # =============================================================================
    # æ–‡æ³•ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè£…ï¼ˆæ®µéšçš„è¿½åŠ ï¼‰
    # =============================================================================
    
    def _handle_relative_clause(self, sentence, base_result: Dict, shared_context: Dict = None) -> Optional[Dict]:
        """
        é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆåˆ†è©æ§‹æ–‡åˆ¶å¾¡ãƒ•ãƒ©ã‚°å¯¾å¿œ + ãƒãƒ³ãƒ‰ãƒ©ãƒ¼é–“æƒ…å ±å…±æœ‰ï¼‰
        
        simple_relative_engine.py ã®æ©Ÿèƒ½ã‚’çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã«ç§»æ¤
        Stanza dependency parsing ã«ã‚ˆã‚‹ç›´æ¥çš„ãªé–¢ä¿‚ç¯€æ¤œå‡ºãƒ»åˆ†è§£
        åˆ†è©æ§‹æ–‡ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€é–¢ä¿‚ç¯€å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
        
        Args:
            sentence: Stanzaè§£ææ¸ˆã¿sentence object
            base_result: ãƒ™ãƒ¼ã‚¹çµæœï¼ˆã‚³ãƒ”ãƒ¼ï¼‰
            shared_context: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼é–“å…±æœ‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            Dict: é–¢ä¿‚ç¯€åˆ†è§£çµæœ or None
        """
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
        if shared_context is None:
            shared_context = {'predefined_slots': {}, 'handler_metadata': {}}
            
        try:
            self.logger.debug("ğŸ” é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè¡Œä¸­...")
            
            # åˆ†è©æ§‹æ–‡åˆ¶å¾¡ãƒ•ãƒ©ã‚°ã‚’ãƒã‚§ãƒƒã‚¯
            grammar_info = base_result.get('grammar_info', {})
            control_flags = grammar_info.get('control_flags', {})
            participle_detected = control_flags.get('participle_detected', False)
            
            if participle_detected:
                self.logger.debug("  ğŸ¯ åˆ†è©æ§‹æ–‡æ¤œå‡ºæ¸ˆã¿ - é–¢ä¿‚ç¯€å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆåˆ†è©æ§‹æ–‡ãŒå„ªå…ˆï¼‰")
                return None
            
            # é–¢ä¿‚ç¯€å­˜åœ¨ãƒã‚§ãƒƒã‚¯
            if not self._has_relative_clause(sentence):
                self.logger.debug("  é–¢ä¿‚ç¯€ãªã— - ã‚¹ã‚­ãƒƒãƒ—")
                return None
            
            self.logger.debug("  âœ… é–¢ä¿‚ç¯€æ¤œå‡º")
            return self._process_relative_clause_structure(sentence, base_result, shared_context)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _has_relative_clause(self, sentence) -> bool:
        """é–¢ä¿‚ç¯€ã‚’å«ã‚€ã‹ãƒã‚§ãƒƒã‚¯"""
        # âœ… whoseæ§‹æ–‡ã®è©³ç´°å‡¦ç†
        has_acl_relcl = any(w.deprel in ['acl:relcl', 'acl'] for w in sentence.words)
        
        if has_acl_relcl and any(w.text.lower() == 'whose' for w in sentence.words):
            # whoseæ§‹æ–‡ã§ã¯å¸¸ã«é–¢ä¿‚ç¯€ã¨ã—ã¦å‡¦ç†
            # ä¸»æ–‡å‹•è©ã¨é–¢ä¿‚ç¯€å‹•è©ã‚’é©åˆ‡ã«åˆ†é›¢ã™ã‚‹ã“ã¨ã§å¯¾å¿œ
            self.logger.debug(f"ğŸ”§ whoseæ§‹æ–‡: é–¢ä¿‚ç¯€ã¨ã—ã¦å‡¦ç†é–‹å§‹")
            return True
        
        return has_acl_relcl
    
    def _process_relative_clause_structure(self, sentence, base_result: Dict, shared_context: Dict = None) -> Dict:
        """é–¢ä¿‚ç¯€æ§‹é€ ã®åˆ†è§£å‡¦ç†"""
        
        # === 1. è¦ç´ ç‰¹å®š ===
        # âœ… whoseæ§‹æ–‡ã®çœŸã®é–¢ä¿‚ç¯€æ¤œå‡º
        rel_verb = None
        antecedent = None
        
        is_whose_construction = any(w.text.lower() == 'whose' for w in sentence.words)
        
        if is_whose_construction:
            # whoseæ§‹æ–‡ã§ã¯ã€ã¾ãšacl:relclé–¢ä¿‚ã®å®Ÿå‹•è©ã‚’æ¢ã™
            acl_word = self._find_word_by_deprel(sentence, 'acl:relcl')
            if acl_word and acl_word.upos == 'VERB':
                # Pattern B: å®Ÿå‹•è©ãŒé–¢ä¿‚ç¯€å‹•è© (ä¾‹: borrowed)
                rel_verb = acl_word
                if acl_word.head > 0:
                    antecedent = self._find_word_by_id(sentence, acl_word.head)
            else:
                # Pattern A: copå‹•è©ãŒé–¢ä¿‚ç¯€å‹•è© (ä¾‹: is in "car is red")  
                for word in sentence.words:
                    if word.deprel == 'cop':
                        rel_verb = word
                        # acl:relclã®headã‹ã‚‰å…ˆè¡Œè©ã‚’æ¢ã™
                        if acl_word and acl_word.head > 0:
                            antecedent = self._find_word_by_id(sentence, acl_word.head)
                        else:
                            # fallback: rootèªã‚’å…ˆè¡Œè©ã¨ã™ã‚‹
                            for w in sentence.words:
                                if w.deprel == 'root':
                                    antecedent = w
                                    break
                        break
                        
            if rel_verb and antecedent:
                self.logger.debug(f"ğŸ”§ whoseæ§‹æ–‡ä¿®æ­£: é–¢ä¿‚ç¯€å‹•è©={rel_verb.text}, å…ˆè¡Œè©={antecedent.text}")
        
        # é€šå¸¸ã®é–¢ä¿‚ç¯€æ¤œå‡º
        if not rel_verb:
            rel_verb = self._find_word_by_deprel(sentence, 'acl:relcl')
            if not rel_verb:
                rel_verb = self._find_word_by_deprel(sentence, 'acl')
            if not rel_verb:
                return base_result
            
            # å…ˆè¡Œè©ï¼ˆé–¢ä¿‚ç¯€å‹•è©ã®é ­ï¼‰
            antecedent = self._find_word_by_id(sentence, rel_verb.head)
            
        if not antecedent:
            return base_result

        self.logger.debug(f"  å…ˆè¡Œè©: {antecedent.text}, é–¢ä¿‚å‹•è©: {rel_verb.text}")
        
        # === 2. é–¢ä¿‚ä»£åè©/é–¢ä¿‚å‰¯è©ç‰¹å®š ===
        rel_pronoun, rel_type = self._identify_relative_pronoun(sentence, rel_verb)
        
        # === 3. é–¢ä¿‚ç¯€å†…è¦ç´ ç‰¹å®š ===
        rel_subject = None
        if rel_type in ['obj', 'advmod']:  # ç›®çš„èªãƒ»é–¢ä¿‚å‰¯è©ã®å ´åˆã®ã¿ä¸»èªæ¤œç´¢
            rel_subject = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'nsubj')
        elif rel_type == 'poss':
            # æ‰€æœ‰æ ¼é–¢ä¿‚ä»£åè©ã®å ´åˆã¯ç‰¹åˆ¥å‡¦ç†
            # whoseæ§‹æ–‡ã§ã¯ã€æ‰€æœ‰ã•ã‚Œã‚‹åè©ä»¥å¤–ã®ç‹¬ç«‹ã—ãŸä¸»èªã‚’æ¢ã™
            nsubj_word = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'nsubj')
            possessed_noun = self._find_word_by_id(sentence, rel_pronoun.head) if rel_pronoun else None
            
            # æ‰€æœ‰ã•ã‚Œã‚‹åè©ã¨ç•°ãªã‚‹ä¸»èªãŒã‚ã‚‹å ´åˆã®ã¿rel_subjectã¨ã—ã¦èªè­˜
            if nsubj_word and possessed_noun and nsubj_word.id != possessed_noun.id:
                rel_subject = nsubj_word
        
        # æ‰€æœ‰æ ¼é–¢ä¿‚ä»£åè©ã®ç‰¹åˆ¥å‡¦ç†
        possessed_noun = None
        if rel_type == 'poss':
            possessed_noun = self._find_word_by_id(sentence, rel_pronoun.head)
        
        self.logger.debug(f"  é–¢ä¿‚ä»£åè©: {rel_pronoun.text if rel_pronoun else 'None'} ({rel_type})")
        if rel_subject:
            self.logger.debug(f"  é–¢ä¿‚ç¯€ä¸»èª: {rel_subject.text}")
        if possessed_noun:
            self.logger.debug(f"  æ‰€æœ‰ã•ã‚Œã‚‹åè©: {possessed_noun.text}")
        
        # === 4. å…ˆè¡Œè©å¥æ§‹ç¯‰ ===
        noun_phrase = self._build_antecedent_phrase(sentence, antecedent, rel_pronoun, possessed_noun)
        self.logger.debug(f"  æ§‹ç¯‰å…ˆè¡Œè©å¥: '{noun_phrase}'")
        
        # === 5. Rephraseã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ ===
        result = base_result.copy()
        
        # âœ… whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†: ãƒ¡ã‚¤ãƒ³å‹•è©å‡¦ç†ã‚’å¦¨å®³ã—ãªã„
        self.logger.debug(f"ğŸ” whoseæ§‹æ–‡æ¡ä»¶ãƒã‚§ãƒƒã‚¯: is_whose={is_whose_construction}, rel_verb={rel_verb.text if rel_verb else None}, deprel={rel_verb.deprel if rel_verb else None}")
        if is_whose_construction and rel_verb and rel_verb.deprel == 'cop':
            self.logger.debug("ğŸ”§ whoseæ§‹æ–‡ç‰¹åˆ¥å‡¦ç†ãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹")
            # é–¢ä¿‚ç¯€ã‚¹ãƒ­ãƒƒãƒˆã®ã¿ç”Ÿæˆã—ã€ãƒ¡ã‚¤ãƒ³æ–‡ã¯5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã«ä»»ã›ã‚‹
            self.logger.debug("ğŸ”§ _generate_whose_relative_clause_slotså‘¼ã³å‡ºã—é–‹å§‹")
            rephrase_slots = self._generate_whose_relative_clause_slots(
                antecedent, rel_verb, sentence
            )
            self.logger.debug(f"ğŸ”§ _generate_whose_relative_clause_slotså‘¼ã³å‡ºã—å®Œäº†: {rephrase_slots}")
            
            # çµæœãƒãƒ¼ã‚¸ï¼ˆãƒ¡ã‚¤ãƒ³æ–‡ã‚¹ãƒ­ãƒƒãƒˆã¯ä¿æŒï¼‰
            if 'slots' not in result:
                result['slots'] = {}
            if 'sub_slots' not in result:
                result['sub_slots'] = {}
            if 'slot_positions' not in result:
                result['slot_positions'] = {}
            
            # é–¢ä¿‚ç¯€ã®sub-slotsã®ã¿ãƒãƒ¼ã‚¸ï¼ˆãƒ¡ã‚¤ãƒ³æ–‡ã‚¹ãƒ­ãƒƒãƒˆã¯å¤‰æ›´ã—ãªã„ï¼‰
            result['sub_slots'].update(rephrase_slots.get('sub_slots', {}))
            
            # ğŸ”¥ whoseæ§‹æ–‡ã§ã‚‚ä½ç½®æƒ…å ±ã‚’è¨­å®š
            antecedent_position = self._determine_antecedent_position(sentence, antecedent)
            for sub_slot_name in rephrase_slots.get('sub_slots', {}):
                result['slot_positions'][sub_slot_name] = antecedent_position
                self.logger.debug(f"ğŸ“ whoseæ§‹æ–‡ä½ç½®æƒ…å ±è¨˜éŒ²: {sub_slot_name} â†’ {antecedent_position}ä½ç½® (å…ˆè¡Œè©: {antecedent.text})")
            
            self.logger.debug(f"ğŸ”§ whoseæ§‹æ–‡: ãƒ¡ã‚¤ãƒ³æ–‡ã‚¹ãƒ­ãƒƒãƒˆä¿æŒ, é–¢ä¿‚ç¯€ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆè¿½åŠ ")
            
            # ğŸ”¥ ä½ç½®æƒ…å ±è¨˜éŒ²: å…ˆè¡Œè©ã®ä½ç½®ã«åŸºã¥ã„ã¦ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆä½ç½®ã‚’æ±ºå®šï¼ˆæ±ç”¨ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œï¼‰
            self.logger.debug("ğŸ”§ whoseæ§‹æ–‡ç‰¹åˆ¥å‡¦ç†: ä½ç½®æƒ…å ±è¨˜éŒ²é–‹å§‹")
            antecedent_position = self._determine_element_position(sentence, antecedent)
            for sub_slot_name in rephrase_slots.get('sub_slots', {}):
                result['slot_positions'][sub_slot_name] = antecedent_position
                self.logger.debug(f"ğŸ“ ä½ç½®æƒ…å ±è¨˜éŒ²[æ±ç”¨ã‚·ã‚¹ãƒ†ãƒ ]: {sub_slot_name} â†’ {antecedent_position}ä½ç½® (å…ˆè¡Œè©: {antecedent.text})")
            
            # ğŸ¤ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼é–“æƒ…å ±å…±æœ‰: å…·ä½“çš„ãªã‚¹ãƒ­ãƒƒãƒˆå€¤ã‚’æä¾›ï¼ˆã‚ãªãŸã®æ­£ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰
            self.logger.debug(f"ğŸ” shared_context: {shared_context is not None}, antecedent_position: '{antecedent_position}'")
            if shared_context is not None and antecedent_position:
                # âœ… æ–°æ–¹å¼: å…·ä½“çš„ãªã‚¹ãƒ­ãƒƒãƒˆå€¤ã‚’æä¾›
                if 'predefined_slots' not in shared_context:
                    shared_context['predefined_slots'] = {}
                
                # é–¢ä¿‚ç¯€ã«ã‚ˆã‚Šè©²å½“ä½ç½®ã¯ç©ºæ–‡å­—åˆ—ã«ç¢ºå®š
                shared_context['predefined_slots'][antecedent_position] = ""
                self.logger.debug(f"ğŸ¤ é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼: predefined_slots[{antecedent_position}] = '' ã‚’è¨­å®š")
                
                shared_context['handler_metadata']['relative_clause'] = {
                    'occupied_slot': antecedent_position,
                    'antecedent': antecedent.text,
                    'processed_sub_slots': list(rephrase_slots.get('sub_slots', {}).keys()),
                    'predefined_value': ""  # æ˜ç¤ºçš„ã«ç©ºæ–‡å­—åˆ—ã‚’è¨­å®š
                }
                self.logger.debug(f"ğŸ¤ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼é–“å…±æœ‰: é–¢ä¿‚ç¯€ã«ã‚ˆã‚Š{antecedent_position}=\"\" ã‚’ç¢ºå®š")
            
        else:
            # é€šå¸¸ã®é–¢ä¿‚ç¯€å‡¦ç†
            rephrase_slots = self._generate_relative_clause_slots(
                rel_type, noun_phrase, rel_subject, rel_verb, sentence
            )
            
            # çµæœãƒãƒ¼ã‚¸
            if 'slots' not in result:
                result['slots'] = {}
            if 'sub_slots' not in result:
                result['sub_slots'] = {}
            if 'slot_positions' not in result:
                result['slot_positions'] = {}
            
            # é€šå¸¸ã®ãƒãƒ¼ã‚¸
            result['slots'].update(rephrase_slots.get('slots', {}))
            result['sub_slots'].update(rephrase_slots.get('sub_slots', {}))
            
            # ğŸ”¥ ä½ç½®æƒ…å ±è¨˜éŒ²: å…ˆè¡Œè©ã®ä½ç½®ã«åŸºã¥ã„ã¦ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆä½ç½®ã‚’æ±ºå®šï¼ˆæ±ç”¨ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œï¼‰
            self.logger.debug("ğŸ”§ whoseæ§‹æ–‡ç‰¹åˆ¥å‡¦ç†: ä½ç½®æƒ…å ±è¨˜éŒ²é–‹å§‹")
            antecedent_position = self._determine_element_position(sentence, antecedent)
            for sub_slot_name in rephrase_slots.get('sub_slots', {}):
                result['slot_positions'][sub_slot_name] = antecedent_position
                self.logger.debug(f"ğŸ“ ä½ç½®æƒ…å ±è¨˜éŒ²[æ±ç”¨ã‚·ã‚¹ãƒ†ãƒ ]: {sub_slot_name} â†’ {antecedent_position}ä½ç½® (å…ˆè¡Œè©: {antecedent.text})")
            
            # ğŸ¤ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼é–“æƒ…å ±å…±æœ‰: å…·ä½“çš„ãªã‚¹ãƒ­ãƒƒãƒˆå€¤ã‚’æä¾›ï¼ˆã‚ãªãŸã®æ­£ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰
            self.logger.debug(f"ğŸ” shared_context: {shared_context is not None}, antecedent_position: '{antecedent_position}'")
            if shared_context is not None and antecedent_position:
                # âŒ æ—§æ–¹å¼: å æœ‰æƒ…å ±ã®ã¿ï¼ˆã‚¹ã‚­ãƒƒãƒ—æ–¹å¼ï¼‰
                # shared_context['occupied_main_slots'].add(antecedent_position)
                
                # âœ… æ–°æ–¹å¼: å…·ä½“çš„ãªã‚¹ãƒ­ãƒƒãƒˆå€¤ã‚’æä¾›
                if 'predefined_slots' not in shared_context:
                    shared_context['predefined_slots'] = {}
                
                # é–¢ä¿‚ç¯€ã«ã‚ˆã‚Šè©²å½“ä½ç½®ã¯ç©ºæ–‡å­—åˆ—ã«ç¢ºå®š
                shared_context['predefined_slots'][antecedent_position] = ""
                self.logger.debug(f"ğŸ¤ é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼: predefined_slots[{antecedent_position}] = '' ã‚’è¨­å®š")
                
                shared_context['handler_metadata']['relative_clause'] = {
                    'occupied_slot': antecedent_position,
                    'antecedent': antecedent.text,
                    'processed_sub_slots': list(rephrase_slots.get('sub_slots', {}).keys()),
                    'predefined_value': ""  # æ˜ç¤ºçš„ã«ç©ºæ–‡å­—åˆ—ã‚’è¨­å®š
                }
                self.logger.debug(f"ğŸ¤ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼é–“å…±æœ‰: é–¢ä¿‚ç¯€ã«ã‚ˆã‚Š{antecedent_position}=\"\" ã‚’ç¢ºå®š")
            
            # æ±ç”¨ã‚¹ãƒ­ãƒƒãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’é©ç”¨
            self._apply_rephrase_slot_structure_rules(result)
            
        # æ–‡æ³•æƒ…å ±è¨˜éŒ²ï¼ˆæ±ç”¨ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œï¼‰
        result['grammar_info'] = {
            'patterns': ['relative_clause'],
            'rel_type': rel_type if not is_whose_construction else 'poss',
            'antecedent': antecedent.text,
            'rel_pronoun': 'whose' if is_whose_construction else (rel_pronoun.text if rel_pronoun else None),
            'rel_verb': rel_verb.text,
            'universal_system': True  # æ±ç”¨ã‚·ã‚¹ãƒ†ãƒ ä½¿ç”¨ãƒ•ãƒ©ã‚°
        }
        
        self.logger.debug(f"  âœ… é–¢ä¿‚ç¯€å‡¦ç†å®Œäº†: {len(result.get('slots', {}))} main slots, {len(result.get('sub_slots', {}))} sub slots")
        return result
    
    def _identify_relative_pronoun(self, sentence, rel_verb) -> Tuple[Optional[Any], str]:
        """é–¢ä¿‚ä»£åè©/é–¢ä¿‚å‰¯è©ã®ç‰¹å®šã¨åˆ†é¡ï¼ˆçœç•¥æ–‡å¯¾å¿œå¼·åŒ–ãƒ»å—å‹•æ…‹è€ƒæ…®ï¼‰"""
        
        # 1. é–¢ä¿‚å‰¯è©æ¤œå‡ºï¼ˆæœ€å„ªå…ˆï¼‰
        advmod_word = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'advmod')
        if advmod_word and advmod_word.text.lower() in ['where', 'when', 'why', 'how']:
            return advmod_word, 'advmod'
        
        # 2. æ‰€æœ‰æ ¼é–¢ä¿‚ä»£åè©æ¤œå‡º
        for word in sentence.words:
            if word.text.lower() == 'whose' and word.deprel == 'nmod:poss':
                return word, 'poss'
        
        # 3. æ˜ç¤ºçš„é–¢ä¿‚ä»£åè©æ¤œå‡º
        # ç›®çš„èªé–¢ä¿‚ä»£åè©
        obj_pronoun = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'obj')
        if obj_pronoun and obj_pronoun.text.lower() in ['who', 'whom', 'which', 'that']:
            return obj_pronoun, 'obj'
        
        # ä¸»èªé–¢ä¿‚ä»£åè©ï¼ˆå—å‹•æ…‹ãƒã‚§ãƒƒã‚¯è¿½åŠ ï¼‰  
        subj_pronoun = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'nsubj')
        if subj_pronoun and subj_pronoun.text.lower() in ['who', 'which', 'that']:
            # å—å‹•æ…‹ã®å ´åˆã¯ä¸»èªé–¢ä¿‚ä»£åè©ã¨ã—ã¦å‡¦ç†
            return subj_pronoun, 'nsubj'
            
        # å—å‹•æ…‹ä¸»èªé–¢ä¿‚ä»£åè©
        pass_subj_pronoun = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'nsubj:pass')
        if pass_subj_pronoun and pass_subj_pronoun.text.lower() in ['who', 'which', 'that']:
            return pass_subj_pronoun, 'nsubj:pass'
        
        # 4. çœç•¥é–¢ä¿‚ä»£åè©ã®æ¨å®šï¼ˆå—å‹•æ…‹æ§‹é€ æ”¹å–„ï¼‰
        inferred_type = self._infer_omitted_relative_pronoun(sentence, rel_verb)
        if inferred_type:
            # ä»®æƒ³çš„ãªé–¢ä¿‚ä»£åè©ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
            virtual_pronoun = self._create_virtual_relative_pronoun(sentence, rel_verb, inferred_type)
            return virtual_pronoun, inferred_type
        
        return None, 'unknown'
    
    def _infer_omitted_relative_pronoun(self, sentence, rel_verb) -> Optional[str]:
        """çœç•¥ã•ã‚ŒãŸé–¢ä¿‚ä»£åè©ã®æ¨å®šï¼ˆå—å‹•æ…‹æ§‹é€ æ”¹å–„ï¼‰"""
        
        # é–¢ä¿‚ç¯€å‹•è©ã®ä¾å­˜æ§‹é€ ã‚’åˆ†æ
        rel_verb_deps = []
        for word in sentence.words:
            if word.head == rel_verb.id:
                rel_verb_deps.append(word.deprel)
        
        self.logger.debug(f"    é–¢ä¿‚å‹•è© '{rel_verb.text}' ã®ä¾å­˜èª: {rel_verb_deps}")
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: å—å‹•æ…‹é–¢ä¿‚ç¯€ã®æ¤œå‡ºï¼ˆæ”¹å–„ï¼‰
        has_nsubj_pass = 'nsubj:pass' in rel_verb_deps or 'nsubjpass' in rel_verb_deps
        has_aux_pass = any(word.deprel in ['aux:pass', 'auxpass'] and word.head == rel_verb.id 
                          for word in sentence.words)
        
        if has_nsubj_pass or has_aux_pass:
            # å—å‹•æ…‹é–¢ä¿‚ç¯€ï¼šå…ˆè¡Œè©ãŒå—å‹•æ…‹ã®ä¸»èª
            self.logger.debug(f"    æ¨å®š: å—å‹•æ…‹ä¸»èªé–¢ä¿‚ä»£åè©")
            return 'nsubj:pass'  # å—å‹•æ…‹ä¸»èªã¨ã—ã¦æ‰±ã†
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: èƒ½å‹•æ…‹ã§ç›®çš„èªãŒãªã„å ´åˆ
        has_nsubj = 'nsubj' in rel_verb_deps
        has_obj = 'obj' in rel_verb_deps or 'dobj' in rel_verb_deps
        
        if has_nsubj and not has_obj:
            # èƒ½å‹•æ…‹ã§ç›®çš„èªãŒãªã„å ´åˆã€å…ˆè¡Œè©ãŒç›®çš„èªã®å¯èƒ½æ€§
            self.logger.debug(f"    æ¨å®š: çœç•¥ç›®çš„èªé–¢ä¿‚ä»£åè©ï¼ˆèƒ½å‹•æ…‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰")  
            return 'obj_omitted'
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ä¸»èªãŒãªãã€é–¢ä¿‚ç¯€ãŒèƒ½å‹•æ…‹ã®å ´åˆ
        if not has_nsubj and not has_nsubj_pass:
            self.logger.debug(f"    æ¨å®š: çœç•¥ä¸»èªé–¢ä¿‚ä»£åè©")
            return 'nsubj_omitted'
        
        return None
    
    def _create_virtual_relative_pronoun(self, sentence, rel_verb, inferred_type):
        """ä»®æƒ³çš„ãªé–¢ä¿‚ä»£åè©ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ"""
        
        # é–¢ä¿‚ç¯€ã®å…ˆè¡Œè©ã‚’å–å¾—
        antecedent = self._find_word_by_id(sentence, rel_verb.head)
        
        # ä»®æƒ³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆè¾æ›¸å½¢å¼ã§ç°¡æ˜“å®Ÿè£…ï¼‰
        virtual_pronoun = type('VirtualWord', (), {
            'text': '[omitted]',  # çœç•¥ãƒãƒ¼ã‚«ãƒ¼
            'id': rel_verb.id - 0.5,  # ä»®æƒ³IDï¼ˆé–¢ä¿‚å‹•è©ã®ç›´å‰ï¼‰
            'head': rel_verb.head,
            'deprel': inferred_type.replace('_omitted', ''),
            'lemma': '[omitted]'
        })()
        
        self.logger.debug(f"    ä»®æƒ³é–¢ä¿‚ä»£åè©ä½œæˆ: type={inferred_type}, text=[omitted]")
        return virtual_pronoun
    
    def _build_antecedent_phrase(self, sentence, antecedent, rel_pronoun, possessed_noun=None) -> str:
        """å…ˆè¡Œè©å¥æ§‹ç¯‰ï¼ˆä¿®é£¾èªå«ã‚€ï¼‰- é–¢ä¿‚ç¯€ã®å‹•è©éƒ¨åˆ†ã¯é™¤å¤–"""
        if not antecedent:
            return rel_pronoun.text if rel_pronoun else ""
        
        # å…ˆè¡Œè©ã®ä¿®é£¾èªåé›†
        modifiers = []
        for word in sentence.words:
            if word.head == antecedent.id and word.deprel in ['det', 'amod', 'compound']:
                modifiers.append(word)
        
        # åŸºæœ¬æ§‹æˆï¼šä¿®é£¾èª + å…ˆè¡Œè© + é–¢ä¿‚ä»£åè©
        phrase_words = modifiers + [antecedent]
        
        # é–¢ä¿‚ä»£åè©ã‚’è¿½åŠ ï¼ˆå‹•è©éƒ¨åˆ†ã¯é™¤å¤–ï¼‰
        if rel_pronoun:
            phrase_words.append(rel_pronoun)
        
        # æ‰€æœ‰æ ¼ã®ç‰¹åˆ¥å‡¦ç†ï¼ˆæ‰€æœ‰ã•ã‚Œã‚‹åè©ã®ã¿ï¼‰
        if possessed_noun and rel_pronoun:
            if possessed_noun not in phrase_words:
                phrase_words.append(possessed_noun)
        
        # IDé †ã‚½ãƒ¼ãƒˆï¼ˆèªé †ä¿æŒï¼‰
        phrase_words.sort(key=lambda w: w.id)
        return ' '.join(w.text for w in phrase_words)
    
    def _generate_relative_clause_slots(self, rel_type: str, noun_phrase: str, rel_subject, rel_verb, sentence) -> Dict:
        """é–¢ä¿‚ç¯€ã‚¿ã‚¤ãƒ—åˆ¥ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆï¼ˆå—å‹•æ…‹å¯¾å¿œæ”¹å–„ï¼‰"""
        
        slots = {}
        sub_slots = {}
        
        # å—å‹•æ…‹è£œåŠ©å‹•è©ã®æ¤œå‡º
        aux_word = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'aux:pass')
        if not aux_word:
            aux_word = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'aux')
        
        # ğŸš« å‰¯è©å‡¦ç†ã‚’å‰¯è©ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã«å®Œå…¨å§”è­²ï¼ˆç«¶åˆå›é¿ï¼‰
        # åŠ©å‹•è©ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã¯å‰¯è©ã‚¹ãƒ­ãƒƒãƒˆè¨­å®šã‚’è¡Œã‚ãªã„
        # adverb_word = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'advmod')
        # if adverb_word:
        #     if adverb_word.text.lower() not in ['where', 'when', 'why', 'how']:
        #         if adverb_word.id < rel_verb.id:
        #             sub_slots["sub-m1"] = adverb_word.text
        #             self.logger.debug(f"ğŸ”§ é–¢ä¿‚ç¯€å†…å‰¯è©æ¤œå‡º: sub-m1 = '{adverb_word.text}' (å‹•è©å‰)")
        #         else:
        #             sub_slots["sub-m2"] = adverb_word.text
        #             self.logger.debug(f"ğŸ”§ é–¢ä¿‚ç¯€å†…å‰¯è©æ¤œå‡º: sub-m2 = '{adverb_word.text}' (å‹•è©å¾Œ)")
        
        # ğŸš« å‰ç½®è©å¥å‡¦ç†ã‚‚å‰¯è©ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã«å§”è­²ï¼ˆå®Œå…¨ãªå‰¯è©å‡¦ç†çµ±ä¸€ï¼‰
        # obl_word = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'obl')
        # if obl_word:
        #     sub_slots["sub-m3"] = obl_word.text
        #     self.logger.debug(f"ğŸ”§ é–¢ä¿‚ç¯€å†…å‰¯è©å¥æ¤œå‡º: sub-m3 = '{obl_word.text}'")
        
        if rel_type == 'obj':
            # ç›®çš„èªé–¢ä¿‚ä»£åè©: "The book that he bought"
            # slots["O1"] = ""  # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã¯5æ–‡å‹ã‚¨ãƒ³ã‚¸ãƒ³ã«ä»»ã›ã‚‹
            sub_slots["sub-o1"] = noun_phrase
            if rel_subject:
                sub_slots["sub-s"] = rel_subject.text
            sub_slots["sub-v"] = rel_verb.text
            
        elif rel_type == 'nsubj':
            # ä¸»èªé–¢ä¿‚ä»£åè©: "The man who runs"
            # slots["S"] = ""  # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã¯5æ–‡å‹ã‚¨ãƒ³ã‚¸ãƒ³ã«ä»»ã›ã‚‹
            sub_slots["sub-s"] = noun_phrase
            sub_slots["sub-v"] = rel_verb.text
            
        elif rel_type == 'nsubj:pass':
            # å—å‹•æ…‹ä¸»èªé–¢ä¿‚ä»£åè©: "The car which was crashed"
            # slots["S"] = ""  # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã¯5æ–‡å‹ã‚¨ãƒ³ã‚¸ãƒ³ã«ä»»ã›ã‚‹
            sub_slots["sub-s"] = noun_phrase
            if aux_word:
                sub_slots["sub-aux"] = aux_word.text
            sub_slots["sub-v"] = rel_verb.text
            
        elif rel_type == 'poss':
            # æ‰€æœ‰æ ¼é–¢ä¿‚ä»£åè©: whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†
            
            # âœ… ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æè£œæ­£ãŒã‚ã‚‹å ´åˆã®ç‰¹åˆ¥å‡¦ç†
            if hasattr(sentence, 'hybrid_corrections'):
                # whoseæ§‹æ–‡ã§å‹•è©ãŒè£œæ­£ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ä¸»æ–‡ãƒ»é–¢ä¿‚ç¯€æ§‹é€ ã‚’æ­£ã—ãåˆ†é›¢
                for word_id, correction in sentence.hybrid_corrections.items():
                    if correction['correction_type'] == 'whose_verb_fix':
                        # è£œæ­£ã•ã‚ŒãŸå‹•è©ï¼ˆä¾‹ï¼šlivesï¼‰ã¯ä¸»æ–‡å‹•è©ãªã®ã§ã€é–¢ä¿‚ç¯€ã®å‡¦ç†ã‹ã‚‰é™¤å¤–
                        main_verb_word = self._find_word_by_id(sentence, word_id)
                        if main_verb_word:
                            self.logger.debug(f"ğŸ”§ whoseæ§‹æ–‡ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è£œæ­£: {main_verb_word.text}ã¯ä¸»æ–‡å‹•è©ã¨ã—ã¦å‡¦ç†")
                            # ã“ã®å ´åˆã€é–¢ä¿‚ç¯€ã¯"car is red"ã®éƒ¨åˆ†
                            # rel_verbã¯copula "is"
                            sub_slots["sub-s"] = noun_phrase  # "The man whose car"
                            sub_slots["sub-v"] = rel_verb.text  # "is"
                            
                            # è£œèªã‚’æ¤œå‡ºï¼ˆ"red"ï¼‰
                            complement = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'amod')
                            if complement:
                                sub_slots["sub-c1"] = complement.text
                            
                            # ãƒ¡ã‚¤ãƒ³æ–‡ã¯åˆ¥é€”åŸºæœ¬5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒå‡¦ç†ã™ã‚‹
                            return {"slots": slots, "sub_slots": sub_slots}
            
            # é€šå¸¸ã®whoseæ§‹æ–‡å‡¦ç†
            if rel_subject:
                # åˆ¥ã®ä¸»èªãŒã‚ã‚‹å ´åˆ: "The student whose book I borrowed"
                # â†’ ç›®çš„èªé–¢ä¿‚ä»£åè©ã¨ã—ã¦å‡¦ç†
                sub_slots["sub-o1"] = noun_phrase
                sub_slots["sub-s"] = rel_subject.text
            else:
                # åˆ¥ã®ä¸»èªãŒãªã„å ´åˆ: "The woman whose dog barks"  
                # â†’ ä¸»èªé–¢ä¿‚ä»£åè©ã¨ã—ã¦å‡¦ç†
                sub_slots["sub-s"] = noun_phrase
            
            # é–¢ä¿‚ç¯€å†…ã®å‹•è©ãƒ»è£œèªã‚’æ­£ã—ãæŠ½å‡º
            if aux_word:
                sub_slots["sub-aux"] = aux_word.text
            sub_slots["sub-v"] = rel_verb.text
            
            # whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†ï¼šStanzaã®èª¤è§£æå¯¾å¿œ
            if any(w.text.lower() == 'whose' for w in sentence.words):
                # acl:relclã¨ã—ã¦è§£æã•ã‚ŒãŸlivesï¼ˆid=7ï¼‰ã®ä¾å­˜èªã‹ã‚‰redã‚’æ¢ã™
                acl_relcl_word = self._find_word_by_deprel(sentence, 'acl:relcl')
                if acl_relcl_word:
                    complement = self._find_word_by_head_and_deprel(sentence, acl_relcl_word.id, 'amod')
                    if complement:
                        sub_slots["sub-c1"] = complement.text
            else:
                # é€šå¸¸ã®è£œèªæ¤œå‡º
                complement = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'acomp')  # å½¢å®¹è©è£œèª
                if not complement:
                    complement = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'attr')  # å±æ€§è£œèª
                if not complement:
                    complement = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'nmod')  # åè©ä¿®é£¾
                if complement:
                    sub_slots["sub-c1"] = complement.text
            
        elif rel_type == 'advmod':
            # é–¢ä¿‚å‰¯è©: "The place where we met" â†’ sub-m2="The place where", sub-s="we", sub-v="met"
            # noun_phraseã«ã¯æ—¢ã«é–¢ä¿‚å‰¯è©ãŒå«ã¾ã‚Œã¦ã„ã‚‹
            
            # sub-m2: å…ˆè¡Œè©å¥ï¼ˆé–¢ä¿‚å‰¯è©ä»˜ãï¼‰
            sub_slots["sub-m2"] = noun_phrase
            
            # sub-s: é–¢ä¿‚ç¯€ä¸»èªã®ã¿
            if rel_subject:
                sub_slots["sub-s"] = rel_subject.text
                
            sub_slots["sub-v"] = rel_verb.text
            
            # é–¢ä¿‚å‰¯è©å¥å†…ã®ç›®çš„èªã‚’æ¤œå‡ºã—ã¦sub-o1ã«é…ç½®
            obj_word = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'obj')
            if obj_word:
                sub_slots["sub-o1"] = obj_word.text
                self.logger.debug(f"ğŸ”§ é–¢ä¿‚å‰¯è©å¥å†…ç›®çš„èªæ¤œå‡º: sub-o1 = '{obj_word.text}'")
            
        # çœç•¥é–¢ä¿‚ä»£åè©ã®å‡¦ç†
        elif rel_type == 'obj_omitted':
            # çœç•¥ç›®çš„èªé–¢ä¿‚ä»£åè©: "The book I read"
            # ğŸ”§ ä¿®æ­£: ç©ºæ–‡å­—ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆã‚’å›é¿
            # slots["S"] = ""  # ä¸»ç¯€ä¸»èªã‚’ç©ºã«è¨­å®šã¯å±é™º â†’ 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã«å§”è­²
            
            # å…ˆè¡Œè©ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰[omitted]ã‚’é™¤å»
            clean_noun_phrase = noun_phrase.replace(" [omitted]", "").replace("[omitted]", "")
            
            # å¾“å±ç¯€ä¸»èªã‚’æ¤œå‡ºï¼ˆé–¢ä¿‚ç¯€å‹•è©ã®nsubjï¼‰
            rel_subject = self._find_word_by_head_and_deprel(sentence, rel_verb.id, 'nsubj')
            if rel_subject and rel_subject.text.strip():  # ç©ºæ–‡å­—ãƒã‚§ãƒƒã‚¯è¿½åŠ 
                sub_slots["sub-o1"] = clean_noun_phrase
                sub_slots["sub-s"] = rel_subject.text
                sub_slots["sub-v"] = rel_verb.text
                self.logger.debug(f"ğŸ”§ çœç•¥ç›®çš„èªé–¢ä¿‚ç¯€: sub-s = '{rel_subject.text}'")
            elif clean_noun_phrase.strip():  # ç©ºæ–‡å­—ãƒã‚§ãƒƒã‚¯è¿½åŠ 
                sub_slots["sub-o1"] = clean_noun_phrase
                if rel_verb.text.strip():  # ç©ºæ–‡å­—ãƒã‚§ãƒƒã‚¯è¿½åŠ 
                    sub_slots["sub-v"] = rel_verb.text
            
        elif rel_type == 'nsubj_omitted':  
            # çœç•¥ä¸»èªé–¢ä¿‚ä»£åè©: "The person standing there"
            # slots["O1"] = ""  # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã¯5æ–‡å‹ã‚¨ãƒ³ã‚¸ãƒ³ã«ä»»ã›ã‚‹
            sub_slots["sub-o1"] = noun_phrase
            sub_slots["sub-v"] = rel_verb.text
            
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆç›®çš„èªæ‰±ã„ï¼‰
            # slots["O1"] = ""  # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã¯5æ–‡å‹ã‚¨ãƒ³ã‚¸ãƒ³ã«ä»»ã›ã‚‹
            sub_slots["sub-o1"] = noun_phrase
            if rel_subject:
                sub_slots["sub-s"] = rel_subject.text
            sub_slots["sub-v"] = rel_verb.text
        
        return {"slots": slots, "sub_slots": sub_slots}
    
    def _generate_whose_relative_clause_slots(self, antecedent, cop_verb, sentence) -> Dict:
        """whoseæ§‹æ–‡å°‚ç”¨ã®é–¢ä¿‚ç¯€ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆï¼ˆãƒ¡ã‚¤ãƒ³æ–‡ã‚’å¦¨å®³ã—ãªã„ï¼‰"""
        
        slots = {}  # ãƒ¡ã‚¤ãƒ³æ–‡ã‚¹ãƒ­ãƒƒãƒˆã¯å¤‰æ›´ã—ãªã„
        sub_slots = {}
        
        # whoseæ§‹æ–‡ã®é–¢ä¿‚ç¯€: "whose car is red"
        # æ‰€æœ‰æ ¼é–¢ä¿‚ä»£åè©ã‚’å«ã‚€å…ˆè¡Œè©å¥ã‚’æ§‹ç¯‰
        whose_word = None
        car_word = None
        
        for word in sentence.words:
            if word.text.lower() == 'whose':
                whose_word = word
                # whoseãŒä¾å­˜ã™ã‚‹èªï¼ˆcarï¼‰ã‚’å–å¾—
                car_word = self._find_word_by_id(sentence, whose_word.head)
                break
        
        if whose_word and car_word:
            # "The man whose car"ã®æ§‹ç¯‰
            man_phrase = self._build_phrase_with_modifiers(sentence, antecedent)
            whose_car_phrase = f"{man_phrase} {whose_word.text} {car_word.text}"
            
            sub_slots["sub-s"] = whose_car_phrase
            sub_slots["sub-v"] = cop_verb.text  # "is"
            
            # è£œèªï¼ˆredï¼‰ã‚’å–å¾—
            complement = self._find_word_by_id(sentence, cop_verb.head)
            if complement:
                sub_slots["sub-c1"] = complement.text
            
            self.logger.debug(f"ğŸ”§ whoseé–¢ä¿‚ç¯€ã‚¹ãƒ­ãƒƒãƒˆ: sub-s='{whose_car_phrase}', sub-v='{cop_verb.text}', sub-c1='{complement.text if complement else ''}'")
        
        return {"slots": slots, "sub_slots": sub_slots}
    
    def _process_main_clause_after_relative(self, sentence, antecedent, rel_verb, noun_phrase) -> Optional[Dict]:
        """é–¢ä¿‚ç¯€å‡¦ç†å¾Œã®ä¸»æ–‡éƒ¨åˆ†ã‚’5æ–‡å‹ã§å‡¦ç†"""
        
        # ä¸»æ–‡ã®å‹•è©ï¼ˆROOTèªï¼‰ã‚’ç‰¹å®š
        main_verb = self._find_root_word(sentence)
        if not main_verb:
            self.logger.debug("  âš ï¸ ä¸»æ–‡å‹•è©ãªã—")
            return None
            
        if main_verb.id == rel_verb.id:
            self.logger.debug(f"  âš ï¸ é–¢ä¿‚ç¯€å‹•è©ãŒROOT - ä¸»æ–‡ãªã— (main_verb={main_verb.text}, rel_verb={rel_verb.text})")
            return None
        
        self.logger.debug(f"  ğŸ” ä¸»æ–‡å‹•è©æ¤œå‡º: {main_verb.text} (id: {main_verb.id}, POS: {main_verb.upos})")
        
        # ä¾å­˜é–¢ä¿‚ãƒãƒƒãƒ—æ§‹ç¯‰ï¼ˆé–¢ä¿‚ç¯€ã‚’é™¤å¤–ï¼‰
        dep_relations = {}
        excluded_words = []
        
        for word in sentence.words:
            # é–¢ä¿‚ç¯€å†…ã®èªã‚’ã‚¹ã‚­ãƒƒãƒ—
            if self._is_word_in_relative_clause(word, rel_verb):
                excluded_words.append(word.text)
                continue
                
            if word.deprel not in dep_relations:
                dep_relations[word.deprel] = []
            dep_relations[word.deprel].append(word)
        
        self.logger.debug(f"  ğŸš« é™¤å¤–èª: {excluded_words}")
        self.logger.debug(f"  ğŸ“ ä¸»æ–‡ä¾å­˜é–¢ä¿‚: {list(dep_relations.keys())}")
        
        # åŸºæœ¬5æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        pattern_result = self._detect_basic_five_pattern(main_verb, dep_relations, None, sentence)
        if not pattern_result:
            self.logger.debug("  âŒ ä¸»æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå¤±æ•—")
            return None
        
        self.logger.debug(f"  ğŸ¯ ä¸»æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º: {pattern_result['pattern']}")
        
        # ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆï¼ˆSã‚¹ãƒ­ãƒƒãƒˆã¯ç©ºã«ã—ã¦æ§‹é€ ã‚’ç¶­æŒï¼‰
        # é–¢ä¿‚ç¯€ãŒSã‚¹ãƒ­ãƒƒãƒˆã‚’å æœ‰ã—ã¦ã„ã‚‹ã“ã¨ã‚’ä¼é”
        five_pattern_slots = self._generate_basic_five_slots(
            pattern_result['pattern'], pattern_result['mapping'], dep_relations, sentence, {'S': ""}
        )
        
        # é–¢ä¿‚ç¯€ã‚’å«ã‚€ä¸»èªã¯ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«ã‚ã‚‹ãŸã‚ã€ä¸Šä½Sã‚¹ãƒ­ãƒƒãƒˆã¯Noneã¾ãŸã¯ç©º
        if 'slots' in five_pattern_slots and 'S' in five_pattern_slots['slots']:
            five_pattern_slots['slots']['S'] = ""  # é–¢ä¿‚ç¯€ãŒã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«å«ã¾ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã™
            
        self.logger.debug(f"  âœ… ä¸»æ–‡å‡¦ç†å®Œäº†: ãƒ‘ã‚¿ãƒ¼ãƒ³={pattern_result['pattern']}")
        return five_pattern_slots
    
    def _is_word_in_relative_clause(self, word, rel_verb) -> bool:
        """èªãŒé–¢ä¿‚ç¯€å†…ã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        
        # é–¢ä¿‚ç¯€å‹•è©è‡ªèº«
        if word.id == rel_verb.id:
            return True
            
        # é–¢ä¿‚ç¯€å‹•è©ã®ç›´æ¥ä¾å­˜èªï¼ˆå…¨ç¨®é¡ï¼‰
        if word.head == rel_verb.id:
            return True
            
        # é–¢ä¿‚ä»£åè©ï¼ˆé–¢ä¿‚ç¯€å‹•è©ã«ä¾å­˜ã™ã‚‹nsubj/objç­‰ï¼‰
        if word.deprel in ['nsubj', 'obj', 'advmod', 'obl', 'aux', 'aux:pass', 'acomp', 'attr', 'nmod'] and word.head == rel_verb.id:
            return True
        
        # é–¢ä¿‚ç¯€ã‚’ä¿®é£¾ã™ã‚‹acl:relclã®ä¾å­˜èª
        if word.deprel == 'acl:relcl':
            return True
            
        return False
    
    def _get_all_dependents(self, head_word) -> List:
        """æŒ‡å®šèªã®ã™ã¹ã¦ã®ä¾å­˜èªã‚’å–å¾—"""
        # ã“ã®å®Ÿè£…ã§ã¯ã€sentenceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã§ããªã„ãŸã‚
        # ç°¡æ˜“å®Ÿè£…ã¨ã—ã¦ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™
        # å®Ÿéš›ã®ä½¿ç”¨ã§ã¯ã€sentence.wordsã‚’é€šã˜ã¦ä¾å­˜èªã‚’æ¤œç´¢ã™ã‚‹
        return []
    
    def _get_relative_clause_text(self, sentence, rel_verb) -> str:
        """é–¢ä¿‚ç¯€ã®ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã‚’æŠ½å‡ºã™ã‚‹"""
        # é–¢ä¿‚ç¯€ã«å«ã¾ã‚Œã‚‹èªã‚’åé›†
        rel_words = []
        
        # é–¢ä¿‚å‹•è©ã«ç›´æ¥ä¾å­˜ã™ã‚‹èªã‚’åé›†
        for word in sentence.words:
            if word.head == rel_verb.id:
                rel_words.append(word)
        
        # é–¢ä¿‚å‹•è©è‡ªèº«ã‚‚å«ã‚ã‚‹
        rel_words.append(rel_verb)
        
        # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
        rel_words.sort(key=lambda w: w.id)
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
        return " ".join([w.text for w in rel_words])
    
    # === Stanzaè§£æãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ ===
    
    # === æ±ç”¨ä½ç½®æ±ºå®šã‚·ã‚¹ãƒ†ãƒ  ===
    
    def _determine_element_position(self, sentence, target_element) -> str:
        """æ±ç”¨ä½ç½®æ±ºå®š: ä»»æ„ã®è¦ç´ ãŒãƒ¡ã‚¤ãƒ³æ–‡ã®ã©ã®ä½ç½®ã«ã‚ã‚‹ã‹ã‚’åˆ¤å®š
        
        Args:
            sentence: Stanza sentence object
            target_element: ä½ç½®ã‚’åˆ¤å®šã—ãŸã„è¦ç´ ï¼ˆå…ˆè¡Œè©ã€å¾“å±ç¯€ã®é ­èªç­‰ï¼‰
            
        Returns:
            str: ä½ç½®åï¼ˆS, O1, O2, C1, C2, M1, M2, M3ï¼‰
        """
        try:
            # ãƒ¡ã‚¤ãƒ³å‹•è©ï¼ˆROOTï¼‰ã‚’å–å¾—
            main_verb = self._find_root_word(sentence)
            if not main_verb:
                self.logger.debug(f"âš ï¸ ãƒ¡ã‚¤ãƒ³å‹•è©ãŒè¦‹ã¤ã‹ã‚‰ãªã„ - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½ç½®: S")
                return 'S'
            
            # è¦ç´ ã®ä¾å­˜é–¢ä¿‚ã‚’ç¢ºèª
            element_deprel = target_element.deprel
            element_head = target_element.head
            
            self.logger.debug(f"ğŸ” æ±ç”¨ä½ç½®åˆ¤å®š: {target_element.text} (deprel: {element_deprel}, head: {element_head}, main_verb: {main_verb.text})")
            
            # è¦ç´ ãŒãƒ¡ã‚¤ãƒ³å‹•è©ã«ç›´æ¥ä¾å­˜ã—ã¦ã„ã‚‹å ´åˆã®ä½ç½®åˆ¤å®š
            if element_head == main_verb.id:
                return self._classify_position_by_deprel(element_deprel)
            
            # è¦ç´ ãŒé–“æ¥çš„ã«é–¢é€£ã—ã¦ã„ã‚‹å ´åˆã®å‡¦ç†
            elif element_deprel in ['compound', 'amod', 'det']:
                # è¦ç´ ã®é ­èªã‚’ç¢ºèªã—ã¦ãã®ä½ç½®ã‚’åˆ¤å®šï¼ˆå†å¸°ï¼‰
                head_word = self._find_word_by_id(sentence, element_head)
                if head_word:
                    return self._determine_element_position(sentence, head_word)
            
            # ãã®ä»–ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            self.logger.debug(f"ğŸ“ è¦ç´ ä½ç½®: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ Sä½ç½®é©ç”¨ (deprel: {element_deprel})")
            return 'S'
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ æ±ç”¨ä½ç½®åˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
            return 'S'
    
    def _classify_position_by_deprel(self, deprel: str) -> str:
        """ä¾å­˜é–¢ä¿‚ãƒ©ãƒ™ãƒ«ã«åŸºã¥ãä½ç½®åˆ†é¡
        
        Args:
            deprel: ä¾å­˜é–¢ä¿‚ãƒ©ãƒ™ãƒ«
            
        Returns:
            str: ä½ç½®å
        """
        # ä¸»èªç³»
        if deprel in ['nsubj', 'nsubj:pass']:
            return 'S'
        
        # ç›®çš„èªç³»
        elif deprel in ['obj', 'dobj']:
            return 'O1'
        elif deprel in ['iobj']:
            return 'O2'
        
        # è£œèªç³»
        elif deprel in ['acomp', 'attr', 'nmod:tmod']:
            return 'C1'
        elif deprel in ['xcomp']:  # ç¬¬2è£œèª
            return 'C2'
        
        # ä¿®é£¾èªç³»ï¼ˆå‰¯è©çš„ï¼‰
        elif deprel in ['obl', 'advmod', 'nmod']:
            return self._determine_m_slot_position(deprel)
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        else:
            return 'S'
    
    def _determine_m_slot_position(self, deprel: str) -> str:
        """M-ã‚¹ãƒ­ãƒƒãƒˆã®è©³ç´°ä½ç½®æ±ºå®š
        
        Args:
            deprel: ä¾å­˜é–¢ä¿‚ãƒ©ãƒ™ãƒ«
            
        Returns:
            str: M1, M2, M3ã®ã„ãšã‚Œã‹
        """
        # ç¾åœ¨ã¯ã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿè£…
        # TODO: æ–‡è„ˆã‚„æ„å‘³ã«åŸºã¥ãã‚ˆã‚Šè©³ç´°ãªåˆ†é¡
        if deprel == 'advmod':
            return 'M2'  # å‰¯è©ã¯M2
        elif deprel == 'obl':
            return 'M3'  # å‰ç½®è©å¥ã¯M3
        else:
            return 'M1'  # ãã®ä»–ã¯M1
    
    def _apply_position_to_sub_slots(self, result: Dict, sub_slots: Dict, position: str, handler_name: str = "") -> None:
        """ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«ä½ç½®æƒ…å ±ã‚’é©ç”¨ï¼ˆæ±ç”¨ç‰ˆï¼‰
        
        Args:
            result: å‡¦ç†çµæœè¾æ›¸
            sub_slots: ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆè¾æ›¸
            position: ä½ç½®å
            handler_name: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼åï¼ˆãƒ­ã‚°ç”¨ï¼‰
        """
        if 'slot_positions' not in result:
            result['slot_positions'] = {}
        
        for sub_slot_name in sub_slots:
            result['slot_positions'][sub_slot_name] = position
            self.logger.debug(f"ğŸ“ æ±ç”¨ä½ç½®æƒ…å ±è¨˜éŒ²[{handler_name}]: {sub_slot_name} â†’ {position}ä½ç½®")
    
    def _determine_antecedent_position(self, sentence, antecedent) -> str:
        """å…ˆè¡Œè©ãŒãƒ¡ã‚¤ãƒ³æ–‡ã®ã©ã®ä½ç½®ã«ã‚ã‚‹ã‹ã‚’åˆ¤å®šï¼ˆäº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼ï¼‰"""
        return self._determine_element_position(sentence, antecedent)
    
    # === æ±ç”¨ç¯€å‡¦ç†ãƒ˜ãƒ«ãƒ‘ãƒ¼ ===
    
    def _process_clause_at_position(self, sentence, clause_elements: List, handler_name: str, **kwargs) -> Dict:
        """æ±ç”¨çš„ãªç¯€å‡¦ç†: ä»»æ„ã®ä½ç½®ã®ç¯€ã‚’å‡¦ç†
        
        Args:
            sentence: Stanza sentence object
            clause_elements: ç¯€ã®è¦ç´ ãƒªã‚¹ãƒˆ
            handler_name: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å
            **kwargs: è¿½åŠ ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å›ºæœ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        Returns:
            Dict: å‡¦ç†çµæœï¼ˆã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆï¼‹ä½ç½®æƒ…å ±ä»˜ãï¼‰
        """
        result = {}
        
        try:
            if not clause_elements:
                return result
            
            # ç¯€ã®ä¸»è¦è¦ç´ ï¼ˆé ­èªï¼‰ã‚’ç‰¹å®š
            main_element = clause_elements[0]  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ€åˆã®è¦ç´ 
            
            # æ„å‘³çš„ãªé ­èªã‚’æ¢ã™ï¼ˆå‹•è©ã€åè©ãªã©ï¼‰
            for element in clause_elements:
                if element.upos in ['VERB', 'NOUN', 'ADJ']:
                    main_element = element
                    break
            
            # ä½ç½®ã‚’æ±ºå®š
            position = self._determine_element_position(sentence, main_element)
            
            self.logger.debug(f"ğŸ”„ æ±ç”¨ç¯€å‡¦ç†[{handler_name}]: ä½ç½®={position}, è¦ç´ æ•°={len(clause_elements)}")
            
            # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å›ºæœ‰ã®å‡¦ç†ã‚’å®Ÿè¡Œ
            if handler_name == "relative_clause":
                sub_slots = self._extract_relative_clause_components(clause_elements, **kwargs)
            elif handler_name == "noun_clause":
                sub_slots = self._extract_noun_clause_components(clause_elements, **kwargs)
            elif handler_name == "adverbial_clause":
                sub_slots = self._extract_adverbial_clause_components(clause_elements, **kwargs)
            else:
                # æ±ç”¨çš„ãªæŠ½å‡º
                sub_slots = self._extract_generic_clause_components(clause_elements)
            
            # ä½ç½®æƒ…å ±ã‚’é©ç”¨
            self._apply_position_to_sub_slots(result, sub_slots, position, handler_name)
            
            # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã‚’çµæœã«è¿½åŠ 
            result.update(sub_slots)
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ æ±ç”¨ç¯€å‡¦ç†ã‚¨ãƒ©ãƒ¼[{handler_name}]: {e}")
            return {}
    
    def _extract_generic_clause_components(self, clause_elements: List) -> Dict:
        """æ±ç”¨çš„ãªç¯€è¦ç´ æŠ½å‡º
        
        Args:
            clause_elements: ç¯€ã®è¦ç´ ãƒªã‚¹ãƒˆ
            
        Returns:
            Dict: ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆè¾æ›¸
        """
        sub_slots = {}
        
        try:
            # åŸºæœ¬çš„ãªå½¹å‰²åˆ†é¡
            subjects = []
            verbs = []
            objects = []
            modifiers = []
            
            for word in clause_elements:
                if word.deprel in ['nsubj', 'nsubj:pass']:
                    subjects.append(word.text)
                elif word.upos == 'VERB':
                    verbs.append(word.text)
                elif word.deprel in ['obj', 'dobj']:
                    objects.append(word.text)
                else:
                    modifiers.append(word.text)
            
            # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®
            if subjects:
                sub_slots['sub-s'] = ' '.join(subjects)
            if verbs:
                sub_slots['sub-v'] = ' '.join(verbs)
            if objects:
                sub_slots['sub-o1'] = ' '.join(objects)
            if modifiers:
                sub_slots['sub-m2'] = ' '.join(modifiers)
            
            return sub_slots
            
        except Exception as e:
            self.logger.error(f"âŒ æ±ç”¨è¦ç´ æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def _extract_noun_clause_components(self, clause_elements: List, **kwargs) -> Dict:
        """åè©ç¯€è¦ç´ æŠ½å‡ºï¼ˆthatç¯€ã€whç¯€ãªã©ï¼‰
        
        Args:
            clause_elements: ç¯€ã®è¦ç´ ãƒªã‚¹ãƒˆ
            **kwargs: è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        Returns:
            Dict: ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆè¾æ›¸
        """
        # ç¾åœ¨ã¯æ±ç”¨å®Ÿè£…ã‚’ä½¿ç”¨
        # TODO: åè©ç¯€å›ºæœ‰ã®å‡¦ç†ã‚’å®Ÿè£…
        return self._extract_generic_clause_components(clause_elements)
    
    def _extract_adverbial_clause_components(self, clause_elements: List, **kwargs) -> Dict:
        """å‰¯è©ç¯€è¦ç´ æŠ½å‡ºï¼ˆwhenç¯€ã€becauseç¯€ãªã©ï¼‰
        
        Args:
            clause_elements: ç¯€ã®è¦ç´ ãƒªã‚¹ãƒˆ
            **kwargs: è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        Returns:
            Dict: ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆè¾æ›¸
        """
        # ç¾åœ¨ã¯æ±ç”¨å®Ÿè£…ã‚’ä½¿ç”¨
        # TODO: å‰¯è©ç¯€å›ºæœ‰ã®å‡¦ç†ã‚’å®Ÿè£…
        return self._extract_generic_clause_components(clause_elements)
    
    def _find_word_by_deprel(self, sentence, deprel: str):
        """ä¾å­˜é–¢ä¿‚ã§èªã‚’æ¤œç´¢"""
        return next((w for w in sentence.words if w.deprel == deprel), None)
    
    def _find_word_by_id(self, sentence, word_id: int):
        """IDã§èªã‚’æ¤œç´¢"""
        return next((w for w in sentence.words if w.id == word_id), None)
    
    def _find_word_by_head_and_deprel(self, sentence, head_id: int, deprel: str):
        """é ­IDã¨ä¾å­˜é–¢ä¿‚ã§èªã‚’æ¤œç´¢"""
        return next((w for w in sentence.words if w.head == head_id and w.deprel == deprel), None)
    
    def _find_main_verb(self, sentence):
        """ä¸»æ–‡ã®å‹•è©ã‚’æ¤œç´¢ï¼ˆé–¢ä¿‚ç¯€ã‚’é™¤å¤–ãƒ»ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æå¯¾å¿œï¼‰"""
        
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æã®è£œæ­£æƒ…å ±ã‚’ãƒã‚§ãƒƒã‚¯
        if hasattr(sentence, 'hybrid_corrections'):
            for word in sentence.words:
                if word.id in sentence.hybrid_corrections:
                    correction = sentence.hybrid_corrections[word.id]
                    if correction['correction_type'] == 'whose_verb_fix':
                        # è£œæ­£ã•ã‚ŒãŸå‹•è©ã‚’ä¸»æ–‡å‹•è©ã¨ã—ã¦è¿”ã™
                        self.logger.debug(f"ğŸ”§ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æ: ä¸»æ–‡å‹•è©ã¨ã—ã¦ {word.text} ã‚’ä½¿ç”¨ (è£œæ­£æ¸ˆã¿)")
                        return word
        
        # é€šå¸¸ã®å ´åˆï¼šrootã‚’æ¤œç´¢ï¼ˆwhoseæ§‹æ–‡ã§ã‚‚å…ˆã«ãƒã‚§ãƒƒã‚¯ï¼‰
        root_word = None
        for word in sentence.words:
            if word.head == 0:  # root
                root_word = word
                break
        
        # whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†ï¼šrootãŒå­˜åœ¨ã›ãšã€acl:relclèªãŒãƒ¡ã‚¤ãƒ³å‹•è©å€™è£œã®å ´åˆã®ã¿
        if any(w.text.lower() == 'whose' for w in sentence.words) and not root_word:
            acl_relcl_word = self._find_word_by_deprel(sentence, 'acl:relcl')
            if (acl_relcl_word and 
                acl_relcl_word.text.lower() in ['lives', 'works', 'runs', 'goes'] and
                acl_relcl_word.lemma in ['live', 'work', 'run', 'go']):
                self.logger.debug(f"ğŸ”§ whoseæ§‹æ–‡: ä¸»æ–‡å‹•è©ã¨ã—ã¦ {acl_relcl_word.text} ã‚’ä½¿ç”¨")
                return acl_relcl_word
        
        # é€šå¸¸ã®å ´åˆï¼šrootã‚’æ¤œç´¢ï¼ˆwhoseæ§‹æ–‡ã§ã‚‚å…ˆã«ãƒã‚§ãƒƒã‚¯ï¼‰
        root_word = None
        for word in sentence.words:
            if word.head == 0:  # root
                root_word = word
                break
        
        # whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†ï¼šrootãŒå­˜åœ¨ã›ãšã€acl:relclèªãŒãƒ¡ã‚¤ãƒ³å‹•è©å€™è£œã®å ´åˆã®ã¿
        if any(w.text.lower() == 'whose' for w in sentence.words) and not root_word:
            acl_relcl_word = self._find_word_by_deprel(sentence, 'acl:relcl')
            if (acl_relcl_word and 
                acl_relcl_word.text.lower() in ['lives', 'works', 'runs', 'goes'] and
                acl_relcl_word.lemma in ['live', 'work', 'run', 'go']):
                self.logger.debug(f"ğŸ”§ whoseæ§‹æ–‡: ä¸»æ–‡å‹•è©ã¨ã—ã¦ {acl_relcl_word.text} ã‚’ä½¿ç”¨")
                return acl_relcl_word
        
        if not root_word:
            return None
            
        # rootãŒå½¢å®¹è©ã®å ´åˆã®ç‰¹åˆ¥å‡¦ç†
        if root_word.upos == 'ADJ':
            # whenæ§‹æ–‡ã§ã¯å½¢å®¹è©ã‚’ä¸»å‹•è©ã¨ã—ã¦æ‰±ã†ï¼ˆRephraseä»•æ§˜ï¼‰
            if any(w.text.lower() == 'when' for w in sentence.words):
                self.logger.debug(f"ğŸ”§ whenæ§‹æ–‡: å½¢å®¹è©ã‚’ä¸»å‹•è©ã¨ã—ã¦ä½¿ç”¨ {root_word.text}")
                return root_word
            else:
                # é€šå¸¸ã®å ´åˆï¼šcopå‹•è©ã‚’ä¸»å‹•è©ã¨ã™ã‚‹ï¼ˆ"The man is strong"æ§‹é€ ï¼‰
                cop_verb = self._find_word_by_head_and_deprel(sentence, root_word.id, 'cop')
                if cop_verb:
                    return cop_verb
        
        return root_word
    
    def _build_full_subject_with_relative_clause(self, sentence, antecedent, rel_verb):
        """é–¢ä¿‚ç¯€ã‚’å«ã‚€å®Œå…¨ãªä¸»èªå¥ã‚’æ§‹ç¯‰"""
        # å…ˆè¡Œè©ã‹ã‚‰é–‹å§‹
        subject_phrase = antecedent.text
        
        # å…ˆè¡Œè©ã®ä¿®é£¾èªã‚’è¿½åŠ 
        modifiers = []
        for word in sentence.words:
            if word.head == antecedent.id and word.id != rel_verb.id:
                if word.deprel in ['det', 'amod', 'compound']:
                    modifiers.append((word.id, word.text))
        
        # ä¿®é£¾èªã‚’ä½ç½®é †ã§ã‚½ãƒ¼ãƒˆ
        modifiers.sort(key=lambda x: x[0])
        
        # å®Œå…¨ãªä¸»èªå¥ã‚’æ§‹ç¯‰
        if modifiers:
            modifier_text = ' '.join([m[1] for m in modifiers])
            subject_phrase = f"{modifier_text} {subject_phrase}"
        
        return subject_phrase
    
    def _is_whose_construction(self, sentence, rel_verb):
        """whoseæ§‹æ–‡ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        # whoseãŒå­˜åœ¨ã—ã€ã‹ã¤rel_verbã®ä¾å­˜èªã«copãŒã‚ã‚‹å ´åˆ
        has_whose = any(w.text.lower() == 'whose' for w in sentence.words)
        has_cop_child = any(w.head == rel_verb.id and w.deprel == 'cop' for w in sentence.words)
        return has_whose and has_cop_child
    
    def _find_cop_verb_in_whose_clause(self, sentence, rel_verb):
        """whoseæ§‹æ–‡ã§ã®å®Ÿéš›ã®é–¢ä¿‚ç¯€å‹•è©ï¼ˆcopï¼‰ã‚’æ¤œç´¢"""
        # rel_verbã®ä¾å­˜èªã§copã®ã‚‚ã®ã‚’æ¢ã™
        cop_verb = next((w for w in sentence.words if w.head == rel_verb.id and w.deprel == 'cop'), None)
        return cop_verb
    
    def _find_whose_antecedent(self, sentence):
        """whoseæ§‹æ–‡ã®å…ˆè¡Œè©ã‚’æ¤œç´¢"""
        # rootä¸»èªã‚’å–å¾—ï¼ˆé€šå¸¸ã¯å…ˆè¡Œè©ï¼‰
        for word in sentence.words:
            if word.head == 0 and word.deprel == 'root':
                return word
        return None
    
    def _handle_basic_five_pattern(self, sentence, base_result: Dict, shared_context: Dict = None) -> Optional[Dict]:
        """
        åŸºæœ¬5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆåˆ†è©æ§‹æ–‡åˆ¶å¾¡ãƒ•ãƒ©ã‚°å¯¾å¿œ + ãƒãƒ³ãƒ‰ãƒ©ãƒ¼é–“æƒ…å ±å…±æœ‰ï¼‰
        
        basic_five_pattern_engine.py ã®æ©Ÿèƒ½ã‚’çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã«ç§»æ¤
        Stanza dependency parsing ã«ã‚ˆã‚‹åŸºæœ¬æ–‡å‹æ¤œå‡ºãƒ»åˆ†è§£
        åˆ†è©æ§‹æ–‡ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€é©åˆ‡ã«ä¸»èªã‚¹ãƒ­ãƒƒãƒˆã‚’åˆ¶å¾¡
        
        Args:
            sentence: Stanza sentence object
            base_result: åŸºæœ¬çµæœè¾æ›¸
            shared_context: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼é–“å…±æœ‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            Optional[Dict]: 5æ–‡å‹å‡¦ç†çµæœ or None
        """
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
        if shared_context is None:
            shared_context = {'predefined_slots': {}, 'handler_metadata': {}}
            
        try:
            self.logger.debug("ğŸ” 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè¡Œä¸­...")
            
            # ğŸ¤ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼é–“æƒ…å ±å…±æœ‰: äº‹å‰ç¢ºå®šã•ã‚ŒãŸã‚¹ãƒ­ãƒƒãƒˆå€¤ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ãªãŸã®æ­£ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰
            predefined_slots = shared_context.get('predefined_slots', {})
            self.logger.debug(f"ğŸ” shared_contextå†…å®¹: {shared_context}")
            if predefined_slots:
                self.logger.debug(f"ğŸ¤ äº‹å‰ç¢ºå®šã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡º: {predefined_slots} - ã“ã‚Œã‚‰ã®å€¤ã‚’ä½¿ç”¨ã—ã¦æ®‹ã‚Šã‚’åˆ†è§£")
            else:
                self.logger.debug(f"ğŸ” äº‹å‰ç¢ºå®šã‚¹ãƒ­ãƒƒãƒˆãªã—: predefined_slots = {predefined_slots}")
            
            # âŒ æ—§æ–¹å¼: å æœ‰æƒ…å ±ã«ã‚ˆã‚‹ã‚¹ã‚­ãƒƒãƒ—æ–¹å¼ï¼ˆå‰Šé™¤ï¼‰
            # occupied_slots = shared_context.get('occupied_main_slots', set())
            # if occupied_slots:
            #     self.logger.debug(f"ğŸ¤ å æœ‰æ¸ˆã¿ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡º: {occupied_slots} - éƒ¨åˆ†çš„ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚’å®Ÿè¡Œ")
            
            # åˆ†è©æ§‹æ–‡åˆ¶å¾¡ãƒ•ãƒ©ã‚°ã‚’ãƒã‚§ãƒƒã‚¯
            grammar_info = base_result.get('grammar_info', {})
            control_flags = grammar_info.get('control_flags', {})
            participle_detected = control_flags.get('participle_detected', False)
            
            if participle_detected:
                self.logger.debug("  âœ… åˆ†è©æ§‹æ–‡æ¤œå‡ºæ¸ˆã¿ - åˆ†è©æ§‹æ–‡ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨")
                return self._process_basic_pattern_with_participle_control(sentence, base_result)
            
            # ä»–ã®ã‚¨ãƒ³ã‚¸ãƒ³ãŒä¸»æ–‡å‹•è©ï¼ˆVï¼‰ã‚’æ—¢ã«å‡¦ç†æ¸ˆã¿ã®å ´åˆã®ã¿ã‚¹ã‚­ãƒƒãƒ—
            # sub-vã¯é–¢ä¿‚ç¯€å‹•è©ãªã®ã§ä¸»æ–‡å‡¦ç†ã«ã¯å½±éŸ¿ã—ãªã„
            if base_result.get('slots', {}).get('V'):
                self.logger.debug("  ä¸»æ–‡å‹•è©(V)ãŒå‡¦ç†æ¸ˆã¿ - ã‚¹ã‚­ãƒƒãƒ—")
                return None
            
            return self._process_basic_five_pattern_structure(sentence, base_result, predefined_slots)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ 5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _process_basic_five_pattern_structure(self, sentence, base_result: Dict, predefined_slots: dict = None) -> Dict:
        """åŸºæœ¬5æ–‡å‹æ§‹é€ ã®åˆ†è§£å‡¦ç†ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æå¯¾å¿œï¼‰"""
        
        # âœ… ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æè£œæ­£æƒ…å ±ã‚’å„ªå…ˆçš„ã«åˆ©ç”¨
        root_word = None
        is_whose_construction = any(w.text.lower() == 'whose' for w in sentence.words)
        
        if hasattr(sentence, 'hybrid_corrections'):
            # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æã§VERBã¨ã—ã¦è£œæ­£ã•ã‚ŒãŸèªã‚’ä¸»æ–‡å‹•è©ã¨ã—ã¦æ¡ç”¨
            for word_id, correction in sentence.hybrid_corrections.items():
                if correction['correction_type'] == 'whose_verb_fix':
                    root_word = self._find_word_by_id(sentence, word_id)
                    if root_word:
                        self.logger.debug(f"ğŸ”§ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æ: {root_word.text} ã‚’ãƒ¡ã‚¤ãƒ³å‹•è©ã¨ã—ã¦ä½¿ç”¨")
                        break
        
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æãŒãªã„å ´åˆã®å¾“æ¥å‡¦ç†        
        if not root_word and is_whose_construction:
            # acl:relclé–¢ä¿‚ã«ã‚ã‚‹èªã‚’ç¢ºèª
            acl_relcl_word = self._find_word_by_deprel(sentence, 'acl:relcl')
            if (acl_relcl_word and 
                acl_relcl_word.text.lower() in ['lives', 'works', 'runs', 'goes', 'sits', 'stands']):
                # ã“ã‚Œã¯å®Ÿéš›ã®ãƒ¡ã‚¤ãƒ³å‹•è©ã¨ã—ã¦è§£é‡ˆã™ã¹ã
                root_word = acl_relcl_word
                self.logger.debug(f"ğŸ”§ whoseæ§‹æ–‡æ¤œå‡º: ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’ {acl_relcl_word.text} ã«ä¿®æ­£")
        
        # é€šå¸¸ã®å ´åˆï¼šROOTèªæ¤œå‡º
        if not root_word:
            root_word = self._find_root_word(sentence)
            if not root_word:
                return base_result

        # ä¾å­˜é–¢ä¿‚ãƒãƒƒãƒ—æ§‹ç¯‰
        dep_relations = {}
        for word in sentence.words:
            if word.deprel not in dep_relations:
                dep_relations[word.deprel] = []
            dep_relations[word.deprel].append(word)
        
        # âœ… whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†ï¼šãƒ¡ã‚¤ãƒ³æ–‡ã®ä¾å­˜é–¢ä¿‚ãƒãƒƒãƒ—ã‚’æ­£ã—ãæ§‹ç¯‰
        if is_whose_construction and root_word:
            # whoseæ§‹æ–‡ã§ã¯çœŸã®ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’ROOTèªã¨ã—ã¦ä½¿ç”¨
            self.logger.debug(f"ğŸ”§ whoseæ§‹æ–‡: çœŸã®ãƒ¡ã‚¤ãƒ³å‹•è©={root_word.text}")
            
            # ä¾å­˜é–¢ä¿‚ãƒãƒƒãƒ—ã‚’ã‚¯ãƒªã‚¢ã—ã¦å†æ§‹ç¯‰
            dep_relations = {}
            
            # ãƒ¡ã‚¤ãƒ³å‹•è©ã®ç›´æ¥ä¾å­˜èªã‚’ä¾å­˜é–¢ä¿‚ãƒãƒƒãƒ—ã«è¿½åŠ 
            for word in sentence.words:
                if word.head == root_word.id:
                    if word.deprel not in dep_relations:
                        dep_relations[word.deprel] = []
                    dep_relations[word.deprel].append(word)
                    
            self.logger.debug(f"ğŸ”§ whoseæ§‹æ–‡: ä¾å­˜é–¢ä¿‚å†æ§‹ç¯‰å®Œäº†, ãƒ¡ã‚¤ãƒ³å‹•è©={root_word.text}")
            
            # ğŸ”„ whoseæ§‹æ–‡ã¯ä¸Šä½ã‚µãƒ–é€£çµæ±ç”¨ã‚·ã‚¹ãƒ†ãƒ ã«å®Œå…¨å§”è¨—ï¼ˆç‹¬è‡ªå‡¦ç†å»ƒæ­¢ï¼‰
            self.logger.debug("ğŸ”„ whoseæ§‹æ–‡: ä¸Šä½ã‚µãƒ–é€£çµã‚·ã‚¹ãƒ†ãƒ ã«å‡¦ç†å§”è¨—")

        # åŸºæœ¬5æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        pattern_result = self._detect_basic_five_pattern(root_word, dep_relations, predefined_slots, sentence)
        if not pattern_result:
            return base_result
        
        # ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ
        result = base_result.copy()
        if 'slots' not in result:
            result['slots'] = {}
        if 'sub_slots' not in result:
            result['sub_slots'] = {}
        
        five_pattern_slots = self._generate_basic_five_slots(
            pattern_result['pattern'], pattern_result['mapping'], dep_relations, sentence, predefined_slots
        )
        
        result['slots'].update(five_pattern_slots.get('slots', {}))
        result['sub_slots'].update(five_pattern_slots.get('sub_slots', {}))
        
        # æ–‡æ³•æƒ…å ±è¨˜éŒ²ï¼ˆ_merge_handler_resultsã¨äº’æ›æ€§ã®ã‚ã‚‹å½¢å¼ï¼‰
        result['grammar_info'] = {
            'detected_patterns': ['basic_five_pattern'],
            'handler_contributions': {
                'basic_five_pattern': {
                    'pattern': pattern_result['pattern'],
                    'confidence': pattern_result.get('confidence', 0.8)
                }
            }
        }
        
        self.logger.debug(f"  âœ… 5æ–‡å‹å‡¦ç†å®Œäº†: ãƒ‘ã‚¿ãƒ¼ãƒ³={pattern_result['pattern']}")
        return result
    
    def _handle_whose_construction_simple(self, sentence, base_result: Dict, main_verb, dep_relations: Dict) -> Dict:
        """whoseæ§‹æ–‡å°‚ç”¨ã®ç°¡æ˜“å‡¦ç†ï¼ˆãƒ¡ã‚¤ãƒ³æ–‡copulaæ¤œå‡ºã€é–¢ä¿‚ç¯€copulaé™¤å¤–ï¼‰"""
        result = base_result.copy()
        if 'slots' not in result:
            result['slots'] = {}
        
        # ğŸ”§ ãƒ¡ã‚¤ãƒ³å‹•è©è‡ªä½“ãŒbeå‹•è©ã®å ´åˆã®ã¿SVCæ§‹é€ æ¤œå‡º
        if main_verb.lemma.lower() in ['be', 'am', 'is', 'are', 'was', 'were']:
            # ãƒ¡ã‚¤ãƒ³å‹•è©ãŒbeå‹•è©ï¼šSVCæ§‹é€ 
            result['slots']['V'] = main_verb.text
            
            # ãƒ¡ã‚¤ãƒ³å‹•è©ã®è£œèªã‚’æ¢ã™ï¼ˆxcompä¾å­˜é–¢ä¿‚ï¼‰
            for word in sentence.words:
                if word.head == main_verb.id and word.deprel == 'xcomp':
                    complement_phrase = self._build_phrase_with_modifiers(sentence, word)
                    result['slots']['C1'] = complement_phrase
                    self.logger.debug(f"ğŸ”§ whoseæ§‹æ–‡SVC: V='{main_verb.text}', C1='{complement_phrase}'")
                    break
            
            pattern_name = 'SVC_whose'
        else:
            # ãƒ¡ã‚¤ãƒ³å‹•è©ãŒé€šå¸¸å‹•è©ï¼šé€šå¸¸å‡¦ç†
            result['slots']['V'] = main_verb.text
            self.logger.debug(f"ğŸ”§ whoseæ§‹æ–‡ãƒ¡ã‚¤ãƒ³å‹•è©: V='{main_verb.text}'")
            pattern_name = 'SV_whose'
            
            # ãƒ¡ã‚¤ãƒ³å‹•è©ã«ç›´æ¥ä¾å­˜ã™ã‚‹è¦ç´ ã®ã¿ã‚’å‡¦ç†ï¼ˆé–¢ä¿‚ç¯€è¦ç´ ã‚’é™¤å¤–ï¼‰
            for word in sentence.words:
                if word.head == main_verb.id:
                    if word.deprel == 'obj':
                        obj_phrase = self._build_phrase_with_modifiers(sentence, word)
                        result['slots']['O1'] = obj_phrase
                        self.logger.debug(f"ğŸ”§ whoseæ§‹æ–‡ç°¡æ˜“å‡¦ç†: O1='{obj_phrase}' è¿½åŠ ")
                        pattern_name = 'SVO_whose'
                    elif word.deprel == 'xcomp':
                        # è£œèªï¼ˆbecame famousç­‰ï¼‰
                        complement_phrase = self._build_phrase_with_modifiers(sentence, word)
                        result['slots']['C1'] = complement_phrase
                        self.logger.debug(f"ğŸ”§ whoseæ§‹æ–‡ç°¡æ˜“å‡¦ç†: C1='{complement_phrase}' è¿½åŠ ")
                        pattern_name = 'SVC_whose'
        
        # ä¸»èªã‚’è¨­å®šï¼ˆå…ˆè¡Œè©ï¼‰
        if 'nsubj' in dep_relations and dep_relations['nsubj']:
            subject = dep_relations['nsubj'][0]
            subject_phrase = self._build_phrase_with_modifiers(sentence, subject)
            result['slots']['S'] = subject_phrase
            self.logger.debug(f"ğŸ”§ whoseæ§‹æ–‡ç°¡æ˜“å‡¦ç†: S='{subject_phrase}'")
        
        # æ–‡å‹æƒ…å ±ã‚’è¨­å®š
        result['grammar_info'] = {
            'detected_patterns': ['basic_five_pattern'],
            'handler_contributions': {
                'basic_five_pattern': {
                    'pattern': pattern_name,
                    'confidence': 0.9
                }
            }
        }
        
        return result
    
    def _find_root_word(self, sentence):
        """ROOTèªã‚’æ¤œç´¢"""
        return next((w for w in sentence.words if w.head == 0), None)
    
    def _detect_basic_five_pattern(self, root_word, dep_relations, predefined_slots: dict = None, sentence=None):
        """åŸºæœ¬5æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºï¼ˆäº‹å‰ç¢ºå®šã‚¹ãƒ­ãƒƒãƒˆå¯¾å¿œï¼‰"""
        
        # ğŸ”§ äººé–“æ–‡æ³•ä¿®æ­£ãƒã‚§ãƒƒã‚¯: å‹•è©/åè©åŒå½¢èªãŒä¿®æ­£ã•ã‚ŒãŸå ´åˆã®ç‰¹åˆ¥å‡¦ç†
        if sentence and hasattr(sentence, 'hybrid_corrections') and sentence.hybrid_corrections:
            for word_id, correction in sentence.hybrid_corrections.items():
                if (correction.get('correction_type') == 'whose_verb_fix' and 
                    correction.get('word_text') == root_word.text):
                    self.logger.debug(f"ğŸ”§ äººé–“æ–‡æ³•ä¿®æ­£é©ç”¨: {root_word.text} NOUNâ†’VERB (ä¾å­˜é–¢ä¿‚å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—)")
                    # è‡ªå‹•è©ã¨ã—ã¦ç°¡å˜ãªSVæ§‹é€ ã§å‡¦ç†
                    return {
                        'pattern': 'SV_human_corrected',
                        'mapping': {'root': 'V'},
                        'confidence': 0.95,
                        'human_grammar_override': True
                    }
        
        # äº‹å‰ç¢ºå®šã‚¹ãƒ­ãƒƒãƒˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        if predefined_slots is None:
            predefined_slots = {}
        
        # åŸºæœ¬5æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©ï¼ˆè©³ç´°â†’å˜ç´”ã®é †åºã§æ¤œå‡ºï¼‰
        patterns = {
            "SVOO": {
                "required": ["nsubj", "obj", "iobj"],
                "optional": [],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "root": "V", "iobj": "O1", "obj": "O2"}
            },
            "SVOC": {
                "required": ["nsubj", "obj", "xcomp"],
                "optional": [],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "root": "V", "obj": "O1", "xcomp": "C2"}
            },
            "SVO": {
                "required": ["nsubj", "obj"],
                "optional": [],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "root": "V", "obj": "O1"}
            },
            "SVC": {
                "required": ["nsubj", "cop"],
                "optional": [],
                "root_pos": ["ADJ", "NOUN"],
                "mapping": {"nsubj": "S", "cop": "V", "root": "C1"}
            },
            "SVC_PRON": {
                "required": ["nsubj", "cop"],
                "optional": [],
                "root_pos": ["PRON"],
                "mapping": {"nsubj": "S", "cop": "V", "root": "C1"}
            },
            "SVC_ALT": {
                "required": ["nsubj", "xcomp"],
                "optional": [],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "root": "V", "xcomp": "C1"}
            },
            "SV": {
                "required": ["nsubj"],
                "optional": [],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "root": "V"}
            },
            "V_ONLY": {
                "required": [],
                "optional": [],
                "root_pos": ["VERB"],
                "mapping": {"root": "V"}
            }
        }
        
        # äº‹å‰ç¢ºå®šã‚¹ãƒ­ãƒƒãƒˆãŒã‚ã‚‹å ´åˆã€ãã‚Œã‚‰ã‚’è€ƒæ…®ã—ã¦ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        if predefined_slots:
            self.logger.debug(f"ğŸ¤ äº‹å‰ç¢ºå®šã‚¹ãƒ­ãƒƒãƒˆã‚’è€ƒæ…®: {predefined_slots}")
            
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼ˆäº‹å‰ç¢ºå®šã‚¹ãƒ­ãƒƒãƒˆã«é–¢ä¿‚ãªãã€ã™ã¹ã¦ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œï¼‰
        for pattern_name, pattern_info in patterns.items():
            if self._matches_five_pattern(pattern_info, dep_relations, root_word, predefined_slots):
                return {
                    'pattern': pattern_name,
                    'mapping': pattern_info['mapping'],
                    'confidence': 0.9,
                    'predefined_slots': predefined_slots  # äº‹å‰ç¢ºå®šã‚¹ãƒ­ãƒƒãƒˆæƒ…å ±ã‚’å«ã‚ã‚‹
                }
        
        return None
    
    def _matches_five_pattern(self, pattern_info, dep_relations, root_word, predefined_slots: dict = None):
        """5æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼ˆäº‹å‰ç¢ºå®šã‚¹ãƒ­ãƒƒãƒˆå¯¾å¿œï¼‰"""
        # å¿…è¦ãªä¾å­˜é–¢ä¿‚ã®ç¢ºèª
        for rel in pattern_info['required']:
            if rel not in dep_relations:
                return False
        
        # ROOTèªã®å“è©ãƒã‚§ãƒƒã‚¯
        if root_word.upos not in pattern_info['root_pos']:
            return False
        
        return True
    
    def _build_phrase_with_modifiers(self, sentence, main_word):
        """
        ä¿®é£¾èªå¥ã‚’å«ã‚€å®Œå…¨ãªå¥ã‚’æ§‹ç¯‰
        
        å¯¾å¿œä¿®é£¾èªã‚¿ã‚¤ãƒ—ï¼š
        - det: é™å®šè© (a, an, the, my, your, his, her, its, our, their)
        - amod: å½¢å®¹è©ä¿®é£¾èª (red, beautiful, smart, old)
        - nummod: æ•°è©ä¿®é£¾èª (one, two, first, second)  
        - nmod:poss: æ‰€æœ‰æ ¼ä¿®é£¾èª (John's, Mary's, my, your)
        - compound: è¤‡åˆåè© (car door, school bus)
        """
        if not main_word:
            return ""
        
        # ä¿®é£¾èªåé›†
        modifiers = []
        for word in sentence.words:
            if word.head == main_word.id:
                if word.deprel in ['det', 'amod', 'nummod', 'nmod:poss', 'compound']:
                    modifiers.append(word)
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ 
        if modifiers:
            self.logger.debug(f"ğŸ”§ ä¿®é£¾èªæ¤œå‡º [{main_word.text}]: {[(m.text, m.deprel) for m in modifiers]}")
        
        # ä¿®é£¾èªã‚’IDé †ã§ã‚½ãƒ¼ãƒˆï¼ˆèªé †ä¿æŒï¼‰
        modifiers.sort(key=lambda w: w.id)
        
        # å¥æ§‹ç¯‰: ä¿®é£¾èª + ãƒ¡ã‚¤ãƒ³èª
        phrase_words = modifiers + [main_word]
        phrase_words.sort(key=lambda w: w.id)  # æœ€çµ‚çš„ãªèªé †ç¢ºä¿
        
        result = ' '.join(word.text for word in phrase_words)
        self.logger.debug(f"ğŸ”§ å¥æ§‹ç¯‰å®Œäº†: '{result}'")
        
        return result
    
    def _generate_basic_five_slots(self, pattern, mapping, dep_relations, sentence, predefined_slots=None):
        """åŸºæœ¬5æ–‡å‹ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆï¼ˆäº‹å‰ç¢ºå®šã‚¹ãƒ­ãƒƒãƒˆå¯¾å¿œï¼‰"""
        slots = {}
        sub_slots = {}
        
        if predefined_slots is None:
            predefined_slots = {}
        
        # ğŸ¤ äº‹å‰ç¢ºå®šã‚¹ãƒ­ãƒƒãƒˆã‚’æœ€å„ªå…ˆã§è¨­å®š
        for slot_name, slot_value in predefined_slots.items():
            slots[slot_name] = slot_value
            self.logger.debug(f"ğŸ¤ äº‹å‰ç¢ºå®šã‚¹ãƒ­ãƒƒãƒˆè¨­å®š: {slot_name} = '{slot_value}'")
        
        # ãƒãƒƒãƒ”ãƒ³ã‚°ã«å¾“ã£ã¦ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆï¼ˆäº‹å‰ç¢ºå®šã•ã‚Œã¦ã„ãªã„ã‚¹ãƒ­ãƒƒãƒˆã®ã¿ï¼‰
        for dep_rel, slot in mapping.items():
            # âœ… äº‹å‰ç¢ºå®šæ¸ˆã¿ã‚¹ãƒ­ãƒƒãƒˆã¯æ—¢ã«è¨­å®šæ¸ˆã¿ãªã®ã§ä¾å­˜é–¢ä¿‚åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—
            if slot in predefined_slots:
                self.logger.debug(f"âœ… äº‹å‰ç¢ºå®šæ¸ˆã¿ã‚¹ãƒ­ãƒƒãƒˆ: {slot} = '{predefined_slots[slot]}' (é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒå‡¦ç†æ¸ˆã¿)")
                slots[slot] = predefined_slots[slot]
                continue
                
            if dep_rel == "root":
                # ROOTèªã®å‡¦ç†ï¼ˆå‹•è©ã¯é€šå¸¸ä¿®é£¾èªãªã—ãªã®ã§å˜èªã®ã¿ï¼‰
                root_word = self._find_root_word(sentence)
                if root_word:
                    # âœ… C1é‡è¤‡é˜²æ­¢: ROOTãƒ¯ãƒ¼ãƒ‰ãŒVã«æ—¢ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯C1ã«è¨­å®šã—ãªã„
                    if slot == "C1" and "V" in slots and slots["V"] == root_word.text:
                        self.logger.debug(f"ğŸš« C1é‡è¤‡é˜²æ­¢: {root_word.text} (Vã¨åŒä¸€)")
                        continue  # C1ã¸ã®è¨­å®šã‚’ã‚¹ã‚­ãƒƒãƒ—
                    
                    # âœ… ç©ºæ–‡å­—ã‚¹ãƒ­ãƒƒãƒˆé˜²æ­¢: æœ‰åŠ¹ãªå€¤ã®ã¿è¨­å®š
                    if root_word.text and root_word.text.strip():
                        self.logger.debug(f"ğŸ”§ ROOTèªè¨­å®š: {slot} = '{root_word.text}' (ROOTèª: {root_word.text})")
                        slots[slot] = root_word.text
                    else:
                        self.logger.debug(f"ğŸš« ç©ºæ–‡å­—ã‚¹ãƒ­ãƒƒãƒˆé˜²æ­¢: {slot} (ROOTèªãŒç©º)")
            elif dep_rel in dep_relations:
                # ä¾å­˜é–¢ä¿‚èªã®å‡¦ç†ï¼ˆä¿®é£¾èªå¥ã‚’å«ã‚€å®Œå…¨ãªå¥ã‚’æ§‹ç¯‰ï¼‰
                words = dep_relations[dep_rel]
                if words:
                    # ãƒ¡ã‚¤ãƒ³ã®èª
                    main_word = words[0]
                    # ä¿®é£¾èªå¥ã‚’æ§‹ç¯‰
                    phrase = self._build_phrase_with_modifiers(sentence, main_word)
                    
                    # âœ… ç©ºæ–‡å­—ã‚¹ãƒ­ãƒƒãƒˆé˜²æ­¢: æœ‰åŠ¹ãªå¥ã®ã¿è¨­å®š
                    if phrase and phrase.strip():
                        self.logger.debug(f"ğŸ”§ ä¾å­˜é–¢ä¿‚èªè¨­å®š: {slot} = '{phrase}'")
                        slots[slot] = phrase
                    else:
                        self.logger.debug(f"ğŸš« ç©ºæ–‡å­—ã‚¹ãƒ­ãƒƒãƒˆé˜²æ­¢: {slot} (å¥ãŒç©º)")
        
        # âœ… è¿½åŠ å‡¦ç†ï¼šROOTãƒ¯ãƒ¼ãƒ‰ã«ã‚‚ä¿®é£¾èªå¥å‡¦ç†ã‚’é©ç”¨ï¼ˆå‹•è©ä»¥å¤–ã®å ´åˆï¼‰
        # ä¾‹: "The woman is my neighbor" ã§neighborãŒROOTã®å ´åˆ
        root_word = self._find_root_word(sentence)
        if root_word and root_word.pos in ['NOUN', 'PRON', 'ADJ']:
            # åè©ãƒ»ä»£åè©ãƒ»å½¢å®¹è©ãŒROOTã®å ´åˆã€ä¿®é£¾èªå¥ã‚’æ§‹ç¯‰
            root_phrase = self._build_phrase_with_modifiers(sentence, root_word)
            
            # ROOTãƒ¯ãƒ¼ãƒ‰å¯¾å¿œã®ã‚¹ãƒ­ãƒƒãƒˆã‚’æ›´æ–°
            for dep_rel, slot in mapping.items():
                if dep_rel == "root" and slot in slots:
                    if slots[slot] == root_word.text:  # å˜èªã®ã¿ã®å ´åˆ
                        slots[slot] = root_phrase  # ä¿®é£¾èªå¥ã«æ›´æ–°
                        self.logger.debug(f"ğŸ”§ ROOTèªä¿®é£¾èªå¥é©ç”¨: {slot} = '{root_phrase}'")
        
        # ä¿®é£¾èªã®å‡¦ç†ï¼ˆåŸºæœ¬çš„ãªã‚‚ã®ã®ã¿ï¼‰
        # é–¢ä¿‚å‰¯è©ã¯é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã«ä»»ã›ã‚‹ãŸã‚é™¤å¤–
        relative_adverbs = ['where', 'when', 'why', 'how']
        
        # âœ… é–¢ä¿‚ç¯€å†…ã®èªã‚’äº‹å‰ã«ç‰¹å®šã—ã¦é™¤å¤–
        rel_verb_candidates = [w for w in sentence.words if w.deprel in ['acl:relcl', 'acl']]
        excluded_word_ids = set()
        for rel_verb_cand in rel_verb_candidates:
            # é–¢ä¿‚ç¯€å‹•è©ã¨ãã®ä¾å­˜èªã‚’ã™ã¹ã¦é™¤å¤–
            excluded_word_ids.add(rel_verb_cand.id)
            for word in sentence.words:
                if word.head == rel_verb_cand.id:
                    excluded_word_ids.add(word.id)
        
        for word in sentence.words:
            # é–¢ä¿‚ç¯€å†…ã®èªã‚’ã‚¹ã‚­ãƒƒãƒ—
            if word.id in excluded_word_ids:
                continue
                
            # âœ… å‰¯è©å‡¦ç†ã¯å°‚é–€ã‚¨ãƒ³ã‚¸ãƒ³ã«å§”è­² - åŸºæœ¬5æ–‡å‹ã§ã¯å‡¦ç†ã—ãªã„
            # if word.deprel == 'advmod' and 'M2' not in slots:
            #     if word.text.lower() not in relative_adverbs:
            #         slots['M2'] = word.text  # é€šå¸¸ã®å‰¯è©ä¿®é£¾èªã®ã¿
            #     else:
            #         self.logger.debug(f"ğŸ” é–¢ä¿‚å‰¯è©é™¤å¤–: {word.text} (é–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã«å§”è­²)")
            # elif word.deprel == 'obl' and 'M3' not in slots:
            #     slots['M3'] = word.text  # å‰ç½®è©å¥ç­‰
        
        return {'slots': slots, 'sub_slots': sub_slots}

    def _handle_adverbial_modifier(self, sentence, base_result: Dict, shared_context: Dict = None) -> Optional[Dict]:
        """
        å‰¯è©å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆRephraseè·é›¢ãƒ™ãƒ¼ã‚¹åŸç† + ä»•æ§˜æ›¸æº–æ‹ è§£æã‚¨ãƒ©ãƒ¼ä¿®æ­£ï¼‰
        Stanza/spaCyåˆ†æçµæœã‚’ä½¿ç”¨ã—ã€è§£æã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œã™ã‚‹ä¿®æ­£æˆ¦ç•¥ã‚’é©ç”¨
        """
        print("å‰¯è©ãƒãƒ³ãƒ‰ãƒ©ãƒ¼é–‹å§‹")
        self.logger.debug("å‰¯è©ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè¡Œä¸­ï¼ˆè·é›¢ãƒ™ãƒ¼ã‚¹åŸç† + è§£æã‚¨ãƒ©ãƒ¼ä¿®æ­£ï¼‰...")
        
        # === è§£æã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³å‡¦ç†ï¼ˆä»•æ§˜æ›¸ã®Error Pattern Managementæº–æ‹ ï¼‰===
        error_corrections = self._apply_analysis_error_corrections(sentence, base_result)
        
        # ğŸ¯ RephraseåŸç†ï¼šãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åˆ†é¡ã¯ä¸è¦
        # Stanza/spaCyã®åˆ†æçµæœã®ã¿ã‚’ä¿¡é ¼ï¼ˆãŸã ã—ã€ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ä¿®æ­£ï¼‰
        
        # === æ—¢å­˜ã‚¹ãƒ­ãƒƒãƒˆç¢ºèªï¼ˆé–¢ä¿‚ç¯€ã‚¹ãƒ­ãƒƒãƒˆå«ã‚€ï¼‰===
        existing_slots = base_result.get('slots', {}) if base_result else {}
        existing_sub_slots = base_result.get('sub_slots', {}) if base_result else {}
        
        existing_adverbs = set()
        
        # ä¸»ç¯€å‰¯è©ã‚’æ—¢å­˜ãƒã‚§ãƒƒã‚¯ã«è¿½åŠ 
        for slot_key, slot_value in existing_slots.items():
            if slot_key.startswith('M') and slot_value:
                existing_adverbs.update(slot_value.split())
        
        # ğŸ”§ é‡è¦ä¿®æ­£: é–¢ä¿‚ç¯€å‰¯è©ã‚‚æ—¢å­˜ãƒã‚§ãƒƒã‚¯ã«è¿½åŠ 
        for slot_key, slot_value in existing_sub_slots.items():
            if slot_key.startswith('sub-m') and slot_value:
                existing_adverbs.update(slot_value.split())
        
        self.logger.debug(f"ğŸ” æ—¢å­˜å‰¯è©ãƒã‚§ãƒƒã‚¯: {existing_adverbs}")
        
        # ã‚¨ãƒ©ãƒ¼ä¿®æ­£é©ç”¨
        if error_corrections:
            existing_sub_slots.update(error_corrections)
            self.logger.debug(f"âœ… è§£æã‚¨ãƒ©ãƒ¼ä¿®æ­£é©ç”¨: {error_corrections}")
        
        # ğŸ†• é–¢ä¿‚å‰¯è©å†é…ç½®ã‚·ã‚¹ãƒ†ãƒ ï¼ˆæ–°ã—ã„ãƒ­ã‚¸ãƒƒã‚¯ã§å¯¾å¿œã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰
        # æ—¢å­˜ã®sub-m1ã‚¹ãƒ­ãƒƒãƒˆã«ã‚ã‚‹é–¢ä¿‚å‰¯è©å¥ã‚’sub-m2ã«ç§»å‹•
        # if existing_sub_slots.get('sub-m1'):
        #     sub_m1_value = existing_sub_slots['sub-m1']
        #     relative_adverbs = ['where', 'when', 'why', 'how', 'as if']
        #     
        #     for rel_adv in relative_adverbs:
        #         if rel_adv in sub_m1_value.lower():
        #             self.logger.debug(f"ğŸ”„ é–¢ä¿‚å‰¯è©å†é…ç½®: '{sub_m1_value}' sub-m1 â†’ sub-m2")
        #             existing_sub_slots['sub-m2'] = sub_m1_value
        #             existing_sub_slots['sub-m1'] = ''
        #             break
        
        # === é–¢ä¿‚ç¯€ãƒ»å¾“å±ç¯€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ ===
        # ğŸ”§ ä¿®æ­£ï¼šbase_resultã‹ã‚‰ä¸»å‹•è©æƒ…å ±ã‚’å–å¾—ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æçµæœåæ˜ ï¼‰
        main_verb_id = None
        main_verb_text = existing_slots.get('V')
        
        print(f"MAIN VERB DETECTION:")
        
        # ğŸ”§ Whoseæ§‹æ–‡å°‚ç”¨æ¤œå‡ºï¼ˆæœ€å„ªå…ˆï¼‰
        sentence_text = " ".join([w.text for w in sentence.words])
        if "whose" in sentence_text:
            print(f"ğŸ¯ WHOSEæ§‹æ–‡æ¤œå‡º: {sentence_text}")
            # whoseä»¥é™ã§é–¢ä¿‚ç¯€ã‚’è¶…ãˆãŸæœ€åˆã®VERBå“è©ã®å‹•è©ã‚’ä¸»å‹•è©ã¨ã™ã‚‹
            for word in sentence.words:
                if (word.upos == 'VERB' and 
                    word.deprel not in ['acl:relcl', 'acl'] and
                    word.text not in ['is', 'are', 'was', 'were']):
                    print(f"   â†’ WHOSEæ§‹æ–‡ä¸»å‹•è©ç¢ºå®š: {word.text} (id={word.id})")
                    main_verb_id = word.id
                    break
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå‹•è©èªå¹¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢ï¼ˆå“è©ç„¡è¦–ï¼‰
            if not main_verb_id:
                for word in sentence.words:
                    if (self._is_verb_pattern(word.text) and 
                        word.text not in ['is', 'are', 'was', 'were']):
                        print(f"   â†’ WHOSEæ§‹æ–‡ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {word.text} (id={word.id})")
                        main_verb_id = word.id
                        break

        if not main_verb_id:
            print(f"   existing_slots V: {main_verb_text}")
            
            # ğŸ¯ é‡è¦ä¿®æ­£ï¼šé–¢ä¿‚ç¯€å‡¦ç†ã§VãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®ä¸»å‹•è©ç‰¹å®š
            if main_verb_text in ['is', 'are', 'was', 'were']:
                print(f"   â†’ é–¢ä¿‚ç¯€å‹•è©æ¤œå‡ºã€çœŸã®ä¸»å‹•è©ã‚’æ¢ç´¢ä¸­...")
                # whoseæ§‹æ–‡ãªã©ã§é–¢ä¿‚ç¯€ã®å‹•è©ãŒä¸»å‹•è©ã¨ã—ã¦èª¤èªã•ã‚Œã¦ã„ã‚‹å ´åˆ
                # çœŸã®ä¸»å‹•è©ï¼ˆlives, needs, etc.ï¼‰ã‚’æ¢ã™
                for word in sentence.words:
                    print(f"     æ¤œè¨: {word.text} (upos={word.upos}, deprel={word.deprel})")
                    if (word.text not in ['is', 'are', 'was', 'were'] and 
                        word.upos == 'VERB' and  # VERBã®ã¿ã«é™å®š
                        word.deprel not in ['acl:relcl', 'acl', 'advcl']):  # å¾“å±ç¯€å‹•è©ã‚’é™¤å¤–
                        main_verb_text = word.text
                        print(f"   â†’ ä¸»å‹•è©ä¿®æ­£: '{main_verb_text}' (é–¢ä¿‚ç¯€å‹•è© '{existing_slots.get('V')}' ã‹ã‚‰å¤‰æ›´)")
                        self.logger.debug(f"ğŸ”§ ä¸»å‹•è©ä¿®æ­£: '{main_verb_text}' (é–¢ä¿‚ç¯€å‹•è© '{existing_slots.get('V')}' ã‹ã‚‰å¤‰æ›´)")
                        break
            
            if main_verb_text:
                print(f"   main_verb_textæœ€çµ‚: {main_verb_text}")
                # ä¸»å‹•è©ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å¯¾å¿œã™ã‚‹word IDã‚’ç‰¹å®š
                for word in sentence.words:
                    print(f"     IDæ¢ç´¢: {word.text} (id={word.id}, upos={word.upos})")
                    if word.text == main_verb_text and word.upos in ['VERB', 'AUX', 'NOUN']:  # NOUNã‚‚å«ã‚ã‚‹ï¼ˆlivesç­‰ï¼‰
                        main_verb_id = word.id
                        print(f"   â†’ ä¸»å‹•è©IDç¢ºå®š: {main_verb_id}")
                        break
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®æ–¹æ³•
            if not main_verb_id:
                main_verb_id = self._find_main_verb(sentence)
        
        print(f"æœ€çµ‚ä¸»å‹•è©ID: {main_verb_id}")
        
        subordinate_verbs = self._find_subordinate_verbs(sentence, main_verb_id)
        
        # === å‰¯è©å€™è£œåé›†ï¼ˆMigration sourceå„ªç§€æ©Ÿèƒ½æ´»ç”¨ï¼‰===
        adverb_phrases = []
        processed_positions = set()
        processed_phrases = set()  # é‡è¤‡ãƒ•ãƒ¬ãƒ¼ã‚ºé˜²æ­¢
        
        self.logger.debug("ğŸ” å‰¯è©å€™è£œã‚¹ã‚­ãƒ£ãƒ³é–‹å§‹ï¼ˆStanza/spaCyåˆ†æãƒ™ãƒ¼ã‚¹ï¼‰...")
        for word in sentence.words:
            # ğŸ¯ RephraseåŸç†ï¼šç´”ç²‹ã«Stanza/spaCyåˆ†æçµæœã‚’ä¿¡é ¼
            is_adverb = (
                word.deprel in ['advmod', 'obl', 'obl:tmod', 'obl:npmod', 'obl:agent', 'obl:unmarked', 'nmod:tmod'] or
                word.upos == 'ADV'  # POS-based detectionï¼ˆä¿¡é ¼æ€§é«˜ã„ï¼‰
            )
            
            self.logger.debug(f"  {word.text}: deprel={word.deprel}, upos={word.upos}, is_adverb={is_adverb}")
            
            if is_adverb:
                if word.text in existing_adverbs:
                    self.logger.debug(f"    â†’ é™¤å¤–ï¼ˆæ—¢å­˜å‰¯è©ï¼‰: {word.text}")
                    continue
                    
                # é‡è¤‡é™¤å»ï¼ˆMigration sourceå„ªç§€æ©Ÿèƒ½ï¼‰
                if word.id in processed_positions:
                    self.logger.debug(f"    â†’ é™¤å¤–ï¼ˆé‡è¤‡ä½ç½®ï¼‰: {word.text}")
                    continue
                
                # Migration sourceå‰ç½®è©å¥æ§‹ç¯‰æ©Ÿèƒ½æ´»ç”¨
                if word.deprel.startswith('obl'):
                    # ğŸ”§ advmodä¿®é£¾èªã‚’é™¤å¤–ã—ã¦åŸºæœ¬èªã®ã¿ã§OBLå¥æ§‹ç¯‰
                    phrase = self._build_prepositional_phrase(sentence, word, exclude_advmod=True)
                    print(f"DEBUG OBL: '{word.text}' -> '{phrase}' (advmodé™¤å¤–)")
                    # å‰ç½®è©å¥ã®å…¨tokensè¨˜éŒ²ï¼ˆé‡è¤‡å›é¿ï¼‰
                    phrase_words = phrase.split()
                    for pw in phrase_words:
                        for w in sentence.words:
                            if w.text == pw:
                                processed_positions.add(w.id)
                else:
                    # ğŸ”§ å‰¯è©ä¿®é£¾èªã‚’å«ã‚€å¥æ§‹ç¯‰ï¼ˆ"very carefully"å¯¾å¿œï¼‰
                    phrase = self._build_adverbial_phrase(sentence, word)
                    print(f"ADVå¥æ§‹ç¯‰: '{word.text}' â†’ '{phrase}'")
                    phrase_words = phrase.split()
                    for pw in phrase_words:
                        for w in sentence.words:
                            if w.text == pw:
                                processed_positions.add(w.id)
                    
                
                # é‡è¤‡ãƒ•ãƒ¬ãƒ¼ã‚ºãƒã‚§ãƒƒã‚¯
                if phrase in processed_phrases:
                    self.logger.debug(f"    â†’ é™¤å¤–ï¼ˆé‡è¤‡ãƒ•ãƒ¬ãƒ¼ã‚ºï¼‰: {phrase}")
                    continue
                
                processed_phrases.add(phrase)
                
                # ğŸ”§ ç›¸å¯¾å‰¯è©å¥ã®æ¤œå‡ºã¨ç‰¹åˆ¥å‡¦ç†
                if self._is_relative_adverb_phrase(phrase):
                    self.logger.debug(f"    â†’ ç›¸å¯¾å‰¯è©å¥æ¤œå‡º: {phrase}")
                    # ç›¸å¯¾å‰¯è©ã¯å¸¸ã«sub-m2ã¾ãŸã¯M2ã«é…ç½®ï¼ˆ1å€‹ãƒ«ãƒ¼ãƒ«ï¼‰
                
                # ğŸ¯ RephraseåŸç†ï¼šåˆ†é¡ä¸è¦ã€ä½ç½®æƒ…å ±ã®ã¿ã§åˆ¤å®š
                # category = self._classify_adverbial_phrase(phrase, time_keywords, location_keywords, manner_keywords)
                category = 'position_based'  # Rephraseè·é›¢ãƒ™ãƒ¼ã‚¹åŸç†
                
                # æ–‡è„ˆåˆ†æ: ä¸»ç¯€ vs å¾“å±ç¯€ï¼ˆMigration sourceåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ï¼‰
                context = self._determine_adverb_context(word, main_verb_id, subordinate_verbs, sentence)
                
                self.logger.debug(f"    â†’ æ¤œå‡º: phrase='{phrase}', category={category}, context={context}")
                
                adverb_phrases.append({
                    'phrase': phrase,
                    'category': category,
                    'position': word.id,
                    'word': word,
                    'context': context  # 'main' or 'subordinate'
                })
        
        if not adverb_phrases:
            self.logger.debug("å‰¯è©ãªã— - ã‚¹ã‚­ãƒƒãƒ—")
            return None

        # === é‡è¤‡å‰¯è©ã®é™¤å» ===
        # ã‚ˆã‚Šå®Œå…¨ãªå¥ï¼ˆä¾‹ï¼š"very carefully"ï¼‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€éƒ¨åˆ†çš„ãªå¥ï¼ˆä¾‹ï¼š"very"ï¼‰ã‚’é™¤å»
        filtered_adverb_phrases = []
        for i, adverb in enumerate(adverb_phrases):
            is_substring = False
            for j, other_adverb in enumerate(adverb_phrases):
                if i != j and adverb['phrase'] in other_adverb['phrase'] and adverb['phrase'] != other_adverb['phrase']:
                    # adverbã®å¥ãŒother_adverbã®å¥ã®ä¸€éƒ¨ã§ã‚ã‚‹å ´åˆ
                    self.logger.debug(f"ğŸ”§ å‰¯è©é‡è¤‡é™¤å»: '{adverb['phrase']}' ã¯ '{other_adverb['phrase']}' ã«å«ã¾ã‚Œã‚‹ãŸã‚é™¤å¤–")
                    is_substring = True
                    break
            if not is_substring:
                filtered_adverb_phrases.append(adverb)
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨
        adverb_phrases = filtered_adverb_phrases

        # === Rephraseä»•æ§˜é…ç½®ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆMigration sourceæ©Ÿèƒ½æ´»ç”¨ï¼‰ ===
        slots = {}
        sub_slots = {}
        
        # ä½ç½®é †ã‚½ãƒ¼ãƒˆ
        adverb_phrases.sort(key=lambda x: x['position'])
        
        # === ã‚·ãƒ³ãƒ—ãƒ«ãƒ«ãƒ¼ãƒ«ä¸€æ‹¬é…ç½®ã‚·ã‚¹ãƒ†ãƒ  ===
        # ä¸»ç¯€å‰¯è©ã¨å¾“å±ç¯€å‰¯è©ã‚’åˆ†é›¢ã—ã¦ã€ãã‚Œãã‚Œã«ã‚·ãƒ³ãƒ—ãƒ«ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨
        
        main_adverbs = [p for p in adverb_phrases if p['context'] == 'main']
        sub_adverbs = [p for p in adverb_phrases if p['context'] == 'subordinate']
        
        self.logger.debug(f"ğŸ¯ ã‚·ãƒ³ãƒ—ãƒ«ãƒ«ãƒ¼ãƒ«é©ç”¨: ä¸»ç¯€å‰¯è©{len(main_adverbs)}å€‹, å¾“å±ç¯€å‰¯è©{len(sub_adverbs)}å€‹")
        
        # ä¸»ç¯€å‰¯è©ã®ã‚·ãƒ³ãƒ—ãƒ«ãƒ«ãƒ¼ãƒ«é…ç½®
        if main_adverbs:
            print(f"Mainå‰¯è©è©³ç´°: {main_adverbs}")
            main_slots = self._apply_simple_rule_to_adverbs(main_adverbs, 'main', main_verb_id, None)
            print(f"Mainå‰¯è©çµæœ: {main_slots}")
            slots.update(main_slots)
        
        # å¾“å±ç¯€å‰¯è©ã®ã‚·ãƒ³ãƒ—ãƒ«ãƒ«ãƒ¼ãƒ«é…ç½®
        if sub_adverbs:
            # æ—¢å­˜ã®sub-slotsã‚’æ¸¡ã—ã¦ã€å…ˆè¡Œè©å¥ã‚’ä¿è­·
            sub_main_slots = self._apply_simple_rule_to_adverbs(sub_adverbs, 'sub', main_verb_id, existing_sub_slots)
            sub_slots.update(sub_main_slots)
        
        self.logger.debug(f"å‰¯è©é…ç½®å®Œäº†: slots={slots}, sub_slots={sub_slots}")
        print("å‰¯è©ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Œäº†: slots={}, sub_slots={}".format(slots, sub_slots))
        return {'slots': slots, 'sub_slots': sub_slots}
    
    def _apply_simple_rule_to_adverbs(self, adverbs, context_type, main_verb_id=None, existing_sub_slots=None):
        """
        ã‚·ãƒ³ãƒ—ãƒ«ãƒ«ãƒ¼ãƒ«ã‚’å‰¯è©ç¾¤ã«ä¸€æ‹¬é©ç”¨
        
        Args:
            adverbs: å‰¯è©ãƒªã‚¹ãƒˆ
            context_type: 'main' or 'sub'
            main_verb_id: ä¸»å‹•è©ã®IDï¼ˆå‹•è©ä¸­å¿ƒåˆ¤å®šç”¨ï¼‰
            existing_sub_slots: æ—¢å­˜ã®sub-slotsï¼ˆé–¢ä¿‚ç¯€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‹ã‚‰ã®å…ˆè¡Œè©å¥ä¿è­·ç”¨ï¼‰
        """
        result_slots = {}
        count = len(adverbs)
        
        self.logger.debug(f"ğŸ¯ {context_type}ç¯€ã‚·ãƒ³ãƒ—ãƒ«ãƒ«ãƒ¼ãƒ«é©ç”¨: {count}å€‹ã®å‰¯è©")
        
        if count == 0:
            return result_slots
        
        # ã‚¹ãƒ­ãƒƒãƒˆåãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
        slot_prefix = 'sub-m' if context_type == 'sub' else 'M'
        
        # æ—¢å­˜ã®sub-m2ä¿è­·ï¼ˆé–¢ä¿‚ç¯€å…ˆè¡Œè©å¥ç”¨ï¼‰
        existing_sub_m2 = None
        if context_type == 'sub' and existing_sub_slots:
            existing_sub_m2 = existing_sub_slots.get('sub-m2')
            if existing_sub_m2:
                self.logger.debug(f"ğŸ›¡ï¸ æ—¢å­˜sub-m2ä¿è­·: '{existing_sub_m2}' (é–¢ä¿‚ç¯€å…ˆè¡Œè©å¥)")
        
        if count == 1:
            # 1å€‹ â†’ M2 (ã¾ãŸã¯ sub-m2)ã€ãŸã ã—é–¢ä¿‚ç¯€å…ˆè¡Œè©å¥ãŒã‚ã‚‹å ´åˆã¯M3ã¸
            if existing_sub_m2:
                slot_name = f"{slot_prefix}3"  # é–¢ä¿‚ç¯€å…ˆè¡Œè©å¥ãŒã‚ã‚‹å ´åˆã¯sub-m3ã¸
            else:
                slot_name = f"{slot_prefix}2"
            result_slots[slot_name] = adverbs[0]['phrase']
            self.logger.debug(f"  1å€‹ãƒ«ãƒ¼ãƒ«: {slot_name} = '{adverbs[0]['phrase']}'")
        
        elif count == 2:
            # 2å€‹ â†’ å‹•è©ä¸­å¿ƒ(M2)ã‚’åŸºæº–ã«M1ã¨M3ã«é…ç½®
            # ğŸ†• ç›¸å¯¾å‰¯è©ã®ç‰¹åˆ¥å‡¦ç†: ç›¸å¯¾å‰¯è©ã¯å¸¸ã«M2ã«é…ç½®
            
            first_adverb = adverbs[0]
            second_adverb = adverbs[1]
            
            # ç›¸å¯¾å‰¯è©ã‚’ç‰¹å®š
            first_is_relative = self._is_relative_adverb_phrase(first_adverb['phrase'])
            second_is_relative = self._is_relative_adverb_phrase(second_adverb['phrase'])
            
            if first_is_relative and not second_is_relative:
                # æœ€åˆãŒç›¸å¯¾å‰¯è©ï¼šç›¸å¯¾å‰¯è©â†’M2ã€é€šå¸¸å‰¯è©â†’M3
                result_slots[f"{slot_prefix}2"] = first_adverb['phrase']
                result_slots[f"{slot_prefix}3"] = second_adverb['phrase']
                self.logger.debug(f"  2å€‹ãƒ«ãƒ¼ãƒ«(ç›¸å¯¾å‰¯è©å„ªå…ˆ): {slot_prefix}2='{first_adverb['phrase']}', {slot_prefix}3='{second_adverb['phrase']}'")
            elif second_is_relative and not first_is_relative:
                # 2ç•ªç›®ãŒç›¸å¯¾å‰¯è©ï¼šç›¸å¯¾å‰¯è©â†’M2ã€é€šå¸¸å‰¯è©â†’M3
                result_slots[f"{slot_prefix}2"] = second_adverb['phrase']
                result_slots[f"{slot_prefix}3"] = first_adverb['phrase']
                self.logger.debug(f"  2å€‹ãƒ«ãƒ¼ãƒ«(ç›¸å¯¾å‰¯è©å„ªå…ˆ): {slot_prefix}2='{second_adverb['phrase']}', {slot_prefix}3='{first_adverb['phrase']}'")
            else:
                # å¾“æ¥ã®å‹•è©ä¸­å¿ƒãƒ­ã‚¸ãƒƒã‚¯ï¼ˆç›¸å¯¾å‰¯è©ãªã—ã¾ãŸã¯ä¸¡æ–¹ç›¸å¯¾å‰¯è©ï¼‰
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šM2ã¯æœ€åˆã®å‰¯è©ã€M3ã¯2ç•ªç›®ã®å‰¯è©
                result_slots[f"{slot_prefix}2"] = first_adverb['phrase']
                result_slots[f"{slot_prefix}3"] = second_adverb['phrase']
                self.logger.debug(f"  2å€‹ãƒ«ãƒ¼ãƒ«é©ç”¨: {result_slots}")
                
            self.logger.debug(f"  è©³ç´°: adverb[0]={first_adverb}, adverb[1]={second_adverb}")
        
        elif count >= 3:
            # 3å€‹ä»¥ä¸Š â†’ M1, M2, M3 (ã¾ãŸã¯ sub-m1, sub-m2, sub-m3)
            result_slots[f"{slot_prefix}1"] = adverbs[0]['phrase']
            result_slots[f"{slot_prefix}2"] = adverbs[1]['phrase']
            result_slots[f"{slot_prefix}3"] = adverbs[2]['phrase']
            self.logger.debug(f"  3å€‹ãƒ«ãƒ¼ãƒ«: {slot_prefix}1/2/3 = '{adverbs[0]['phrase']}'/'{adverbs[1]['phrase']}'/'{adverbs[2]['phrase']}'")
            
            # 4å€‹ä»¥ä¸Šã¯ç„¡è¦–ï¼ˆè­¦å‘Šï¼‰
            if count > 3:
                ignored = [a['phrase'] for a in adverbs[3:]]
                self.logger.warning(f"  âš ï¸ 4å€‹ä»¥ä¸Šã®å‰¯è©ã‚’ç„¡è¦–: {ignored}")
        
        return result_slots
    
    def _is_relative_adverb_phrase(self, phrase):
        """ç›¸å¯¾å‰¯è©å¥ã‹ã©ã†ã‹ã‚’åˆ¤å®šï¼ˆé–¢ä¿‚ç¯€æ§‹é€ é™¤å¤–ï¼‰"""
        # é–¢ä¿‚ç¯€æ§‹é€ ã¯é™¤å¤–ï¼ˆå…ˆè¡Œè© + where/when/why/howæ§‹é€ ï¼‰
        if any(pattern in phrase.lower() for pattern in ['the place where', 'the time when', 'the reason why', 'the way how']):
            return False  # é–¢ä¿‚ç¯€ã¨ã—ã¦å‡¦ç†ã•ã‚Œã‚‹ã¹ã
        
        # å˜ç‹¬ã®ç›¸å¯¾å‰¯è©ã®ã¿
        relative_patterns = ['where', 'when', 'why', 'how', 'as if']
        phrase_lower = phrase.lower()
        return any(pattern == phrase_lower for pattern in relative_patterns)
    
    def _find_main_verb(self, sentence):
        """ä¸»å‹•è©ã‚’ç‰¹å®šï¼ˆæ§‹é€ çš„ä¿®æ­£ç‰ˆï¼‰"""
        
        print(f"MAIN VERB ANALYSIS:")
        for word in sentence.words:
            print(f"   Word: {word.text} (id={word.id}, upos={word.upos}, deprel={word.deprel})")
        
        # ğŸ”§ Step 0: whoseæ§‹æ–‡ç‰¹æ®Šå‡¦ç†
        sentence_text = getattr(sentence, 'text', ' '.join(w.text for w in sentence.words))
        if 'whose' in sentence_text.lower():
            print(f"   ğŸ¯ whoseæ§‹æ–‡æ¤œå‡º - ç‰¹æ®Šå‡¦ç†é–‹å§‹")
            
            # whoseä»¥é™ã®ç¯€ã‚’ç‰¹å®š
            whose_index = None
            for i, word in enumerate(sentence.words):
                if word.text.lower() == 'whose':
                    whose_index = i
                    break
            
            if whose_index is not None:
                # whoseç¯€å¾Œã®æœ€åˆã®å‹•è©ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã€ãã®æ¬¡ã®å‹•è©ã‚’ä¸»å‹•è©ã¨ã™ã‚‹
                relative_clause_verbs = []
                main_clause_candidates = []
                
                for i, word in enumerate(sentence.words[whose_index:], whose_index):
                    if word.upos == 'VERB' or self._is_verb_by_pattern(word.text):
                        if not relative_clause_verbs:
                            # æœ€åˆã®å‹•è©ã¯é–¢ä¿‚ç¯€å‹•è©
                            relative_clause_verbs.append(word)
                            print(f"     é–¢ä¿‚ç¯€å‹•è©: {word.text} (id={word.id})")
                        else:
                            # 2ç•ªç›®ä»¥é™ã¯ä¸»ç¯€å€™è£œ
                            main_clause_candidates.append(word)
                            print(f"     ä¸»ç¯€å€™è£œ: {word.text} (id={word.id})")
                
                if main_clause_candidates:
                    main_verb = main_clause_candidates[0]
                    print(f"   â†’ whoseæ§‹æ–‡ä¸»å‹•è©: {main_verb.text} (id={main_verb.id})")
                    return main_verb.id
        
        # ğŸ¯ Step 1: ROOTå‹•è©ã‚’å„ªå…ˆ
        for word in sentence.words:
            if word.deprel == 'root' and word.upos == 'VERB':
                self.logger.debug(f"ğŸ¯ ä¸»å‹•è©ï¼ˆROOTå‹•è©ï¼‰: {word.text} (id={word.id})")
                print(f"   â†’ ROOTå‹•è©ãŒä¸»å‹•è©: {word.text} (id={word.id})")
                return word.id
        
        # ğŸ”§ Step 2: ROOTå½¢å®¹è©ã§å—å‹•æ…‹ã®å ´åˆã€ROOTè‡ªä½“ã‚’ä¸»å‹•è©ã¨ã—ã¦æ‰±ã†
        root_word = None
        for word in sentence.words:
            if word.deprel == 'root':
                root_word = word
                break
        
        print(f"   Root word: {root_word.text if root_word else None} (upos={root_word.upos if root_word else None})")
        
        if root_word and root_word.upos == 'ADJ':
            # å—å‹•æ…‹æ§‹é€ ï¼šROOTå½¢å®¹è©ã‚’ä¸»å‹•è©ã¨ã™ã‚‹
            # "was unexpected" â†’ unexpected ãŒä¸»å‹•è©ç›¸å½“
            self.logger.debug(f"ğŸ¯ ä¸»å‹•è©ï¼ˆå—å‹•æ…‹ROOTå½¢å®¹è©ï¼‰: {root_word.text} (id={root_word.id})")
            print(f"   â†’ ROOTå½¢å®¹è©ãŒä¸»å‹•è©: {root_word.text} (id={root_word.id})")
            return root_word.id
        
        if root_word and root_word.upos != 'VERB':
            # æ§‹é€ çš„éšå±¤ã§ä¸»å‹•è©å€™è£œã‚’è©•ä¾¡ï¼ˆå“è©èª¤èªè­˜å¯¾ç­–å«ã‚€ï¼‰
            verb_candidates = []
            for w in sentence.words:
                if w.upos == 'VERB' or self._is_verb_by_pattern(w.text):
                    verb_candidates.append(w)
            
            print(f"   Verb candidates: {[(v.text, v.id, v.upos) for v in verb_candidates]}")
            if verb_candidates:
                # æœ€ã‚‚æ–‡ã®ä¸­å¿ƒã«è¿‘ã„å‹•è©ã‚’ä¸»å‹•è©ã¨ã™ã‚‹
                main_verb = min(verb_candidates, key=lambda v: abs(v.id - root_word.id))
                self.logger.debug(f"ğŸ¯ ä¸»å‹•è©ï¼ˆæ§‹é€ çš„é¸æŠï¼‰: {main_verb.text} (id={main_verb.id})")
                print(f"   â†’ æ§‹é€ çš„é¸æŠã§ä¸»å‹•è©: {main_verb.text} (id={main_verb.id})")
                return main_verb.id
        
        # ğŸ”„ Fallback: æœ€åˆã®å‹•è©ï¼ˆå“è©èª¤èªè­˜å¯¾ç­–å«ã‚€ï¼‰
        for word in sentence.words:
            if word.upos == 'VERB' or self._is_verb_by_pattern(word.text):
                self.logger.debug(f"ğŸ¯ ä¸»å‹•è©ï¼ˆFallbackï¼‰: {word.text} (id={word.id})")
                print(f"   â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¸»å‹•è©: {word.text} (id={word.id})")
                return word.id
        
        return None

    def _is_verb_by_pattern(self, word_text):
        """å‹•è©èªå¹¹ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¤å®šï¼ˆStanzaèª¤èªè­˜å¯¾ç­–ï¼‰"""
        # ä¸€èˆ¬çš„ãªå‹•è©èªå°¾ãƒ‘ã‚¿ãƒ¼ãƒ³
        verb_endings = ['s', 'es', 'ed', 'ing', 'en']
        verb_patterns = [
            'live', 'work', 'play', 'study', 'run', 'walk', 'talk', 'sing',
            'dance', 'write', 'read', 'teach', 'learn', 'help', 'love'
        ]
        
        word_lower = word_text.lower()
        
        # ç›´æ¥çš„ãªå‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒ
        for pattern in verb_patterns:
            if word_lower == pattern:
                return True
            for ending in verb_endings:
                if word_lower == pattern + ending:
                    return True
        
        # ç‰¹å®šèª: lives (live + s)
        if word_lower == 'lives':
            return True
            
        return False
    
    def _find_subordinate_verbs(self, sentence, main_verb_id):
        """å¾“å±ç¯€å‹•è©ã‚’ç‰¹å®šï¼ˆæ§‹é€ çš„ä¿®æ­£ç‰ˆï¼‰"""
        subordinate_verbs = []
        
        # ğŸ¯ ä¸»å‹•è©ã‚’é™¤å¤–ã—ã¦ã€æ˜ç¢ºãªå¾“å±ç¯€å‹•è©ã®ã¿ã‚’ç‰¹å®š
        for word in sentence.words:
            if word.id == main_verb_id:
                continue  # ä¸»å‹•è©ã¯é™¤å¤–
                
            # æ˜ç¢ºãªå¾“å±ç¯€ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿ã‚’å¾“å±ç¯€å‹•è©ã¨ã—ã¦èªè­˜
            if word.deprel in ['acl:relcl', 'acl', 'advcl', 'ccomp', 'xcomp']:
                # ãŸã ã—ã€ä¸»å‹•è©ã¨ã—ã¦ç‰¹å®šæ¸ˆã¿ã®å ´åˆã¯é™¤å¤–
                if word.upos == 'VERB':
                    subordinate_verbs.append(word.id)
                    self.logger.debug(f"ğŸ” å¾“å±ç¯€å‹•è©æ¤œå‡º: {word.text} (id={word.id}, deprel={word.deprel})")
        
        return subordinate_verbs
    
    def _is_verb_pattern(self, word_text):
        """å‹•è©èªå¹¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ¤å®šï¼ˆå“è©ã«ä¾å­˜ã—ãªã„ï¼‰"""
        word = word_text.lower()
        
        # æ˜ç¢ºãªå‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³
        verb_patterns = [
            'live', 'lives', 'lived', 'living',
            'need', 'needs', 'needed', 'needing',  
            'work', 'works', 'worked', 'working',
            'run', 'runs', 'ran', 'running',
            'come', 'comes', 'came', 'coming',
            'go', 'goes', 'went', 'going',
            'see', 'sees', 'saw', 'seeing',
            'get', 'gets', 'got', 'getting',
            'make', 'makes', 'made', 'making',
            'take', 'takes', 'took', 'taking',
            'know', 'knows', 'knew', 'knowing',
            'think', 'thinks', 'thought', 'thinking',
            'find', 'finds', 'found', 'finding',
            'give', 'gives', 'gave', 'giving',
            'tell', 'tells', 'told', 'telling',
            'become', 'becomes', 'became', 'becoming',
            'leave', 'leaves', 'left', 'leaving',
            'feel', 'feels', 'felt', 'feeling',
            'try', 'tries', 'tried', 'trying',
            'ask', 'asks', 'asked', 'asking',
            'seem', 'seems', 'seemed', 'seeming',
            'help', 'helps', 'helped', 'helping',
            'talk', 'talks', 'talked', 'talking',
            'turn', 'turns', 'turned', 'turning',
            'start', 'starts', 'started', 'starting',
            'show', 'shows', 'showed', 'showing',
            'hear', 'hears', 'heard', 'hearing',
            'play', 'plays', 'played', 'playing',
            'move', 'moves', 'moved', 'moving',
            'pay', 'pays', 'paid', 'paying',
            'meet', 'meets', 'met', 'meeting',
            'include', 'includes', 'included', 'including',
            'continue', 'continues', 'continued', 'continuing',
            'set', 'sets', 'setting',
            'learn', 'learns', 'learned', 'learning',
            'change', 'changes', 'changed', 'changing',
            'lead', 'leads', 'led', 'leading',
            'understand', 'understands', 'understood', 'understanding',
            'watch', 'watches', 'watched', 'watching',
            'follow', 'follows', 'followed', 'following',
            'stop', 'stops', 'stopped', 'stopping',
            'create', 'creates', 'created', 'creating',
            'speak', 'speaks', 'spoke', 'speaking',
            'read', 'reads', 'reading',
            'allow', 'allows', 'allowed', 'allowing',
            'add', 'adds', 'added', 'adding',
            'spend', 'spends', 'spent', 'spending',
            'grow', 'grows', 'grew', 'growing',
            'open', 'opens', 'opened', 'opening',
            'walk', 'walks', 'walked', 'walking',
            'win', 'wins', 'won', 'winning',
            'offer', 'offers', 'offered', 'offering',
            'remember', 'remembers', 'remembered', 'remembering',
            'love', 'loves', 'loved', 'loving',
            'consider', 'considers', 'considered', 'considering',
            'appear', 'appears', 'appeared', 'appearing',
            'buy', 'buys', 'bought', 'buying',
            'wait', 'waits', 'waited', 'waiting',
            'serve', 'serves', 'served', 'serving',
            'die', 'dies', 'died', 'dying',
            'send', 'sends', 'sent', 'sending',
            'expect', 'expects', 'expected', 'expecting',
            'build', 'builds', 'built', 'building',
            'stay', 'stays', 'stayed', 'staying',
            'fall', 'falls', 'fell', 'falling',
            'cut', 'cuts', 'cutting',
            'reach', 'reaches', 'reached', 'reaching',
            'kill', 'kills', 'killed', 'killing',
            'remain', 'remains', 'remained', 'remaining'
        ]
        
        return word in verb_patterns

    def _determine_adverb_context(self, adverb_word, main_verb_id, subordinate_verbs, sentence):
        """å‰¯è©ã®æ–‡è„ˆï¼ˆä¸»ç¯€ vs å¾“å±ç¯€ï¼‰ã‚’åˆ¤å®šï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ 
        print(f"ï¿½ ADVERB CONTEXT ANALYSIS: {adverb_word.text}")
        print(f"   - adverb_word.head: {adverb_word.head}")
        print(f"   - main_verb_id: {main_verb_id}")
        print(f"   - subordinate_verbs: {subordinate_verbs}")
        
        # ğŸ”§ Whoseæ§‹æ–‡ä½ç½®ãƒ™ãƒ¼ã‚¹åˆ¤å®šï¼ˆStanzaèª¤è§£æå¯¾ç­–ï¼‰- æœ€å„ªå…ˆ

        
        sentence_text = " ".join([w.text for w in sentence.words])

        
        if "whose" in sentence_text:

        
            whose_pos = -1

        
            main_verb_pos = -1

        
            adverb_pos = adverb_word.id

        
            

        
            for word in sentence.words:

        
                if word.text.lower() == "whose":

        
                    whose_pos = word.id

        
                elif word.id == main_verb_id:

        
                    main_verb_pos = word.id

        
            

        
            # whoseç¯€å†…ï¼ˆwhoseã€œä¸»å‹•è©å‰ï¼‰ã®å‰¯è©ã¯å¾“å±ç¯€

        
            if whose_pos > 0 and main_verb_pos > 0:

        
                if whose_pos < adverb_pos < main_verb_pos:

        
                    print(f"   â†’ WHOSEæ§‹æ–‡ä½ç½®åˆ¤å®š: SUBORDINATE (whose:{whose_pos} less_than adverb:{adverb_pos} less_than main:{main_verb_pos})")

        
                    return 'subordinate'

        
                elif adverb_pos > main_verb_pos:

        
                    print(f"   â†’ WHOSEæ§‹æ–‡ä½ç½®åˆ¤å®š: MAIN (adverb:{adverb_pos} greater_than main:{main_verb_pos})")

        
                    return 'main'

        
        

        
        # ğŸ”§ ä¸»å‹•è©ç›´æ¥ä¿®é£¾ãƒã‚§ãƒƒã‚¯ï¼ˆwhoseæ§‹æ–‡åˆ¤å®šå¾Œï¼‰
        if adverb_word.head == main_verb_id:
            self.logger.debug(f"ğŸ¯ ä¸»å‹•è©ç›´æ¥ä¿®é£¾: {adverb_word.text} â†’ ä¸»å‹•è© (head={main_verb_id})")
            print(f"   â†’ ä¸»å‹•è©ç›´æ¥ä¿®é£¾: MAIN")
            return 'main'
        
        # ç›´æ¥ã®å‹•è©ä¾å­˜é–¢ä¿‚ã‚’ãƒã‚§ãƒƒã‚¯
        head_id = adverb_word.head
        
        # ã¾ãšã€å‰¯è©ãŒç›´æ¥ä¿®é£¾ã—ã¦ã„ã‚‹å‹•è©ã‚’ç¢ºèª
        direct_head = None
        for word in sentence.words:
            if word.id == head_id:
                direct_head = word
                break
        
        print(f"   - direct_head: {direct_head.text if direct_head else None} (id={head_id})")
        
        # ğŸ”§ å¼·åŒ–ï¼šå¾“å±å‹•è©ç›´æ¥ä¿®é£¾ãƒã‚§ãƒƒã‚¯
        if direct_head and direct_head.id in subordinate_verbs:
            self.logger.debug(f"ğŸ¯ å¾“å±å‹•è©ç›´æ¥ä¿®é£¾: {adverb_word.text} â†’ {direct_head.text} (subordinate)")
            print(f"   â†’ å¾“å±å‹•è©ç›´æ¥ä¿®é£¾: SUBORDINATE")
            return 'subordinate'
        
        # ç›´æ¥ä¿®é£¾ã—ã¦ã„ã‚‹å‹•è©ãŒé–¢ä¿‚è©ç¯€å‹•è©ã®å ´åˆ
        if direct_head and direct_head.deprel in ['acl:relcl', 'acl']:
            self.logger.debug(f"ğŸ¯ é–¢ä¿‚è©ç¯€å‰¯è©æ¤œå‡º: {adverb_word.text} â†’ {direct_head.text} ({direct_head.deprel})")
            print(f"   â†’ é–¢ä¿‚è©ç¯€å‰¯è©: SUBORDINATE")
            return 'subordinate'
        
        # ğŸ”§ Whoseæ§‹æ–‡ä½ç½®ãƒ™ãƒ¼ã‚¹åˆ¤å®šï¼ˆStanzaèª¤è§£æå¯¾ç­–ï¼‰
        sentence_text = " ".join([w.text for w in sentence.words])
        if "whose" in sentence_text:
            whose_pos = -1
            main_verb_pos = -1
            adverb_pos = adverb_word.id
            
            for word in sentence.words:
                if word.text.lower() == "whose":
                    whose_pos = word.id
                elif word.id == main_verb_id:
                    main_verb_pos = word.id
            
            # whoseç¯€å†…ï¼ˆwhoseã€œä¸»å‹•è©å‰ï¼‰ã®å‰¯è©ã¯å¾“å±ç¯€
            if whose_pos > 0 and main_verb_pos > 0:
                if whose_pos < adverb_pos < main_verb_pos:
                    print(f"   â†’ WHOSEæ§‹æ–‡ä½ç½®åˆ¤å®š: SUBORDINATE (whose:{whose_pos} < adverb:{adverb_pos} < main:{main_verb_pos})")
                    return 'subordinate'
                elif adverb_pos > main_verb_pos:
                    print(f"   â†’ WHOSEæ§‹æ–‡ä½ç½®åˆ¤å®š: MAIN (adverb:{adverb_pos} > main:{main_verb_pos})")
                    return 'main'
        
        # ğŸ”§ é‡è¦ä¿®æ­£ï¼šä¸»å‹•è©ã¸ã®ä¾å­˜çµŒè·¯ãƒã‚§ãƒƒã‚¯ï¼ˆä¿®æ­£ç‰ˆï¼‰
        # å‰¯è© â†’ ... â†’ main_verb ã®çµŒè·¯ãŒã‚ã‚‹ã‹ã‚’ç¢ºèª
        visited = set()
        check_word = direct_head
        
        print(f"   - çµŒè·¯ãƒã‚§ãƒƒã‚¯é–‹å§‹...")
        while check_word and check_word.id not in visited:
            visited.add(check_word.id)
            print(f"     çµŒè·¯: {check_word.text} (id={check_word.id}, deprel={check_word.deprel})")
            
            if check_word.id == main_verb_id:
                self.logger.debug(f"ğŸ¯ ä¸»å‹•è©çµŒè·¯æ¤œå‡º: {adverb_word.text} â†’ ä¸»ç¯€")
                print(f"   â†’ ä¸»å‹•è©çµŒè·¯æ¤œå‡º: MAIN")
                return 'main'
            
            # é–¢ä¿‚è©ç¯€ãƒãƒ¼ã‚«ãƒ¼ãƒã‚§ãƒƒã‚¯
            if check_word.deprel in ['acl:relcl', 'acl', 'advcl']:
                self.logger.debug(f"ğŸ¯ å¾“å±ç¯€ãƒãƒ¼ã‚«ãƒ¼æ¤œå‡º: {adverb_word.text} â†’ {check_word.deprel}")
                print(f"   â†’ å¾“å±ç¯€ãƒãƒ¼ã‚«ãƒ¼: SUBORDINATE")
                return 'subordinate'
            
            # æ¬¡ã®headã‚’æ¢ã™
            if check_word.head == 0:
                break
                
            next_word = None
            for w in sentence.words:
                if w.id == check_word.head:
                    next_word = w
                    break
            check_word = next_word
        
        # ğŸ”§ å®‰å…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šä¸»ç¯€å‰¯è©ã¨ã—ã¦å‡¦ç†
        self.logger.debug(f"ğŸ¯ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ¤å®š: {adverb_word.text} â†’ ä¸»ç¯€ï¼ˆå®‰å…¨å´ï¼‰")
        print(f"   â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: MAIN")
        return 'main'

    def _determine_optimal_main_adverb_slot(self, phrase, category, position, main_verb_position, existing_slots):
        """
        ğŸ¯ çœŸã®ã‚·ãƒ³ãƒ—ãƒ«å‰¯è©é…ç½®ãƒ«ãƒ¼ãƒ«ï¼ˆè’¸ã—è¿”ã—å•é¡Œå®Œå…¨è§£æ±ºç‰ˆï¼‰
        
        æ ¸å¿ƒåŸç†ï¼šå€‹æ•°ã«åŸºã¥ãå›ºå®šé…ç½®
        1å€‹ã®ã¿ â†’ M2ï¼ˆã©ã“ã«ã‚ã£ã¦ã‚‚ï¼‰
        2å€‹ â†’ M2, M3ï¼ˆä½ç½®é †ï¼‰ 
        3å€‹ â†’ M1, M2, M3ï¼ˆä½ç½®é †ï¼‰
        
        å¾“æ¥ã®è¤‡é›‘ãªåˆ¤å®šã‚’æ’é™¤ã—ã€äºˆæ¸¬å¯èƒ½æ€§ã‚’æœ€å¤§åŒ–
        """
        
        # å…¨ä¿®é£¾èªã‚’åé›†ï¼ˆç¾åœ¨ã®å‡¦ç†å¯¾è±¡å«ã‚€ï¼‰
        all_modifiers = []
        
        # æ—¢å­˜ã®Mã‚¹ãƒ­ãƒƒãƒˆã‹ã‚‰ä¿®é£¾èªã‚’åé›†
        for slot in ['M1', 'M2', 'M3']:
            if slot in existing_slots and existing_slots[slot]:
                all_modifiers.append(existing_slots[slot])
        
        # ç¾åœ¨ã®ä¿®é£¾èªã‚’è¿½åŠ 
        all_modifiers.append(phrase)
        
        total_count = len(all_modifiers)
        
        self.logger.debug(f"ğŸ¯ çœŸã‚·ãƒ³ãƒ—ãƒ«Mã‚¹ãƒ­ãƒƒãƒˆåˆ¤å®š: phrase='{phrase}', ç·ä¿®é£¾èªæ•°={total_count}")
        self.logger.debug(f"  å…¨ä¿®é£¾èª: {all_modifiers}")
        
        # === çœŸã®ã‚·ãƒ³ãƒ—ãƒ«ãƒ«ãƒ¼ãƒ«é©ç”¨ ===
        
        if total_count == 1:
            # 1å€‹ã®ã¿ â†’ M2
            self.logger.debug(f"  â†’ M2é¸æŠï¼ˆ1å€‹ãƒ«ãƒ¼ãƒ«ï¼‰")
            return 'M2'
        
        elif total_count == 2:
            # 2å€‹ â†’ M2, M3
            # ç¾åœ¨ã®ä¿®é£¾èªãŒæœ€åˆã®å ´åˆã¯M2ã€2ç•ªç›®ã®å ´åˆã¯M3
            current_index = all_modifiers.index(phrase)
            if current_index == 0:
                target_slot = 'M2'
            else:
                target_slot = 'M3'
            
            self.logger.debug(f"  â†’ {target_slot}é¸æŠï¼ˆ2å€‹ãƒ«ãƒ¼ãƒ«ãƒ»ä½ç½®{current_index + 1}ï¼‰")
            return target_slot
        
        elif total_count >= 3:
            # 3å€‹ä»¥ä¸Š â†’ M1, M2, M3
            current_index = all_modifiers.index(phrase)
            slot_mapping = ['M1', 'M2', 'M3']
            
            if current_index < 3:
                target_slot = slot_mapping[current_index]
                self.logger.debug(f"  â†’ {target_slot}é¸æŠï¼ˆ3å€‹+ãƒ«ãƒ¼ãƒ«ãƒ»ä½ç½®{current_index + 1}ï¼‰")
                return target_slot
            else:
                # 3å€‹ã‚’è¶…ãˆã‚‹å ´åˆã¯ç„¡è¦–ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
                self.logger.debug(f"  â†’ Noneï¼ˆ3å€‹è¶…éãƒ»ä½ç½®{current_index + 1}ï¼‰")
                return None
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆé€šå¸¸ã¯åˆ°é”ã—ãªã„ï¼‰
        self.logger.debug(f"  â†’ M2é¸æŠï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
        return 'M2'

    def _build_prepositional_phrase(self, sentence, word, exclude_advmod=True):
        """å‰ç½®è©å¥ã®æ§‹ç¯‰ï¼ˆå®Œå…¨æ€§å¼·åŒ–ç‰ˆï¼‰"""
        print(f"DEBUG METHOD1: '{word.text}' (id={word.id})")
        
        # å‰ç½®è©å¥ã®å®Œå…¨æ§‹ç¯‰
        phrase_parts = []
        
        # å‰ç½®è©ã‚’æ¢ã™
        preposition = None
        for w in sentence.words:
            if w.head == word.id and w.deprel == 'case':
                preposition = w.text
                break
        
        if preposition:
            phrase_parts.append(preposition)
        
        # ğŸ”§ ä¿®é£¾èªåé›†ã‚’æ‹¡å¼µï¼ˆã‚ˆã‚Šå¤šãã®ä¿®é£¾é–¢ä¿‚ã‚’å«ã‚ã‚‹ï¼‰
        # ãŸã ã—ã€advmod ã¯é™¤å¤–ï¼ˆå‰¯è©ã¯ç‹¬ç«‹å‡¦ç†ï¼‰
        modifiers = []
        for w in sentence.words:
            if w.head == word.id and w.deprel in ['det', 'amod', 'compound', 'nmod', 'nmod:poss']:
                modifiers.append((w.id, w.text))
                print(f"DEBUG MOD: {w.text} (deprel={w.deprel})")
            elif w.head == word.id and w.deprel == 'advmod':
                # advmod ã¯å‰¯è©ã¨ã—ã¦ç‹¬ç«‹å‡¦ç†ã™ã‚‹ãŸã‚é™¤å¤–
                print(f"DEBUG ADVMOD EXCLUDED: {w.text} (advmod)")
            elif w.head == word.id:
                print(f"DEBUG OTHER: {w.text} (deprel={w.deprel})")
        
        # ğŸ”§ é–“æ¥ä¿®é£¾èªã‚‚åé›†ï¼ˆ"the morning breeze"ã®"morning"ã‚’ã‚­ãƒ£ãƒƒãƒï¼‰
        for w in sentence.words:
            # wordã®ç›´æ¥ä¿®é£¾èªã®ä¿®é£¾èªã‚‚åé›†
            if any(mod[0] == w.head for mod in modifiers) and w.deprel in ['amod', 'compound']:
                modifiers.append((w.id, w.text))
                print(f"DEBUG INDIRECT: {w.text} (deprel={w.deprel})")
        
        # ä½ç½®é †ã‚½ãƒ¼ãƒˆ
        modifiers.sort()
        phrase_parts.extend([mod[1] for mod in modifiers])
        phrase_parts.append(word.text)
        
        constructed_phrase = ' '.join(phrase_parts)
        self.logger.debug(f"ğŸ”§ å‰ç½®è©å¥æ§‹ç¯‰: '{word.text}' â†’ '{constructed_phrase}'")
        print(f"DEBUG RESULT: '{word.text}' -> '{constructed_phrase}'")
        
        return constructed_phrase
    
    def _build_adverbial_phrase(self, sentence, word):
        """å‰¯è©ä¿®é£¾èªã‚’å«ã‚€å¥æ§‹ç¯‰ï¼ˆ"very carefully"å¯¾å¿œï¼‰"""
        phrase_parts = []
        modifiers = []
        
        # å‰¯è©ã®ä¿®é£¾èªã‚’åé›†ï¼ˆadvmodï¼‰
        for w in sentence.words:
            if w.head == word.id and w.deprel == 'advmod':
                modifiers.append((w.id, w.text))
        
        # ä½ç½®é †ã‚½ãƒ¼ãƒˆ
        modifiers.sort()
        phrase_parts.extend([mod[1] for mod in modifiers])
        phrase_parts.append(word.text)
        
        constructed_phrase = ' '.join(phrase_parts)
        self.logger.debug(f"ğŸ”§ å‰¯è©å¥æ§‹ç¯‰: '{word.text}' â†’ '{constructed_phrase}'")
        
        return constructed_phrase
    
    # ğŸ—‘ï¸ å‰Šé™¤ï¼šãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åˆ†é¡æ©Ÿèƒ½ï¼ˆRephraseè·é›¢ãƒ™ãƒ¼ã‚¹åŸç†ã¨çŸ›ç›¾ï¼‰
    # def _classify_adverbial_phrase(...) -> ä¸è¦

    # ==== PASSIVE VOICE HANDLER ====

    def _handle_passive_voice(self, sentence, base_result: Dict, shared_context: Dict = None) -> Optional[Dict]:
        """
        å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆPhase 2å®Ÿè£…ï¼‰
        
        passive_voice_engine.py ã®æ©Ÿèƒ½ã‚’çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã«ç§»æ¤
        Stanza dependency parsing ã«ã‚ˆã‚‹å—å‹•æ…‹æ¤œå‡ºãƒ»åˆ†è§£
        
        Args:
            sentence: Stanzaè§£ææ¸ˆã¿sentence object
            base_result: ãƒ™ãƒ¼ã‚¹çµæœï¼ˆã‚³ãƒ”ãƒ¼ï¼‰
            
        Returns:
            Dict: å—å‹•æ…‹åˆ†è§£çµæœ or None
        """
        try:
            self.logger.debug("ğŸ” å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè¡Œä¸­...")
            
            # åˆ†è©æ§‹æ–‡åˆ¶å¾¡ãƒ•ãƒ©ã‚°ã‚’ãƒã‚§ãƒƒã‚¯
            grammar_info = base_result.get('grammar_info', {})
            control_flags = grammar_info.get('control_flags', {})
            participle_detected = control_flags.get('participle_detected', False)
            
            if participle_detected:
                # åˆ†è©æ§‹æ–‡ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã‚‹å ´åˆã€Sã‚¹ãƒ­ãƒƒãƒˆã®ä¸Šæ›¸ãã‚’é˜²ã
                existing_s = base_result.get('slots', {}).get('S', '')
                if existing_s == '':
                    self.logger.debug("  ğŸ›¡ï¸ åˆ†è©æ§‹æ–‡ä¿è­·: S=''ã‚’ç¶­æŒ")
            
            # å—å‹•æ…‹æ§‹é€ åˆ†æ
            passive_info = self._analyze_passive_structure(sentence)
            if not passive_info:
                self.logger.debug("  å—å‹•æ…‹ãªã— - ã‚¹ã‚­ãƒƒãƒ—")
                return None
                
            self.logger.debug("  âœ… å—å‹•æ…‹æ¤œå‡º")
            return self._process_passive_construction(sentence, passive_info, base_result, participle_detected)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ å—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _analyze_passive_structure(self, sentence) -> Optional[Dict]:
        """å—å‹•æ…‹æ§‹é€ ã®åˆ†æ"""
        passive_features = {
            'auxiliary': None,      # beå‹•è©
            'main_verb': None,      # éå»åˆ†è©
            'subject': None,        # ä¸»èª
            'agent': None,          # byå¥å‹•ä½œä¸»
            'agent_phrase': None,   # byå¥å…¨ä½“
            'type': None            # å—å‹•æ…‹ã®ç¨®é¡
        }
        
        # å…¸å‹çš„ãªéå»åˆ†è©ãƒªã‚¹ãƒˆ
        common_past_participles = {
            'written', 'bought', 'sold', 'made', 'taken', 'given', 'seen', 'done',
            'broken', 'stolen', 'found', 'lost', 'taught', 'caught', 'brought',
            'eaten', 'driven', 'shown', 'known', 'grown', 'thrown', 'chosen',
            'unexpected'  # å½¢å®¹è©å‹å—å‹•æ…‹ã®è¿½åŠ 
        }
        
        # æ§‹é€ è¦ç´ ã®æ¤œå‡º
        for word in sentence.words:
            # å—å‹•æ…‹ä¸»èªæ¤œå‡º
            if word.deprel == 'nsubj:pass':
                passive_features['subject'] = word
            elif word.deprel == 'nsubjpass':  # æ—§ç‰ˆStanzaå¯¾å¿œ
                passive_features['subject'] = word
            elif word.deprel == 'nsubj':  # å½¢å®¹è©å—å‹•æ…‹ã®å ´åˆ
                if not passive_features['subject']:  # ã¾ã è¦‹ã¤ã‹ã£ã¦ã„ãªã„å ´åˆã®ã¿
                    passive_features['subject'] = word
                    
            # å—å‹•æ…‹è£œåŠ©å‹•è©æ¤œå‡º
            elif word.deprel == 'aux:pass':
                passive_features['auxiliary'] = word
            elif word.deprel == 'auxpass':  # æ—§ç‰ˆStanzaå¯¾å¿œ
                passive_features['auxiliary'] = word
            elif word.deprel == 'cop' and word.lemma == 'be':
                passive_features['auxiliary'] = word
                
            # ä¸»å‹•è©æ¤œå‡ºï¼ˆéå»åˆ†è©ï¼‰
            elif word.deprel == 'root':
                if word.upos == 'VERB' and word.xpos == 'VBN':  # éå»åˆ†è©
                    passive_features['main_verb'] = word
                elif word.upos == 'ADJ' and word.text.lower() in common_past_participles:
                    passive_features['main_verb'] = word
            
            # érootèªã§ã®å½¢å®¹è©å—å‹•æ…‹æ¤œå‡ºï¼ˆè¤‡æ–‡å¯¾å¿œï¼‰
            elif word.upos == 'ADJ' and word.text.lower() in common_past_participles:
                if not passive_features['main_verb']:  # ã¾ã è¦‹ã¤ã‹ã£ã¦ã„ãªã„å ´åˆ
                    passive_features['main_verb'] = word
                    
            # byå¥å‹•ä½œä¸»æ¤œå‡º
            elif word.deprel == 'obl:agent':
                passive_features['agent'] = word
                passive_features['agent_phrase'] = self._build_agent_phrase(sentence, word)
            elif word.deprel == 'agent':  # æ—§ç‰ˆå¯¾å¿œ
                passive_features['agent'] = word
                passive_features['agent_phrase'] = self._build_agent_phrase(sentence, word)
        
        # å—å‹•æ…‹åˆ¤å®š
        if (passive_features['auxiliary'] and 
            passive_features['main_verb'] and 
            passive_features['subject']):
            
            passive_features['type'] = 'agent_passive' if passive_features['agent'] else 'simple_passive'
            
            self.logger.debug(f"  ä¸»èª: {passive_features['subject'].text}")
            self.logger.debug(f"  è£œåŠ©å‹•è©: {passive_features['auxiliary'].text}")
            self.logger.debug(f"  ä¸»å‹•è©: {passive_features['main_verb'].text}")
            self.logger.debug(f"  å‹•ä½œä¸»: {passive_features['agent'].text if passive_features['agent'] else 'ãªã—'}")
            self.logger.debug(f"  ç¨®é¡: {passive_features['type']}")
            
            return passive_features
        
        return None
    
    def _process_passive_construction(self, sentence, passive_info: Dict, base_result: Dict, participle_detected: bool = False) -> Dict:
        """å—å‹•æ…‹æ§‹æ–‡ã®å‡¦ç†"""
        result = base_result.copy()
        
        auxiliary = passive_info['auxiliary']
        main_verb = passive_info['main_verb']
        subject = passive_info['subject']
        agent_phrase = passive_info['agent_phrase']
        passive_type = passive_info['type']
        
        self.logger.debug(f"  å—å‹•æ…‹å‡¦ç†: {passive_type}")
        
        # ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ
        rephrase_slots = self._generate_passive_voice_slots(
            passive_type, subject, auxiliary, main_verb, agent_phrase, passive_info['agent'], sentence, participle_detected
        )
        
        # çµæœãƒãƒ¼ã‚¸
        if 'slots' not in result:
            result['slots'] = {}
        if 'sub_slots' not in result:
            result['sub_slots'] = {}
        
        result['slots'].update(rephrase_slots.get('slots', {}))
        result['sub_slots'].update(rephrase_slots.get('sub_slots', {}))
        
        # æ–‡æ³•æƒ…å ±è¨˜éŒ²
        result['grammar_info'] = {
            'patterns': ['passive_voice'],
            'passive_type': passive_type,
            'subject': subject.text,
            'auxiliary': auxiliary.text,
            'main_verb': main_verb.text,
            'agent': passive_info['agent'].text if passive_info['agent'] else None
        }
        
        self.logger.debug(f"  âœ… å—å‹•æ…‹å‡¦ç†å®Œäº†: {len(rephrase_slots.get('slots', {}))} main slots, {len(rephrase_slots.get('sub_slots', {}))} sub slots")
        return result
    
    def _generate_passive_voice_slots(self, passive_type: str, subject, auxiliary, main_verb, 
                                     agent_phrase: str, agent, sentence, participle_detected: bool = False) -> Dict:
        """å—å‹•æ…‹ã‚¿ã‚¤ãƒ—åˆ¥ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆï¼ˆå‰¯è©å‡¦ç†ã¯å°‚é–€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã«å§”è­²ï¼‰"""
        
        slots = {}
        sub_slots = {}
        
        # åŸºæœ¬ã‚¹ãƒ­ãƒƒãƒˆï¼ˆå…±é€šï¼‰
        # åˆ†è©æ§‹æ–‡ä¿è­·: åˆ†è©æ§‹æ–‡ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã‚‹å ´åˆã¯Sã‚¹ãƒ­ãƒƒãƒˆã‚’ä¸Šæ›¸ãã—ãªã„
        if not participle_detected:
            slots['S'] = self._build_subject_phrase(sentence, subject)
        else:
            self.logger.debug("åˆ†è©æ§‹æ–‡ä¿è­·: S ç©ºæ–‡å­—ä¿æŒ (by participle_construction)")
            
        slots['Aux'] = auxiliary.text
        slots['V'] = main_verb.text
        
        # âœ… å‰¯è©å‡¦ç†ã‚’é™¤å»ï¼šbyå¥ã¯å‰¯è©ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã«å§”è­²
        # byå¥ä»˜ãå—å‹•æ…‹ã§ã‚‚ã€M1ã¯è¨­å®šã›ãšå‰¯è©ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã«ä»»ã›ã‚‹
        # agent_phraseã®æƒ…å ±ã¯æ–‡æ³•æƒ…å ±ã¨ã—ã¦è¨˜éŒ²ã™ã‚‹ãŒã€ã‚¹ãƒ­ãƒƒãƒˆã«ã¯è¨­å®šã—ãªã„
        
        return {'slots': slots, 'sub_slots': sub_slots}
    
    def _build_agent_phrase(self, sentence, agent_word) -> str:
        """byå¥å…¨ä½“ã®æ§‹ç¯‰"""
        if not agent_word:
            return None
        
        # byå‰ç½®è©ã‚’æ¢ã™
        by_preposition = None
        for word in sentence.words:
            if word.text.lower() == 'by' and word.deprel == 'case' and word.head == agent_word.id:
                by_preposition = word
                break
        
        if by_preposition:
            # by + å‹•ä½œä¸» + ä¿®é£¾èª
            phrase_words = [by_preposition, agent_word]
            
            # å‹•ä½œä¸»ã®ä¿®é£¾èªã‚’è¿½åŠ 
            for word in sentence.words:
                if word.head == agent_word.id and word.deprel in ['det', 'amod', 'nmod', 'compound', 'nmod:poss']:
                    phrase_words.append(word)
            
            # IDé †ã‚½ãƒ¼ãƒˆï¼ˆèªé †ä¿æŒï¼‰
            phrase_words.sort(key=lambda w: w.id)
            return ' '.join(w.text for w in phrase_words)
        
        return f"by {agent_word.text}"
    
    def _build_subject_phrase(self, sentence, subject) -> str:
        """ä¸»èªå¥ã®æ§‹ç¯‰ï¼ˆä¿®é£¾èªå«ã‚€ï¼‰"""
        if not subject:
            return ""
            
        subject_words = [subject]
        
        # ä¸»èªã®ä¿®é£¾èªã‚’åé›†
        for word in sentence.words:
            if word.head == subject.id and word.deprel in ['det', 'amod', 'compound', 'nmod']:
                subject_words.append(word)
        
        # IDé †ã‚½ãƒ¼ãƒˆï¼ˆèªé †ä¿æŒï¼‰
        subject_words.sort(key=lambda w: w.id)
        return ' '.join(w.text for w in subject_words)

    # =============================================================================
    # åˆ†è©æ§‹æ–‡å‡¦ç†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ (Phase 3)
    # =============================================================================
    
    def _handle_participle_construction(self, sentence, base_result: Dict, shared_context: Dict = None) -> Optional[Dict]:
        """
        åˆ†è©æ§‹æ–‡ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        
        åˆ†è©æ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡ºãƒ»åˆ†è§£:
        - The team working overtime (ç¾åœ¨åˆ†è©ä¿®é£¾)
        - The woman standing quietly (ç¾åœ¨åˆ†è©ä¿®é£¾)
        - The documents being reviewed (being + éå»åˆ†è©)
        
        Args:
            sentence: Stanzaè§£ææ¸ˆã¿sentence object
            base_result: ãƒ™ãƒ¼ã‚¹çµæœï¼ˆã‚³ãƒ”ãƒ¼ï¼‰
            
        Returns:
            Dict: åˆ†è©æ§‹æ–‡åˆ†è§£çµæœ or None
        """
        print("PARTICIPLE HANDLER CALLED")
        try:
            self.logger.debug("åˆ†è©æ§‹æ–‡ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè¡Œä¸­...")
            
            # åˆ†è©æ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
            participle_info = self._analyze_participle_structure(sentence)
            print(f"PARTICIPLE INFO: {participle_info}")
            if not participle_info:
                self.logger.debug("  åˆ†è©æ§‹æ–‡ãªã— - ã‚¹ã‚­ãƒƒãƒ—")
                return None
                
            self.logger.debug("  åˆ†è©æ§‹æ–‡æ¤œå‡º")
            print("PROCESSING PARTICIPLE")
            return self._process_participle_construction(sentence, participle_info, base_result)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ åˆ†è©æ§‹æ–‡ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _analyze_participle_structure(self, sentence) -> Optional[Dict]:
        """åˆ†è©æ§‹æ–‡ã®åˆ†æ"""
        participle_info = {
            'participle_verb': None,    # åˆ†è©å‹•è©
            'subject': None,            # ä¸»èª
            'participle_type': None,    # åˆ†è©ã®ã‚¿ã‚¤ãƒ— (present/past/being)
            'modifiers': []             # ä¿®é£¾èª
        }
        
        # ç¾åœ¨åˆ†è©ã®æ¤œå‡º (VBG) - dep:acl ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å„ªå…ˆæ¤œå‡º
        for word in sentence.words:
            if word.xpos == 'VBG' and word.deprel == 'acl':
                participle_info['participle_verb'] = word
                participle_info['participle_type'] = 'present'
                
                # åˆ†è©ã®ä¸»èªã‚’æ¢ã™ï¼ˆhead ãŒ NOUN ã®å ´åˆï¼‰
                if word.head > 0:
                    head_word = next((w for w in sentence.words if w.id == word.head), None)
                    if head_word and head_word.upos == 'NOUN':
                        participle_info['subject'] = head_word
                
                # åˆ†è©ã®ä¿®é£¾èªã‚’åé›†ï¼ˆCase 49 "overtime"å•é¡Œå¯¾å¿œï¼‰
                participle_info['modifiers'] = self._find_participle_modifiers(sentence, word)
                
                self.logger.debug(f"  ç¾åœ¨åˆ†è©æ¤œå‡º: {word.text} (ID:{word.id}, HEAD:{word.head}, DEP:{word.deprel})")
                return participle_info
        
        # being + éå»åˆ†è©ã®æ¤œå‡º
        for word in sentence.words:
            if word.text.lower() == 'being' and word.deprel == 'aux:pass':
                self.logger.debug(f"  beingæ¤œå‡º: {word.text} (deprel={word.deprel}, head={word.head})")
                # beingãŒä¿®é£¾ã™ã‚‹éå»åˆ†è©ã‚’æ¢ã™
                for reviewed_word in sentence.words:
                    self.logger.debug(f"    ğŸ“ å€™è£œèª: {reviewed_word.text} (id={reviewed_word.id}, xpos={reviewed_word.xpos}, deprel={reviewed_word.deprel})")
                    if (reviewed_word.id == word.head and  # being â†’ reviewed
                        reviewed_word.xpos == 'VBN' and 
                        reviewed_word.deprel == 'acl'):
                        
                        participle_info['participle_verb'] = reviewed_word
                        participle_info['participle_type'] = 'being_past'
                        
                        # beingã®ä¸»èªã‚’æ¢ã™ï¼ˆreviewed ã® headï¼‰
                        if reviewed_word.head > 0:
                            head_word = next((w for w in sentence.words if w.id == reviewed_word.head), None)
                            if head_word and head_word.upos == 'NOUN':
                                participle_info['subject'] = head_word
                        
                        self.logger.debug(f"  being+éå»åˆ†è©æ¤œå‡º: being {reviewed_word.text} (è¢«ä¿®é£¾èª:{head_word.text if head_word else 'unknown'})")
                        return participle_info
        
        return None
    
    def _process_participle_construction(self, sentence, participle_info: Dict, base_result: Dict) -> Dict:
        """åˆ†è©ä¿®é£¾å¥ã®æ±ç”¨å‡¦ç†ï¼ˆä»•æ§˜æ›¸æº–æ‹ ï¼šã©ã®ã‚¹ãƒ­ãƒƒãƒˆã§ã‚‚å¯¾å¿œå¯èƒ½ï¼‰
        
        åˆ†è©ä¿®é£¾å¥ãŒä¿®é£¾ã™ã‚‹åè©ãŒã©ã®ã‚¹ãƒ­ãƒƒãƒˆï¼ˆS/O1/O2/C1/C2ï¼‰ã«ã‚ã£ã¦ã‚‚ã€
        é©åˆ‡ã«sub-ã‚¹ãƒ­ãƒƒãƒˆã«ç§»å‹•ã•ã›ã‚‹æ±ç”¨å®Ÿè£…
        """
        result = base_result.copy()
        
        # åˆ†è©æ§‹æ–‡ã®ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ
        slots = result.get('slots', {})
        sub_slots = result.get('sub_slots', {})
        
        participle_verb = participle_info['participle_verb']
        subject = participle_info['subject']  # å®Ÿéš›ã¯'modified_target'ãŒé©åˆ‡
        participle_type = participle_info['participle_type']
        
        self.logger.debug(f"  æ±ç”¨åˆ†è©å‡¦ç†: type={participle_type}, verb={participle_verb.text}, target={subject.text if subject else 'None'}")
        
        if not subject:
            return result
        
        # Step 1: ä¿®é£¾å¯¾è±¡ã®åè©ãŒãƒ¡ã‚¤ãƒ³æ–‡ã®ã©ã®ã‚¹ãƒ­ãƒƒãƒˆã«ã‚ã‚‹ã‹ã‚’ç‰¹å®š
        target_slot = self._identify_modified_noun_slot(subject, slots)
        
        if participle_type == 'present':
            # ç¾åœ¨åˆ†è©æ§‹æ–‡å‡¦ç†
            subject_phrase = self._build_noun_phrase_for_subject(sentence, subject)
            sub_v_content = f"{subject_phrase} {participle_verb.text}"
            
            # æ–‡é ­ã‚’å°æ–‡å­—åŒ–ï¼ˆRephraseä»•æ§˜æº–æ‹ ï¼‰
            sub_v_content = sub_v_content[0].lower() + sub_v_content[1:] if sub_v_content else sub_v_content
            
            # Step 2: è©²å½“ã‚¹ãƒ­ãƒƒãƒˆã‚’ç©ºã«ã—ã¦ã€sub-vã«ç§»å‹•
            if target_slot:
                slots[target_slot] = ""  # ä¿®é£¾å¯¾è±¡ã‚¹ãƒ­ãƒƒãƒˆã‚’ç©ºã«ã™ã‚‹
                sub_slots['sub-v'] = sub_v_content
                self.logger.debug(f"  åˆ†è©ä¿®é£¾å‡¦ç†: {target_slot} â†’ sub-v = '{sub_v_content}'")
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Sã‚’ç©ºã«ã™ã‚‹ï¼ˆå¾“æ¥ã®æŒ™å‹•ï¼‰
                slots['S'] = ""
                sub_slots['sub-v'] = sub_v_content
                self.logger.debug(f"  ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: S â†’ sub-v = '{sub_v_content}'")
                
        elif participle_type == 'being_past':
            # Case 52ãƒ‘ã‚¿ãƒ¼ãƒ³: The documents being reviewed
            subject_phrase = self._build_noun_phrase_for_subject(sentence, subject)
            print(f"SUBJECT PHRASE: '{subject_phrase}'")
            
            # Test 52æœŸå¾…å€¤ã«åˆã‚ã›ã¦: "The documents being"ï¼ˆå¤§æ–‡å­—ä¿æŒï¼‰
            sub_aux_content = f"{subject_phrase} being"
            print(f"SUB_AUX_CONTENT: '{sub_aux_content}'")
            
            # Step 2: è©²å½“ã‚¹ãƒ­ãƒƒãƒˆã‚’ç©ºã«ã—ã¦ã€sub-aux/sub-vã«åˆ†å‰²
            if target_slot:
                print(f"TARGET SLOT: {target_slot}")
                slots[target_slot] = ""
                sub_slots['sub-aux'] = sub_aux_content
                sub_slots['sub-v'] = participle_verb.text
                print(f"SET SUB_AUX: '{sub_aux_content}'")
                self.logger.debug(f"  being+éå»åˆ†è©å‡¦ç†: {target_slot} â†’ sub-aux='{sub_aux_content}' sub-v='{participle_verb.text}'")
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                print("FALLBACK CASE")
                slots['S'] = ""
                sub_slots['sub-aux'] = sub_aux_content
                sub_slots['sub-v'] = participle_verb.text
                print(f"FALLBACK SET SUB_AUX: '{sub_aux_content}'")
                self.logger.debug(f"  being+éå»åˆ†è©ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: S â†’ sub-aux='{sub_aux_content}' sub-v='{participle_verb.text}'")
        
        # çµæœã‚’æ›´æ–°
        result['slots'] = slots
        result['sub_slots'] = sub_slots
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼æƒ…å ±ã‚’è¨˜éŒ²
        grammar_info = result.get('grammar_info', {})
        grammar_info['detected_patterns'] = grammar_info.get('detected_patterns', [])
        if 'participle_construction' not in grammar_info['detected_patterns']:
            grammar_info['detected_patterns'].append('participle_construction')
        
        # åˆ¶å¾¡ãƒ•ãƒ©ã‚°è¨­å®šï¼šåˆ†è©æ§‹æ–‡ãŒæ¤œå‡ºã•ã‚ŒãŸã“ã¨ã‚’ãƒãƒ¼ã‚¯ï¼ˆä»•æ§˜æ›¸ã®handler control systemæº–æ‹ ï¼‰
        grammar_info['control_flags'] = grammar_info.get('control_flags', {})
        grammar_info['control_flags']['participle_detected'] = True
        grammar_info['control_flags']['modified_slot'] = target_slot  # ä¿®é£¾å¯¾è±¡ã‚¹ãƒ­ãƒƒãƒˆã‚’è¨˜éŒ²
        
        # Stanza/spaCyè§£æã‚¨ãƒ©ãƒ¼å¯¾å¿œï¼šå•é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒãƒ¼ã‚¯ï¼ˆä»•æ§˜æ›¸ã®Error Pattern Managementæº–æ‹ ï¼‰
        self._mark_analysis_error_patterns(sentence, participle_info, result)
        
        result['grammar_info'] = grammar_info
        
        self.logger.debug(f"  æ±ç”¨åˆ†è©å‡¦ç†å®Œäº†: slots={slots}, sub_slots={sub_slots}")
        return result
    
    def _is_standalone_participle(self, sentence, subject, participle_verb) -> bool:
        """åˆ†è©ãŒç‹¬ç«‹ã—ãŸä¿®é£¾èªã‹ï¼ˆCase 49ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰ã‚’åˆ¤å®š"""
        # ãƒ¡ã‚¤ãƒ³å‹•è©ãŒå­˜åœ¨ã—ã€åˆ†è©ã¨ã¯åˆ¥ã®å ´åˆã¯ç‹¬ç«‹åˆ†è©
        main_verb = None
        for word in sentence.words:
            if word.deprel == 'root' and word.upos == 'VERB' and word.id != participle_verb.id:
                main_verb = word
                break
        
        # ãƒ¡ã‚¤ãƒ³å‹•è©ãŒå­˜åœ¨ã—ã€ä¸»èªãŒåŒã˜å ´åˆã¯ç‹¬ç«‹åˆ†è©ï¼ˆCase 49ï¼‰
        if main_verb and subject:
            # ä¸»èªãŒãƒ¡ã‚¤ãƒ³å‹•è©ã®ä¸»èªã§ã‚‚ã‚ã‚‹å ´åˆ
            main_subj = None
            for word in sentence.words:
                if word.head == main_verb.id and word.deprel == 'nsubj':
                    main_subj = word
                    break
            
            if main_subj and main_subj.id == subject.id:
                return True
        
        return False
    
    def _mark_analysis_error_patterns(self, sentence, participle_info: Dict, result: Dict) -> None:
        """Stanza/spaCyè§£æã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒãƒ¼ã‚¯ï¼ˆä»•æ§˜æ›¸ã®Error Pattern Managementæº–æ‹ ï¼‰
        
        Case 49ã®"overtime"å•é¡Œã®ã‚ˆã†ã«ã€å“è©ãƒ»ä¾å­˜é–¢ä¿‚ã®èª¤åˆ†é¡ã‚’ãƒãƒ¼ã‚¯ã—ã¦
        adverbial_modifierãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã§ã®ä¿®æ­£æˆ¦ç•¥ã‚’æŒ‡ç¤º
        """
        try:
            grammar_info = result.get('grammar_info', {})
            if 'analysis_error_patterns' not in grammar_info:
                grammar_info['analysis_error_patterns'] = []
            
            # Pattern 1: åˆ†è©ç›´å¾Œã®åè©ãŒå‰¯è©çš„ä¿®é£¾ã¨ã—ã¦èª¤åˆ†é¡ã•ã‚Œã‚‹å•é¡Œ
            participle_verb = participle_info['participle_verb']
            sentence_text = sentence.text.lower()
            
            # "overtime", "quickly", "carefully"ãªã©ã®ç‰¹å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
            problematic_modifiers = []
            
            # åˆ†è©ã®ç›´å¾Œã®èªã‚’ç¢ºèª
            words = [w.text.lower() for w in sentence.words]
            participle_idx = None
            for i, word in enumerate(words):
                if word == participle_verb.text.lower():
                    participle_idx = i
                    break
            
            if participle_idx is not None and participle_idx + 1 < len(words):
                next_word = words[participle_idx + 1]
                
                # "overtime"ã®ã‚ˆã†ãªæ™‚é–“å‰¯è©ãŒåè©ã¨ã—ã¦èª¤åˆ†é¡ã•ã‚Œã‚‹å•é¡Œ
                time_adverbs = ['overtime', 'today', 'yesterday', 'tomorrow', 'tonight', 'now', 'then']
                if next_word in time_adverbs:
                    problematic_modifiers.append({
                        'type': 'time_adverb_misclassified_as_noun',
                        'word': next_word,
                        'expected_pos': 'ADV',
                        'correction_strategy': 'force_adverbial_classification',
                        'target_slot': 'sub-m2'
                    })
                
                # ãã®ä»–ã®å‰¯è©çš„ä¿®é£¾èªãƒ‘ã‚¿ãƒ¼ãƒ³
                elif next_word.endswith('ly'):
                    problematic_modifiers.append({
                        'type': 'manner_adverb_in_participle',
                        'word': next_word,
                        'expected_pos': 'ADV',
                        'correction_strategy': 'sub_slot_placement',
                        'target_slot': 'sub-m2'
                    })
            
            # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¨˜éŒ²
            for pattern in problematic_modifiers:
                grammar_info['analysis_error_patterns'].append(pattern)
                self.logger.debug(f"  âš ï¸ åˆ†æã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º: {pattern['type']} - {pattern['word']}")
            
            result['grammar_info'] = grammar_info
            
        except Exception as e:
            self.logger.error(f"Error in marking analysis error patterns: {e}")
    
    def _structural_main_verb_fallback(self, tokens: List[str], participle_index: int) -> List[Tuple[str, int]]:
        """æ§‹é€ çš„ä¸»å‹•è©åˆ¤å®šã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆä»•æ§˜æ›¸ã®StructuralGrammarAnalyzeræº–æ‹ ï¼‰
        
        Stanzaè§£æãŒå¤±æ•—ã—ãŸå ´åˆã®æ§‹é€ çš„åˆ¤å®š
        """
        candidates = []
        
        try:
            # åˆ†è©ã‚ˆã‚Šå¾Œã‚ã«ã‚ã‚‹å‹•è©å€™è£œã‚’æ¢ç´¢
            for i in range(participle_index + 1, len(tokens)):
                token = tokens[i]
                
                # åŸºæœ¬çš„ãªå‹•è©å½¢ãƒ‘ã‚¿ãƒ¼ãƒ³
                verb_patterns = [
                    lambda w: w.endswith('ed'),  # éå»å½¢ãƒ»éå»åˆ†è©
                    lambda w: w in ['was', 'were', 'is', 'are', 'am', 'be', 'been', 'being'],  # beå‹•è©
                    lambda w: w in ['have', 'has', 'had', 'will', 'would', 'can', 'could', 'should'],  # åŠ©å‹•è©
                    lambda w: w.endswith('s') and len(w) > 2,  # ä¸‰äººç§°å˜æ•°ç¾åœ¨
                ]
                
                for pattern in verb_patterns:
                    if pattern(token.lower()):
                        candidates.append((token, i))
                        break
                
        except Exception as e:
            self.logger.error(f"Error in structural main verb fallback: {e}")
        
        return candidates
    
    def _identify_modified_noun_slot(self, modified_noun, current_slots: Dict) -> Optional[str]:
        """ä¿®é£¾å¯¾è±¡ã®åè©ãŒãƒ¡ã‚¤ãƒ³æ–‡ã®ã©ã®ã‚¹ãƒ­ãƒƒãƒˆã«ã‚ã‚‹ã‹ã‚’ç‰¹å®š
        
        Args:
            modified_noun: åˆ†è©ã«ä¿®é£¾ã•ã‚Œã‚‹åè©ï¼ˆStanza word objectï¼‰
            current_slots: ç¾åœ¨ã®ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ­ãƒƒãƒˆçŠ¶æ³
            
        Returns:
            str: è©²å½“ã‚¹ãƒ­ãƒƒãƒˆåï¼ˆS/O1/O2/C1/C2ï¼‰ã¾ãŸã¯ None
        """
        try:
            modified_text = modified_noun.text.lower()
            
            # å„ã‚¹ãƒ­ãƒƒãƒˆã®å†…å®¹ã¨ç…§åˆ
            for slot_name, slot_value in current_slots.items():
                if slot_name in ['S', 'O1', 'O2', 'C1', 'C2'] and slot_value:
                    # ã‚¹ãƒ­ãƒƒãƒˆå€¤ã«ä¿®é£¾å¯¾è±¡åè©ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    slot_words = slot_value.lower().split()
                    if modified_text in slot_words:
                        self.logger.debug(f"  ğŸ¯ ä¿®é£¾å¯¾è±¡ç‰¹å®š: {modified_text} â†’ {slot_name} ('{slot_value}')")
                        return slot_name
                    
                    # éƒ¨åˆ†ä¸€è‡´ã‚‚ãƒã‚§ãƒƒã‚¯ï¼ˆ"team" in "The team"ï¼‰
                    if modified_text in slot_value.lower():
                        self.logger.debug(f"  ğŸ¯ ä¿®é£¾å¯¾è±¡ç‰¹å®šï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰: {modified_text} â†’ {slot_name} ('{slot_value}')")
                        return slot_name
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šä¸»èªã‚’ä»®å®š
            self.logger.debug(f"  âš ï¸ ä¿®é£¾å¯¾è±¡ç‰¹å®šå¤±æ•—: {modified_text} â†’ Sï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰")
            return 'S'
            
        except Exception as e:
            self.logger.error(f"Error identifying modified noun slot: {e}")
            return 'S'  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    
    def _process_basic_pattern_with_participle_control(self, sentence, base_result: Dict) -> Dict:
        """åˆ†è©æ§‹æ–‡åˆ¶å¾¡ãƒ•ãƒ©ã‚°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã®åŸºæœ¬æ–‡å‹å‡¦ç†
        
        åˆ†è©æ§‹æ–‡ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã§æ—¢ã«è¨­å®šã•ã‚ŒãŸã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ ã‚’å°Šé‡ã—ã€
        ä¸»èªã‚¹ãƒ­ãƒƒãƒˆï¼ˆSï¼‰ã¯çµ¶å¯¾ã«å¤‰æ›´ã›ãšã€ãƒ¡ã‚¤ãƒ³å‹•è©ãƒ»ç›®çš„èªãƒ»è£œèªã®ã¿ã‚’å‡¦ç†
        """
        result = base_result.copy()
        slots = result.get('slots', {})
        
        # ğŸš¨ é‡è¦ï¼šåˆ†è©æ§‹æ–‡ã§ã¯ä¸»èªã¯çµ¶å¯¾ã«å¤‰æ›´ã—ãªã„
        original_subject = slots.get('S', '')
        self.logger.debug(f"  ğŸ¯ åˆ†è©æ§‹æ–‡åˆ¶å¾¡ãƒ¢ãƒ¼ãƒ‰: ä¸»èª'{original_subject}'ã‚’ä¿è­·ã€ä»–è¦ç´ ã®ã¿å‡¦ç†")
        
        # Step 1: ãƒ¡ã‚¤ãƒ³å‹•è©ã®ç‰¹å®šï¼ˆåˆ†è©æ§‹æ–‡ã§ãªã„çœŸã®ä¸»å‹•è©ï¼‰
        main_verb = self._find_main_verb_excluding_participles(sentence)
        if main_verb:
            slots['V'] = main_verb.text
            self.logger.debug(f"    âœ… ãƒ¡ã‚¤ãƒ³å‹•è©: {main_verb.text}")
            
            # Step 2: ãƒ¡ã‚¤ãƒ³å‹•è©ã®ç›´æ¥ç›®çš„èªã‚’ç‰¹å®š
            main_object = self._find_verb_direct_object(sentence, main_verb)
            if main_object and not slots.get('O1'):
                object_phrase = self._build_noun_phrase_for_subject(sentence, main_object)
                slots['O1'] = object_phrase
                self.logger.debug(f"    âœ… ç›®çš„èª: {object_phrase}")
            
            # Step 3: ãƒ¡ã‚¤ãƒ³å‹•è©ã®è£œèªã‚’ç‰¹å®š
            main_complement = self._find_verb_complement(sentence, main_verb)
            if main_complement and not slots.get('C1'):
                complement_phrase = self._build_noun_phrase_for_subject(sentence, main_complement)
                
                # ğŸ”§ é‡è¤‡é˜²æ­¢ï¼šä¸»å‹•è©ã¨åŒã˜å˜èªã¯è£œèªã«è¨­å®šã—ãªã„
                main_verb_text = slots.get('V', '')
                if complement_phrase != main_verb_text:
                    slots['C1'] = complement_phrase
                    self.logger.debug(f"    âœ… è£œèª: {complement_phrase}")
                else:
                    self.logger.debug(f"    ğŸš« è£œèªé‡è¤‡å›é¿: {complement_phrase} (ä¸»å‹•è©ã¨åŒä¸€)")
                    
        # ğŸš¨ ä¸»èªä¿è­·ï¼šåˆ†è©æ§‹æ–‡ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒè¨­å®šã—ãŸä¸»èªã‚’çµ¶å¯¾ã«ç¶­æŒ
        slots['S'] = original_subject
        self.logger.debug(f"    ğŸ›¡ï¸ ä¸»èªä¿è­·: S='{original_subject}' (åˆ†è©æ§‹æ–‡ã«ã‚ˆã‚Šå›ºå®š)")
        
        result['slots'] = slots
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼æƒ…å ±ã‚’è¨˜éŒ²
        grammar_info = result.get('grammar_info', {})
        grammar_info['detected_patterns'] = grammar_info.get('detected_patterns', [])
        if 'basic_five_pattern' not in grammar_info['detected_patterns']:
            grammar_info['detected_patterns'].append('basic_five_pattern')
        
        result['grammar_info'] = grammar_info
        
        self.logger.debug(f"  âœ… åˆ†è©æ§‹æ–‡åˆ¶å¾¡å‡¦ç†å®Œäº†: slots={slots}")
        return result
    
    def _find_main_verb_excluding_participles(self, sentence):
        """åˆ†è©ã‚’é™¤ã„ãŸçœŸã®ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’ç‰¹å®š"""
        for word in sentence.words:
            # ROOTå‹•è©ã§ã€åˆ†è©ï¼ˆVBG/VBNï¼‰ã§ãªã„ã‚‚ã®
            if (word.deprel == 'root' and 
                word.upos == 'VERB' and 
                word.xpos not in ['VBG', 'VBN']):
                return word
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ROOTå‹•è©
        for word in sentence.words:
            if word.deprel == 'root' and word.upos == 'VERB':
                return word
                
        return None
    
    def _find_verb_direct_object(self, sentence, verb_word):
        """å‹•è©ã®ç›´æ¥ç›®çš„èªã‚’ç‰¹å®š"""
        for word in sentence.words:
            if word.head == verb_word.id and word.deprel == 'obj':
                return word
        return None
    
    def _find_verb_complement(self, sentence, verb_word):
        """å‹•è©ã®è£œèªã‚’ç‰¹å®š"""
        for word in sentence.words:
            if word.head == verb_word.id and word.deprel in ['xcomp', 'ccomp', 'nsubj:xsubj']:
                return word
        return None
    
    def _apply_analysis_error_corrections(self, sentence, base_result: Dict) -> Dict:
        """è§£æã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œã™ã‚‹ä¿®æ­£æˆ¦ç•¥ã‚’é©ç”¨ï¼ˆä»•æ§˜æ›¸æº–æ‹ ï¼‰
        
        åˆ†è©æ§‹æ–‡ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒæ¤œå‡ºã—ãŸè§£æã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ã„ã¦ã€
        æ­£ã—ã„ã‚¹ãƒ­ãƒƒãƒˆé…ç½®ã‚’è¡Œã†
        """
        corrections = {}
        
        try:
            grammar_info = base_result.get('grammar_info', {})
            error_patterns = grammar_info.get('analysis_error_patterns', [])
            
            if not error_patterns:
                return corrections
            
            self.logger.debug(f"ğŸ”§ è§£æã‚¨ãƒ©ãƒ¼ä¿®æ­£é–‹å§‹: {len(error_patterns)}å€‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³")
            
            for pattern in error_patterns:
                pattern_type = pattern.get('type')
                word = pattern.get('word')
                target_slot = pattern.get('target_slot')
                strategy = pattern.get('correction_strategy')
                
                if pattern_type == 'time_adverb_misclassified_as_noun':
                    # Case 49ã® "overtime" å•é¡Œ
                    if strategy == 'force_adverbial_classification' and target_slot:
                        corrections[target_slot] = word
                        self.logger.debug(f"  âœ… æ™‚é–“å‰¯è©ä¿®æ­£: {word} â†’ {target_slot}")
                
                elif pattern_type == 'manner_adverb_in_participle':
                    # -ly å‰¯è©ã®åˆ†è©æ§‹æ–‡å†…é…ç½®
                    if strategy == 'sub_slot_placement' and target_slot:
                        corrections[target_slot] = word
                        self.logger.debug(f"  âœ… æ–¹æ³•å‰¯è©ä¿®æ­£: {word} â†’ {target_slot}")
            
            return corrections
            
        except Exception as e:
            self.logger.error(f"Error in applying analysis error corrections: {e}")
            return corrections
    
    def _find_participle_modifiers(self, sentence, participle_word) -> List:
        """åˆ†è©ã®ä¿®é£¾èªã‚’æ¤œå‡ºï¼ˆStanzaè§£æã‚¨ãƒ©ãƒ¼å¯¾å¿œï¼‰"""
        modifiers = []
        
        # ç›´æ¥ã®ä¿®é£¾èªï¼ˆStanzaè§£æçµæœãƒ™ãƒ¼ã‚¹ï¼‰
        for modifier in sentence.words:
            if modifier.head == participle_word.id:
                modifiers.append(modifier)
        
        # æ§‹é€ çš„ä¿®é£¾èªæ¤œå‡ºï¼ˆCase 49 "overtime"ã®ã‚ˆã†ãªèª¤åˆ†é¡å¯¾å¿œï¼‰
        words = [w.text.lower() for w in sentence.words]
        participle_idx = None
        
        for i, word in enumerate(sentence.words):
            if word.id == participle_word.id:
                participle_idx = i
                break
        
        if participle_idx is not None and participle_idx + 1 < len(sentence.words):
            next_word = sentence.words[participle_idx + 1]
            
            # æ™‚é–“å‰¯è©ãŒåè©ã¨ã—ã¦èª¤åˆ†é¡ã•ã‚Œã¦ã„ã‚‹å ´åˆ
            time_adverbs = ['overtime', 'today', 'yesterday', 'tomorrow', 'tonight', 'now', 'then']
            if (next_word.text.lower() in time_adverbs and 
                next_word.upos == 'NOUN' and 
                next_word.deprel == 'obj'):
                
                modifiers.append(next_word)
                self.logger.debug(f"  ğŸ”§ æ§‹é€ çš„ä¿®é£¾èªæ¤œå‡º: {next_word.text} (èª¤åˆ†é¡ä¿®æ­£)")
        
        return modifiers
    
    def _detect_expanded_participle_patterns(self, sentence) -> Optional[Dict]:
        """æ‹¡å¼µåˆ†è©ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºï¼ˆæ§‹é€ çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰"""
        # ã‚ˆã‚Šå¹…åºƒã„åˆ†è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã™ã‚‹å ´åˆã«ä½¿ç”¨
        # ç¾åœ¨ã¯åŸºæœ¬æ¤œå‡ºã«å§”è­²
        return None
    
    def _find_main_verb_object(self, sentence):
        """ãƒ¡ã‚¤ãƒ³å‹•è©ã®ç›®çš„èªã‚’æ¢ã™ï¼ˆCase 49ç”¨ï¼‰"""
        # rootã®ç›´æ¥ç›®çš„èªã‚’æ¢ã™
        for word in sentence.words:
            if word.deprel == 'root' and word.upos == 'VERB':
                # ã“ã®å‹•è©ã®ç›®çš„èªã‚’æ¢ã™
                for obj_word in sentence.words:
                    if obj_word.head == word.id and obj_word.deprel == 'obj':
                        return obj_word
        return None
    
    def _build_noun_phrase_for_subject(self, sentence, subject_word) -> str:
        """ä¸»èªã®åè©å¥ã‚’æ§‹ç¯‰"""
        # å† è©ãƒ»ä¿®é£¾èªã‚’å«ã‚€åè©å¥ã‚’æ§‹ç¯‰
        phrase_words = []
        
        # å† è©ã‚’æ¢ã™
        for word in sentence.words:
            if word.head == subject_word.id and word.deprel == 'det':
                phrase_words.append((word.id, word.text))
        
        # ä¸»èªæœ¬ä½“ã‚’è¿½åŠ 
        phrase_words.append((subject_word.id, subject_word.text))
        
        # IDé †ã§ã‚½ãƒ¼ãƒˆ
        phrase_words.sort(key=lambda x: x[0])
        
        return " ".join([w[1] for w in phrase_words])
    
    def _assign_participle_modifiers(self, modifiers, sub_slots, sentence, is_single_modifier_case):
        """åˆ†è©ã®ä¿®é£¾èªã‚’é©åˆ‡ãª sub-m ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®"""
        modifier_texts = []
        
        for modifier in modifiers:
            # ä¿®é£¾èªã®æ§‹ç¯‰ï¼ˆå‰ç½®è©å¥ãªã©ã‚‚å«ã‚€ï¼‰
            if modifier.deprel == 'obl':
                # å‰ç½®è©å¥ã®å ´åˆ
                prep_phrase = self._build_prepositional_phrase(sentence, modifier, exclude_advmod=False)
                modifier_texts.append(prep_phrase)
            else:
                # å˜ç´”ãªä¿®é£¾èª
                modifier_texts.append(modifier.text)
        
        # ä¿®é£¾èªã®é…ç½®ãƒ«ãƒ¼ãƒ«
        if is_single_modifier_case:
            # Case 49: 1ã¤ã®ä¿®é£¾èª -> sub-m2
            if len(modifier_texts) >= 1:
                sub_slots['sub-m2'] = modifier_texts[0]
        else:
            # Case 50, 51: è¤‡æ•°ä¿®é£¾èª -> sub-m2, sub-m3
            if len(modifier_texts) >= 1:
                sub_slots['sub-m2'] = modifier_texts[0]
            if len(modifier_texts) >= 2:
                sub_slots['sub-m3'] = modifier_texts[1]
    
    def _assign_modifiers_to_sub_slots(self, modifiers, sub_slots, sentence):
        """ä¿®é£¾èªã‚’ sub-m ã‚¹ãƒ­ãƒƒãƒˆã«å‰²ã‚Šå½“ã¦"""
        modifier_texts = []
        
        for modifier in modifiers:
            # ä¿®é£¾èªã®æ§‹ç¯‰ï¼ˆå‰ç½®è©å¥ãªã©ã‚‚å«ã‚€ï¼‰
            if modifier.deprel == 'obl':
                # å‰ç½®è©å¥ã®å ´åˆ
                prep_phrase = self._build_prepositional_phrase(sentence, modifier, exclude_advmod=False)
                modifier_texts.append(prep_phrase)
            else:
                # å˜ç´”ãªä¿®é£¾èª
                modifier_texts.append(modifier.text)
        
        # Simple Ruleã«å¾“ã£ã¦ M ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®
        self._apply_simple_rule_to_sub_modifiers(modifier_texts, sub_slots)
    
    def _build_prepositional_phrase(self, sentence, obl_word, exclude_advmod=True) -> str:
        """å‰ç½®è©å¥ã‚’æ§‹ç¯‰"""
        print(f"DEBUG METHOD2: '{obl_word.text}' (id={obl_word.id}) exclude_advmod={exclude_advmod}")
        
        # obl_wordã¯å‰ç½®è©å¥ã®ç›®çš„èªãªã®ã§ã€å‰ç½®è©ã‚’æ¢ã™
        prep = None
        for word in sentence.words:
            if word.head == obl_word.id and word.deprel == 'case':
                prep = word
                break
        
        # ğŸ”§ advmodé™¤å¤–å‡¦ç†ã‚’è¿½åŠ 
        advmod_modifiers = []
        for word in sentence.words:
            if word.head == obl_word.id and word.deprel == 'advmod':
                advmod_modifiers.append(word.text)
                print(f"DEBUG ADVMOD FOUND: {word.text} modifies {obl_word.text}")
        
        if prep:
            # å† è©ã‚‚å«ã‚ã¦æ§‹ç¯‰
            det = None
            for word in sentence.words:
                if word.head == obl_word.id and word.deprel == 'det':
                    det = word
                    break
            
            # ğŸ”§ exclude_advmodãŒTrueã®å ´åˆã€advmodã‚’å«ã‚ãªã„
            if exclude_advmod and advmod_modifiers:
                print(f"DEBUG EXCLUDING ADVMOD: {advmod_modifiers}")
                if det:
                    result = f"{prep.text} {det.text} {obl_word.text}"
                else:
                    result = f"{prep.text} {obl_word.text}"
            else:
                # å¾“æ¥ã®å‡¦ç†ï¼ˆadvmodã‚‚å«ã‚ã‚‹ï¼‰
                advmod_text = ' '.join(advmod_modifiers)
                if det:
                    if advmod_modifiers:
                        result = f"{prep.text} {det.text} {advmod_text} {obl_word.text}"
                    else:
                        result = f"{prep.text} {det.text} {obl_word.text}"
                else:
                    if advmod_modifiers:
                        result = f"{prep.text} {advmod_text} {obl_word.text}"
                    else:
                        result = f"{prep.text} {obl_word.text}"
        else:
            # å‰ç½®è©ãŒãªã„å ´åˆ
            if exclude_advmod and advmod_modifiers:
                print(f"DEBUG NO PREP, EXCLUDING ADVMOD: {advmod_modifiers}")
                result = obl_word.text
            else:
                advmod_text = ' '.join(advmod_modifiers)
                if advmod_modifiers:
                    result = f"{advmod_text} {obl_word.text}"
                else:
                    result = obl_word.text
        
        print(f"DEBUG METHOD2 RESULT: '{result}'")
        return result
    
    def _apply_simple_rule_to_sub_modifiers(self, modifier_texts, sub_slots):
        """Simple Ruleã‚’ sub-m ã‚¹ãƒ­ãƒƒãƒˆã«é©ç”¨"""
        if not modifier_texts:
            return
        
        count = len(modifier_texts)
        
        if count == 1:
            sub_slots['sub-m2'] = modifier_texts[0]
        elif count == 2:
            sub_slots['sub-m1'] = modifier_texts[0]
            sub_slots['sub-m2'] = modifier_texts[1]
        elif count == 3:
            sub_slots['sub-m1'] = modifier_texts[0]
            sub_slots['sub-m2'] = modifier_texts[1]
            sub_slots['sub-m3'] = modifier_texts[2]
    
    def _is_object_of_main_verb(self, sentence, word) -> bool:
        """èªãŒä¸»å‹•è©ã®ç›®çš„èªã‹ãƒã‚§ãƒƒã‚¯"""
        # ä¸»å‹•è©ã‚’æ¢ã™
        main_verb = None
        for w in sentence.words:
            if w.deprel == 'root':
                main_verb = w
                break
        
        if main_verb and word.head == main_verb.id and word.deprel in ['obj', 'dobj']:
            return True
        return False
    
    def _build_prepositional_phrase(self, sentence, prep, exclude_advmod=True) -> str:
        """å‰ç½®è©å¥ã®æ§‹ç¯‰"""
        print(f"DEBUG METHOD3: called with prep='{prep.text}' exclude_advmod={exclude_advmod}")
        
        # ğŸ”§ æ™‚é–“å‰¯è©ã®ç‰¹åˆ¥å‡¦ç†ï¼ˆ"yesterday", "today", "tomorrow"ãªã©ï¼‰
        if prep.text.lower() in ['yesterday', 'today', 'tomorrow', 'now', 'then']:
            print(f"DEBUG METHOD3: temporal adverb '{prep.text}' treated as standalone")
            return prep.text
        
        phrase_words = [prep]
        
        # å‰ç½®è©ã®ç›®çš„èªã‚’æ¢ã™
        for word in sentence.words:
            if word.head == prep.id:
                phrase_words.append(word)
                print(f"DEBUG METHOD3: found object '{word.text}' for prep '{prep.text}'")
                
                # ç›®çš„èªã®ä¿®é£¾èªã‚‚è¿½åŠ 
                for modifier in sentence.words:
                    if modifier.head == word.id and modifier.deprel in ['det', 'amod']:
                        phrase_words.append(modifier)
                        print(f"DEBUG METHOD3: added modifier '{modifier.text}' (deprel={modifier.deprel})")
                    elif modifier.head == word.id and modifier.deprel == 'advmod':
                        if not exclude_advmod:
                            phrase_words.append(modifier)
                            print(f"DEBUG METHOD3: added advmod '{modifier.text}' (exclude_advmod=False)")
                        else:
                            print(f"DEBUG METHOD3: excluded advmod '{modifier.text}' (exclude_advmod=True)")
        
        # IDé †ã‚½ãƒ¼ãƒˆï¼ˆèªé †ä¿æŒï¼‰
        phrase_words.sort(key=lambda w: w.id)
        result = ' '.join(w.text for w in phrase_words)
        print(f"DEBUG METHOD3 RESULT: '{result}'")
        return result
    
    def _build_noun_phrase(self, sentence, noun) -> str:
        """åè©å¥ã®æ§‹ç¯‰ï¼ˆä¿®é£¾èªå«ã‚€ï¼‰"""
        if not noun:
            return ""
            
        noun_words = [noun]
        
        # åè©ã®ä¿®é£¾èªã‚’åé›†
        for word in sentence.words:
            if word.head == noun.id and word.deprel in ['det', 'amod', 'compound', 'nmod']:
                noun_words.append(word)
        
        # IDé †ã‚½ãƒ¼ãƒˆï¼ˆèªé †ä¿æŒï¼‰
        noun_words.sort(key=lambda w: w.id)
        return ' '.join(w.text for w in noun_words)

    # =============================================================================
    # åŠ©å‹•è©è¤‡åˆä½“å‡¦ç†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ (Phase 3)
    # =============================================================================
    
    def _handle_auxiliary_complex(self, sentence, base_result: Dict, shared_context: Dict = None) -> Dict[str, Any]:
        """
        åŠ©å‹•è©è¤‡åˆä½“å‡¦ç†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ (Phase 3)
        
        è¤‡åˆåŠ©å‹•è©ãƒã‚§ãƒ¼ãƒ³ã®å‡¦ç†:
        - is being (ç¾åœ¨é€²è¡Œå—å‹•æ…‹)
        - will be (æœªæ¥æ™‚åˆ¶)
        - has finished (ç¾åœ¨å®Œäº†)
        - will have been (æœªæ¥å®Œäº†)
        
        Migration Source: perfect_progressive_engine.py ã®ãƒ­ã‚¸ãƒƒã‚¯ç¶™æ‰¿
        """
        print(f"  åŠ©å‹•è©è¤‡åˆå‡¦ç†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼é–‹å§‹")
        
        result = {
            'handler': 'auxiliary_complex',
            'analysis_type': 'auxiliary_chain_processing',
            'metadata': {}
        }
        
        # åŠ©å‹•è©ãƒã‚§ãƒ¼ãƒ³æ¤œå‡º
        auxiliary_chain = []
        main_verb = None
        subject = None
        
        # ç¬¬ä¸€ãƒ‘ã‚¹: ä¸»å‹•è©ã‚’ç‰¹å®š
        for word in sentence.words:
            if word.deprel == 'root' and word.upos == 'VERB':
                main_verb = word
                print(f"    ä¸»å‹•è©æ¤œå‡º: {word.text}")
                break
        
        # ç¬¬äºŒãƒ‘ã‚¹: åŠ©å‹•è©ã‚’ç¯€ãƒ¬ãƒ™ãƒ«ã§åˆ†é¡ã—ã¦åé›†
        main_auxiliary_words = []  # ä¸»ç¯€åŠ©å‹•è©
        sub_auxiliary_words = []   # å¾“å±ç¯€åŠ©å‹•è©
        
        for word in sentence.words:
            # åŠ©å‹•è©æ¤œå‡º
            is_auxiliary = False
            if word.deprel in ['aux', 'aux:pass']:
                is_auxiliary = True
                print(f"    æ¨™æº–åŠ©å‹•è©: {word.text} ({word.deprel})")
            elif word.deprel == 'cop' and word.lemma == 'be':
                # é€£çµè©ã¯åŠ©å‹•è©ã§ã¯ãªã„ï¼ˆè£œèªæ§‹æ–‡ã®beå‹•è©ï¼‰
                # å—å‹•æ…‹ãƒ»é€²è¡Œå½¢ã®æ–‡è„ˆã§ã®ã¿åŠ©å‹•è©ã¨ã—ã¦æ‰±ã†
                is_auxiliary_context = False
                
                # å—å‹•æ…‹ãƒã‚§ãƒƒã‚¯: è¿‘ãã«éå»åˆ†è©ãŒã‚ã‚‹ã‹
                for next_word in sentence.words:
                    if (next_word.id > word.id and 
                        next_word.upos == 'VERB' and 
                        (next_word.xpos in ['VBN'] or next_word.text.endswith('ed'))):
                        is_auxiliary_context = True
                        break
                        
                # é€²è¡Œå½¢ãƒã‚§ãƒƒã‚¯: è¿‘ãã«beingãŒã‚ã‚‹ã‹
                for next_word in sentence.words:
                    if (next_word.id > word.id and 
                        next_word.text.lower() == 'being'):
                        is_auxiliary_context = True
                        break
                
                if is_auxiliary_context:
                    is_auxiliary = True
                    print(f"    æ–‡è„ˆçš„åŠ©å‹•è©be: {word.text}")
                else:
                    print(f"    âŒ é€£çµè©be (éåŠ©å‹•è©): {word.text}")
                    continue
            elif (word.upos == 'VERB' and 
                  word.text.lower() in ['can', 'could', 'will', 'would', 'shall', 'should', 'may', 'might', 'must']):
                is_auxiliary = True
                print(f"    æ³•åŠ©å‹•è©: {word.text}")
            elif word.text.lower() == 'being' and word.upos in ['AUX', 'VERB']:
                is_auxiliary = True
                print(f"    beingæ¤œå‡º: {word.text}")
            
            # åŠ©å‹•è©ã®ç¯€ãƒ¬ãƒ™ãƒ«åˆ†é¡
            if is_auxiliary:
                # ä¸»ç¯€åŠ©å‹•è©: ä¸»å‹•è©ã«ç›´æ¥ä¾å­˜
                if main_verb and (word.head == main_verb.id or 
                                  (word.deprel == 'cop' and word.text.lower() in ['am', 'is', 'are', 'was', 'were'])):
                    main_auxiliary_words.append(word)
                    print(f"      â†’ ä¸»ç¯€åŠ©å‹•è©: {word.text}")
                else:
                    sub_auxiliary_words.append(word)
                    print(f"      â†’ å¾“å±ç¯€åŠ©å‹•è©: {word.text}")
            
            # ä¸»èªæ¤œå‡º (ä¸»æ–‡ã®ã¿)
            elif word.deprel == 'nsubj' and main_verb and word.head == main_verb.id:
                subject = word
                print(f"    ä¸»èªæ¤œå‡º: {word.text}")
        
        # ä¸»ç¯€åŠ©å‹•è©ã‚’ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆã—ã¦çµ±åˆ
        if main_auxiliary_words:
            main_auxiliary_words.sort(key=lambda x: x.id)
            auxiliary_chain = [word.text for word in main_auxiliary_words]
            print(f"    ä¸»ç¯€åŠ©å‹•è©ãƒã‚§ãƒ¼ãƒ³: {auxiliary_chain}")
        else:
            auxiliary_chain = []
        
        # ç¬¬ä¸‰ãƒ‘ã‚¹: å¾“å±ç¯€åŠ©å‹•è©ã‚’sub-auxã¨ã—ã¦å‡¦ç†
        subordinate_auxiliaries = []
        for aux_word in sub_auxiliary_words:
            subordinate_auxiliaries.append(aux_word.text.lower())
            print(f"    å¾“å±ç¯€åŠ©å‹•è©çµ±åˆ: {aux_word.text}")
        
        # åŠ©å‹•è©ãƒã‚§ãƒ¼ãƒ³ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å‡¦ç†
        if len(auxiliary_chain) >= 1:
            print(f"    ä¸»ç¯€åŠ©å‹•è©ãƒã‚§ãƒ¼ãƒ³ç™ºè¦‹: {auxiliary_chain}")
            
            # åŠ©å‹•è©ãƒã‚§ãƒ¼ãƒ³çµåˆ (æ ¸å¿ƒãƒ­ã‚¸ãƒƒã‚¯)
            auxiliary_phrase = ' '.join(auxiliary_chain)
            result['metadata']['auxiliary_chain'] = auxiliary_phrase
            result['metadata']['auxiliary_count'] = len(auxiliary_chain)
            
            # ã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ ã®åˆæœŸåŒ–
            slots = {}
            sub_slots = {}
            
            # ä¸»æ–‡è¦ç´ ã®é…ç½®
            if subject:
                subject_phrase = self._build_phrase_with_modifiers(sentence, subject)
                slots['S'] = subject_phrase
            
            # åŠ©å‹•è©å¥ã‚’Auxã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®ï¼ˆä¸»æ–‡ã®ã¿ï¼‰
            slots['Aux'] = auxiliary_phrase
            
            # ä¸»å‹•è©å‡¦ç†
            if main_verb:
                verb_phrase = self._build_phrase_with_modifiers(sentence, main_verb)
                slots['V'] = verb_phrase
            
            # å¾“å±ç¯€åŠ©å‹•è©ã®å‡¦ç†
            if subordinate_auxiliaries:
                # åˆ†è©æ§‹æ–‡ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒæ—¢ã«sub-auxã‚’è¨­å®šã—ã¦ã„ã‚‹å ´åˆã¯ä¸Šæ›¸ãã—ãªã„
                existing_sub_aux = base_result.get('sub_slots', {}).get('sub-aux')
                print(f"    CHECK existing_sub_aux: '{existing_sub_aux}', new: {subordinate_auxiliaries}")
                # åˆ†è©æ§‹æ–‡ã®å ´åˆã¯"being"ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if existing_sub_aux and ('being' in existing_sub_aux or len(existing_sub_aux.split()) > 1):
                    print(f"    åˆ†è©æ§‹æ–‡sub-auxä¿è­·: '{existing_sub_aux}' (åŠ©å‹•è©: {subordinate_auxiliaries})")
                else:
                    sub_slots['sub-aux'] = ' '.join(subordinate_auxiliaries)
                    print(f"    å¾“å±ç¯€åŠ©å‹•è©: sub-aux = {sub_slots['sub-aux']}")
            
            print(f"    åŠ©å‹•è©è¤‡åˆå‡¦ç†å®Œäº†: Aux='{auxiliary_phrase}'")
            return {'slots': slots, 'sub_slots': sub_slots}
        
        elif subordinate_auxiliaries:
            # ä¸»ç¯€åŠ©å‹•è©ãªã—ã€å¾“å±ç¯€åŠ©å‹•è©ã®ã¿ã®å ´åˆ
            # åˆ†è©æ§‹æ–‡ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒæ—¢ã«sub-auxã‚’è¨­å®šã—ã¦ã„ã‚‹å ´åˆã¯ä¸Šæ›¸ãã—ãªã„
            existing_sub_aux = base_result.get('sub_slots', {}).get('sub-aux')
            print(f"    CHECK SUB-ONLY existing_sub_aux: '{existing_sub_aux}', new: {subordinate_auxiliaries}")
            # åˆ†è©æ§‹æ–‡ã®å ´åˆã¯"being"ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if existing_sub_aux and ('being' in existing_sub_aux or len(existing_sub_aux.split()) > 1):
                print(f"    åˆ†è©æ§‹æ–‡sub-auxä¿è­·(å¾“å±ã®ã¿): '{existing_sub_aux}' (åŠ©å‹•è©: {subordinate_auxiliaries})")
                return {'slots': {}, 'sub_slots': {}}
            else:
                print(f"    å¾“å±ç¯€åŠ©å‹•è©ã®ã¿: {subordinate_auxiliaries}")
                return {'slots': {}, 'sub_slots': {'sub-aux': ' '.join(subordinate_auxiliaries)}}
        
        else:
            print(f"    åŠ©å‹•è©ãƒã‚§ãƒ¼ãƒ³æœªæ¤œå‡º")
            return None

    def _is_main_clause_auxiliary(self, word, main_verb) -> bool:
        """ä¸»æ–‡ã®åŠ©å‹•è©ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        # åŸºæœ¬çš„ãªåŠ©å‹•è©åˆ¤å®š
        is_auxiliary = (
            word.upos == 'AUX' or 
            (word.upos == 'VERB' and word.deprel in ['aux', 'cop']) or
            word.text.lower() in ['be', 'have', 'will', 'can', 'should', 'would', 'could', 'may', 'might', 'must']
        )
        
        if not is_auxiliary:
            return False
        
        # ä¸»å‹•è©ã«ç›´æ¥é–¢é€£ã™ã‚‹åŠ©å‹•è©ã®ã¿ï¼ˆä¸»æ–‡ãƒ¬ãƒ™ãƒ«ï¼‰
        if word.deprel in ['aux', 'cop'] and word.head == main_verb.id:
            return True
            
        return False

    def _handle_conjunction(self, sentence, base_result: Dict, shared_context: Dict = None) -> Optional[Dict]:
        """
        æ¥ç¶šè©å‡¦ç†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆ"as if"ç­‰ã®å¾“å±æ¥ç¶šè©å¯¾å¿œï¼‰
        migrationã‚¨ãƒ³ã‚¸ãƒ³ã‹ã‚‰ã®ç§»æ¤ç‰ˆ
        """
        self.logger.debug("æ¥ç¶šè©ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè¡Œä¸­...")
        
        # å¾“å±æ¥ç¶šè©ã®æ¤œå‡ºï¼ˆmark + advcl ã®çµ„ã¿åˆã‚ã›ï¼‰
        mark_words = []
        advcl_verbs = []
        
        for word in sentence.words:
            if word.deprel == 'mark' and word.upos == 'SCONJ':
                mark_words.append(word)
            elif word.deprel == 'advcl':
                advcl_verbs.append(word)
        
        if not mark_words or not advcl_verbs:
            self.logger.debug("  â†’ æ¥ç¶šè©æ§‹æ–‡æœªæ¤œå‡º")
            return None
        
        # "as if"ç­‰ã®è¤‡åˆæ¥ç¶šè©ã‚’æ¤œå‡º
        conjunction_phrase = self._detect_compound_conjunction(sentence, mark_words)
        if not conjunction_phrase:
            self.logger.debug("  â†’ è¤‡åˆæ¥ç¶šè©æœªæ¤œå‡º")
            return None
        
        self.logger.debug(f"  ğŸ”— è¤‡åˆæ¥ç¶šè©æ¤œå‡º: '{conjunction_phrase}'")
        
        # å¾“å±ç¯€ã®è¦ç´ ã‚’æŠ½å‡º
        advcl_verb = advcl_verbs[0]  # æœ€åˆã®advclå‹•è©ã‚’ä½¿ç”¨
        sub_slots = self._extract_subordinate_conjunction_elements(sentence, advcl_verb, conjunction_phrase)
        
        # ä¸»ç¯€ã¯æ—¢å­˜ã®base_resultã‚’ä½¿ç”¨ï¼ˆæ¥ç¶šè©æ§‹æ–‡ã§ã¯ç§»è¡Œã—ãªã„ï¼‰
        main_slots = base_result.get('slots', {}) if base_result else {}
        
        # å¾“å±ç¯€è¦ç´ ã‚’ä¸»ç¯€ã‹ã‚‰é™¤å»
        self._remove_subordinate_elements_from_main(main_slots, sub_slots, advcl_verb)
        
        # â˜… M2ä½ç½®ã‚’ç©ºã«è¨­å®šï¼ˆæ¥ç¶šè©ã¯å¾“å±ç¯€ã®sub-m2ã«é…ç½®ï¼‰
        main_slots['M2'] = ''
        
        # â˜… M1ä½ç½®ã«æ¥ç¶šè©ã‚’é…ç½®ï¼ˆä½†ã—æ—¢å­˜ã®Mã‚¹ãƒ­ãƒƒãƒˆã¯ä¿è­·ï¼‰
        # æ¥ç¶šè©æ§‹é€ ã§ã¯ä¸€èˆ¬çš„ã«M1ã¯ä½¿ã‚ãªã„ãŸã‚ã€ã“ã®å‡¦ç†ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
        # if not main_slots.get('M1'):
        #     main_slots['M1'] = ''
        
        result = {
            'slots': main_slots,
            'sub_slots': sub_slots,
            'grammar_info': {
                'detected_patterns': ['conjunction'],
                'conjunction_type': conjunction_phrase,
                'subordinate_verb': advcl_verb.text
            }
        }
        
        self.logger.debug(f"  âœ… æ¥ç¶šè©å‡¦ç†å®Œäº†: {len(sub_slots)}å€‹ã®å¾“å±ç¯€è¦ç´ ")
        return result
    
    def _detect_compound_conjunction(self, sentence, mark_words) -> Optional[str]:
        """è¤‡åˆæ¥ç¶šè©ã®æ¤œå‡ºï¼ˆ"as if"ç­‰ï¼‰"""
        if len(mark_words) < 2:
            return None
        
        # é€£ç¶šã™ã‚‹mark wordã‚’æ¤œå‡º
        mark_words.sort(key=lambda x: x.id)
        
        # "as if"ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
        for i in range(len(mark_words) - 1):
            word1 = mark_words[i]
            word2 = mark_words[i + 1]
            
            # é€£ç¶šã™ã‚‹ä½ç½®ã«ã‚ã‚‹å ´åˆ
            if word2.id == word1.id + 1:
                phrase = f"{word1.text} {word2.text}"
                if phrase.lower() in ['as if', 'even if', 'as though']:
                    return phrase
        
        return None
    
    def _extract_subordinate_conjunction_elements(self, sentence, advcl_verb, conjunction_phrase) -> Dict[str, str]:
        """å¾“å±ç¯€è¦ç´ ã®æŠ½å‡º"""
        sub_slots = {}
        
        # æ¥ç¶šè©ã‚’sub-m2ã«é…ç½®
        sub_slots['sub-m2'] = conjunction_phrase
        
        # å¾“å±ç¯€ã®ä¸»èª
        for word in sentence.words:
            if word.head == advcl_verb.id and word.deprel == 'nsubj':
                sub_slots['sub-s'] = word.text
                break
        
        # å¾“å±ç¯€ã®å‹•è©
        sub_slots['sub-v'] = advcl_verb.text
        
        # å¾“å±ç¯€ã®ç›®çš„èª
        for word in sentence.words:
            if word.head == advcl_verb.id and word.deprel == 'obj':
                sub_slots['sub-o1'] = word.text
                break
        
        return sub_slots

    def _remove_subordinate_elements_from_main(self, main_slots: Dict[str, str], sub_slots: Dict[str, str], advcl_verb) -> None:
        """å¾“å±ç¯€è¦ç´ ã‚’ä¸»ç¯€ã‹ã‚‰é™¤å»ï¼ˆä¸»ç¯€ã®ä¸»èªãƒ»å‹•è©ã¯ä¿æŒï¼‰"""
        # å¾“å±ç¯€ã«ã®ã¿å­˜åœ¨ã™ã‚‹è¦ç´ ã‚’ç‰¹å®š
        subordinate_only_elements = set()
        
        # å¾“å±ç¯€ã®ç›®çš„èªãƒ»è£œèªç­‰ï¼ˆä¸»èªãƒ»å‹•è©ä»¥å¤–ï¼‰ã‚’å–å¾—
        for sub_key, sub_value in sub_slots.items():
            if sub_value and sub_key.startswith('sub-') and sub_key not in ['sub-m2', 'sub-s', 'sub-v']:
                subordinate_only_elements.add(sub_value.lower())
        
        # ä¸»ç¯€ã‚¹ãƒ­ãƒƒãƒˆã‹ã‚‰å¾“å±ç¯€ã«ã®ã¿å­˜åœ¨ã™ã‚‹è¦ç´ ã‚’é™¤å»
        for main_key, main_value in list(main_slots.items()):
            if main_value and main_value.lower() in subordinate_only_elements:
                # Case 28å¯¾å¿œ: O1ã‚¹ãƒ­ãƒƒãƒˆè‡ªä½“ã‚’å‰Šé™¤ã›ãšã€ç©ºæ–‡å­—è¨­å®šã‚’å›é¿
                if main_key == 'O1':
                    # O1ã‚¹ãƒ­ãƒƒãƒˆã¯å‰Šé™¤ï¼ˆæœŸå¾…å€¤ã«å­˜åœ¨ã—ãªã„ï¼‰
                    del main_slots[main_key]
                    self.logger.debug(f"  ğŸ”„ å¾“å±ç¯€å°‚ç”¨è¦ç´ O1ã‚¹ãƒ­ãƒƒãƒˆå‰Šé™¤: {main_key}='{main_value}'")
                else:
                    main_slots[main_key] = ''
                    self.logger.debug(f"  ğŸ”„ å¾“å±ç¯€å°‚ç”¨è¦ç´ ã‚’ä¸»ç¯€ã‹ã‚‰é™¤å»: {main_key}='{main_value}' â†’ ''")


# =============================================================================
# Phase 0 ãƒ†ã‚¹ãƒˆç”¨ åŸºæœ¬ãƒ†ã‚¹ãƒˆãƒãƒ¼ãƒã‚¹
# =============================================================================

def clean_result_for_json(result: Dict) -> Dict:
    """
    JSONå‡ºåŠ›ç”¨ã«çµæœã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    å¾ªç’°å‚ç…§ã‚„éJSONå¯¾å¿œã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’é™¤å»
    """
    def clean_value(obj, visited=None):
        if visited is None:
            visited = set()
        
        # å¾ªç’°å‚ç…§ãƒã‚§ãƒƒã‚¯
        obj_id = id(obj)
        if obj_id in visited:
            return "<circular_reference>"
        
        if isinstance(obj, (str, int, float, bool, type(None))):
            return obj
        elif isinstance(obj, dict):
            visited.add(obj_id)
            cleaned = {}
            for k, v in obj.items():
                # ç‰¹å®šã®ã‚­ãƒ¼ã¯é™¤å¤–
                if k in ['stanza_doc', 'spacy_doc', '__dict__', '__weakref__']:
                    continue
                try:
                    cleaned[k] = clean_value(v, visited.copy())
                except (RecursionError, RuntimeError):
                    cleaned[k] = f"<error_cleaning_{k}>"
            return cleaned
        elif isinstance(obj, list):
            visited.add(obj_id)
            try:
                return [clean_value(item, visited.copy()) for item in obj[:100]]  # æœ€å¤§100è¦ç´ 
            except (RecursionError, RuntimeError):
                return ["<error_cleaning_list>"]
        else:
            # ãã®ä»–ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯æ–‡å­—åˆ—è¡¨ç¾
            try:
                return str(obj)[:200]  # æœ€å¤§200æ–‡å­—
            except:
                return "<unrepresentable_object>"


def process_batch_sentences(input_file: str, output_file: str = None) -> str:
    """
    ãƒãƒƒãƒå‡¦ç†ï¼š53ä¾‹æ–‡ä¸€æ‹¬å®Ÿè¡Œ
    
    Args:
        input_file: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ« (JSON)
        output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« (çœç•¥æ™‚ã¯ auto-generated)
    
    Returns:
        output_file: ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«å
    """
    import argparse
    from datetime import datetime
    
    print(f"ğŸ”„ ãƒãƒƒãƒå‡¦ç†é–‹å§‹: {input_file}")
    
    # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
    except FileNotFoundError:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ - {input_file}")
        return None
    except json.JSONDecodeError as e:
        print(f"âŒ JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
        return None
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
    if output_file is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"batch_results_{timestamp}.json"
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    mapper = UnifiedStanzaRephraseMapper()
    print("âœ… ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    # çµæœæ ¼ç´
    results = {
        "meta": {
            "input_file": input_file,
            "processed_at": datetime.now().isoformat(),
            "total_sentences": 0,
            "success_count": 0,
            "error_count": 0
        },
        "results": {}
    }
    
    # ãƒ‡ãƒ¼ã‚¿å½¢å¼åˆ¤å®šã¨å‡¦ç†
    if "data" in test_data:
        # final_54_test_data.json å½¢å¼
        sentences_data = test_data["data"]
        results["meta"]["total_sentences"] = len(sentences_data)
        
        print(f"ğŸ“Š å‡¦ç†å¯¾è±¡: {len(sentences_data)}ä¾‹æ–‡")
        
        for test_id, test_case in sentences_data.items():
            try:
                sentence = test_case["sentence"]
                print(f"Processing [{test_id}]: {sentence}")
                
                # æ–‡è§£æå®Ÿè¡Œ
                result = mapper.process(sentence)
                
                # åŸºæœ¬ã‚¹ãƒ­ãƒƒãƒˆæƒ…å ±ã®ã¿æŠ½å‡ºï¼ˆå¾ªç’°å‚ç…§å•é¡Œã‚’å›é¿ï¼‰
                clean_result = {
                    "sentence": result.get("sentence", ""),
                    "slots": result.get("slots", {}),
                    "sub_slots": result.get("sub_slots", {}),
                    "meta": {
                        "processing_time": result.get("meta", {}).get("processing_time", 0.0),
                        "sentence_id": result.get("meta", {}).get("sentence_id", 0),
                        "active_handlers": result.get("meta", {}).get("active_handlers", 0)
                    }
                }
                
                results["results"][test_id] = {
                    "sentence": sentence,
                    "analysis_result": clean_result,
                    "expected": test_case.get("expected", {}),
                    "status": "success"
                }
                results["meta"]["success_count"] += 1
                
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼ [{test_id}]: {e}")
                results["results"][test_id] = {
                    "sentence": test_case.get("sentence", ""),
                    "error": str(e),
                    "status": "error"
                }
                results["meta"]["error_count"] += 1
    
    elif isinstance(test_data, list):
        # ã‚·ãƒ³ãƒ—ãƒ«ãƒªã‚¹ãƒˆå½¢å¼ ["sentence1", "sentence2", ...]
        results["meta"]["total_sentences"] = len(test_data)
        
        for i, sentence in enumerate(test_data):
            try:
                print(f"Processing [{i+1}]: {sentence}")
                result = mapper.process(sentence)
                
                # åŸºæœ¬ã‚¹ãƒ­ãƒƒãƒˆæƒ…å ±ã®ã¿æŠ½å‡ºï¼ˆå¾ªç’°å‚ç…§å•é¡Œã‚’å›é¿ï¼‰
                clean_result = {
                    "sentence": result.get("sentence", ""),
                    "slots": result.get("slots", {}),
                    "sub_slots": result.get("sub_slots", {}),
                    "meta": {
                        "processing_time": result.get("meta", {}).get("processing_time", 0.0),
                        "sentence_id": result.get("meta", {}).get("sentence_id", 0),
                        "active_handlers": result.get("meta", {}).get("active_handlers", 0)
                    }
                }
                
                results["results"][str(i+1)] = {
                    "sentence": sentence,
                    "analysis_result": clean_result,
                    "status": "success"
                }
                results["meta"]["success_count"] += 1
                
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼ [{i+1}]: {e}")
                results["results"][str(i+1)] = {
                    "sentence": sentence,
                    "error": str(e),
                    "status": "error"
                }
                results["meta"]["error_count"] += 1
    
    else:
        print("âŒ æœªå¯¾å¿œã®ãƒ‡ãƒ¼ã‚¿å½¢å¼ã§ã™")
        return None
    
    # çµæœä¿å­˜
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… å‡¦ç†å®Œäº†ï¼")
        print(f"ğŸ“ çµæœä¿å­˜: {output_file}")
        print(f"ğŸ“Š çµ±è¨ˆ:")
        print(f"   ç·æ•°: {results['meta']['total_sentences']}")
        print(f"   æˆåŠŸ: {results['meta']['success_count']}")
        print(f"   ã‚¨ãƒ©ãƒ¼: {results['meta']['error_count']}")
        print(f"   æˆåŠŸç‡: {results['meta']['success_count']/results['meta']['total_sentences']*100:.1f}%")
        
        return output_file
        
    except Exception as e:
        print(f"âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def main():
    """CLI ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Unified Stanza-Rephrase Mapper - ãƒãƒƒãƒå‡¦ç†ç‰ˆ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # 53ä¾‹æ–‡ä¸€æ‹¬å‡¦ç†
  python unified_stanza_rephrase_mapper.py --input final_test_system/final_54_test_data.json
  
  # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®š
  python unified_stanza_rephrase_mapper.py --input sentences.json --output my_results.json
  
  # ã‚·ãƒ³ãƒ—ãƒ«ãƒªã‚¹ãƒˆå½¢å¼ã®JSONã‚‚å¯¾å¿œ
  python unified_stanza_rephrase_mapper.py --input simple_sentences.json
        """
    )
    
    parser.add_argument(
        '--input', '-i',
        required=True,
        help='å…¥åŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¾‹æ–‡ãƒ‡ãƒ¼ã‚¿ï¼‰'
    )
    
    parser.add_argument(
        '--output', '-o',
        help='å‡ºåŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆçœç•¥æ™‚ã¯è‡ªå‹•ç”Ÿæˆï¼‰'
    )
    
    args = parser.parse_args()
    
    # ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰
    result_file = process_batch_sentences(args.input, args.output)
    if result_file:
        print(f"\nğŸ‰ ãƒãƒƒãƒå‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
        print(f"çµæœãƒ•ã‚¡ã‚¤ãƒ«: {result_file}")
    else:
        print("\nâŒ ãƒãƒƒãƒå‡¦ç†ãŒå¤±æ•—ã—ã¾ã—ãŸ")
        exit(1)

if __name__ == "__main__":
    main()
