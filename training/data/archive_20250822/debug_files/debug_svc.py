#!/usr/bin/env python3
"""
SVCæ–‡å‹ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"The car is red."ã®è©³ç´°åˆ†æ
"""

from spacy_human_grammar_mapper import SpacyHumanGrammarMapper

def debug_svc_recognition():
    """SVCèªè­˜ã®ãƒ‡ãƒãƒƒã‚°"""
    mapper = SpacyHumanGrammarMapper()
    sentence = "The car is red."
    
    print(f"=== SVCæ–‡å‹ãƒ‡ãƒãƒƒã‚°: '{sentence}' ===")
    
    # èªå½™è§£æ
    lexical_info = mapper._extract_lexical_knowledge(sentence)
    tokens = lexical_info['tokens']
    
    print(f"\nğŸ“‹ ãƒˆãƒ¼ã‚¯ãƒ³åŒ–çµæœ ({len(tokens)}èª):")
    for i, token in enumerate(tokens):
        print(f"  [{i}] '{token['text']}' - POS:{token['pos']} TAG:{token['tag']}")
    
    # å„ãƒˆãƒ¼ã‚¯ãƒ³ã®åˆ¤å®šçµæœ
    print(f"\nğŸ” å„ãƒˆãƒ¼ã‚¯ãƒ³ã®å“è©åˆ¤å®š:")
    for i, token in enumerate(tokens):
        text = token['text']
        print(f"  [{i}] '{text}':")
        print(f"      determiner: {mapper._is_determiner_human(token)}")
        print(f"      noun: {mapper._is_noun_human(token)}")
        print(f"      linking_verb: {mapper._is_linking_verb_human(token)}")
        print(f"      complement: {mapper._is_complement_human(token)}")
    
    # SVCèªè­˜ãƒ†ã‚¹ãƒˆ
    print(f"\nğŸ¯ SVCèªè­˜ãƒ†ã‚¹ãƒˆ:")
    svc_result = mapper._detect_svc_pattern_human(tokens)
    print(f"  çµæœ: {svc_result}")
    
    # å…¨ä½“çš„ãªæ–‡å‹èªè­˜
    print(f"\nğŸ“Š å…¨ä½“ã®æ–‡å‹èªè­˜:")
    full_result = mapper.analyze_sentence(sentence)
    print(f"  çµæœ: {full_result}")

if __name__ == '__main__':
    debug_svc_recognition()
