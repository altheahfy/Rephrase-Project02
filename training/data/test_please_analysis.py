#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
spaCyè§£æã«ã‚ˆã‚‹ã€Œpleaseã€ã®å“è©ãƒ»ä¾å­˜é–¢ä¿‚ç¢ºèª
"""

import spacy

def analyze_please_sentences():
    """pleaseã‚’å«ã‚€æ–‡ã®spaCyè§£æç¢ºèª"""
    # spaCyãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    nlp = spacy.load("en_core_web_sm")
    
    test_sentences = [
        "Help me, please.",
        "Call her back, please.",
        "Come here quickly, please.",
        "Give it to him, please.",
    ]
    
    print("=== spaCyè§£æï¼šã€Œpleaseã€ã®å“è©ãƒ»ä¾å­˜é–¢ä¿‚ç¢ºèª ===\n")
    
    for sentence in test_sentences:
        print(f"ğŸ“ ä¾‹æ–‡: '{sentence}'")
        doc = nlp(sentence)
        
        for token in doc:
            if token.text.lower() == 'please':
                print(f"  ğŸ” ã€Œpleaseã€è©³ç´°:")
                print(f"    - token.text: {token.text}")
                print(f"    - token.lemma_: {token.lemma_}")
                print(f"    - token.pos_: {token.pos_}")
                print(f"    - token.tag_: {token.tag_}")
                print(f"    - token.dep_: {token.dep_}")
                print(f"    - token.head: {token.head}")
                print(f"    - token.i (ä½ç½®): {token.i}")
                
        print(f"  ğŸ“Š å…¨tokens: {[(t.text, t.pos_, t.dep_) for t in doc]}")
        print()

if __name__ == "__main__":
    analyze_please_sentences()
