# ===== Rephrase Parsing Engine =====
# å®Œå…¨çµ±åˆç‰ˆ: ChatGPT 34ãƒ«ãƒ¼ãƒ« + ã‚µãƒ–ã‚¯ãƒ­ãƒ¼ã‚ºåˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³ + spaCyèªå½™èªè­˜
# ç›®æ¨™: ã™ã¹ã¦ã®è‹±æ–‡æ³•ãƒ«ãƒ¼ãƒ«ã«å¯¾å¿œã—ãŸå“è©åˆ†è§£ã‚·ã‚¹ãƒ†ãƒ 

import json
import re
import os

# spaCyèªå½™èªè­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    import spacy
    nlp = spacy.load("en_core_web_sm")
    SPACY_AVAILABLE = True
    print("âœ… spaCyèªå½™èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
except ImportError:
    SPACY_AVAILABLE = False
    nlp = None
    print("âš ï¸ spaCyæœªä½¿ç”¨ï¼ˆå¾“æ¥ã®å½¢æ…‹ç´ ãƒ«ãƒ¼ãƒ«ã®ã¿ï¼‰")

class RephraseParsingEngine:
    """å®Œå…¨çµ±åˆç‰ˆRephraseå“è©åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.engine_name = "Rephrase Parsing Engine v1.1 + spaCyèªå½™èªè­˜"
        self.rules_data = self.load_rules()
        self.nlp = nlp if SPACY_AVAILABLE else None  # spaCyã‚¨ãƒ³ã‚¸ãƒ³
        
    def load_rules(self):
        """æ–‡æ³•ãƒ«ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        rules_file = os.path.join(os.path.dirname(__file__), 'rephrase_rules_v1.0.json')
        try:
            with open(rules_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"è­¦å‘Š: {rules_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åŸºæœ¬ãƒ«ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            return self.get_basic_rules()
    
    def get_basic_rules(self):
        """åŸºæœ¬çš„ãªæ–‡æ³•ãƒ«ãƒ¼ãƒ«ã‚»ãƒƒãƒˆï¼ˆfallbackï¼‰"""
        return {
            "cognitive_verbs": ["think", "believe", "know", "realize", "understand", "feel", "guess", "suppose"],
            "modal_verbs": ["will", "would", "can", "could", "may", "might", "must", "should", "ought"],
            "be_verbs": ["am", "is", "are", "was", "were", "being", "been"],
            "have_verbs": ["have", "has", "had"],
            "copular_verbs": ["become", "seem", "appear", "look", "sound", "feel", "taste", "smell"],
            "ditransitive_verbs": ["give", "tell", "show", "send", "teach", "buy", "make", "get"]
        }
    
    def detect_phrasal_verbs_with_spacy(self, sentence):
        """spaCyã‚’ä½¿ç”¨ã—ãŸå¥å‹•è©ã®è‡ªå‹•æ¤œå‡º"""
        if not self.nlp:
            return {}
            
        doc = self.nlp(sentence)
        phrasal_verbs = {}
        
        # prt (particle) ä¾å­˜é–¢ä¿‚ã‚’æ¢ã™
        for token in doc:
            if token.dep_ == "prt" and token.head.pos_ == "VERB":
                verb_text = token.head.text.lower()
                particle_text = token.text.lower()
                
                # å¥å‹•è©ã®çµ„ã¿åˆã‚ã›ã‚’è¨˜éŒ²
                phrasal_verbs[verb_text] = {
                    'particle': particle_text,
                    'verb_token': token.head,
                    'particle_token': token,
                    'separated': token.i > token.head.i + 1  # åˆ†é›¢ã•ã‚Œã¦ã„ã‚‹ã‹
                }
        
        return phrasal_verbs
    
    def extract_phrasal_verb_components(self, sentence, phrasal_verbs):
        """å¥å‹•è©ã®æˆåˆ†ã‚’æŠ½å‡ºã—ã¦ã‚¹ãƒ­ãƒƒãƒˆã«åˆ†é…ï¼ˆãƒ‡ãƒãƒƒã‚°ç‰ˆï¼‰"""
        if not phrasal_verbs:
            return {}
            
        doc = self.nlp(sentence)
        result = {}
        
        for verb, pv_info in phrasal_verbs.items():
            verb_token = pv_info['verb_token']
            particle_token = pv_info['particle_token']
            
            # å‹•è©ã‚’V ã‚¹ãƒ­ãƒƒãƒˆã«
            result['V'] = [{'value': verb_token.text, 'type': 'phrasal_verb', 'rule_id': 'spacy-phrasal-verb'}]
            
            # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚¯ãƒ«ã‚’M2ã‚¹ãƒ­ãƒƒãƒˆã«
            result['M2'] = result.get('M2', [])
            result['M2'].append({
                'value': particle_token.text, 
                'type': 'phrasal_verb_particle', 
                'rule_id': 'spacy-phrasal-verb-particle'
            })
            
            # å…¨ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’èª¿ã¹ã¦ä¾å­˜é–¢ä¿‚ã‚’æŠ½å‡º
            for token in doc:
                # ä¸»èªã®æ¤œå‡º
                if token.dep_ == "nsubj" and token.head.i == verb_token.i:
                    subject_phrase = self.get_noun_phrase(token)
                    result['S'] = [{'value': subject_phrase, 'type': 'subject', 'rule_id': 'spacy-phrasal-verb-subject'}]
                
                # ç›®çš„èªã®æ¤œå‡º
                elif token.dep_ == "dobj" and token.head.i == verb_token.i:
                    object_phrase = self.get_noun_phrase(token)
                    result['O1'] = [{'value': object_phrase, 'type': 'direct_object', 'rule_id': 'spacy-phrasal-verb-object'}]
                
                # åŠ©å‹•è©ã®æ¤œå‡ºï¼ˆãƒ¢ãƒ¼ãƒ€ãƒ«ç–‘å•æ–‡ã§ãªã„å ´åˆï¼‰
                elif token.dep_ == "aux" and token.head.i == verb_token.i and 'Aux' not in result:
                    result['Aux'] = [{'value': token.text, 'type': 'auxiliary', 'rule_id': 'spacy-phrasal-verb-aux'}]
        
        return result
    
    def get_noun_phrase(self, noun_token):
        """åè©å¥ã‚’å† è©ãƒ»ä¿®é£¾èªã¨ä¸€ç·’ã«æŠ½å‡ºï¼ˆä»£åè©å¯¾å¿œç‰ˆï¼‰"""
        # ä»£åè©ã®å ´åˆã¯å˜ç‹¬ã§è¿”ã™
        if noun_token.pos_ == "PRON":
            return noun_token.text
            
        phrase_parts = []
        
        # å† è©ãƒ»ä¿®é£¾èªã‚’å«ã‚€å¥ã‚’æ§‹ç¯‰
        for child in noun_token.children:
            if child.dep_ in ["det", "amod", "poss"]:
                phrase_parts.append((child.i, child.text))
        
        # ä¸­å¿ƒã®åè©
        phrase_parts.append((noun_token.i, noun_token.text))
        
        # èªé †ã§ã‚½ãƒ¼ãƒˆ
        phrase_parts.sort()
        return ' '.join([part[1] for part in phrase_parts])

    def clean_punctuation(self, text):
        """å¥èª­ç‚¹ã‚’é™¤å»"""
        import string
        # å¥èª­ç‚¹ã‚’é™¤å»
        return text.translate(str.maketrans('', '', string.punctuation))

    def analyze_sentence_with_spacy(self, sentence):
        """spaCyã‚’ä½¿ç”¨ã—ãŸåŒ…æ‹¬çš„ãªæ–‡è§£æ"""
        if not self.nlp:
            return None
            
        doc = self.nlp(sentence)
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›
        print(f"ğŸ” spaCyè§£æ: '{sentence}'")
        for token in doc:
            print(f"  {token.text:10} | POS: {token.pos_:6} | DEP: {token.dep_:10} | HEAD: {token.head.text}")
        
        result = {}
        
        # å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‡¦ç†ã—ã¦ã‚¹ãƒ­ãƒƒãƒˆã«åˆ†é¡
        for token in doc:
            # ä¸»èªã®æ¤œå‡º (nsubj, nsubjpass)
            if token.dep_ in ["nsubj", "nsubjpass"]:
                subject_phrase = self.get_spacy_noun_phrase(token)
                result['S'] = [{'value': self.clean_punctuation(subject_phrase), 'type': 'subject', 'rule_id': 'spacy-analysis'}]
            
            # å‹•è©ã®æ¤œå‡º (ROOT)
            elif token.dep_ == "ROOT" and token.pos_ == "VERB":
                result['V'] = [{'value': self.clean_punctuation(token.text), 'type': 'main_verb', 'rule_id': 'spacy-analysis'}]
            
            # åŠ©å‹•è©ã®æ¤œå‡º (aux)
            elif token.dep_ == "aux":
                result['Aux'] = [{'value': self.clean_punctuation(token.text), 'type': 'auxiliary', 'rule_id': 'spacy-analysis'}]
            
            # ç›´æ¥ç›®çš„èªã®æ¤œå‡º (dobj)
            elif token.dep_ == "dobj":
                object_phrase = self.get_spacy_noun_phrase(token)
                result['O1'] = [{'value': self.clean_punctuation(object_phrase), 'type': 'direct_object', 'rule_id': 'spacy-analysis'}]
            
            # æ™‚é–“ä¿®é£¾èªã®æ¤œå‡º (temporal modifier)
            elif token.dep_ in ["npadvmod", "advmod"] and self.is_temporal_expression(token):
                temporal_phrase = self.get_spacy_temporal_phrase(token)
                result['M3'] = result.get('M3', [])
                result['M3'].append({'value': self.clean_punctuation(temporal_phrase), 'type': 'temporal_modifier', 'rule_id': 'spacy-temporal'})
            
            # å‰ç½®è©å¥ã®æ¤œå‡º (prep)
            elif token.dep_ == "prep":
                prep_phrase = self.get_spacy_prepositional_phrase(token)
                # å‰ç½®è©å¥ã®ç¨®é¡ã«ã‚ˆã£ã¦ã‚¹ãƒ­ãƒƒãƒˆã‚’æ±ºå®š
                slot_type = self.classify_prepositional_phrase(token.text.lower())
                if slot_type not in result:
                    result[slot_type] = []
                result[slot_type].append({'value': self.clean_punctuation(prep_phrase), 'type': 'prepositional_phrase', 'rule_id': 'spacy-prep'})
        
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£èªè­˜ã«ã‚ˆã‚‹æ™‚é–“è¡¨ç¾ã®æ¤œå‡º
        for ent in doc.ents:
            if ent.label_ in ["DATE", "TIME"]:
                # æ—¢ã«æ¤œå‡ºã•ã‚ŒãŸæ™‚é–“ä¿®é£¾èªã¨é‡è¤‡ã—ãªã„ã‹ãƒã‚§ãƒƒã‚¯
                if 'M3' not in result or not any(ent.text in item['value'] for item in result['M3']):
                    if 'M3' not in result:
                        result['M3'] = []
                    result['M3'].append({'value': self.clean_punctuation(ent.text), 'type': 'temporal_entity', 'rule_id': 'spacy-ner'})
        
        return result if result else None

    def get_spacy_noun_phrase(self, noun_token):
        """spaCyãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰åè©å¥ã‚’æ§‹ç¯‰"""
        # ä»£åè©ã®å ´åˆã¯å˜ç‹¬ã§è¿”ã™
        if noun_token.pos_ == "PRON":
            return noun_token.text
            
        # åè©å¥ã®æ§‹æˆè¦ç´ ã‚’åé›†
        phrase_parts = []
        
        # å­è¦ç´ ã‚’ç¢ºèªï¼ˆé™å®šè©ã€å½¢å®¹è©ãªã©ï¼‰
        for child in noun_token.children:
            if child.dep_ in ["det", "amod", "poss", "nummod", "compound"]:
                phrase_parts.append((child.i, child.text))
        
        # ä¸­å¿ƒã®åè©
        phrase_parts.append((noun_token.i, noun_token.text))
        
        # èªé †ã§ã‚½ãƒ¼ãƒˆ
        phrase_parts.sort()
        return ' '.join([part[1] for part in phrase_parts])

    def is_temporal_expression(self, token):
        """ãƒˆãƒ¼ã‚¯ãƒ³ãŒæ™‚é–“è¡¨ç¾ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        temporal_words = [
            'ago', 'yesterday', 'today', 'tomorrow', 'now', 'then', 'soon',
            'morning', 'afternoon', 'evening', 'night', 'week', 'month', 'year',
            'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday',
            'january', 'february', 'march', 'april', 'may', 'june',
            'july', 'august', 'september', 'october', 'november', 'december'
        ]
        
        # ãƒˆãƒ¼ã‚¯ãƒ³è‡ªä½“ãŒæ™‚é–“å˜èªã®å ´åˆ
        if token.text.lower() in temporal_words:
            return True
            
        # å­è¦ç´ ã«æ™‚é–“å˜èªãŒå«ã¾ã‚Œã‚‹å ´åˆ
        for child in token.children:
            if child.text.lower() in temporal_words:
                return True
                
        # è¦ªè¦ç´ ãŒæ™‚é–“è¡¨ç¾ã®å ´åˆ
        if token.head.text.lower() in temporal_words:
            return True
            
        return False

    def get_spacy_temporal_phrase(self, token):
        """æ™‚é–“ä¿®é£¾èªå¥ã‚’æ§‹ç¯‰"""
        phrase_parts = []
        
        # æ™‚é–“ä¿®é£¾èªã®ç¯„å›²ã‚’æ±ºå®š
        temporal_tokens = [token]
        
        # å­è¦ç´ ã‚’è¿½åŠ 
        for child in token.children:
            temporal_tokens.append(child)
            
        # å…„å¼Ÿè¦ç´ ã§æ™‚é–“é–¢é€£ã®ã‚‚ã®ã‚’è¿½åŠ 
        for sibling in token.head.children:
            if sibling != token and self.is_temporal_expression(sibling):
                temporal_tokens.append(sibling)
                
        # ã‚½ãƒ¼ãƒˆã—ã¦å¥ã‚’æ§‹ç¯‰
        temporal_tokens.sort(key=lambda t: t.i)
        return ' '.join([t.text for t in temporal_tokens])

    def get_spacy_prepositional_phrase(self, prep_token):
        """å‰ç½®è©å¥ã‚’æ§‹ç¯‰"""
        phrase_parts = [prep_token.text]
        
        # å‰ç½®è©ã®ç›®çš„èªã‚’å–å¾—
        for child in prep_token.children:
            if child.dep_ == "pobj":
                obj_phrase = self.get_spacy_noun_phrase(child)
                phrase_parts.append(obj_phrase)
                
        return ' '.join(phrase_parts)

    def classify_prepositional_phrase(self, preposition):
        """å‰ç½®è©ã«åŸºã¥ã„ã¦ã‚¹ãƒ­ãƒƒãƒˆã‚¿ã‚¤ãƒ—ã‚’æ±ºå®š"""
        if preposition in ['in', 'on', 'at', 'during', 'for', 'since', 'until']:
            return 'M3'  # æ™‚é–“ãƒ»å ´æ‰€ä¿®é£¾èª
        elif preposition in ['to', 'from', 'with', 'by']:
            return 'M2'  # æ–¹æ³•ãƒ»æ‰‹æ®µä¿®é£¾èª
        else:
            return 'M1'  # ãã®ä»–ã®ä¿®é£¾èª

    def analyze_sentence(self, sentence):
        """æ–‡ã‚’è§£æã—ã¦ã‚¹ãƒ­ãƒƒãƒˆã«åˆ†è§£ï¼ˆspaCyå„ªå…ˆç‰ˆï¼‰"""
        sentence = sentence.strip()
        if not sentence:
            return {}
        
        # ã¾ãšspaCyã«ã‚ˆã‚‹åŒ…æ‹¬è§£æã‚’è©¦è¡Œ
        if self.nlp:
            spacy_result = self.analyze_sentence_with_spacy(sentence)
            if spacy_result:
                print("âœ… spaCyè§£ææˆåŠŸ")
                return spacy_result
        
        print("âš ï¸ spaCyè§£æå¤±æ•—ã€å¾“æ¥æ–¹å¼ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            
        # ç–‘å•æ–‡ãƒã‚§ãƒƒã‚¯ã‚’æœ€å„ªå…ˆ
        if self.is_question(sentence):
            return self.analyze_question(sentence)
            
        # å‘½ä»¤æ–‡ãƒ»å‘¼ã³ã‹ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
        if self.is_imperative_with_vocative(sentence):
            return self.analyze_imperative_with_vocative(sentence)
            
        # è¤‡æ–‡ã®å ´åˆã€ã‚µãƒ–ã‚¯ãƒ­ãƒ¼ã‚ºåˆ†è§£ã‚’è©¦è¡Œ
        if self.contains_subclause(sentence):
            return self.analyze_complex_sentence(sentence)
        else:
            return self.analyze_simple_sentence(sentence)
    
    def is_question(self, sentence):
        """ç–‘å•æ–‡ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        sentence = sentence.strip()
        
        # ç–‘å•ç¬¦ã§çµ‚ã‚ã‚‹
        if sentence.endswith('?'):
            return True
            
        # ç–‘å•è©ã§å§‹ã¾ã‚‹
        wh_words = ['what', 'who', 'where', 'when', 'why', 'how', 'which']
        first_word = sentence.split()[0].lower() if sentence.split() else ""
        if first_word in wh_words:
            return True
            
        # åŠ©å‹•è©ã§å§‹ã¾ã‚‹ (Do, Does, Did, Can, Willç­‰)
        auxiliary_starts = ['do', 'does', 'did', 'can', 'could', 'will', 'would', 'should', 'may', 'might', 'must', 'am', 'is', 'are', 'was', 'were']
        if first_word in auxiliary_starts:
            return True
            
        return False
    
    def is_imperative_with_vocative(self, sentence):
        """å‘¼ã³ã‹ã‘+å‘½ä»¤æ–‡ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        words = sentence.split()
        if len(words) < 2:
            return False
            
        # "You,"ã§å§‹ã¾ã‚‹å ´åˆï¼ˆå‘¼ã³ã‹ã‘ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
        if words[0].lower().rstrip(',') == 'you' and words[0].endswith(','):
            return True
            
        # ãã®ä»–ã®å‘¼ã³ã‹ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³: "Name,"
        if words[0].endswith(','):
            return True
            
        return False
    
    def analyze_imperative_with_vocative(self, sentence):
        """å‘¼ã³ã‹ã‘+å‘½ä»¤æ–‡ã‚’è§£æï¼ˆå¥å‹•è©å¯¾å¿œç‰ˆï¼‰"""
        # æœ€åˆã«spaCyã§å¥å‹•è©æ¤œå‡ºã‚’è©¦è¡Œ
        phrasal_verbs = self.detect_phrasal_verbs_with_spacy(sentence)
        
        if phrasal_verbs:
            # å¥å‹•è©ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆ
            result = self.extract_phrasal_verb_components(sentence, phrasal_verbs)
            
            # å‘¼ã³ã‹ã‘éƒ¨åˆ†ã‚’è¿½åŠ 
            words = sentence.split()
            if words and words[0].endswith(','):
                vocative = words[0]
                result['M1'] = [{'value': vocative, 'type': 'vocative', 'rule_id': 'imperative-vocative'}]
            
            return result
        
        # å¾“æ¥ã®ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå¥å‹•è©ãŒæ¤œå‡ºã•ã‚Œãªã„å ´åˆï¼‰
        words = sentence.split()
        
        # å‘¼ã³ã‹ã‘éƒ¨åˆ†ã‚’æŠ½å‡º
        vocative = words[0]  # "You,", "John," ãªã©
        command_words = words[1:]  # å‘½ä»¤æ–‡éƒ¨åˆ†
        
        if len(command_words) < 1:
            return {}
        
        # å‘½ä»¤æ–‡éƒ¨åˆ†ã‚’è§£æ
        verb = command_words[0]  # give
        remaining = command_words[1:] if len(command_words) > 1 else []
        
        result = {
            'M1': [{'value': vocative, 'type': 'vocative', 'rule_id': 'imperative-vocative'}],
            'V': [{'value': verb, 'type': 'imperative_verb', 'rule_id': 'imperative-vocative'}]
        }
        
        # æ®‹ã‚Šã®èªå¥ã‚’åˆ†æ
        if remaining:
            objects_and_modifiers = self.parse_imperative_objects_modifiers(remaining)
            result.update(objects_and_modifiers)
        
        return result
    
    def parse_imperative_objects_modifiers(self, words):
        """å‘½ä»¤æ–‡ã®ç›®çš„èªã¨ä¿®é£¾èªã‚’åˆ†æ"""
        result = {}
        
        if not words:
            return result
        
        # "it to me straight"ã®ã‚ˆã†ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
        i = 0
        
        # æœ€åˆã®èªã¯é€šå¸¸ç›´æ¥ç›®çš„èª
        if i < len(words):
            result['O1'] = [{'value': words[i], 'type': 'direct_object', 'rule_id': 'imperative-object'}]
            i += 1
        
        # æ®‹ã‚Šã®èªå¥ã‚’ä¿®é£¾èªã¨ã—ã¦åˆ†æ
        remaining_words = words[i:]
        modifiers = self.extract_imperative_modifiers(remaining_words)
        result.update(modifiers)
        
        return result
    
    def extract_imperative_modifiers(self, words):
        """å‘½ä»¤æ–‡ã®ä¿®é£¾èªã‚’æŠ½å‡ºï¼ˆto me straight ã‚’åˆ†é›¢ï¼‰"""
        modifiers = {}
        
        if not words:
            return modifiers
        
        i = 0
        while i < len(words):
            # å‰ç½®è©å¥ã®æ¤œå‡ºï¼ˆto me, from him ãªã©ï¼‰
            if words[i].lower() in ['to', 'from', 'for', 'with', 'in', 'at', 'on']:
                if i + 1 < len(words):
                    prep_phrase = f"{words[i]} {words[i+1]}"
                    modifiers['M2'] = [{'value': prep_phrase, 'type': 'word', 'rule_id': 'imperative-modifier'}]
                    i += 2
                    continue
            
            # å˜ç‹¬ã®å‰¯è©ï¼ˆstraight, quickly ãªã©ï¼‰
            if self.is_manner_adverb(words[i].rstrip('.,!?')):
                modifiers['M3'] = [{'value': words[i].rstrip('.,!?'), 'type': 'manner_adverb', 'rule_id': 'imperative-modifier'}]
                i += 1
                continue
                
            i += 1
        
        return modifiers
    
    def is_manner_adverb(self, word):
        """æ§˜æ…‹å‰¯è©ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        manner_adverbs = [
            'straight', 'quickly', 'slowly', 'carefully', 'well', 'badly', 
            'fast', 'hard', 'softly', 'loudly', 'quietly', 'clearly',
            'directly', 'honestly', 'simply', 'easily', 'perfectly'
        ]
        return word.lower() in manner_adverbs
    
    def analyze_question(self, sentence):
        """ç–‘å•æ–‡ã‚’è§£æ"""
        words = sentence.split()
        if not words:
            return {}
            
        first_word = words[0].lower()
        
        # whç–‘å•æ–‡ã®å‡¦ç†
        if first_word in ['what', 'who', 'where', 'when', 'why', 'how', 'which']:
            return self.analyze_wh_question(words)
            
        # yes/noç–‘å•æ–‡ã®å‡¦ç† (Do, Does, Didç­‰ã§å§‹ã¾ã‚‹)
        if first_word in ['do', 'does', 'did']:
            return self.analyze_do_question(words)
            
        # beå‹•è©ç–‘å•æ–‡ã®å‡¦ç† (Are, Isç­‰ã§å§‹ã¾ã‚‹)  
        if first_word in ['am', 'is', 'are', 'was', 'were']:
            return self.analyze_be_question(words)
            
        # modalç–‘å•æ–‡ã®å‡¦ç† (Can, Willç­‰ã§å§‹ã¾ã‚‹)
        modal_verbs = ['can', 'could', 'will', 'would', 'should', 'may', 'might', 'must']
        if first_word in modal_verbs:
            return self.analyze_modal_question(words)
            
        # ãã®ä»–ã¯é€šå¸¸è§£æã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return self.analyze_simple_sentence(sentence)
    
    def analyze_do_question(self, words):
        """Do/Does/Didç–‘å•æ–‡ã‚’è§£æ"""
        if len(words) < 3:
            return {}
            
        aux = words[0]  # Do/Does/Did
        subject = words[1]  # you/he/sheç­‰
        
        # å‹•è©ã¨ãã®ä»–ã‚’åˆ†é›¢
        rest = words[2:]
        main_verb = rest[0] if rest else ""
        remaining_text = " ".join(rest[1:]) if len(rest) > 1 else ""
        
        result = {
            'Aux': [{'value': aux, 'type': 'auxiliary', 'rule_id': 'do-question'}],
            'S': [{'value': subject, 'type': 'subject', 'rule_id': 'do-question'}]
        }
        
        if main_verb:
            result['V'] = [{'value': main_verb, 'type': 'verb', 'rule_id': 'do-question'}]
            
        # æ®‹ã‚Šã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æã—ã¦é©åˆ‡ãªã‚¹ãƒ­ãƒƒãƒˆã«åˆ†é¡
        if remaining_text:
            # pleaseåˆ†é›¢å‡¦ç†
            main_part, please_part = self.separate_please_from_phrase(remaining_text)
            
            # ãƒ¡ã‚¤ãƒ³éƒ¨åˆ†ã‚’åˆ†é¡
            if main_part:
                slot_info = self.classify_remaining_phrase(main_part)
                result[slot_info['slot']] = [{'value': slot_info['value'], 'type': slot_info['type'], 'rule_id': 'do-question'}]
            
            # pleaseéƒ¨åˆ†ãŒã‚ã‚Œã°è¿½åŠ 
            if please_part:
                result['M3'] = [{'value': please_part, 'type': 'polite_expression', 'rule_id': 'do-question'}]
            
        return result
    
    def separate_please_from_phrase(self, phrase):
        """ãƒ•ãƒ¬ãƒ¼ã‚ºã‹ã‚‰pleaseã‚’åˆ†é›¢"""
        phrase = phrase.strip()
        phrase_lower = phrase.lower()
        
        # æ–‡æœ«ã®pleaseã‚’æ¤œå‡º
        if phrase_lower.endswith(' please?') or phrase_lower.endswith(' please'):
            please_pos = phrase_lower.rfind(' please')
            main_part = phrase[:please_pos].strip()
            return main_part, 'please'
        elif phrase_lower == 'please' or phrase_lower == 'please?':
            return '', 'please'
        
        return phrase, ''
    
    def extract_phrasal_verb(self, words):
        """å¥å‹•è©ã‚’æŠ½å‡ºã—ã¦ã€å‹•è©éƒ¨åˆ†ã¨æ®‹ã‚Šã®éƒ¨åˆ†ã‚’åˆ†é›¢"""
        if not words:
            return '', ''
        
        # å¥å‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®šç¾©
        phrasal_verbs = {
            'sit': ['down', 'up'],
            'stand': ['up', 'down'],
            'wake': ['up'],
            'get': ['up', 'down', 'in', 'out', 'on', 'off'],
            'come': ['in', 'out', 'up', 'down', 'back'],
            'go': ['out', 'in', 'up', 'down', 'away', 'back'],
            'put': ['on', 'off', 'down', 'up'],
            'take': ['off', 'on', 'out', 'up', 'down'],
            'turn': ['on', 'off', 'up', 'down'],
            'pick': ['up', 'out'],
            'look': ['up', 'down', 'out', 'in'],
            'write': ['down', 'up', 'out'],
            'fill': ['in', 'out', 'up'],
            'work': ['out', 'up'],
            'give': ['up', 'out', 'back', 'away'],
            'bring': ['up', 'back', 'out'],
            'call': ['up', 'back', 'off', 'out']
        }
        
        verb = words[0]
        verb_lower = verb.lower()
        
        # å¥å‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        if len(words) >= 2 and verb_lower in phrasal_verbs:
            particle_candidate = words[1].rstrip('?!.')  # å¥èª­ç‚¹ã‚’é™¤å»ã—ã¦æ¯”è¼ƒ
            particle_lower = particle_candidate.lower()
            
            if particle_lower in phrasal_verbs[verb_lower]:
                # å¥å‹•è©ã¨ã—ã¦çµåˆ
                phrasal_verb = verb + " " + particle_candidate
                remaining_words = words[2:] if len(words) > 2 else []
                remaining_text = " ".join(remaining_words)
                return phrasal_verb, remaining_text
        
        # å¥å‹•è©ã§ãªã„å ´åˆã¯é€šå¸¸ã®å‹•è©
        remaining_words = words[1:] if len(words) > 1 else []
        remaining_text = " ".join(remaining_words)
        return verb, remaining_text
    
    def classify_verb_complement(self, verb, complement_text):
        """å‹•è©ã®è£œèªã‚’é©åˆ‡ã«åˆ†é¡ï¼ˆå¥å‹•è©ã®ç²’å­ã€æ–¹å‘å‰¯è©ç­‰ã‚’è€ƒæ…®ï¼‰"""
        complement_text = complement_text.strip().rstrip('?')
        complement_words = complement_text.split()
        
        if not complement_words:
            return {'slot': '', 'value': '', 'type': ''}
        
        # æ–¹å‘å‰¯è©ã®ãƒªã‚¹ãƒˆ
        directional_adverbs = [
            'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'away', 'back',
            'forward', 'backward', 'upward', 'downward', 'inside', 'outside', 
            'upstairs', 'downstairs', 'ahead', 'behind', 'here', 'there'
        ]
        
        # å˜ä¸€ã®æ–¹å‘å‰¯è©ã®å ´åˆ
        if len(complement_words) == 1 and complement_words[0].lower() in directional_adverbs:
            return {'slot': 'M2', 'value': complement_text, 'type': 'directional_adverb'}
        
        # å¥å‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆverb + particleï¼‰ã®åˆ¤å®š
        phrasal_verbs = {
            'sit': ['down', 'up'],
            'stand': ['up', 'down'],
            'wake': ['up'],
            'get': ['up', 'down', 'in', 'out', 'on', 'off'],
            'come': ['in', 'out', 'up', 'down', 'back'],
            'go': ['out', 'in', 'up', 'down', 'away', 'back'],
            'put': ['on', 'off', 'down', 'up'],
            'take': ['off', 'on', 'out', 'up', 'down'],
            'turn': ['on', 'off', 'up', 'down'],
            'pick': ['up', 'out'],
            'look': ['up', 'down', 'out', 'in']
        }
        
        verb_lower = verb.lower()
        first_word = complement_words[0].lower()
        
        # å¥å‹•è©+ç›®çš„èªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¾‹ï¼šturn on the lightï¼‰
        if (verb_lower in phrasal_verbs and 
            first_word in phrasal_verbs[verb_lower] and 
            len(complement_words) > 1):
            particle = complement_words[0]
            object_part = " ".join(complement_words[1:])
            return {
                'slot': 'compound',  # è¤‡åˆè¦ç´ ã‚’ç¤ºã™ãƒ•ãƒ©ã‚°
                'particle': {'slot': 'M2', 'value': particle, 'type': 'phrasal_verb_particle'},
                'object': {'slot': 'O1', 'value': object_part, 'type': 'object'}
            }
        
        # å˜ç´”ãªå¥å‹•è©ç²’å­
        if verb_lower in phrasal_verbs and first_word in phrasal_verbs[verb_lower] and len(complement_words) == 1:
            return {'slot': 'M2', 'value': complement_text, 'type': 'phrasal_verb_particle'}
        
        # å‰ç½®è©å¥ã®å ´åˆ
        if any(complement_text.lower().startswith(prep + ' ') for prep in ['to', 'from', 'in', 'at', 'by', 'with', 'for']):
            return {'slot': 'M2', 'value': complement_text, 'type': 'prepositional_phrase'}
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ç›®çš„èª
        return {'slot': 'O1', 'value': complement_text, 'type': 'object'}
    
    def classify_remaining_phrase(self, phrase):
        """ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’é©åˆ‡ãªã‚¹ãƒ­ãƒƒãƒˆï¼ˆO1, M1, M2, M3ç­‰ï¼‰ã«åˆ†é¡"""
        phrase = phrase.strip().rstrip('?')  # ç–‘å•ç¬¦ã‚’é™¤å»
        phrase_lower = phrase.lower()
        
        # æ™‚é–“è¡¨ç¾ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        time_patterns = [
            'every day', 'every morning', 'every evening', 'every week', 'every month', 'every year',
            'yesterday', 'today', 'tomorrow', 'now', 'then', 'always', 'never', 'often', 'sometimes',
            'usually', 'frequently', 'rarely', 'daily', 'weekly', 'monthly', 'yearly'
        ]
        
        # å ´æ‰€è¡¨ç¾ã®ãƒ‘ã‚¿ãƒ¼ãƒ³  
        place_patterns = [
            'at home', 'at school', 'at work', 'in the park', 'in the city', 'there', 'here',
            'downtown', 'upstairs', 'downstairs', 'outside', 'inside'
        ]
        
        # æ–¹æ³•ãƒ»æ‰‹æ®µè¡¨ç¾ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        manner_patterns = [
            'quickly', 'slowly', 'carefully', 'well', 'fast', 'hard', 'softly', 'loudly',
            'by car', 'by train', 'by bus', 'on foot'
        ]
        
        phrase_lower = phrase.lower()
        
        # æ™‚é–“è¡¨ç¾ãƒã‚§ãƒƒã‚¯
        for time_expr in time_patterns:
            if time_expr in phrase_lower:
                return {'slot': 'M3', 'value': phrase, 'type': 'time_adverb'}
                
        # å ´æ‰€è¡¨ç¾ãƒã‚§ãƒƒã‚¯
        for place_expr in place_patterns:
            if place_expr in phrase_lower:
                return {'slot': 'M2', 'value': phrase, 'type': 'place_adverb'}
        
        # å‰ç½®è©å¥ã«ã‚ˆã‚‹èµ·ç‚¹ãƒ»æ–¹å‘ãƒ»å ´æ‰€è¡¨ç¾ã®ãƒã‚§ãƒƒã‚¯
        if phrase_lower.startswith(('from ', 'to ', 'in ', 'at ', 'on ', 'for ', 'with ', 'by ', 'into ', 'onto ', 'toward ', 'towards ', 'during ', 'since ')):
            return {'slot': 'M2', 'value': phrase, 'type': 'prepositional_phrase'}
                
        # æ–¹æ³•è¡¨ç¾ãƒã‚§ãƒƒã‚¯  
        for manner_expr in manner_patterns:
            if manner_expr in phrase_lower:
                return {'slot': 'M1', 'value': phrase, 'type': 'manner_adverb'}
        
        # é »åº¦ãƒ»ç¨‹åº¦å‰¯è©ã®åˆ¤å®š
        if any(word in phrase_lower for word in ['very', 'quite', 'really', 'extremely', 'totally']):
            return {'slot': 'M1', 'value': phrase, 'type': 'degree_adverb'}
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ç›®çš„èªã¨ã—ã¦æ‰±ã†
        return {'slot': 'O1', 'value': phrase, 'type': 'object'}
    
    def analyze_wh_question(self, words):
        """whç–‘å•æ–‡ã‚’è§£æ"""
        if len(words) < 2:
            return {}
            
        wh_word = words[0]
        rest = words[1:]
        
        # wh-wordã®å¾ŒãŒdo/did/doesã®å ´åˆ
        if rest and rest[0].lower() in ['do', 'does', 'did']:
            if len(rest) >= 3:
                aux = rest[0]
                subject = rest[1] 
                verb = rest[2]
                objects = " ".join(rest[3:]) if len(rest) > 3 else ""
                
                result = {
                    'Aux': [{'value': aux, 'type': 'auxiliary', 'rule_id': 'wh-question', 'order': 2}],
                    'S': [{'value': subject, 'type': 'subject', 'rule_id': 'wh-question', 'order': 3}],
                    'V': [{'value': verb, 'type': 'verb', 'rule_id': 'wh-question', 'order': 4}]
                }
                
                # wh-wordãŒã©ã®ã‚¹ãƒ­ãƒƒãƒˆã«å¯¾å¿œã™ã‚‹ã‹ã‚’åˆ¤å®š
                wh_slot = self.determine_wh_slot(wh_word.lower())
                result[wh_slot] = [{'value': wh_word, 'type': 'wh-word', 'rule_id': 'wh-question', 'order': 1}]
                
                if objects:
                    # wh-wordãŒç›®çš„èªã§ãªã„å ´åˆã®ã¿O1ã‚’è¿½åŠ 
                    if wh_slot not in ['O1', 'O2']:
                        result['O1'] = [{'value': objects.rstrip('?'), 'type': 'object', 'rule_id': 'wh-question', 'order': 5}]
                        
                return result
        
        # ãã®ä»–ã®å ´åˆã¯åŸºæœ¬è§£æ
        return self.analyze_simple_sentence(" ".join(words))
    
    def determine_wh_slot(self, wh_word):
        """wh-wordãŒã©ã®ã‚¹ãƒ­ãƒƒãƒˆã«å¯¾å¿œã™ã‚‹ã‹ã‚’åˆ¤å®š"""
        wh_slot_map = {
            'what': 'O1',
            'who': 'S', 
            'where': 'M1',
            'when': 'M1',
            'why': 'M1',
            'how': 'M1',
            'which': 'O1'
        }
        return wh_slot_map.get(wh_word, 'O1')
    
    def analyze_be_question(self, words):
        """beå‹•è©ç–‘å•æ–‡ã‚’è§£æ"""
        if len(words) < 3:
            return {}
            
        be_verb = words[0]
        subject = words[1]
        complement = " ".join(words[2:])
        
        return {
            'Aux': [{'value': be_verb, 'type': 'be_auxiliary', 'rule_id': 'be-question'}],
            'S': [{'value': subject, 'type': 'subject', 'rule_id': 'be-question'}],
            'C': [{'value': complement, 'type': 'complement', 'rule_id': 'be-question'}] if complement else []
        }
    
    def analyze_modal_question(self, words):
        """modalå‹•è©ç–‘å•æ–‡ã‚’è§£æï¼ˆå¥å‹•è©å¯¾å¿œç‰ˆï¼‰"""
        if len(words) < 3:
            return {}
        
        sentence = ' '.join(words)
        modal = words[0]
        
        # å¥å‹•è©æ¤œå‡ºã‚’æœ€å„ªå…ˆã§è©¦è¡Œ
        phrasal_verbs = self.detect_phrasal_verbs_with_spacy(sentence)
        if phrasal_verbs:
            result = self.extract_phrasal_verb_components(sentence, phrasal_verbs)
            # ãƒ¢ãƒ¼ãƒ€ãƒ«å‹•è©ã‚’ä¸Šæ›¸ãã§ã¯ãªãã€ãªã‘ã‚Œã°è¿½åŠ 
            if 'Aux' not in result:
                result['Aux'] = [{'value': modal, 'type': 'modal', 'rule_id': 'modal-phrasal-question'}]
            return result
            
        modal = words[0]
        subject = words[1]
        
        # å‹•è©éƒ¨åˆ†ã‚’åˆ†é›¢
        rest_words = words[2:]
        
        # "Would you please verb..." ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‡¦ç†
        if rest_words and rest_words[0].lower() == 'please' and len(rest_words) > 1:
            verb_part, remaining_text = self.extract_phrasal_verb(rest_words[1:])
            
            result = {
                'Aux': [{'value': modal, 'type': 'modal', 'rule_id': 'modal-question'}],
                'S': [{'value': subject, 'type': 'subject', 'rule_id': 'modal-question'}],
                'M3': [{'value': 'please', 'type': 'polite_expression', 'rule_id': 'modal-question'}],
                'V': [{'value': verb_part, 'type': 'verb', 'rule_id': 'modal-question'}]
            }
            
            if remaining_text:
                main_part, please_part = self.separate_please_from_phrase(remaining_text)
                if main_part:
                    result['O1'] = [{'value': main_part.rstrip('?'), 'type': 'object', 'rule_id': 'modal-question'}]
                if please_part:
                    if 'M3' not in result:
                        result['M3'] = [{'value': please_part, 'type': 'polite_expression', 'rule_id': 'modal-question'}]
                
        else:
            # é€šå¸¸ã® "modal subject verb object please" ãƒ‘ã‚¿ãƒ¼ãƒ³
            verb_part, remaining_text = self.extract_phrasal_verb(rest_words)
            
            result = {
                'Aux': [{'value': modal, 'type': 'modal', 'rule_id': 'modal-question'}],
                'S': [{'value': subject, 'type': 'subject', 'rule_id': 'modal-question'}]
            }
            
            if verb_part:
                result['V'] = [{'value': verb_part, 'type': 'verb', 'rule_id': 'modal-question'}]
                
            if remaining_text:
                # pleaseåˆ†é›¢å‡¦ç†
                main_part, please_part = self.separate_please_from_phrase(remaining_text)
                
                # ãƒ¡ã‚¤ãƒ³éƒ¨åˆ†ã‚’ç›®çš„èªã¨ã—ã¦è¿½åŠ 
                if main_part:
                    result['O1'] = [{'value': main_part.rstrip('?'), 'type': 'object', 'rule_id': 'modal-question'}]
                
                # pleaseéƒ¨åˆ†ãŒã‚ã‚Œã°è¿½åŠ 
                if please_part:
                    result['M3'] = [{'value': please_part, 'type': 'polite_expression', 'rule_id': 'modal-question'}]
            
        return result
    
    def contains_subclause(self, sentence):
        """ã‚µãƒ–ã‚¯ãƒ­ãƒ¼ã‚ºï¼ˆè¤‡æ–‡ï¼‰ã‚’å«ã‚€ã‹ãƒã‚§ãƒƒã‚¯"""
        subclause_indicators = [
            r'\bthat\b',  # thatç¯€
            r'\bwhat\b',  # whatç¯€
            r'\bwhere\b', # whereç¯€  
            r'\bwhen\b',  # whenç¯€
            r'\bwhy\b',   # whyç¯€
            r'\bhow\b',   # howç¯€
            r'\bwhich\b', # whichç¯€
            r'\bwho\b',   # whoç¯€
        ]
        
        for indicator in subclause_indicators:
            if re.search(indicator, sentence, re.IGNORECASE):
                return True
        return False
    
    def analyze_complex_sentence(self, sentence):
        """è¤‡æ–‡ã‚’è§£æï¼ˆã‚µãƒ–ã‚¯ãƒ­ãƒ¼ã‚ºåˆ†è§£ï¼‰"""
        slots = {}
        words = sentence.split()
        
        # èªçŸ¥å‹•è© + thatç¯€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        cognitive_result = self.detect_cognitive_verb_pattern(words, sentence)
        if cognitive_result:
            slots.update(cognitive_result)
            return slots
            
        # ãã®ä»–ã®è¤‡æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚ã“ã“ã§å‡¦ç†å¯èƒ½
        return self.analyze_simple_sentence(sentence)
    
    def detect_cognitive_verb_pattern(self, words, sentence):
        """èªçŸ¥å‹•è© + thatç¯€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºãƒ»åˆ†è§£"""
        cognitive_verbs = self.rules_data.get("cognitive_verbs", ["think", "believe", "know"])
        
        for i, word in enumerate(words):
            if word.lower() in cognitive_verbs:
                # ä¸»èªã‚’æ¤œå‡º
                subject = " ".join(words[:i]) if i > 0 else "I"
                
                # thatç¯€ã‚’æ¤œå‡ºãƒ»åˆ†è§£
                remaining = " ".join(words[i+1:])
                
                if "that" in remaining.lower():
                    that_clause = self.extract_that_clause(remaining)
                    subslots = self.analyze_that_clause(that_clause)
                    
                    return {
                        'S': [{'value': subject.strip(), 'type': 'subject', 'rule_id': 'cognitive-main'}],
                        'V': [{'value': word, 'type': 'cognitive_verb', 'rule_id': 'cognitive-main', 
                               'subslots': subslots, 'note': f'thatç¯€åˆ†è§£æ¸ˆã¿: {that_clause[:20]}...'}]
                    }
                else:
                    # thatçœç•¥ãƒ‘ã‚¿ãƒ¼ãƒ³
                    subslots = self.analyze_that_clause(remaining)
                    
                    return {
                        'S': [{'value': subject.strip(), 'type': 'subject', 'rule_id': 'cognitive-main'}],
                        'V': [{'value': word, 'type': 'cognitive_verb', 'rule_id': 'cognitive-main',
                               'subslots': subslots, 'note': f'thatçœç•¥ç¯€åˆ†è§£æ¸ˆã¿: {remaining[:20]}...'}]
                    }
        
        return None
    
    def extract_that_clause(self, text):
        """thatç¯€éƒ¨åˆ†ã‚’æŠ½å‡º"""
        text = text.strip()
        
        # "that" ã§å§‹ã¾ã‚‹å ´åˆ
        if text.lower().startswith("that "):
            return text[5:].strip()  # "that "ã‚’é™¤å»
            
        # "that" ãŒé€”ä¸­ã«ã‚ã‚‹å ´åˆ
        that_match = re.search(r'\bthat\s+(.+)$', text, re.IGNORECASE)
        if that_match:
            return that_match.group(1)
            
        return text
    
    def analyze_that_clause(self, clause_text):
        """thatç¯€ã‚’åˆ†è§£ã—ã¦ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«"""
        if not clause_text.strip():
            return {}
            
        words = clause_text.strip().split()
        if not words:
            return {}
            
        # åŸºæœ¬çš„ãªSVOæ§‹é€ ã‚’æ¤œå‡º
        subslots = {}
        
        # beå‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³
        be_verbs = ["is", "are", "was", "were", "am"]
        for i, word in enumerate(words):
            if word.lower() in be_verbs:
                if i > 0:
                    subslots['sub-s'] = " ".join(words[:i])
                subslots['sub-v'] = word
                if i < len(words) - 1:
                    subslots['sub-c'] = " ".join(words[i+1:])
                return subslots
        
        # ä¸€èˆ¬å‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³
        if len(words) >= 2:
            # æœ€åˆã®èªã‚’ä¸»èªã€2ç•ªç›®ã‚’å‹•è©ã¨ä»®å®š
            subslots['sub-s'] = words[0]
            subslots['sub-v'] = words[1]
            if len(words) > 2:
                subslots['sub-o1'] = " ".join(words[2:])
                
        return subslots
    
    def analyze_simple_sentence(self, sentence):
        """å˜æ–‡ã‚’è§£æï¼ˆå¥å‹•è©å¯¾å¿œç‰ˆï¼‰"""
        slots = {}
        words = sentence.split()
        
        # å¥å‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡ºï¼ˆæœ€å„ªå…ˆï¼‰
        phrasal_verbs = self.detect_phrasal_verbs_with_spacy(sentence)
        if phrasal_verbs:
            phrasal_result = self.extract_phrasal_verb_components(sentence, phrasal_verbs)
            if phrasal_result:
                slots.update(phrasal_result)
                return slots
        
        # åŠ©å‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
        modal_result = self.detect_modal_pattern(words)
        if modal_result:
            slots.update(modal_result)
            return slots
        
        # ç¾åœ¨æ™‚åˆ¶å®Œäº†ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
        perfect_result = self.detect_perfect_pattern(words)
        if perfect_result:
            slots.update(perfect_result)
            return slots
            
        # å—å‹•æ…‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º  
        passive_result = self.detect_passive_pattern(words)
        if passive_result:
            slots.update(passive_result)
            return slots
        
        # beå‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
        be_result = self.detect_be_verb_pattern(words)
        if be_result:
            slots.update(be_result)
            return slots
            
        # åŸºæœ¬çš„ãªSVOãƒ‘ã‚¿ãƒ¼ãƒ³
        basic_result = self.detect_basic_svo_pattern(words)
        if basic_result:
            slots.update(basic_result)
            
        return slots
    
    def detect_modal_pattern(self, words):
        """åŠ©å‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºï¼ˆç¸®ç´„å½¢å¯¾å¿œç‰ˆï¼‰"""
        modal_verbs = self.rules_data.get("modal_verbs", ["will", "would", "can", "could"])
        
        # ç¸®ç´„å½¢ã®åŠ©å‹•è©ãƒãƒƒãƒ”ãƒ³ã‚°
        modal_contractions = {
            "can't": "can",
            "won't": "will", 
            "wouldn't": "would",
            "couldn't": "could",
            "shouldn't": "should",
            "mustn't": "must",
            "mightn't": "might"
        }
        
        # å…¨ã¦ã®åŠ©å‹•è©å€™è£œï¼ˆé€šå¸¸å½¢ + ç¸®ç´„å½¢ï¼‰
        all_modals = modal_verbs + list(modal_contractions.keys())
        
        for i, word in enumerate(words):
            word_lower = word.lower()
            
            # é€šå¸¸ã®åŠ©å‹•è©ãƒã‚§ãƒƒã‚¯
            if word_lower in modal_verbs:
                subject = " ".join(words[:i]) if i > 0 else "I"
                
                # modal + have + past participle ãƒ‘ã‚¿ãƒ¼ãƒ³
                if i + 2 < len(words) and words[i+1].lower() == "have":
                    return {
                        'S': [{'value': subject.strip(), 'type': 'subject', 'rule_id': 'modal-perfect'}],
                        'Aux': [{'value': f"{word} have", 'type': 'modal_perfect', 'rule_id': 'modal-perfect'}],
                        'V': [{'value': words[i+2], 'type': 'past_participle', 'rule_id': 'modal-perfect'}]
                    }
                
                # åŸºæœ¬çš„ãª modal + verb ãƒ‘ã‚¿ãƒ¼ãƒ³
                if i + 1 < len(words):
                    verb_word = self.clean_punctuation(words[i+1])
                    remaining_words = words[i+2:] if i + 2 < len(words) else []
                    
                    result = {
                        'S': [{'value': subject.strip(), 'type': 'subject', 'rule_id': 'modal-basic'}],
                        'Aux': [{'value': word, 'type': 'modal_verb', 'rule_id': 'modal-basic'}],
                        'V': [{'value': verb_word, 'type': 'base_verb', 'rule_id': 'modal-basic'}]
                    }
                    
                    # æ®‹ã‚Šã®å˜èªã‹ã‚‰ç›®çš„èªã¨ä¿®é£¾èªã‚’åˆ†é›¢
                    if remaining_words:
                        object_and_modifiers = self.extract_object_and_modifiers(remaining_words)
                        
                        # ç›®çš„èªã¨ä¿®é£¾èªã‚’è¿½åŠ ï¼ˆå¥èª­ç‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼‰
                        cleaned_modifiers = {}
                        for key, value_list in object_and_modifiers.items():
                            cleaned_list = []
                            for item in value_list:
                                cleaned_item = item.copy()
                                cleaned_item['value'] = self.clean_punctuation(item['value'])
                                cleaned_list.append(cleaned_item)
                            cleaned_modifiers[key] = cleaned_list
                        result.update(cleaned_modifiers)
                    
                    return result
            
            # ç¸®ç´„å½¢ã®åŠ©å‹•è©ãƒã‚§ãƒƒã‚¯
            elif word_lower in modal_contractions:
                subject = " ".join(words[:i]) if i > 0 else "I"
                
                # åŸºæœ¬çš„ãª modal contraction + verb ãƒ‘ã‚¿ãƒ¼ãƒ³
                if i + 1 < len(words):
                    verb_word = self.clean_punctuation(words[i+1])
                    remaining_words = words[i+2:] if i + 2 < len(words) else []
                    
                    result = {
                        'S': [{'value': subject.strip(), 'type': 'subject', 'rule_id': 'modal-negative'}],
                        'Aux': [{'value': word, 'type': 'modal_negative', 'rule_id': 'modal-negative'}],
                        'V': [{'value': verb_word, 'type': 'base_verb', 'rule_id': 'modal-negative'}]
                    }
                    
                    # æ®‹ã‚Šã®å˜èªã‹ã‚‰ç›®çš„èªã¨ä¿®é£¾èªã‚’åˆ†é›¢
                    if remaining_words:
                        object_and_modifiers = self.extract_object_and_modifiers(remaining_words)
                        
                        # ç›®çš„èªã¨ä¿®é£¾èªã‚’è¿½åŠ ï¼ˆå¥èª­ç‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼‰
                        cleaned_modifiers = {}
                        for key, value_list in object_and_modifiers.items():
                            cleaned_list = []
                            for item in value_list:
                                cleaned_item = item.copy()
                                cleaned_item['value'] = self.clean_punctuation(item['value'])
                                cleaned_list.append(cleaned_item)
                            cleaned_modifiers[key] = cleaned_list
                        result.update(cleaned_modifiers)
                    
                    return result
                
                # modal contractionå˜ä½“ã®å ´åˆï¼ˆå‹•è©ãŒãªã„å ´åˆï¼‰
                else:
                    return {
                        'S': [{'value': subject.strip(), 'type': 'subject', 'rule_id': 'modal-negative-no-verb'}],
                        'Aux': [{'value': word, 'type': 'modal_negative', 'rule_id': 'modal-negative-no-verb'}]
                    }
        
        return None
    
    def detect_perfect_pattern(self, words):
        """ç¾åœ¨æ™‚åˆ¶å®Œäº†ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º (has/have + past participle)"""
        have_verbs = ["have", "has", "had"]
        # çŸ­ç¸®å½¢ã‚‚è¿½åŠ 
        contraction_map = {
            "haven't": "have",
            "hasn't": "has", 
            "hadn't": "had"
        }
        
        for i, word in enumerate(words):
            # é€šå¸¸å½¢ã®ãƒã‚§ãƒƒã‚¯
            if word.lower() in have_verbs and i + 1 < len(words):
                # have/has + past participle ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
                next_word = words[i+1]
                if self.looks_like_past_participle(next_word):
                    subject = " ".join(words[:i]) if i > 0 else "I"
                    
                    # æ®‹ã‚Šã®éƒ¨åˆ†ã‹ã‚‰ç›®çš„èªã¨ä¿®é£¾èªã‚’åˆ†é›¢
                    remaining_words = words[i+2:]
                    object_and_modifiers = self.extract_object_and_modifiers(remaining_words)
                    
                    result = {
                        'S': [{'value': subject.strip(), 'type': 'subject', 'rule_id': 'present-perfect'}],
                        'Aux': [{'value': word, 'type': 'auxiliary', 'rule_id': 'present-perfect'}],
                        'V': [{'value': next_word, 'type': 'past_participle', 'rule_id': 'present-perfect'}]
                    }
                    
                    # ç›®çš„èªã¨ä¿®é£¾èªã‚’è¿½åŠ 
                    result.update(object_and_modifiers)
                    
                    return result
            
            # çŸ­ç¸®å½¢ã®ãƒã‚§ãƒƒã‚¯
            elif word.lower() in contraction_map and i + 1 < len(words):
                next_word = words[i+1]
                if self.looks_like_past_participle(next_word):
                    subject = " ".join(words[:i]) if i > 0 else "I"
                    base_aux = contraction_map[word.lower()]
                    
                    # æ®‹ã‚Šã®éƒ¨åˆ†ã‹ã‚‰ç›®çš„èªã¨ä¿®é£¾èªã‚’åˆ†é›¢
                    remaining_words = words[i+2:]
                    object_and_modifiers = self.extract_object_and_modifiers(remaining_words)
                    
                    result = {
                        'S': [{'value': subject.strip(), 'type': 'subject', 'rule_id': 'present-perfect-negative'}],
                        'Aux': [{'value': word, 'type': 'auxiliary_negative', 'rule_id': 'present-perfect-negative'}],
                        'V': [{'value': next_word, 'type': 'past_participle', 'rule_id': 'present-perfect-negative'}]
                    }
                    
                    # ç›®çš„èªã¨ä¿®é£¾èªã‚’è¿½åŠ 
                    result.update(object_and_modifiers)
                    
                    return result
        
        return None
    
    def detect_passive_pattern(self, words):
        """å—å‹•æ…‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º"""
        be_verbs = ["is", "are", "was", "were", "am", "be", "been", "being"]
        
        for i, word in enumerate(words):
            if word.lower() in be_verbs and i + 1 < len(words):
                # be + past participle ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
                next_word = words[i+1]
                if self.looks_like_past_participle(next_word):
                    subject = " ".join(words[:i]) if i > 0 else "it"
                    
                    return {
                        'S': [{'value': subject.strip(), 'type': 'subject', 'rule_id': 'passive'}],
                        'Aux': [{'value': word, 'type': 'be_verb', 'rule_id': 'passive'}], 
                        'V': [{'value': next_word, 'type': 'past_participle', 'rule_id': 'passive'}]
                    }
        
        return None
    
    def looks_like_past_participle(self, word):
        """éå»åˆ†è©ã‚‰ã—ã„èªã‹ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        word = word.lower()
        
        # è¦å‰‡å¤‰åŒ– (-ed)
        if word.endswith('ed'):
            return True
            
        # ã‚ˆãä½¿ã‚ã‚Œã‚‹ä¸è¦å‰‡éå»åˆ†è©
        irregular_past_participles = [
            'written', 'taken', 'given', 'made', 'done', 'seen', 'known',
            'broken', 'spoken', 'chosen', 'driven', 'eaten', 'fallen'
        ]
        
        return word in irregular_past_participles
    
    def detect_be_verb_pattern(self, words):
        """beå‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º"""
        be_verbs = ["am", "is", "are", "was", "were"]
        
        for i, word in enumerate(words):
            if word.lower() in be_verbs:
                subject = " ".join(words[:i]) if i > 0 else "I"
                complement = " ".join(words[i+1:]) if i + 1 < len(words) else ""
                
                return {
                    'S': [{'value': subject.strip(), 'type': 'subject', 'rule_id': 'be-verb'}],
                    'V': [{'value': word, 'type': 'be_verb', 'rule_id': 'be-verb'}],
                    'C': [{'value': complement, 'type': 'complement', 'rule_id': 'be-verb'}] if complement else []
                }
        
        return None
    
    def detect_basic_svo_pattern(self, words):
        """åŸºæœ¬çš„ãªSV(O)ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºï¼ˆè‡ªå‹•è©ãƒ»ä»–å‹•è©ã‚’è€ƒæ…®ï¼‰"""
        if len(words) < 2:
            return None
            
        # æœ€åˆã®èªã‚’ä¸»èªã€2ç•ªç›®ã‚’å‹•è©ã¨ä»®å®š
        subject = words[0]
        verb = words[1]
        remaining_words = words[2:] if len(words) > 2 else []
        
        result = {
            'S': [{'value': subject, 'type': 'subject', 'rule_id': 'basic-sv'}],
            'V': [{'value': verb, 'type': 'verb', 'rule_id': 'basic-sv'}]
        }
        
        if remaining_words:
            # è‡ªå‹•è©ã‹ã©ã†ã‹ã‚’åˆ¤å®š
            if self.is_intransitive_verb(verb.lower()):
                # è‡ªå‹•è©ã®å ´åˆï¼šæ®‹ã‚Šã¯ä¿®é£¾èªã¨ã—ã¦åˆ†é¡
                modifiers = self.extract_modifiers_from_words(remaining_words)
                result.update(modifiers)
            else:
                # ä»–å‹•è©ã®å ´åˆï¼šæ®‹ã‚Šã‚’ã¾ãšç›®çš„èªå€™è£œã¨ã—ã¦ã€å¿…è¦ã«å¿œã˜ã¦ä¿®é£¾èªã‚’åˆ†é›¢
                remaining_text = " ".join(remaining_words)
                
                # æ™‚é–“ä¿®é£¾èªã¾ãŸã¯å‰ç½®è©å¥ãŒã‚ã‚‹å ´åˆã¯åˆ†é›¢
                has_time_or_prep = (
                    any(word.lower() in ['from', 'to', 'in', 'at', 'on', 'by', 'with', 'for', 'during', 'since'] 
                        for word in remaining_words) or
                    self.has_time_expression(remaining_text)
                )
                
                if has_time_or_prep:
                    # æ™‚é–“ä¿®é£¾èªãƒ»å‰ç½®è©å¥ä»¥å‰ã‚’ç›®çš„èªã€æ™‚é–“ä¿®é£¾èªãƒ»å‰ç½®è©å¥ã‚’ä¿®é£¾èªã¨ã—ã¦åˆ†é›¢
                    object_part, modifiers = self.separate_object_and_modifiers(remaining_words)
                    if object_part:
                        object_part = self.clean_punctuation(object_part)
                        result['O1'] = [{'value': object_part, 'type': 'object', 'rule_id': 'basic-svo'}]
                    result.update(modifiers)
                else:
                    # æ™‚é–“ä¿®é£¾èªãƒ»å‰ç½®è©å¥ãŒãªã„å ´åˆã¯å…¨ã¦ç›®çš„èª
                    remaining_text = self.clean_punctuation(remaining_text)
                    result['O1'] = [{'value': remaining_text, 'type': 'object', 'rule_id': 'basic-svo'}]
            
        return result
    
    def is_intransitive_verb(self, verb):
        """å‹•è©ãŒè‡ªå‹•è©ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        # ä¸»è¦ãªè‡ªå‹•è©ãƒªã‚¹ãƒˆ
        intransitive_verbs = [
            'lie', 'sit', 'stand', 'sleep', 'die', 'come', 'go', 'arrive', 'depart', 
            'exist', 'happen', 'occur', 'appear', 'disappear', 'remain', 'stay',
            'rise', 'fall', 'fly', 'swim', 'dance', 'sing', 'laugh', 'cry', 'smile'
        ]
        
        return verb in intransitive_verbs
    
    def has_time_expression(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã«æ™‚é–“ä¿®é£¾èªãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        import re
        time_expressions = [
            r'\b(a few|several|many) (days?|weeks?|months?|years?) ago\b',
            r'\b(yesterday|today|tomorrow)\b',
            r'\b(last|next) (week|month|year|Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)\b',
            r'\b(this|that) (morning|afternoon|evening|night)\b',
            r'\b(in|at) \d+:\d+\b',
            r'\b(at) (dawn|noon|midnight)\b',
            r'\b(during|throughout) (the )?(day|night|week|month|year)\b',
            r'\b(now|then|soon|recently|lately)\b',
            r'\b\d+ (minutes?|hours?|days?|weeks?|months?|years?) ago\b',
            r'\b(two|three|four|five) (days?|weeks?|months?|years?) ago\b'
        ]
        
        for pattern in time_expressions:
            if re.search(pattern, text, re.IGNORECASE):
                return True
        return False
    
    def separate_object_and_modifiers(self, words):
        """ä»–å‹•è©æ–‡ã§ç›®çš„èªã¨ä¿®é£¾èªã‚’åˆ†é›¢ï¼ˆæ™‚é–“ä¿®é£¾èªå¯¾å¿œç‰ˆï¼‰"""
        import re
        
        # æ™‚é–“ä¿®é£¾èªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        text = " ".join(words)
        time_expressions = [
            r'\b(a few|several|many) (days?|weeks?|months?|years?) ago\b',
            r'\b(yesterday|today|tomorrow)\b',
            r'\b(last|next) (week|month|year|Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)\b',
            r'\b(this|that) (morning|afternoon|evening|night)\b',
            r'\b(in|at) \d+:\d+\b',
            r'\b(at) (dawn|noon|midnight)\b',
            r'\b(during|throughout) (the )?(day|night|week|month|year)\b',
            r'\b(now|then|soon|recently|lately)\b',
            r'\b(one|two|three|four|five|six|seven|eight|nine|ten|\d+) (minutes?|hours?|days?|weeks?|months?|years?) ago\b',
            r'\bnext (week|month|year|Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)\b',
            r'\blast (week|month|year|Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)\b'
        ]
        
        time_modifier = None
        object_text = text
        
        # æ™‚é–“ä¿®é£¾èªã‚’æ¤œå‡ºãƒ»é™¤å»
        for pattern in time_expressions:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                time_modifier = match.group(0)
                # æ™‚é–“ä¿®é£¾èªã‚’é™¤å»ã—ã¦ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆéƒ¨åˆ†ã‚’å–å¾—
                object_text = re.sub(pattern, '', text, flags=re.IGNORECASE).strip()
                # è¤‡æ•°ã®ç©ºç™½ã‚’å˜ä¸€ã®ç©ºç™½ã«å¤‰æ›ã—ã€å…ˆé ­ãƒ»æœ«å°¾ã®ç©ºç™½ã‚’é™¤å»
                object_text = re.sub(r'\s+', ' ', object_text).strip()
                break
        
        # ãã®ä»–ã®å‰ç½®è©å¥ã«ã‚ˆã‚‹åˆ†é›¢ã‚‚å‡¦ç†
        object_words = []
        modifier_start = -1
        remaining_words = object_text.split() if object_text else []
        
        # å‰ç½®è©ã‚’æ¢ã—ã¦åˆ†é›¢ç‚¹ã‚’ç‰¹å®š
        for i, word in enumerate(remaining_words):
            if word.lower() in ['from', 'to', 'in', 'at', 'on', 'by', 'with', 'for', 'during', 'since']:
                modifier_start = i
                break
        
        modifiers = {}
        
        if modifier_start >= 0:
            object_words = remaining_words[:modifier_start]
            modifier_words = remaining_words[modifier_start:]
            
            object_part = " ".join(object_words) if object_words else ""
            prep_modifiers = self.extract_modifiers_from_words(modifier_words)
            modifiers.update(prep_modifiers)
        else:
            # å‰ç½®è©å¥ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€æ®‹ã‚Šã®ãƒ†ã‚­ã‚¹ãƒˆãŒç›®çš„èª
            object_part = object_text
        
        # æ™‚é–“ä¿®é£¾èªã‚’M3ã«è¿½åŠ 
        if time_modifier:
            if 'M3' not in modifiers:
                modifiers['M3'] = []
            modifiers['M3'].append({'value': time_modifier, 'type': 'time_expression', 'rule_id': 'time-modifier'})
        
        return object_part, modifiers
    
    def extract_object_and_modifiers(self, words):
        """ç›®çš„èªã¨ä¿®é£¾èªã‚’åˆ†é›¢ï¼ˆç¾åœ¨å®Œäº†ç”¨ï¼‰"""
        if not words:
            return {}
            
        result = {}
        
        # å‰ç½®è©ã®ä½ç½®ã‚’æ¢ã™
        preposition_words = ['for', 'to', 'from', 'in', 'at', 'on', 'by', 'with', 'during', 'since']
        prep_index = -1
        
        for i, word in enumerate(words):
            if word.lower() in preposition_words:
                prep_index = i
                break
        
        if prep_index > 0:
            # ç›®çš„èªéƒ¨åˆ†ï¼ˆå‰ç½®è©ã®å‰ã¾ã§ï¼‰
            object_words = words[:prep_index]
            if object_words:
                object_text = self.clean_punctuation(' '.join(object_words))
                result['O1'] = [{'value': object_text, 'type': 'object', 'rule_id': 'present-perfect-object'}]
            
            # ä¿®é£¾èªéƒ¨åˆ†ï¼ˆå‰ç½®è©å¥ï¼‰
            modifier_words = words[prep_index:]
            if modifier_words:
                modifiers = self.extract_modifiers_from_words(modifier_words)
                result.update(modifiers)
                
        elif prep_index == 0:
            # æœ€åˆã‹ã‚‰å‰ç½®è©å¥ï¼ˆç›®çš„èªãªã—ï¼‰
            modifiers = self.extract_modifiers_from_words(words)
            result.update(modifiers)
            
        else:
            # å‰ç½®è©å¥ãŒãªã„å ´åˆã¯å…¨ã¦ç›®çš„èª
            if words:
                # å¥èª­ç‚¹ã‚’é™¤å»ã—ã¦ã‹ã‚‰ç›®çš„èªã¨ã—ã¦è¨­å®š
                object_text = self.clean_punctuation(' '.join(words))
                result['O1'] = [{'value': object_text, 'type': 'object', 'rule_id': 'present-perfect-object'}]
                
        return result

    def extract_modifiers_from_words(self, words):
        """å˜èªãƒªã‚¹ãƒˆã‹ã‚‰ä¿®é£¾èªã‚’æŠ½å‡ºã—ã¦ã‚¹ãƒ­ãƒƒãƒˆã«åˆ†é¡ï¼ˆæ™‚é–“ä¿®é£¾èªå¯¾å¿œç‰ˆï¼‰"""
        if not words:
            return {}
            
        modifiers = {}
        remaining_phrase = " ".join(words)
        
        # æ™‚é–“ä¿®é£¾èªãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡ºï¼ˆæœ€å„ªå…ˆï¼‰
        time_expressions = [
            r'\b(a few|several|many) (days?|weeks?|months?|years?) ago\b',
            r'\b(yesterday|today|tomorrow)\b',
            r'\b(last|next) (week|month|year|Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)\b',
            r'\b(this|that) (morning|afternoon|evening|night)\b',
            r'\b(in|at) \d+:\d+\b',
            r'\b(at) (dawn|noon|midnight)\b',
            r'\b(during|throughout) (the )?(day|night|week|month|year)\b',
            r'\b(now|then|soon|recently|lately)\b',
            r'\b\d+ (minutes?|hours?|days?|weeks?|months?|years?) ago\b',
            r'\b(two|three|four|five) (days?|weeks?|months?|years?) ago\b'
        ]
        
        import re
        for pattern in time_expressions:
            match = re.search(pattern, remaining_phrase, re.IGNORECASE)
            if match:
                time_phrase = match.group(0)
                # æ¤œå‡ºã—ãŸæ™‚é–“ä¿®é£¾èªã‚’é™¤å»
                remaining_phrase = re.sub(pattern, '', remaining_phrase, flags=re.IGNORECASE).strip()
                words = remaining_phrase.split() if remaining_phrase else []
                
                # M3ã‚¹ãƒ­ãƒƒãƒˆã«æ™‚é–“ä¿®é£¾èªã‚’è¿½åŠ 
                if 'M3' not in modifiers:
                    modifiers['M3'] = []
                modifiers['M3'].append({'value': time_phrase, 'type': 'time_expression', 'rule_id': 'time-modifier'})
                break
        
        # pleaseã®åˆ†é›¢å‡¦ç†ï¼ˆæ–‡æœ«ã¾ãŸã¯å¥èª­ç‚¹ã®å‰ã«ã‚ã‚‹å ´åˆï¼‰
        please_extracted = False
        if words and words[-1].lower().rstrip('?!.') == 'please':
            # æ–‡æœ«ã®please
            if 'M3' not in modifiers:
                modifiers['M3'] = []
            modifiers['M3'].append({'value': 'please', 'type': 'polite_expression', 'rule_id': 'polite-please'})
            words = words[:-1]  # pleaseã‚’é™¤å¤–
            please_extracted = True
        elif len(words) >= 2 and words[-2].lower() == 'please' and words[-1] in ['?', '!', '.']:
            # å¥èª­ç‚¹ã®å‰ã®please
            if 'M3' not in modifiers:
                modifiers['M3'] = []
            modifiers['M3'].append({'value': 'please', 'type': 'polite_expression', 'rule_id': 'polite-please'})
            words = words[:-2] + [words[-1]]  # pleaseã®ã¿é™¤å¤–
            please_extracted = True
        
        # è¤‡æ•°ã®ä¿®é£¾èªãŒå«ã¾ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ã‚’è€ƒæ…®ã—ã¦åˆ†æ
        phrases_to_classify = []
        current_phrase = []
        
        i = 0
        while i < len(words):
            word = words[i]
            current_phrase.append(word)
            
            # å‰ç½®è©å¥ã®é–‹å§‹ã‚’æ¤œå‡º (from, to, in, at, on, etc.)
            if word.lower() in ['from', 'to', 'in', 'at', 'on', 'by', 'with', 'for', 'during', 'since']:
                # å‰ç½®è©å¥ã®çµ‚ã‚ã‚Šã‚’æ¢ã™ï¼ˆæ¬¡ã®å‰ç½®è©ã‚„æ–‡æœ«ã¾ã§ï¼‰
                phrase_end = i + 1
                while phrase_end < len(words) and words[phrase_end].lower() not in ['from', 'to', 'in', 'at', 'on', 'by', 'with', 'for', 'during', 'since']:
                    phrase_end += 1
                
                # å‰ç½®è©å¥å…¨ä½“ã‚’å–å¾—
                prep_phrase = " ".join(words[i:phrase_end])
                phrases_to_classify.append(prep_phrase)
                current_phrase = []
                i = phrase_end
                continue
                
            # å‰¯è©ã®æ¤œå‡º
            elif word.lower() in ['quickly', 'slowly', 'carefully', 'quietly', 'loudly', 'well', 'badly', 'fast', 'hard', 'early', 'late']:
                phrases_to_classify.append(word)
                current_phrase = []
                
            i += 1
        
        # æ®‹ã‚Šã®èªå¥ãŒã‚ã‚Œã°è¿½åŠ 
        if current_phrase:
            phrases_to_classify.append(" ".join(current_phrase))
        
        # å„ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’åˆ†é¡
        for phrase in phrases_to_classify:
            if phrase.strip():
                slot_info = self.classify_remaining_phrase(phrase.strip())
                slot = slot_info['slot']
                if slot not in modifiers:
                    modifiers[slot] = []
                modifiers[slot].append({
                    'value': slot_info['value'],
                    'type': slot_info['type'],
                    'rule_id': 'present-perfect-modifier'
                })
        
        return modifiers

    # ===== spaCyèªå½™èªè­˜æ©Ÿèƒ½ =====
    
    def enhance_word_recognition(self, text):
        """spaCyã‚’ä½¿ç”¨ã—ã¦èªå½™èªè­˜ã‚’å¼·åŒ–ï¼ˆè§£æãƒ­ã‚¸ãƒƒã‚¯ã¯å¤‰æ›´ã—ãªã„ï¼‰"""
        if not self.nlp:
            return None  # spaCyæœªä½¿ç”¨æ™‚ã¯Noneã‚’è¿”ã™
            
        doc = self.nlp(text)
        enhanced_words = {}
        
        for token in doc:
            if token.is_punct:
                continue
                
            # is_oovã®ä»£æ›¿åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
            has_pos = token.pos_ != 'X'  # å“è©ãŒç‰¹å®šã•ã‚Œã¦ã„ã‚‹
            is_alpha = token.is_alpha    # è‹±å­—ã®ã¿
            is_recognized = has_pos and is_alpha
            
            word_info = {
                'text': token.text,
                'lemma': token.lemma_,
                'pos': token.pos_,
                'is_known': is_recognized,  # ä»£æ›¿åˆ¤å®šã‚’ä½¿ç”¨
                'is_alpha': token.is_alpha,
                'confidence': 0.95 if is_recognized else 0.7
            }
            enhanced_words[token.text.lower()] = word_info
            
        return enhanced_words
    
    def is_word_recognized(self, word):
        """å˜èªãŒèªè­˜å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆspaCyå°‚ç”¨ï¼‰"""
        if not self.nlp:
            return False  # spaCyæœªä½¿ç”¨æ™‚ã¯èªè­˜ä¸å¯
            
        try:
            doc = self.nlp(word)
            if len(doc) == 1:
                token = doc[0]
                # POSï¼ˆå“è©ï¼‰ãŒç‰¹å®šã•ã‚Œã€è‹±å­—ã®ã¿ã®å ´åˆã¯èªè­˜æ¸ˆã¿
                has_pos = token.pos_ != 'X'  # Xã¯æœªçŸ¥å“è©
                is_alpha = token.is_alpha
                return has_pos and is_alpha
        except Exception:
            return False
            
        return False
    
    def is_known_word_traditional(self, word):
        """å¾“æ¥ã®å½¢æ…‹ç´ ãƒ«ãƒ¼ãƒ«ã«ã‚ˆã‚‹èªå½™èªè­˜"""
        word_lower = word.lower()
        
        # åŸºæœ¬èªå½™ãƒã‚§ãƒƒã‚¯
        basic_words = [
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'about', 'into', 'through', 'during',
            'before', 'after', 'above', 'below', 'up', 'down', 'out', 'off', 'over',
            'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when',
            'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',
            'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own',
            'same', 'so', 'than', 'too', 'very', 'can', 'will', 'just', 'should'
        ]
        
        return word_lower in basic_words


def test_parsing_engine():
    """Parsing Engineã®ãƒ†ã‚¹ãƒˆ"""
    print("=== Rephrase Parsing Engine ãƒ†ã‚¹ãƒˆ ===")
    
    engine = RephraseParsingEngine()
    
    test_sentences = [
        # åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³
        "I run fast",
        "She is happy", 
        "I will go",
        
        # åŠ©å‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³
        "I could have done it",
        "She must have finished",
        
        # å—å‹•æ…‹ãƒ‘ã‚¿ãƒ¼ãƒ³  
        "The book is written by John",
        "The window was broken",
        
        # èªçŸ¥å‹•è© + thatç¯€
        "I think that he is smart",
        "She believes that we are ready",
        "I know what he thinks",
        
        # è¤‡åˆãƒ‘ã‚¿ãƒ¼ãƒ³
        "I think that the book should be written",
        "She believes that I could have done better",
    ]
    
    for sentence in test_sentences:
        print(f"\nå…¥åŠ›: {sentence}")
        try:
            slots = engine.analyze_sentence(sentence)
            
            if not slots:
                print("  âŒ ã‚¹ãƒ­ãƒƒãƒˆãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                continue
                
            for slot, candidates in slots.items():
                if candidates:
                    candidate = candidates[0]
                    value = candidate['value']
                    note = candidate.get('note', candidate.get('type', ''))
                    rule_id = candidate.get('rule_id', '')
                    
                    print(f"  {slot}: {value} ({note}) [rule: {rule_id}]")
                    
                    # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæƒ…å ±ãŒã‚ã‚Œã°è¡¨ç¤º
                    if 'subslots' in candidate and candidate['subslots']:
                        for sub_slot, sub_value in candidate['subslots'].items():
                            print(f"    â””â”€ {sub_slot}: {sub_value}")
                            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    test_parsing_engine()
