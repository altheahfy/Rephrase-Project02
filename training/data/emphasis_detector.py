#!/usr/bin/env python3
"""
Phase 3: å¼·èª¿æ§‹æ–‡æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…
V7ã‚·ã‚¹ãƒ†ãƒ ã«å¼·èª¿æ§‹æ–‡æ¤œå‡ºæ©Ÿèƒ½ã‚’è¿½åŠ 

å¼·èª¿æ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³:
1. Itåˆ†è£‚æ–‡: It is John who did this.
2. ç–‘ä¼¼åˆ†è£‚æ–‡: What I need is rest.
3. Doå¼·èª¿: I do believe you.
4. å‰¯è©å¼·èª¿: Never, ever do that again.
5. èªé †å¼·èª¿: This I cannot accept.
6. åå¾©å¼·èª¿: Very, very important.
7. æ„Ÿå˜†å¼·èª¿: What a beautiful day!
8. å€’ç½®å¼·èª¿: Down came the rain. (æ—¢å­˜ã®å€’ç½®ã¨çµ±åˆ)
"""

import spacy
import re
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from enum import Enum

class EmphasisType(Enum):
    """å¼·èª¿æ§‹æ–‡ã®ç¨®é¡"""
    CLEFT_SENTENCE = "cleft_sentence"           # It is John who did this
    PSEUDO_CLEFT = "pseudo_cleft"               # What I need is rest
    WH_CLEFT = "wh_cleft"                       # What John did was leave
    DO_EMPHASIS = "do_emphasis"                 # I do believe you
    ADVERB_EMPHASIS = "adverb_emphasis"         # Never, ever do that
    FRONTING_EMPHASIS = "fronting_emphasis"     # This I cannot accept
    REPETITION_EMPHASIS = "repetition_emphasis" # Very, very important
    EXCLAMATION_EMPHASIS = "exclamation_emphasis" # What a beautiful day!
    INTENSIFIER_EMPHASIS = "intensifier_emphasis" # So very important
    NO_EMPHASIS = "no_emphasis"                 # å¼·èª¿ãªã—

@dataclass
class EmphasisAnalysis:
    """å¼·èª¿æ§‹æ–‡åˆ†æçµæœ"""
    emphasis_type: EmphasisType
    emphasized_element: str     # å¼·èª¿ã•ã‚Œã‚‹è¦ç´ 
    emphasis_marker: str        # å¼·èª¿ãƒãƒ¼ã‚«ãƒ¼
    base_sentence: str         # åŸºæœ¬æ–‡æ¨å®š
    confidence: float           # ç¢ºä¿¡åº¦
    components: Dict[str, str]  # æ§‹æˆè¦ç´ 
    explanation: str            # èª¬æ˜
    intensity_level: int        # å¼·èª¿åº¦ (1-5)

class EmphasisDetector:
    """å¼·èª¿æ§‹æ–‡æ¤œå‡ºã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        print("ğŸ’ª Emphasis Detector åˆæœŸåŒ–ä¸­...")
        self.nlp = spacy.load("en_core_web_sm")
        
        # å¼·èª¿ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©
        self.emphasis_patterns = {
            EmphasisType.CLEFT_SENTENCE: {
                'patterns': [
                    r'^It\s+(is|was)\s+.+\s+(who|that|which)\s+',
                    r'^It\s+(is|was)\s+(not\s+)?.+\s+(who|that|which)\s+'
                ],
                'confidence_base': 0.95,
                'intensity': 4
            },
            
            EmphasisType.PSEUDO_CLEFT: {
                'patterns': [
                    r'^(What|Where|When|Why|How)\s+.+\s+(is|was)\s+',
                    r'^(All|Everything|Nothing)\s+.+\s+(is|was)\s+'
                ],
                'confidence_base': 0.90,
                'intensity': 4
            },
            
            EmphasisType.DO_EMPHASIS: {
                'patterns': [
                    r'\b(do|does|did)\s+(believe|think|know|understand|agree|love|like|want|need)\b',
                    r'\bI\s+(do|does|did)\s+\w+\s+you\b'
                ],
                'confidence_base': 0.85,
                'intensity': 3
            },
            
            EmphasisType.EXCLAMATION_EMPHASIS: {
                'patterns': [
                    r'^What\s+(a|an)\s+.+!$',
                    r'^How\s+.+!$',
                    r'^Such\s+(a|an)\s+.+!$'
                ],
                'confidence_base': 0.92,
                'intensity': 5
            }
        }
        
        # å¼·èª¿å‰¯è©ãƒ»å½¢å®¹è©
        self.intensifiers = {
            'extreme': ['extremely', 'incredibly', 'absolutely', 'completely', 'totally', 'utterly', 'perfectly'],
            'high': ['very', 'really', 'quite', 'rather', 'pretty', 'fairly', 'highly'],
            'repetitive': ['so', 'such', 'too'],
            'negative': ['never', 'not', 'hardly', 'scarcely', 'barely']
        }
        
        # èªé †å¼·èª¿ãƒãƒ¼ã‚«ãƒ¼
        self.fronting_markers = {
            'object_fronting': ['this', 'that', 'such'],
            'complement_fronting': ['happy', 'sad', 'angry', 'tired'],
            'adverbial_fronting': ['never', 'rarely', 'here', 'there', 'now', 'then']
        }
    
    def detect_emphasis(self, sentence: str) -> EmphasisAnalysis:
        """å¼·èª¿æ§‹æ–‡ã‚’æ¤œå‡º"""
        doc = self.nlp(sentence)
        
        # å„å¼·èª¿ã‚¿ã‚¤ãƒ—ã‚’ãƒã‚§ãƒƒã‚¯
        for emphasis_type in EmphasisType:
            if emphasis_type == EmphasisType.NO_EMPHASIS:
                continue
                
            analysis = self._check_emphasis_type(sentence, doc, emphasis_type)
            if analysis.confidence > 0.5:
                return analysis
        
        # å¼·èª¿ãªã—
        return EmphasisAnalysis(
            emphasis_type=EmphasisType.NO_EMPHASIS,
            emphasized_element="",
            emphasis_marker="",
            base_sentence=sentence,
            confidence=0.0,
            components={},
            explanation="No emphasis detected",
            intensity_level=0
        )
    
    def _check_emphasis_type(self, sentence: str, doc, emphasis_type: EmphasisType) -> EmphasisAnalysis:
        """ç‰¹å®šã®å¼·èª¿ã‚¿ã‚¤ãƒ—ã‚’ãƒã‚§ãƒƒã‚¯"""
        
        if emphasis_type == EmphasisType.CLEFT_SENTENCE:
            return self._analyze_cleft_sentence(sentence, doc)
        
        elif emphasis_type == EmphasisType.PSEUDO_CLEFT:
            return self._analyze_pseudo_cleft(sentence, doc)
        
        elif emphasis_type == EmphasisType.DO_EMPHASIS:
            return self._analyze_do_emphasis(sentence, doc)
        
        elif emphasis_type == EmphasisType.EXCLAMATION_EMPHASIS:
            return self._analyze_exclamation_emphasis(sentence, doc)
        
        elif emphasis_type == EmphasisType.ADVERB_EMPHASIS:
            return self._analyze_adverb_emphasis(sentence, doc)
        
        elif emphasis_type == EmphasisType.FRONTING_EMPHASIS:
            return self._analyze_fronting_emphasis(sentence, doc)
        
        elif emphasis_type == EmphasisType.REPETITION_EMPHASIS:
            return self._analyze_repetition_emphasis(sentence, doc)
        
        elif emphasis_type == EmphasisType.INTENSIFIER_EMPHASIS:
            return self._analyze_intensifier_emphasis(sentence, doc)
        
        return self._create_empty_analysis()
    
    def _analyze_cleft_sentence(self, sentence: str, doc) -> EmphasisAnalysis:
        """Itåˆ†è£‚æ–‡åˆ†æ"""
        # Pattern: It is/was + NP + who/that/which + clause
        cleft_pattern = r'^It\s+(is|was)\s+([^,]+?)\s+(who|that|which)\s+(.+)$'
        match = re.match(cleft_pattern, sentence, re.IGNORECASE)
        
        if not match:
            return self._create_empty_analysis()
        
        be_verb = match.group(1)
        emphasized_np = match.group(2).strip()
        relative_pronoun = match.group(3)
        clause = match.group(4).strip()
        
        # åŸºæœ¬æ–‡ã‚’æ¨å®š
        if relative_pronoun.lower() == 'who':
            base_sentence = f"{emphasized_np} {clause}"
        elif relative_pronoun.lower() == 'that':
            base_sentence = f"{emphasized_np} {clause}"
        else:
            base_sentence = f"{clause} {emphasized_np}"
        
        return EmphasisAnalysis(
            emphasis_type=EmphasisType.CLEFT_SENTENCE,
            emphasized_element=emphasized_np,
            emphasis_marker=f"It {be_verb} ... {relative_pronoun}",
            base_sentence=base_sentence,
            confidence=0.95,
            components={
                'copula': f"It {be_verb}",
                'focus': emphasized_np,
                'relative_pronoun': relative_pronoun,
                'presupposition': clause
            },
            explanation=f"It-cleft emphasizes '{emphasized_np}' as the focused element",
            intensity_level=4
        )
    
    def _analyze_pseudo_cleft(self, sentence: str, doc) -> EmphasisAnalysis:
        """ç–‘ä¼¼åˆ†è£‚æ–‡åˆ†æ"""
        # Pattern: WH-word + clause + is/was + NP
        pseudo_pattern = r'^(What|Where|When|Why|How|All|Everything|Nothing)\s+(.+?)\s+(is|was)\s+(.+)$'
        match = re.match(pseudo_pattern, sentence, re.IGNORECASE)
        
        if not match:
            return self._create_empty_analysis()
        
        wh_word = match.group(1)
        wh_clause = match.group(2).strip()
        be_verb = match.group(3)
        focus_element = match.group(4).strip()
        
        # åŸºæœ¬æ–‡ã‚’æ¨å®š
        if wh_word.lower() == 'what':
            base_sentence = f"You need {focus_element}" if 'need' in wh_clause else f"The answer is {focus_element}"
        else:
            base_sentence = f"{focus_element} {wh_clause}"
        
        return EmphasisAnalysis(
            emphasis_type=EmphasisType.PSEUDO_CLEFT,
            emphasized_element=focus_element,
            emphasis_marker=f"{wh_word} ... {be_verb}",
            base_sentence=base_sentence,
            confidence=0.90,
            components={
                'wh_element': wh_word,
                'presupposition': wh_clause,
                'focus': focus_element
            },
            explanation=f"Pseudo-cleft emphasizes '{focus_element}' using {wh_word}-clause",
            intensity_level=4
        )
    
    def _analyze_do_emphasis(self, sentence: str, doc) -> EmphasisAnalysis:
        """Doå¼·èª¿åˆ†æ"""
        words = sentence.split()
        
        # Doå¼·èª¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™
        for i, word in enumerate(words):
            if word.lower() in ['do', 'does', 'did'] and i + 1 < len(words):
                next_word = words[i + 1].lower()
                
                # æ‹¡å¼µã•ã‚ŒãŸä¸€èˆ¬çš„ãªå‹•è©ã§doå¼·èª¿ã‚’ãƒã‚§ãƒƒã‚¯
                emphasis_verbs = ['believe', 'think', 'know', 'understand', 'agree', 
                                'love', 'like', 'want', 'need', 'hope', 'wish',
                                'finish', 'complete', 'work', 'study', 'help',
                                'come', 'go', 'try', 'care', 'matter', 'exist']
                
                # ã‚ˆã‚Šç·©å’Œã•ã‚ŒãŸæ¡ä»¶ï¼šå‹•è©ãƒªã‚¹ãƒˆã¾ãŸã¯å‹•è©ã®å½¢æ…‹çš„ç‰¹å¾´
                if (next_word in emphasis_verbs or 
                    # å‹•è©ã‚‰ã—ã„èªå°¾ãƒ‘ã‚¿ãƒ¼ãƒ³
                    (len(next_word) > 3 and next_word not in ['have', 'will', 'can', 'may', 'must']) or
                    # æ–‡è„ˆã‹ã‚‰å¼·èª¿åˆ¤å®šï¼ˆæ„Ÿæƒ…çš„ãªæ–‡ï¼‰
                    any(emotion_word in sentence.lower() for emotion_word in 
                        ['really', 'truly', 'certainly', 'definitely', 'absolutely'])):
                    
                    emphasized_verb = words[i + 1]
                    base_sentence = ' '.join(words[:i] + words[i+1:])  # doã‚’é™¤å»
                    
                    return EmphasisAnalysis(
                        emphasis_type=EmphasisType.DO_EMPHASIS,
                        emphasized_element=emphasized_verb,
                        emphasis_marker=word,
                        base_sentence=base_sentence,
                        confidence=0.85,
                        components={
                            'emphasis_auxiliary': word,
                            'emphasized_verb': emphasized_verb
                        },
                        explanation=f"Do-emphasis strengthens the assertion of '{emphasized_verb}'",
                        intensity_level=3
                    )
        
        return self._create_empty_analysis()
    
    def _analyze_exclamation_emphasis(self, sentence: str, doc) -> EmphasisAnalysis:
        """æ„Ÿå˜†å¼·èª¿åˆ†æ"""
        # What a/an + NP!
        what_pattern = r'^What\s+(a|an)\s+(.+?)!$'
        match = re.match(what_pattern, sentence, re.IGNORECASE)
        
        if match:
            article = match.group(1)
            noun_phrase = match.group(2).strip()
            base_sentence = f"It is {article} {noun_phrase}."
            
            return EmphasisAnalysis(
                emphasis_type=EmphasisType.EXCLAMATION_EMPHASIS,
                emphasized_element=noun_phrase,
                emphasis_marker="What a/an ... !",
                base_sentence=base_sentence,
                confidence=0.92,
                components={'exclamative': 'What', 'focus': noun_phrase},
                explanation=f"Exclamative 'What a/an' emphasizes the degree of '{noun_phrase}'",
                intensity_level=5
            )
        
        # How + ADJ/ADV!
        how_pattern = r'^How\s+(.+?)!$'
        match = re.match(how_pattern, sentence, re.IGNORECASE)
        
        if match:
            adjective_phrase = match.group(1).strip()
            base_sentence = f"It is {adjective_phrase}."
            
            return EmphasisAnalysis(
                emphasis_type=EmphasisType.EXCLAMATION_EMPHASIS,
                emphasized_element=adjective_phrase,
                emphasis_marker="How ... !",
                base_sentence=base_sentence,
                confidence=0.92,
                components={'exclamative': 'How', 'focus': adjective_phrase},
                explanation=f"Exclamative 'How' emphasizes the degree of '{adjective_phrase}'",
                intensity_level=5
            )
        
        # Such a/an + NP! ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ 
        such_pattern = r'^Such\s+(a|an)\s+(.+?)!?$'
        match = re.match(such_pattern, sentence, re.IGNORECASE)
        
        if match:
            article = match.group(1)
            noun_phrase = match.group(2).strip().rstrip('!')
            base_sentence = f"It is {article} {noun_phrase}."
            
            return EmphasisAnalysis(
                emphasis_type=EmphasisType.EXCLAMATION_EMPHASIS,
                emphasized_element=noun_phrase,
                emphasis_marker="Such a/an ... !",
                base_sentence=base_sentence,
                confidence=0.88,
                components={'exclamative': 'Such', 'focus': noun_phrase},
                explanation=f"Exclamative 'Such a/an' emphasizes the degree of '{noun_phrase}'",
                intensity_level=4
            )
        
        return self._create_empty_analysis()
    
    def _analyze_adverb_emphasis(self, sentence: str, doc) -> EmphasisAnalysis:
        """å‰¯è©å¼·èª¿åˆ†æ"""
        words = sentence.split()
        
        # åå¾©å‰¯è©ã‚’ãƒã‚§ãƒƒã‚¯
        for i in range(len(words) - 1):
            if words[i].lower() == words[i + 1].lower():
                repeated_word = words[i]
                
                # å¼·èª¿å‰¯è©ã®åå¾©
                if repeated_word.lower() in ['very', 'so', 'never', 'really', 'quite']:
                    base_sentence = ' '.join(words[:i] + words[i+1:])  # 1ã¤ã®åå¾©ã‚’é™¤å»
                    
                    return EmphasisAnalysis(
                        emphasis_type=EmphasisType.REPETITION_EMPHASIS,
                        emphasized_element=repeated_word,
                        emphasis_marker=f"{repeated_word}, {repeated_word}",
                        base_sentence=base_sentence,
                        confidence=0.88,
                        components={'repeated_element': repeated_word},
                        explanation=f"Repetition of '{repeated_word}' for emphasis",
                        intensity_level=3
                    )
        
        # å¼·èª¿å‰¯è©ã®ç´¯ç©
        intensifier_count = 0
        intensifiers_found = []
        
        for word in words:
            for level, intensifier_list in self.intensifiers.items():
                if word.lower() in intensifier_list:
                    intensifier_count += 1
                    intensifiers_found.append((word, level))
        
        if intensifier_count >= 2:
            intensifier_words = [word for word, level in intensifiers_found]
            base_sentence = sentence  # ç°¡æ˜“ç‰ˆã§ã¯åŒã˜
            
            return EmphasisAnalysis(
                emphasis_type=EmphasisType.INTENSIFIER_EMPHASIS,
                emphasized_element=' '.join(intensifier_words),
                emphasis_marker='multiple intensifiers',
                base_sentence=base_sentence,
                confidence=0.75,
                components={'intensifiers': intensifiers_found},
                explanation=f"Multiple intensifiers {intensifier_words} create emphasis",
                intensity_level=min(intensifier_count, 5)
            )
        
        return self._create_empty_analysis()
    
    def _analyze_fronting_emphasis(self, sentence: str, doc) -> EmphasisAnalysis:
        """èªé †å¼·èª¿ï¼ˆå‰ç½®ï¼‰åˆ†æ"""
        words = sentence.split()
        
        if len(words) < 3:
            return self._create_empty_analysis()
        
        first_word = words[0].lower()
        
        # ç›®çš„èªå‰ç½®: "This I cannot accept"
        if first_word in ['this', 'that', 'these', 'those']:
            # ä»£åè© + ä¸»èª + å‹•è© ãƒ‘ã‚¿ãƒ¼ãƒ³
            if len(words) >= 3:
                object_element = words[0]
                subject = words[1]
                verb_phrase = ' '.join(words[2:])
                
                # é€šå¸¸èªé †ã«å¤‰æ›
                base_sentence = f"{subject} {verb_phrase} {object_element.lower()}"
                
                return EmphasisAnalysis(
                    emphasis_type=EmphasisType.FRONTING_EMPHASIS,
                    emphasized_element=object_element,
                    emphasis_marker="object fronting",
                    base_sentence=base_sentence,
                    confidence=0.80,
                    components={'fronted_element': object_element, 'type': 'object'},
                    explanation=f"Object '{object_element}' fronted for emphasis",
                    intensity_level=3
                )
        
        # å½¢å®¹è©å‰ç½®: "Happy I am not"
        if first_word in ['happy', 'sad', 'angry', 'tired', 'excited', 'worried']:
            complement = words[0]
            rest_sentence = ' '.join(words[1:])
            base_sentence = f"{rest_sentence} {complement.lower()}"
            
            return EmphasisAnalysis(
                emphasis_type=EmphasisType.FRONTING_EMPHASIS,
                emphasized_element=complement,
                emphasis_marker="complement fronting",
                base_sentence=base_sentence,
                confidence=0.75,
                components={'fronted_element': complement, 'type': 'complement'},
                explanation=f"Complement '{complement}' fronted for emphasis",
                intensity_level=3
            )
        
        return self._create_empty_analysis()
    
    def _analyze_repetition_emphasis(self, sentence: str, doc) -> EmphasisAnalysis:
        """åå¾©å¼·èª¿åˆ†æ"""
        words = sentence.split()
        
        # åå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        for i in range(len(words) - 1):
            current_word = words[i].lower().strip(',')
            next_word = words[i + 1].lower().strip(',')
            
            # åŒã˜å˜èªã®åå¾©ï¼ˆvery, very / so, so / such, suchï¼‰
            if current_word == next_word and current_word in ['very', 'so', 'such', 'really', 'quite']:
                # å¼·èª¿å¯¾è±¡ã‚’æ¢ã™ï¼ˆé€šå¸¸ã¯åå¾©èªã®å¾Œã«ç¶šãå½¢å®¹è©ãƒ»åè©ï¼‰
                emphasized_element = ""
                if i + 2 < len(words):
                    emphasized_element = words[i + 2]
                elif i > 0:
                    emphasized_element = words[i - 1]
                
                base_sentence = sentence.replace(f"{words[i]}, {words[i + 1]}", words[i])
                
                return EmphasisAnalysis(
                    emphasis_type=EmphasisType.REPETITION_EMPHASIS,
                    emphasized_element=emphasized_element,
                    emphasis_marker=f"{current_word}, {current_word}",
                    base_sentence=base_sentence,
                    confidence=0.82,
                    components={
                        'repeated_word': current_word,
                        'target': emphasized_element
                    },
                    explanation=f"Repetition of '{current_word}' emphasizes intensity",
                    intensity_level=4
                )
        
        return self._create_empty_analysis()
    
    def _analyze_intensifier_emphasis(self, sentence: str, doc) -> EmphasisAnalysis:
        """å¼·èª¿èªå¼·èª¿åˆ†æï¼ˆæ—¢ã«_analyze_adverb_emphasisã§ã‚«ãƒãƒ¼ï¼‰"""
        return self._create_empty_analysis()
    
    def _create_empty_analysis(self) -> EmphasisAnalysis:
        """ç©ºã®åˆ†æçµæœ"""
        return EmphasisAnalysis(
            emphasis_type=EmphasisType.NO_EMPHASIS,
            emphasized_element="",
            emphasis_marker="",
            base_sentence="",
            confidence=0.0,
            components={},
            explanation="",
            intensity_level=0
        )

def test_emphasis_detection():
    """å¼·èª¿æ§‹æ–‡æ¤œå‡ºãƒ†ã‚¹ãƒˆ"""
    detector = EmphasisDetector()
    
    test_sentences = [
        # Itåˆ†è£‚æ–‡
        "It is John who broke the window.",
        "It was the book that I needed.",
        
        # ç–‘ä¼¼åˆ†è£‚æ–‡
        "What I need is rest.",
        "What John did was leave early.",
        "All I want is peace.",
        
        # Doå¼·èª¿
        "I do believe you are right.",
        "She does love classical music.",
        "They did finish on time.",
        
        # æ„Ÿå˜†å¼·èª¿
        "What a beautiful day!",
        "How wonderful!",
        "Such a lovely garden!",
        
        # åå¾©å¼·èª¿
        "Very, very important decision.",
        "So, so tired today.",
        
        # èªé †å¼·èª¿
        "This I cannot accept.",
        "Happy I am not.",
        
        # å¼·èª¿ãªã—ï¼ˆæ¯”è¼ƒç”¨ï¼‰
        "I read books every day.",
        "She is a good student.",
    ]
    
    print("ğŸ’ª å¼·èª¿æ§‹æ–‡æ¤œå‡ºãƒ†ã‚¹ãƒˆ")
    print("=" * 80)
    
    for sentence in test_sentences:
        print(f"\nğŸ” åˆ†æ: {sentence}")
        result = detector.detect_emphasis(sentence)
        
        if result.confidence > 0.5:
            print(f"   ğŸ“Š å¼·èª¿ã‚¿ã‚¤ãƒ—: {result.emphasis_type.value}")
            print(f"   ğŸ¯ å¼·èª¿è¦ç´ : {result.emphasized_element}")
            print(f"   ğŸ”§ å¼·èª¿ãƒãƒ¼ã‚«ãƒ¼: {result.emphasis_marker}")
            print(f"   ğŸ“ åŸºæœ¬æ–‡æ¨å®š: {result.base_sentence}")
            print(f"   ğŸ’¡ èª¬æ˜: {result.explanation}")
            print(f"   ğŸ’ª å¼·èª¿åº¦: {result.intensity_level}/5")
            print(f"   ğŸ“ˆ ç¢ºä¿¡åº¦: {result.confidence:.2f}")
        else:
            print(f"   âœ… å¼·èª¿ãªã—")

if __name__ == "__main__":
    test_emphasis_detection()
