"""
Basic Five Pattern Handler - 5æ–‡å‹å°‚é–€å‡¦ç†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
Phase 1: 100%ç²¾åº¦ç›®æ¨™

Human Grammar Pattern:
- spaCy POSè§£æã‚’æƒ…å ±æºã¨ã—ãŸæ–‡æ³•ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜
- äººé–“ãŒæ–‡æ³•ä½“ç³»ã‚’ç†è§£ã™ã‚‹ã‚ˆã†ã«å…¨ä½“æ§‹é€ ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ç…§åˆ
- ä¾å­˜é–¢ä¿‚è§£æã¯ä½¿ç”¨ã›ãšã€POSã‚¿ã‚°ãƒ™ãƒ¼ã‚¹ã®èªè­˜
"""

import spacy
from typing import Dict, List, Any, Optional


class BasicFivePatternHandler:
    """
    5æ–‡å‹å°‚é–€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    
    è²¬ä»»:
    - 5æ–‡å‹ã®è­˜åˆ¥ãƒ»åˆ†è§£å‡¦ç†
    - Rephraseã‚¹ãƒ­ãƒƒãƒˆé…ç½®
    - 100%ç²¾åº¦ã®å®Ÿç¾
    
    ç¦æ­¢:
    - ä¾å­˜é–¢ä¿‚è§£æã®ä½¿ç”¨
    - ä»–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã¨ã®ç›´æ¥é€šä¿¡
    - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.nlp = spacy.load('en_core_web_sm')
        
        # 5æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©
        self.patterns = {
            'SV': ['S', 'V'],                    # ç¬¬1æ–‡å‹
            'SVC': ['S', 'V', 'C1'],             # ç¬¬2æ–‡å‹  
            'SVO': ['S', 'V', 'O1'],             # ç¬¬3æ–‡å‹
            'SVOO': ['S', 'V', 'O1', 'O2'],      # ç¬¬4æ–‡å‹
            'SVOC': ['S', 'V', 'O1', 'C2']       # ç¬¬5æ–‡å‹
        }
        
        # æ–‡å‹åˆ¤å®šç”¨å‹•è©åˆ†é¡
        self.verb_types = {
            'linking': ['be', 'seem', 'become', 'appear', 'look', 'sound', 'feel', 'taste', 'smell', 'remain', 'stay', 'turn', 'grow'],
            'transitive': ['love', 'like', 'see', 'hear', 'make', 'take', 'give', 'send', 'show', 'read', 'play', 'study'],
            'ditransitive': ['give', 'send', 'show', 'tell', 'teach', 'buy', 'make', 'get', 'find', 'offer'],
            'causative': ['make', 'let', 'have', 'get', 'help', 'see', 'hear', 'watch', 'call', 'find', 'consider']
        }
    
    def process(self, text: str) -> Dict[str, Any]:
        """
        5æ–‡å‹å‡¦ç†ãƒ¡ã‚¤ãƒ³
        
        Args:
            text: å‡¦ç†å¯¾è±¡ã®è‹±èªæ–‡
            
        Returns:
            Dict: å‡¦ç†çµæœï¼ˆsuccess, slots, errorï¼‰
        """
        try:
            doc = self.nlp(text)
            
            # 1. åŸºæœ¬è¦ç´ æŠ½å‡ºï¼ˆPOSè§£æãƒ™ãƒ¼ã‚¹ï¼‰
            elements = self._extract_basic_elements(doc)
            
            if not elements:
                return {'success': False, 'error': 'åŸºæœ¬è¦ç´ ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ'}
            
            # 2. æ–‡å‹åˆ¤å®š
            pattern_type = self._identify_pattern(elements, doc)
            
            if not pattern_type:
                return {'success': False, 'error': 'æ–‡å‹ã‚’åˆ¤å®šã§ãã¾ã›ã‚“ã§ã—ãŸ'}
            
            # 3. ã‚¹ãƒ­ãƒƒãƒˆé…ç½®
            slots = self._assign_slots(elements, pattern_type)
            
            return {
                'success': True,
                'slots': slots,
                'pattern_type': pattern_type
            }
            
        except Exception as e:
            return {'success': False, 'error': f'å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}'}
    
    def _extract_basic_elements(self, doc) -> Dict[str, str]:
        """
        åŸºæœ¬è¦ç´ æŠ½å‡º: S, V, O, C ã®å€™è£œã‚’æŠ½å‡º
        
        é‡è¦: å‰ç½®è©å¥ã®åè©ã¯åŸºæœ¬ã‚¹ãƒ­ãƒƒãƒˆï¼ˆS, O1, O2, C1, C2ï¼‰ã‹ã‚‰å®Œå…¨é™¤å¤–
        
        Args:
            doc: spaCy Doc ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            
        Returns:
            Dict: æŠ½å‡ºã•ã‚ŒãŸè¦ç´ 
        """
        elements = {}
        
        # ä¸»èªå€™è£œï¼ˆæ–‡é ­ã®åè©å¥å…¨ä½“ã‚’æŠ½å‡ºï¼‰
        subject_tokens = []
        for token in doc:
            if token.pos_ in ['DET', 'ADJ', 'NOUN', 'PRON', 'PROPN']:
                subject_tokens.append(token.text)
            elif token.pos_ in ['VERB', 'AUX']:
                break  # å‹•è©ã«åˆ°é”ã—ãŸã‚‰ä¸»èªçµ‚äº†
            elif subject_tokens:  # ä¸»èªå€™è£œãŒã‚ã‚Šã€å‹•è©ä»¥å¤–ã«åˆ°é”ã—ãŸã‚‰çµ‚äº†
                break
        
        if subject_tokens:
            elements['S'] = ' '.join(subject_tokens)
        
        # å‹•è©æŠ½å‡ºã¨ä½ç½®ç‰¹å®š
        verb_idx = None
        for i, token in enumerate(doc):
            if token.pos_ == 'VERB' and not token.lemma_ in ['be']:
                elements['V'] = token.text
                verb_idx = i
                break
            elif token.pos_ == 'AUX' and token.lemma_ == 'be':
                elements['V'] = token.text
                verb_idx = i
                break
        
        if verb_idx is None:
            return elements
        
        # ğŸ”¥ é‡è¦: å‰ç½®è©å¥æ¤œå‡ºã¨é™¤å¤–
        # å‹•è©å¾Œã®è¦ç´ ã‹ã‚‰å‰ç½®è©å¥ã‚’å®Œå…¨ã«é™¤å¤–
        core_elements = []  # åŸºæœ¬ã‚¹ãƒ­ãƒƒãƒˆå€™è£œï¼ˆå‰ç½®è©å¥é™¤å¤–å¾Œï¼‰
        
        i = verb_idx + 1
        while i < len(doc):
            token = doc[i]
            
            if token.pos_ == 'PUNCT':
                i += 1
                continue
                
            elif token.pos_ == 'ADP':  # å‰ç½®è©ç™ºè¦‹
                # å‰ç½®è©å¥å…¨ä½“ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå‰ç½®è©ï¼‹åè©å¥ï¼‰
                print(f"ğŸš« å‰ç½®è©å¥æ¤œå‡ºãƒ»é™¤å¤–: '{token.text}' ã‹ã‚‰æ–‡æœ«ã¾ã§")
                while i < len(doc) and doc[i].pos_ != 'PUNCT':
                    i += 1
                break  # å‰ç½®è©å¥ä»¥é™ã¯å…¨ã¦ã‚¹ã‚­ãƒƒãƒ—
                
            elif token.pos_ in ['NOUN', 'PRON', 'PROPN', 'ADJ', 'DET']:
                core_elements.append(token)
                i += 1
                
            else:
                i += 1
        
        print(f"ğŸ” å‰ç½®è©å¥é™¤å¤–å¾Œã®æ ¸è¦ç´ : {[token.text for token in core_elements]}")
        
        if not core_elements:
            return elements
        
        # å‹•è©ã®ç¨®é¡åˆ¤å®šï¼ˆé€£çµå‹•è©ã‹ã©ã†ã‹ï¼‰
        verb_lemma = doc[verb_idx].lemma_
        is_linking_verb = verb_lemma in self.verb_types['linking']
        is_causative_verb = verb_lemma in self.verb_types['causative']
        is_ditransitive_verb = verb_lemma in self.verb_types['ditransitive']
        
        # è¦ç´ åˆ†é¡ï¼ˆå‹•è©ã®ç¨®é¡ã«å¿œã˜ã¦å‡¦ç†ï¼‰
        elements_found = []
        
        if is_causative_verb or is_ditransitive_verb:
            # ä½¿å½¹å‹•è©ãƒ»æˆä¸å‹•è©ã®å ´åˆï¼šç‰¹åˆ¥ãªå‡¦ç†
            current_phrase = []
            
            for token in core_elements:
                if token.pos_ in ['DET', 'ADJ', 'NOUN', 'PRON', 'PROPN']:
                    if token.pos_ == 'PRON' and current_phrase:
                        # ä»£åè©ãŒæ¥ãŸå ´åˆã€å‰ã®å¥ã‚’çµ‚äº†ã—ã¦æ–°ã—ã„å¥ã‚’é–‹å§‹
                        if current_phrase:
                            phrase_text = ' '.join([t.text for t in current_phrase])
                            elements_found.append(('NOUN', phrase_text))
                        current_phrase = [token]
                    elif token.pos_ == 'PRON':
                        # å˜ç‹¬ã®ä»£åè©
                        elements_found.append(('NOUN', token.text))
                    elif token.pos_ == 'ADJ':
                        # å˜ç‹¬ã®å½¢å®¹è©
                        if current_phrase:
                            phrase_text = ' '.join([t.text for t in current_phrase])
                            elements_found.append(('NOUN', phrase_text))
                            current_phrase = []
                        elements_found.append(('ADJ', token.text))
                    else:
                        current_phrase.append(token)
            
            # æœ€å¾Œã®å¥ã‚’å‡¦ç†
            if current_phrase:
                phrase_text = ' '.join([t.text for t in current_phrase])
                elements_found.append(('NOUN', phrase_text))
        else:
            # é€šå¸¸ã®å ´åˆï¼šå¥ãƒ™ãƒ¼ã‚¹ã§å‡¦ç†
            current_phrase = []
            
            for token in core_elements:
                if token.pos_ in ['DET', 'ADJ', 'NOUN', 'PRON', 'PROPN']:
                    current_phrase.append(token)
                elif current_phrase:  # å¥ã®çµ‚äº†
                    phrase_text = ' '.join([t.text for t in current_phrase])
                    
                    # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®POSã§åˆ¤å®š
                    last_token_pos = current_phrase[-1].pos_
                    
                    if last_token_pos == 'ADJ':
                        elements_found.append(('ADJ', phrase_text))
                    else:
                        elements_found.append(('NOUN', phrase_text))
                    current_phrase = []
            
            # æœ€å¾Œã®å¥ã‚’å‡¦ç†
            if current_phrase:
                phrase_text = ' '.join([t.text for t in current_phrase])
                
                # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®POSã§åˆ¤å®š
                last_token_pos = current_phrase[-1].pos_
                
                if last_token_pos == 'ADJ':
                    elements_found.append(('ADJ', phrase_text))
                else:
                    elements_found.append(('NOUN', phrase_text))
        
        # æ–‡å‹åˆ¤å®šã¨ã‚¹ãƒ­ãƒƒãƒˆé…ç½®
        noun_count = len([e for e in elements_found if e[0] == 'NOUN'])
        adj_count = len([e for e in elements_found if e[0] == 'ADJ'])
        
        if is_linking_verb and adj_count > 0:
            # ç¬¬2æ–‡å‹: é€£çµå‹•è© + å½¢å®¹è©
            adj_elements = [e[1] for e in elements_found if e[0] == 'ADJ']
            elements['C1'] = adj_elements[0]
            
        elif is_linking_verb and noun_count > 0:
            # ç¬¬2æ–‡å‹: é€£çµå‹•è© + åè©ï¼ˆè£œèªï¼‰
            noun_elements = [e[1] for e in elements_found if e[0] == 'NOUN']
            elements['C1'] = noun_elements[0]
            
        elif is_ditransitive_verb and noun_count == 2:
            # ç¬¬4æ–‡å‹: æˆä¸å‹•è© + é–“æ¥ç›®çš„èª + ç›´æ¥ç›®çš„èª
            noun_elements = [e[1] for e in elements_found if e[0] == 'NOUN']
            elements['O1'] = noun_elements[0]
            elements['O2'] = noun_elements[1]
            
        elif is_causative_verb and noun_count == 2:
            # ç¬¬5æ–‡å‹: ä½¿å½¹å‹•è© + ç›®çš„èª + è£œèªï¼ˆåè©ï¼‰
            noun_elements = [e[1] for e in elements_found if e[0] == 'NOUN']
            elements['O1'] = noun_elements[0]
            elements['C2'] = noun_elements[1]
            
        elif is_causative_verb and noun_count == 1 and adj_count == 1:
            # ç¬¬5æ–‡å‹: ä½¿å½¹å‹•è© + ç›®çš„èª + è£œèªï¼ˆå½¢å®¹è©ï¼‰
            noun_elements = [e[1] for e in elements_found if e[0] == 'NOUN']
            adj_elements = [e[1] for e in elements_found if e[0] == 'ADJ']
            elements['O1'] = noun_elements[0]
            elements['C2'] = adj_elements[0]
            
        elif noun_count == 1 and adj_count == 0:
            # ç¬¬3æ–‡å‹: ä»–å‹•è© + ç›®çš„èª
            noun_elements = [e[1] for e in elements_found if e[0] == 'NOUN']
            elements['O1'] = noun_elements[0]
            
        elif noun_count == 2 and adj_count == 0:
            # ç¬¬4æ–‡å‹: æˆä¸å‹•è© + é–“æ¥ç›®çš„èª + ç›´æ¥ç›®çš„èª
            noun_elements = [e[1] for e in elements_found if e[0] == 'NOUN']
            elements['O1'] = noun_elements[0]
            elements['O2'] = noun_elements[1]
                
        return elements
    
    def _identify_pattern(self, elements: Dict[str, str], doc) -> Optional[str]:
        """
        æ–‡å‹åˆ¤å®š: æŠ½å‡ºã•ã‚ŒãŸè¦ç´ ã‹ã‚‰5æ–‡å‹ã‚’åˆ¤å®š
        
        Args:
            elements: æŠ½å‡ºã•ã‚ŒãŸåŸºæœ¬è¦ç´ 
            doc: spaCy Doc ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            
        Returns:
            Optional[str]: åˆ¤å®šã•ã‚ŒãŸæ–‡å‹
        """
        if not elements.get('S') or not elements.get('V'):
            return None
        
        # å‹•è©ã®é€£çµå‹•è©åˆ¤å®š
        verb_lemma = None
        for token in doc:
            if token.pos_ in ['VERB', 'AUX'] and token.text.lower() == elements['V'].lower():
                verb_lemma = token.lemma_
                break
        
        is_linking_verb = verb_lemma in self.verb_types['linking'] if verb_lemma else False
        
        # ç¬¬2æ–‡å‹: é€£çµå‹•è© + è£œèª
        if is_linking_verb and elements.get('C1'):
            return 'SVC'
        
        # ç¬¬5æ–‡å‹: ä½¿å½¹å‹•è© + O + C
        if elements.get('O1') and elements.get('C2'):
            return 'SVOC'
        
        # ç¬¬4æ–‡å‹: æˆä¸å‹•è© + O1 + O2
        if elements.get('O1') and elements.get('O2'):
            return 'SVOO'
        
        # ç¬¬3æ–‡å‹: ä»–å‹•è© + O
        if elements.get('O1'):
            return 'SVO'
        
        # ç¬¬1æ–‡å‹: è‡ªå‹•è©ã®ã¿
        return 'SV'
    
    def _assign_slots(self, elements: Dict[str, str], pattern_type: str) -> Dict[str, str]:
        """
        ã‚¹ãƒ­ãƒƒãƒˆé…ç½®: æ–‡å‹ã«å¿œã˜ãŸRephraseã‚¹ãƒ­ãƒƒãƒˆé…ç½®
        
        Args:
            elements: æŠ½å‡ºã•ã‚ŒãŸè¦ç´ 
            pattern_type: åˆ¤å®šã•ã‚ŒãŸæ–‡å‹
            
        Returns:
            Dict: ã‚¹ãƒ­ãƒƒãƒˆé…ç½®çµæœ
        """
        slots = {}
        pattern = self.patterns[pattern_type]
        
        for slot in pattern:
            if slot == 'S' and elements.get('S'):
                slots['S'] = elements['S']
            elif slot == 'V' and elements.get('V'):
                slots['V'] = elements['V']
            elif slot == 'O1' and elements.get('O1'):
                slots['O1'] = elements['O1']
            elif slot == 'O2' and elements.get('O2'):
                slots['O2'] = elements['O2']
            elif slot == 'C1' and elements.get('C1'):
                slots['C1'] = elements['C1']
            elif slot == 'C2' and elements.get('C2'):
                slots['C2'] = elements['C2']
        
        return slots


if __name__ == "__main__":
    # åŸºæœ¬ãƒ†ã‚¹ãƒˆ
    handler = BasicFivePatternHandler()
    
    test_cases = [
        ("She is happy.", "SVC"),
        ("I love you.", "SVO"),
        ("He gave me a book.", "SVOO"),
        ("We made him happy.", "SVOC"),
        ("Birds fly.", "SV")
    ]
    
    print("=== BasicFivePatternHandler ãƒ†ã‚¹ãƒˆ ===")
    for sentence, expected in test_cases:
        print(f"\nå…¥åŠ›: {sentence}")
        print(f"æœŸå¾…: {expected}")
        result = handler.process(sentence)
        print(f"çµæœ: {result}")
