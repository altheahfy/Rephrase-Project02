"""
Phase A2: çœŸã®BasicFivePatternHandlerå®Ÿè£…
ãƒ¬ã‚¬ã‚·ãƒ¼åˆ†è§£æ©Ÿèƒ½ã‚’ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å†…ã«å®Œå…¨ç§»è¡Œ

ä½œæˆæ—¥: 2025å¹´8æœˆ25æ—¥
ç›®çš„: Central Controllerã‹ã‚‰åˆ†è§£æ©Ÿèƒ½ã‚’ç§»è¡Œã—ã€ç´”ç²‹ä¸­å¤®ç®¡ç†ã‚’å®Ÿç¾
"""

from typing import Dict, List, Any, Optional, Tuple
import logging
from collections import namedtuple

# GrammarElementå®šç¾©ï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¨ã®äº’æ›æ€§ï¼‰
GrammarElement = namedtuple('GrammarElement', [
    'text', 'tokens', 'role', 'start_idx', 'end_idx', 'confidence'
])

class BasicFivePatternHandler:
    """
    ğŸ¯ æ‹¡å¼µç‰ˆåŸºæœ¬5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    
    Phase A2å®Ÿè£…:
    â”œâ”€ ãƒ¬ã‚¬ã‚·ãƒ¼åˆ†è§£æ©Ÿèƒ½ã®å®Œå…¨çµ±åˆ
    â”œâ”€ ä¸»èªãƒ»å‹•è©ãƒ»ç›®çš„èªãƒ»è£œèªã®ç²¾å¯†ç‰¹å®š
    â”œâ”€ æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³è‡ªå‹•åˆ¤å®š  
    â””â”€ ã‚¹ãƒ­ãƒƒãƒˆé…ç½®ã®å®Œå…¨è‡ªå‹•åŒ–
    
    ç§»è¡Œå¯¾è±¡æ©Ÿèƒ½:
    â”œâ”€ _identify_core_elements() â†’ identify_core_elements_enhanced()
    â”œâ”€ _determine_sentence_pattern() â†’ determine_pattern_enhanced()
    â””â”€ _assign_grammar_roles() â†’ assign_roles_enhanced()
    """
    
    def __init__(self, logger=None):
        self.logger = logger or logging.getLogger(__name__)
        
        # ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œ: èªå½™ãƒªã‚¹ãƒˆ
        self.linking_verbs = {
            'be', 'is', 'am', 'are', 'was', 'were', 'been', 'being',
            'become', 'became', 'get', 'got', 'seem', 'seemed', 'appear', 'appeared',
            'look', 'looked', 'sound', 'sounded', 'feel', 'felt', 'taste', 'tasted',
            'smell', 'smelled', 'remain', 'remained', 'stay', 'stayed', 'keep', 'kept',
            'turn', 'turned', 'grow', 'grew', 'prove', 'proved'
        }
        
        self.ditransitive_verbs = {
            'give', 'gave', 'given', 'tell', 'told', 'show', 'showed', 'shown',
            'send', 'sent', 'bring', 'brought', 'teach', 'taught', 'offer', 'offered',
            'sell', 'sold', 'buy', 'bought', 'lend', 'lent', 'hand', 'handed',
            'pass', 'passed', 'throw', 'threw', 'thrown'
        }
        
        # åŒå½¢èªå‡¦ç†ï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ç¶™æ‰¿ï¼‰
        self.ambiguous_words = {
            'works': ['NOUN', 'VERB'],   # workè¤‡æ•°å½¢ vs workä¸‰äººç§°å˜æ•°
            'runs': ['NOUN', 'VERB'],    # runè¤‡æ•°å½¢ vs runä¸‰äººç§°å˜æ•°
            'calls': ['NOUN', 'VERB'],   # callè¤‡æ•°å½¢ vs callä¸‰äººç§°å˜æ•°
            'studies': ['NOUN', 'VERB'], # studyè¤‡æ•°å½¢ vs studyä¸‰äººç§°å˜æ•°
            'rides': ['NOUN', 'VERB'],   # rideè¤‡æ•°å½¢ vs rideä¸‰äººç§°å˜æ•°
            'sits': ['NOUN', 'VERB']     # sitè¤‡æ•°å½¢ vs sitä¸‰äººç§°å˜æ•°
        }
    
    def handle(self, tokens: List[Dict], context: Dict) -> Dict[str, Any]:
        """
        ğŸ¯ ãƒ¡ã‚¤ãƒ³ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å‡¦ç†
        
        çµ±åˆã•ã‚ŒãŸãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ã‚’é †æ¬¡å®Ÿè¡Œ:
        Step 1: åŸºæœ¬è¦ç´ ç‰¹å®š
        Step 2: æ–‡å‹åˆ¤å®š
        Step 3: å½¹å‰²å‰²ã‚Šå½“ã¦
        Step 4: çµæœç”Ÿæˆ
        """
        try:
            # Step 1: åŸºæœ¬è¦ç´ ç‰¹å®šï¼ˆæ—§_identify_core_elementsçµ±åˆï¼‰
            core_elements = self.identify_core_elements_enhanced(tokens)
            
            # Step 2: æ–‡å‹åˆ¤å®šï¼ˆæ—§_determine_sentence_patternçµ±åˆï¼‰
            pattern = self.determine_pattern_enhanced(tokens, core_elements)
            
            # Step 3: å½¹å‰²å‰²ã‚Šå½“ã¦ï¼ˆæ—§_assign_grammar_rolesçµ±åˆï¼‰
            grammar_elements = self.assign_roles_enhanced(tokens, pattern, core_elements)
            
            # Step 4: çµæœç”Ÿæˆ
            result = self._convert_to_handler_result(grammar_elements, pattern, tokens)
            
            return result
            
        except Exception as e:
            self.logger.error(f"BasicFivePatternHandler error: {e}")
            return {'success': False, 'error': str(e)}
    
    def identify_core_elements_enhanced(self, tokens: List[Dict]) -> Dict[str, Any]:
        """
        ğŸ¯ ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½çµ±åˆç‰ˆ: åŸºæœ¬è¦ç´ ç‰¹å®š
        
        æ—§_identify_core_elements()ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Œå…¨ç§»è¡Œãƒ»å¼·åŒ–
        """
        core = {
            'subject': None,
            'verb': None,
            'subject_indices': [],
            'verb_indices': [],
            'auxiliary': None,
            'auxiliary_indices': []
        }
        
        # å‹•è©ã‚’æ¢ã™ï¼ˆæœ€ã‚‚é‡è¦ï¼‰
        main_verb_idx = self._find_main_verb_enhanced(tokens)
        if main_verb_idx is not None:
            core['verb'] = tokens[main_verb_idx]
            core['verb_indices'] = [main_verb_idx]
            
            # åŠ©å‹•è©ã‚’æ¢ã™
            aux_idx = self._find_auxiliary_enhanced(tokens, main_verb_idx)
            if aux_idx is not None:
                core['auxiliary'] = tokens[aux_idx]
                core['auxiliary_indices'] = [aux_idx]
        
        # ä¸»èªã‚’æ¢ã™ï¼ˆå‹•è©ã®å‰ã§æœ€ã‚‚é©åˆ‡ãªåè©å¥ï¼‰
        if main_verb_idx is not None:
            subject_indices = self._find_subject_enhanced(tokens, main_verb_idx)
            if subject_indices:
                core['subject_indices'] = subject_indices
                core['subject'] = ' '.join([tokens[i]['text'] for i in subject_indices])
        
        return core
    
    def determine_pattern_enhanced(self, tokens: List[Dict], core_elements: Dict) -> str:
        """
        ğŸ¯ ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½çµ±åˆç‰ˆ: æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¤å®š
        
        æ—§_determine_sentence_pattern()ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Œå…¨ç§»è¡Œãƒ»å¼·åŒ–
        """
        if not core_elements['verb']:
            return 'UNKNOWN'
        
        verb = core_elements['verb']
        verb_lemma = verb['lemma'].lower()
        verb_indices = core_elements['verb_indices'] + core_elements.get('auxiliary_indices', [])
        subject_indices = core_elements['subject_indices']
        
        # è‡ªå‹•è©ãƒªã‚¹ãƒˆï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼ç¶™æ‰¿ï¼‰
        intransitive_verbs = {
            'arrive', 'arrived', 'come', 'came', 'go', 'went', 'sleep', 'slept',
            'walk', 'walked', 'run', 'ran', 'happen', 'happened', 'occur', 'occurred',
            'exist', 'existed', 'fall', 'fell', 'rise', 'rose', 'sit', 'sat',
            'stand', 'stood', 'lie', 'lay', 'work', 'worked', 'laugh', 'laughed',
            'cry', 'cried', 'smile', 'smiled', 'die', 'died'
        }
        
        # ä½¿ç”¨æ¸ˆã¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        used_indices = set(verb_indices + subject_indices)
        
        # æ®‹ã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åˆ†æ
        remaining_tokens = [
            (i, token) for i, token in enumerate(tokens) 
            if i not in used_indices and token['pos'] != 'PUNCT'
        ]
        
        # è‡ªå‹•è©ã®å ´åˆã¯å¼·åˆ¶çš„ã«SVãƒ‘ã‚¿ãƒ¼ãƒ³
        if verb_lemma in intransitive_verbs or verb['text'].lower() in intransitive_verbs:
            return 'SV'
        
        # é€£çµå‹•è©ã®å ´åˆ â†’ SVCå€™è£œ
        if verb_lemma in self.linking_verbs:
            if remaining_tokens:
                # è£œèªãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                for i, token in remaining_tokens:
                    if self._can_be_complement_enhanced(token):
                        return 'SVC'
            return 'SV'  # è£œèªãŒãªã„å ´åˆ
        
        # æˆä¸å‹•è©ã®å ´åˆ â†’ SVOOå€™è£œ
        if verb_lemma in self.ditransitive_verbs:
            if len(remaining_tokens) >= 2:
                return 'SVOO'
            elif len(remaining_tokens) == 1:
                return 'SVO'
        
        # ãã®ä»–ã®å ´åˆï¼šæ®‹ã‚Šãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§åˆ¤å®š
        if len(remaining_tokens) >= 2:
            return 'SVOC'  # SVOCå€™è£œ
        elif len(remaining_tokens) == 1:
            return 'SVO'   # SVOå€™è£œ
        else:
            return 'SV'    # SV
    
    def assign_roles_enhanced(self, tokens: List[Dict], pattern: str, core_elements: Dict) -> List[GrammarElement]:
        """
        ğŸ¯ ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½çµ±åˆç‰ˆ: æ–‡æ³•çš„å½¹å‰²å‰²ã‚Šå½“ã¦
        
        æ—§_assign_grammar_roles()ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Œå…¨ç§»è¡Œãƒ»å¼·åŒ–
        """
        elements = []
        used_indices = set()
        
        # ä¸»èªå‡¦ç†
        if core_elements['subject_indices']:
            subject_element = GrammarElement(
                text=core_elements['subject'],
                tokens=[tokens[i] for i in core_elements['subject_indices']],
                role='S',
                start_idx=min(core_elements['subject_indices']),
                end_idx=max(core_elements['subject_indices']),
                confidence=0.95
            )
            elements.append(subject_element)
            used_indices.update(core_elements['subject_indices'])
        
        # åŠ©å‹•è©å‡¦ç†
        if core_elements['auxiliary_indices']:
            aux_element = GrammarElement(
                text=core_elements['auxiliary']['text'],
                tokens=[core_elements['auxiliary']],
                role='Aux',
                start_idx=core_elements['auxiliary_indices'][0],
                end_idx=core_elements['auxiliary_indices'][0],
                confidence=0.9
            )
            elements.append(aux_element)
            used_indices.update(core_elements['auxiliary_indices'])
        
        # å‹•è©å‡¦ç†
        if core_elements['verb_indices']:
            verb_element = GrammarElement(
                text=core_elements['verb']['text'],
                tokens=[core_elements['verb']],
                role='V',
                start_idx=core_elements['verb_indices'][0],
                end_idx=core_elements['verb_indices'][0],
                confidence=0.95
            )
            elements.append(verb_element)
            used_indices.update(core_elements['verb_indices'])
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥å‡¦ç†
        remaining_tokens = [
            (i, token) for i, token in enumerate(tokens)
            if i not in used_indices and token['pos'] != 'PUNCT'
        ]
        
        if pattern == 'SVO' and len(remaining_tokens) >= 1:
            # ç›®çš„èª
            obj_idx, obj_token = remaining_tokens[0]
            obj_element = GrammarElement(
                text=obj_token['text'],
                tokens=[obj_token],
                role='O1',
                start_idx=obj_idx,
                end_idx=obj_idx,
                confidence=0.8
            )
            elements.append(obj_element)
        
        elif pattern == 'SVC' and len(remaining_tokens) >= 1:
            # è£œèª
            comp_idx, comp_token = remaining_tokens[0]
            comp_element = GrammarElement(
                text=comp_token['text'],
                tokens=[comp_token],
                role='C1',
                start_idx=comp_idx,
                end_idx=comp_idx,
                confidence=0.8
            )
            elements.append(comp_element)
        
        elif pattern == 'SVOO' and len(remaining_tokens) >= 2:
            # é–“æ¥ç›®çš„èª
            obj1_idx, obj1_token = remaining_tokens[0]
            obj1_element = GrammarElement(
                text=obj1_token['text'],
                tokens=[obj1_token],
                role='O1',
                start_idx=obj1_idx,
                end_idx=obj1_idx,
                confidence=0.8
            )
            elements.append(obj1_element)
            
            # ç›´æ¥ç›®çš„èª
            obj2_idx, obj2_token = remaining_tokens[1]
            obj2_element = GrammarElement(
                text=obj2_token['text'],
                tokens=[obj2_token],
                role='O2',
                start_idx=obj2_idx,
                end_idx=obj2_idx,
                confidence=0.8
            )
            elements.append(obj2_element)
        
        return elements
    
    def _find_main_verb_enhanced(self, tokens: List[Dict]) -> Optional[int]:
        """
        ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œ: ãƒ¡ã‚¤ãƒ³å‹•è©ç‰¹å®š
        æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶™æ‰¿ãƒ»å¼·åŒ–
        """
        # POSãƒ™ãƒ¼ã‚¹ã¨æ–‡è„ˆãƒ™ãƒ¼ã‚¹ã®ä¸¡æ–¹ã‚’å–å¾—
        pos_candidates = []
        for i, token in enumerate(tokens):
            # å‹•è©ã®å“è©ã‚¿ã‚°
            if (token['tag'].startswith('VB') and token['pos'] == 'VERB') or token['pos'] == 'AUX':
                pos_candidates.append((i, token))
        
        # æ–‡è„ˆçš„å‹•è©è­˜åˆ¥ï¼ˆPOSèª¤èªè­˜å¯¾ç­–ï¼‰
        contextual_candidates = self._find_contextual_verbs_enhanced(tokens)
        
        # ä¸¡æ–¹ã‚’çµ±åˆï¼ˆé‡è¤‡é™¤å»ï¼‰
        verb_candidates = pos_candidates.copy()
        for i, token in contextual_candidates:
            # æ—¢ã«å­˜åœ¨ã—ãªã„å ´åˆã®ã¿è¿½åŠ 
            if not any(existing_i == i for existing_i, _ in verb_candidates):
                verb_candidates.append((i, token))
        
        if not verb_candidates:
            return None
        
        # äººé–“çš„åˆ¤å®šï¼šé–¢ä¿‚ç¯€ã‚’é™¤å¤–ã—ã¦ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’ç‰¹å®š
        non_relative_verbs = []
        
        for i, token in verb_candidates:
            # é–¢ä¿‚ä»£åè©ã®ç›´å¾Œã®å‹•è©ã¯é–¢ä¿‚ç¯€å†…å‹•è©ã¨ã—ã¦é™¤å¤–
            is_in_relative_clause = False
            
            # å‰ã®å˜èªã‚’ç¢ºèª
            for j in range(max(0, i-5), i):  # 5èªå‰ã¾ã§ç¢ºèª
                prev_token = tokens[j]
                if prev_token['text'].lower() in ['who', 'whom', 'which', 'that', 'whose', 'where', 'when']:
                    # whoseæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†: å‹•è©/åè©åŒå½¢èªã¯é–¢ä¿‚ç¯€å¤–ã®ãƒ¡ã‚¤ãƒ³å‹•è©ã¨ã—ã¦æ‰±ã†
                    if (prev_token['text'].lower() == 'whose' and 
                        token['text'].lower() in self.ambiguous_words and
                        token.get('contextual_override', False)):
                        # whoseæ§‹æ–‡ã§ã®åŒå½¢èªå‹•è©ã¯é–¢ä¿‚ç¯€å¤–ã¨ã—ã¦æ‰±ã†
                        is_in_relative_clause = False
                        break
                    
                    # é–¢ä¿‚ä»£åè©ã‹ã‚‰å‹•è©ã¾ã§ã®è·é›¢ãŒè¿‘ã„å ´åˆã€é–¢ä¿‚ç¯€å†…å‹•è©
                    if i - j <= 4:  # 4èªä»¥å†…ãªã‚‰é–¢ä¿‚ç¯€å†…
                        is_in_relative_clause = True
                        break
            
            if not is_in_relative_clause:
                non_relative_verbs.append((i, token))
        
        if non_relative_verbs:
            # ãƒ¡ã‚¤ãƒ³å‹•è©å€™è£œã‹ã‚‰åŠ©å‹•è©ã§ãªã„ã‚‚ã®ã‚’å„ªå…ˆ
            main_verbs = [(i, token) for i, token in non_relative_verbs if not self._is_auxiliary_verb_enhanced(token)]
            if main_verbs:
                # æ–‡ã®å¾ŒåŠã«ã‚ã‚‹ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’å„ªå…ˆï¼ˆé–¢ä¿‚ç¯€ã®å¾Œï¼‰
                return main_verbs[-1][0]
            return non_relative_verbs[-1][0]
        
        # æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦ã€ã©ã®å‹•è©ã§ã‚‚é¸æŠ
        return verb_candidates[-1][0]
    
    def _find_contextual_verbs_enhanced(self, tokens: List[Dict]) -> List[Tuple[int, Dict]]:
        """
        ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œ: æ–‡è„ˆçš„å‹•è©è­˜åˆ¥
        """
        contextual_verbs = []
        sentence_text = ' '.join([token['text'] for token in tokens])
        
        for i, token in enumerate(tokens):
            # æ—¢ã«å‹•è©ã¨ã—ã¦èªè­˜ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®
            if token['pos'] in ['VERB', 'AUX']:
                continue
            
            # åŒå½¢èªãƒã‚§ãƒƒã‚¯
            if token['text'].lower() in self.ambiguous_words:
                if 'VERB' in self.ambiguous_words[token['text'].lower()]:
                    # æ–‡è„ˆã‹ã‚‰å‹•è©ã¨ã—ã¦åˆ¤å®š
                    if self._contextual_verb_check_enhanced(tokens, i, sentence_text):
                        # ãƒ•ãƒ©ã‚°ã‚’è¿½åŠ ã—ã¦å‹•è©ã¨ã—ã¦æ‰±ã†
                        enhanced_token = token.copy()
                        enhanced_token['contextual_override'] = True
                        enhanced_token['pos'] = 'VERB'
                        contextual_verbs.append((i, enhanced_token))
        
        return contextual_verbs
    
    def _contextual_verb_check_enhanced(self, tokens: List[Dict], token_idx: int, sentence: str) -> bool:
        """
        ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œ: æ–‡è„ˆçš„å‹•è©åˆ¤å®š
        """
        token = tokens[token_idx]
        
        # å¾Œç¶šã«ç›®çš„èªã‚‰ã—ãè¦ç´ ãŒã‚ã‚‹ã‹
        for i in range(token_idx + 1, min(len(tokens), token_idx + 3)):
            if i < len(tokens) and tokens[i]['pos'] in ['NOUN', 'PRON']:
                return True
        
        # å‰ã«ä¸»èªã‚‰ã—ãè¦ç´ ãŒã‚ã‚‹ã‹
        for i in range(max(0, token_idx - 3), token_idx):
            if tokens[i]['pos'] in ['NOUN', 'PRON']:
                return True
        
        return False
    
    def _find_auxiliary_enhanced(self, tokens: List[Dict], main_verb_idx: int) -> Optional[int]:
        """
        ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œ: åŠ©å‹•è©ç‰¹å®š
        """
        # ãƒ¡ã‚¤ãƒ³å‹•è©ã®å‰ã‚’æ¢ç´¢
        for i in range(main_verb_idx - 1, -1, -1):
            token = tokens[i]
            if self._is_auxiliary_verb_enhanced(token):
                return i
            # åè©ãŒæ¥ãŸã‚‰æ¢ç´¢çµ‚äº†
            if token['pos'] in ['NOUN', 'PRON']:
                break
        return None
    
    def _find_subject_enhanced(self, tokens: List[Dict], verb_idx: int) -> List[int]:
        """
        ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œ: ä¸»èªç‰¹å®š
        """
        subject_indices = []
        
        # å‹•è©ã®å‰ã§æœ€åˆã«è¦‹ã¤ã‹ã‚‹åè©å¥ã‚’ä¸»èªã¨ã™ã‚‹
        for i in range(verb_idx - 1, -1, -1):
            token = tokens[i]
            if token['pos'] in ['NOUN', 'PRON']:
                subject_indices.insert(0, i)  # èªé †ã‚’ä¿æŒ
            elif token['pos'] in ['DET', 'ADJ']:
                subject_indices.insert(0, i)  # ä¿®é£¾èªã‚‚å«ã‚ã‚‹
            elif subject_indices:  # åè©å¥ãŒé–‹å§‹ã•ã‚ŒãŸã‚‰ç¶™ç¶š
                break
        
        return subject_indices
    
    def _is_auxiliary_verb_enhanced(self, token: Dict) -> bool:
        """
        ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œ: åŠ©å‹•è©åˆ¤å®š
        """
        aux_verbs = {
            'am', 'is', 'are', 'was', 'were', 'be', 'being', 'been',
            'have', 'has', 'had', 'having',
            'do', 'does', 'did',
            'will', 'would', 'shall', 'should',
            'can', 'could', 'may', 'might',
            'must', 'ought'
        }
        return token['lemma'].lower() in aux_verbs or token['pos'] == 'AUX'
    
    def _can_be_complement_enhanced(self, token: Dict) -> bool:
        """
        ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œ: è£œèªåˆ¤å®š
        """
        return token['pos'] in ['NOUN', 'ADJ', 'PRON']
    
    def _convert_to_handler_result(self, grammar_elements: List[GrammarElement], pattern: str, tokens: List[Dict]) -> Dict[str, Any]:
        """
        GrammarElementsã‚’çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼å½¢å¼ã«å¤‰æ›
        """
        slots = {}
        
        for element in grammar_elements:
            if element.text and element.text.strip():
                slots[element.role] = element.text.strip()
        
        return {
            'success': True,
            'slots': slots,
            'pattern_detected': pattern,
            'confidence': 0.9,
            'handler_name': 'basic_five_pattern_enhanced',
            'processing_notes': f'Enhanced pattern: {pattern}, elements: {len(grammar_elements)}'
        }
