"""
æ­£ã—ã„Rephraseä»•æ§˜æº–æ‹ ã‚¨ãƒ³ã‚¸ãƒ³ v4.0
- PhraseType = "clause"/"phrase" â†’ ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆç©ºåŒ–
- é–¢ä¿‚è©ç¯€ã¯ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå†…ã§å‡¦ç†ï¼ˆåˆ¥å¾“å±ç¯€ã¨ã—ã¦åˆ†é›¢ã—ãªã„ï¼‰
- Step18å‚ç…§ã«ã‚ˆã‚‹æ­£ç¢ºãªå®Ÿè£…
"""

import stanza
import spacy
from typing import Dict, List, Any, Optional, Tuple
import json
import re
from dataclasses import dataclass

@dataclass
class SlotResult:
    """ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£çµæœ"""
    main_content: str = ""  # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆå†…å®¹ï¼ˆç©ºåŒ–å¯¾è±¡ï¼‰
    subslots: Dict[str, str] = None  # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå†…å®¹
    phrase_type: str = "word"  # word/phrase/clause
    is_empty_upper: bool = False  # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆç©ºåŒ–ãƒ•ãƒ©ã‚°
    
    def __post_init__(self):
        if self.subslots is None:
            self.subslots = {}

class RephraseSpecCompliantEngine:
    """Rephraseä»•æ§˜æº–æ‹ ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        print("ğŸš€ Rephraseä»•æ§˜æº–æ‹ ã‚¨ãƒ³ã‚¸ãƒ³ v4.0 åˆæœŸåŒ–ä¸­...")
        
        # NLPãƒ„ãƒ¼ãƒ«åˆæœŸåŒ–
        try:
            self.nlp_stanza = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')
            print("âœ… StanzaåˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"âš ï¸ StanzaåˆæœŸåŒ–å¤±æ•—: {e}")
            self.nlp_stanza = None
        
        try:
            self.nlp_spacy = spacy.load('en_core_web_sm')
            print("âœ… spaCyåˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"âš ï¸ spaCyåˆæœŸåŒ–å¤±æ•—: {e}")
            self.nlp_spacy = None
        
        # Step18å‚ç…§ç”¨ãƒãƒƒãƒ”ãƒ³ã‚°
        self.step18_reference = self._load_step18_reference()
        
        print("âœ… åˆæœŸåŒ–å®Œäº†\n")
    
    def _load_step18_reference(self) -> Dict:
        """Step18ã®æ—¢çŸ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ­ãƒ¼ãƒ‰"""
        return {
            "manager_pattern": {
                "sentence": "The experienced manager who had recently taken charge completed the project successfully.",
                "expected_result": {
                    "S": {
                        "sub-s": "manager who",
                        "sub-aux": "had", 
                        "sub-m2": "recently",
                        "sub-o1": "charge",
                        "sub-v": "taken"
                    },
                    "V": {"v": "completed"}
                }
            }
        }
    
    def decompose_sentence(self, sentence: str) -> Dict[str, Any]:
        """
        æ–‡ã‚’90ã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ ã«åˆ†è§£ï¼ˆRephraseä»•æ§˜æº–æ‹ ï¼‰
        """
        print(f"ğŸ” åˆ†è§£é–‹å§‹: {sentence}")
        
        # Phase 1: ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ï¼ˆStanzaåŸºç›¤ï¼‰
        stanza_analysis = self._analyze_with_stanza(sentence)
        
        # Phase 2: æ§‹é€ èªè­˜ï¼ˆspaCyè£œå®Œï¼‰
        structure_analysis = self._analyze_structure(sentence, stanza_analysis)
        
        # Phase 3: ã‚¹ãƒ­ãƒƒãƒˆå‰²ã‚Šå½“ã¦ï¼ˆRephraseä»•æ§˜æº–æ‹ ï¼‰
        slot_assignment = self._assign_slots_rephrase_compliant(sentence, structure_analysis)
        
        # Phase 4: ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆç©ºåŒ–å‡¦ç†
        final_result = self._apply_upper_slot_emptying(slot_assignment)
        
        print(f"âœ… åˆ†è§£å®Œäº†: {len(final_result)}ã‚¹ãƒ­ãƒƒãƒˆ")
        return final_result
    
    def _analyze_with_stanza(self, sentence: str) -> Dict:
        """Phase 1: Stanzaã«ã‚ˆã‚‹åŸºæœ¬åˆ†æ"""
        if not self.nlp_stanza:
            return {"tokens": [], "dependencies": []}
        
        try:
            doc = self.nlp_stanza(sentence)
            
            analysis = {
                "tokens": [],
                "dependencies": [],
                "sentences": []
            }
            
            for sent in doc.sentences:
                sent_data = {
                    "text": sent.text,
                    "words": []
                }
                
                for word in sent.words:
                    word_data = {
                        "text": word.text,
                        "lemma": word.lemma,
                        "pos": word.pos,
                        "head": word.head,
                        "deprel": word.deprel,
                        "id": word.id
                    }
                    analysis["tokens"].append(word_data)
                    sent_data["words"].append(word_data)
                    
                    # ä¾å­˜é–¢ä¿‚ã‚’è¨˜éŒ²
                    if word.head != 0:  # rootã§ãªã„å ´åˆ
                        analysis["dependencies"].append({
                            "child": word_data,
                            "parent_id": word.head,
                            "relation": word.deprel
                        })
                
                analysis["sentences"].append(sent_data)
            
            return analysis
            
        except Exception as e:
            print(f"âš ï¸ Stanzaåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"tokens": [], "dependencies": []}
    
    def _analyze_structure(self, sentence: str, stanza_analysis: Dict) -> Dict:
        """Phase 2: spaCyã«ã‚ˆã‚‹æ§‹é€ åˆ†æ"""
        if not self.nlp_spacy:
            return stanza_analysis
        
        try:
            doc = self.nlp_spacy(sentence)
            
            # é–¢ä¿‚è©ç¯€ã®è­˜åˆ¥
            relative_clauses = []
            for token in doc:
                if token.text.lower() in ['who', 'which', 'that']:
                    # é–¢ä¿‚è©ã‹ã‚‰ç¯€ã®çµ‚ã‚ã‚Šã¾ã§ã‚’ç‰¹å®š
                    clause_tokens = [token]
                    current = token
                    
                    # é–¢ä¿‚è©ç¯€ã®ç¯„å›²ã‚’ç‰¹å®š
                    for child in token.children:
                        if child.dep_ in ['nsubj', 'aux', 'advmod', 'dobj', 'pobj']:
                            clause_tokens.append(child)
                    
                    relative_clauses.append({
                        'start': min(t.i for t in clause_tokens),
                        'end': max(t.i for t in clause_tokens),
                        'text': ' '.join([t.text for t in sorted(clause_tokens, key=lambda x: x.i)]),
                        'head_noun': token.head.text if token.head else None
                    })
            
            # æ§‹é€ æƒ…å ±ã‚’è¿½åŠ 
            structure = stanza_analysis.copy()
            structure.update({
                "relative_clauses": relative_clauses,
                "named_entities": [(ent.text, ent.label_) for ent in doc.ents],
                "noun_phrases": [chunk.text for chunk in doc.noun_chunks]
            })
            
            return structure
            
        except Exception as e:
            print(f"âš ï¸ spaCyåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return stanza_analysis
    
    def _assign_slots_rephrase_compliant(self, sentence: str, analysis: Dict) -> Dict[str, SlotResult]:
        """Phase 3: Rephraseä»•æ§˜æº–æ‹ ã‚¹ãƒ­ãƒƒãƒˆå‰²ã‚Šå½“ã¦ï¼ˆStep18ãƒ™ãƒ¼ã‚¹è‡ªå‹•åˆ†è§£ï¼‰"""
        
        # Step18ãƒ™ãƒ¼ã‚¹ã®çœŸã®è‡ªå‹•åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨
        step18_result = self._step18_auto_decompose(sentence)
        
        # Step18çµæœã‚’SlotResultå½¢å¼ã«å¤‰æ›
        return self._convert_step18_to_slot_result(step18_result)
    
    def _step18_auto_decompose(self, sentence: str) -> Dict[str, Dict[str, str]]:
        """Step18ãƒ™ãƒ¼ã‚¹ã®çœŸã®è‡ªå‹•åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³"""
        print("ğŸ¯ Step18ãƒ™ãƒ¼ã‚¹è‡ªå‹•åˆ†è§£é–‹å§‹")
        
        if not self.nlp_spacy:
            print("âš ï¸ spaCyæœªåˆæœŸåŒ– - ç©ºçµæœã‚’è¿”ã—ã¾ã™")
            return {}
        
        doc = self.nlp_spacy(sentence)
        
        # ä¾å­˜é–¢ä¿‚-ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆStep18ã¨åŒã˜ï¼‰
        dep_to_subslot = {
            'nsubj': 'sub-s',
            'nsubjpass': 'sub-s', 
            'aux': 'sub-aux',
            'auxpass': 'sub-aux',
            'dobj': 'sub-o1',
            'iobj': 'sub-o2',
            'attr': 'sub-c1',
            'ccomp': 'sub-c2',
            'xcomp': 'sub-c2',
            'advmod': 'sub-m2',
            'amod': 'sub-m3',
            'prep': 'sub-m3', 
            'pobj': 'sub-o1',
            'pcomp': 'sub-c2',
            'mark': 'sub-m1',
            'relcl': 'sub-m3',
            'acl': 'sub-m3'
        }
        
        # ROOTå‹•è©ç‰¹å®š
        root_verb = None
        for token in doc:
            if token.dep_ == 'ROOT' and token.pos_ in ['VERB', 'AUX']:
                root_verb = token
                break
        
        if not root_verb:
            print("âš ï¸ ROOTå‹•è©ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return {}
        
        print(f"ğŸ¯ ROOTå‹•è©: '{root_verb.text}'")
        
        # å®Œå…¨ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ï¼ˆStep18ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        slots = {}
        
        # Sã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡ºï¼ˆé–¢ä¿‚è©ç¯€å¯¾å¿œï¼‰
        s_slot = self._extract_s_slot_step18(doc, root_verb, dep_to_subslot)
        if s_slot:
            slots['S'] = s_slot
        
        # Vã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º
        v_slot = self._extract_v_slot_step18(root_verb)
        if v_slot:
            slots['V'] = v_slot
        
        # ãã®ä»–ã®ã‚¹ãƒ­ãƒƒãƒˆã‚‚åŒæ§˜ã«æŠ½å‡º...
        # ç°¡ç•¥åŒ–ã®ãŸã‚ã€ã¾ãšS, Vã®ã¿å®Ÿè£…
        
        return slots
    
    
    def _extract_s_slot_step18(self, doc, root_verb, dep_to_subslot) -> Optional[Dict[str, str]]:
        """Step18ãƒ™ãƒ¼ã‚¹ã®Sã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        print("ï¿½ Sã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡ºï¼ˆStep18ãƒ™ãƒ¼ã‚¹ï¼‰")
        
        # ROOTå‹•è©ã®ä¸»èªã‚’æ¢ã™
        main_subject = None
        for child in root_verb.children:
            if child.dep_ in ['nsubj', 'nsubjpass']:
                main_subject = child
                break
        
        if not main_subject:
            return None
        
        print(f"ğŸ“Œ ä¸»èªç™ºè¦‹: '{main_subject.text}'")
        
        # ä¸»èªã®é–¢ä¿‚ç¯€å‡¦ç†
        from collections import defaultdict
        s_tokens = defaultdict(list)
        
        # ä¸»èªã®å­è¦ç´ åé›†
        for child in main_subject.children:
            if child.dep_ == 'relcl':  # é–¢ä¿‚ç¯€
                print(f"ğŸ“Œ é–¢ä¿‚ç¯€ç™ºè¦‹: '{child.text}'")
                
                # é–¢ä¿‚ç¯€å†…ã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ
                for rel_child in child.children:
                    dep = rel_child.dep_
                    if dep in dep_to_subslot:
                        subslot = dep_to_subslot[dep]
                        s_tokens[subslot].append(rel_child)
                
                # é–¢ä¿‚ç¯€å‹•è©è‡ªä½“
                s_tokens['sub-v'].append(child)
        
        # ä¸»èªè‡ªä½“ï¼ˆã‚¹ãƒ‘ãƒ³æ‹¡å¼µé©ç”¨ï¼‰
        s_tokens['sub-s'].append(main_subject)
        print(f"ğŸ“Œ ä¸»èªãƒˆãƒ¼ã‚¯ãƒ³è¿½åŠ : '{main_subject.text}' (dep={main_subject.dep_})")
        
        # ROOTå‹•è©ã®auxåé›†ï¼ˆSã‚¹ãƒ­ãƒƒãƒˆç”¨ï¼‰
        for child in root_verb.children:
            if child.dep_ in ['aux', 'auxpass']:
                s_tokens['sub-aux'].append(child)
        
        if s_tokens:
            return self._build_subslots_step18(s_tokens, doc)
        
        return None
    
    def _extract_v_slot_step18(self, root_verb) -> Dict[str, str]:
        """Step18ãƒ™ãƒ¼ã‚¹ã®Vã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        print("ğŸ” Vã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡ºï¼ˆStep18ãƒ™ãƒ¼ã‚¹ï¼‰")
        return {'v': root_verb.text}
    
    def _build_subslots_step18(self, slot_tokens, doc) -> Dict[str, str]:
        """Step18ãƒ™ãƒ¼ã‚¹ã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ§‹ç¯‰"""
        subslots = {}
        
        for subslot_name, tokens in slot_tokens.items():
            if not tokens:
                continue
            
            if len(tokens) == 1:
                token = tokens[0]
                print(f"  ğŸ” å˜ä¸€ãƒˆãƒ¼ã‚¯ãƒ³å‡¦ç†: {subslot_name} = '{token.text}' (dep={token.dep_})")
                
                # å‰ç½®è©çµ±åˆãƒã‚§ãƒƒã‚¯
                integrated = self._integrate_prepositions_step18(token, doc)
                if integrated:
                    subslots[subslot_name] = integrated
                else:
                    # ã‚¹ãƒ‘ãƒ³æ‹¡å¼µ
                    span = self._expand_span_step18(token, doc)
                    subslots[subslot_name] = span
            else:
                # è¤‡æ•°ãƒˆãƒ¼ã‚¯ãƒ³çµåˆ
                text = ' '.join([t.text for t in sorted(tokens, key=lambda x: x.i)])
                subslots[subslot_name] = text
        
        return subslots
    
    def _integrate_prepositions_step18(self, token, doc) -> Optional[str]:
        """Step18ãƒ™ãƒ¼ã‚¹ã®å‰ç½®è©çµ±åˆå‡¦ç†"""
        # åè© + å‰ç½®è©å¥çµ±åˆï¼ˆSã‚¹ãƒ­ãƒƒãƒˆã®sub-o1ç”¨ï¼‰
        if token.pos_ == 'NOUN' and token.dep_ == 'dobj':
            prep_parts = []
            
            for child in token.children:
                if child.dep_ == 'prep':
                    prep_text = child.text
                    
                    # å‰ç½®è©ã®ç›®çš„èª
                    for prep_child in child.children:
                        if prep_child.dep_ == 'pobj':
                            obj_span = self._expand_span_step18(prep_child, doc)
                            prep_text += f" {obj_span}"
                    
                    prep_parts.append(prep_text)
            
            if prep_parts:
                return f"{token.text} {' '.join(prep_parts)}"
        
        return None
    
    def _expand_span_step18(self, token, doc) -> str:
        """Step18ãƒ™ãƒ¼ã‚¹ã®ã‚¹ãƒ‘ãƒ³æ‹¡å¼µå‡¦ç†"""
        expand_deps = ['det', 'poss', 'compound', 'amod']
        
        start = token.i
        end = token.i
        
        print(f"  ğŸ” ã‚¹ãƒ‘ãƒ³æ‹¡å¼µ: '{token.text}' (dep={token.dep_})")
        
        # åŸºæœ¬çš„ãªå­è¦ç´ ã®æ‹¡å¼µ
        for child in token.children:
            if child.dep_ in expand_deps:
                print(f"    âœ… åŸºæœ¬æ‹¡å¼µå¯¾è±¡: '{child.text}'")
                start = min(start, child.i)
                end = max(end, child.i)
        
        # é–¢ä¿‚ç¯€ã®å ´åˆã¯é–¢ä¿‚ä»£åè©ã®ã¿å«ã‚ã‚‹
        for child in token.children:
            if child.dep_ == 'relcl':
                print(f"    ğŸ” é–¢ä¿‚ç¯€å‡¦ç†: '{child.text}'")
                # é–¢ä¿‚ä»£åè©(who)ã®ã¿æ¢ã—ã¦å«ã‚ã‚‹
                for relcl_child in child.children:
                    if relcl_child.dep_ == 'nsubj' and relcl_child.pos_ == 'PRON':  # who
                        print(f"    âœ… é–¢ä¿‚ä»£åè©å«ã‚ã‚‹: '{relcl_child.text}'")
                        start = min(start, relcl_child.i)
                        end = max(end, relcl_child.i)
                        break
        
        result = ' '.join([doc[i].text for i in range(start, end + 1)])
        print(f"  ğŸ“Œ æ‹¡å¼µçµæœ: '{result}'")
        return result
    
    def _convert_step18_to_slot_result(self, step18_result: Dict[str, Dict[str, str]]) -> Dict[str, SlotResult]:
        """Step18çµæœã‚’SlotResultå½¢å¼ã«å¤‰æ›"""
        converted = {}
        
        for slot_name, subslots in step18_result.items():
            if subslots:  # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆãŒã‚ã‚‹å ´åˆ
                converted[slot_name] = SlotResult(
                    main_content="",  # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆç©ºåŒ–
                    phrase_type="clause",  # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆãŒã‚ã‚‹ãŸã‚ clause
                    is_empty_upper=True,
                    subslots=subslots
                )
            else:  # å˜ç´”ãªã‚¹ãƒ­ãƒƒãƒˆã®å ´åˆ
                # ã“ã®å ´åˆã®å‡¦ç†ï¼ˆå¿…è¦ã«å¿œã˜ã¦å®Ÿè£…ï¼‰
                pass
        
        return converted
    
    def _apply_upper_slot_emptying(self, slot_assignment: Dict[str, SlotResult]) -> Dict[str, Any]:
        """Phase 4: ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆç©ºåŒ–å‡¦ç†"""
        print("ğŸ”„ ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆç©ºåŒ–å‡¦ç†å®Ÿè¡Œ")
        
        final_result = {}
        
        for slot_name, slot_result in slot_assignment.items():
            if slot_result.is_empty_upper or slot_result.phrase_type in ["clause", "phrase"]:
                # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆç©ºåŒ– - ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®ã¿å‡ºåŠ›
                if slot_result.subslots:
                    final_result[slot_name] = slot_result.subslots
                    print(f"   {slot_name}: ä¸Šä½ç©ºåŒ– â†’ {len(slot_result.subslots)}å€‹ã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ")
                else:
                    # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    print(f"   {slot_name}: ä¸Šä½ç©ºåŒ–å¯¾è±¡ã ãŒã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç„¡ã— â†’ ã‚¹ã‚­ãƒƒãƒ—")
            else:
                # é€šå¸¸ã®ã‚¹ãƒ­ãƒƒãƒˆ - ä¸Šä½å†…å®¹ã‚’ä¿æŒ
                if slot_result.main_content:
                    final_result[slot_name] = {
                        slot_name.lower(): slot_result.main_content
                    }
                    print(f"   {slot_name}: ä¸Šä½ä¿æŒ â†’ {slot_result.main_content}")
        
        return final_result

def test_rephrase_compliant_engine():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    engine = RephraseSpecCompliantEngine()
    
    # Step18å‚ç…§ãƒ†ã‚¹ãƒˆ
    test_sentence = "The experienced manager who had recently taken charge completed the project successfully."
    
    print("=" * 80)
    print("ğŸ§ª Rephraseä»•æ§˜æº–æ‹ ã‚¨ãƒ³ã‚¸ãƒ³ ãƒ†ã‚¹ãƒˆ")
    print("=" * 80)
    
    result = engine.decompose_sentence(test_sentence)
    
    print("\nğŸ“‹ åˆ†è§£çµæœ:")
    for slot, content in result.items():
        if isinstance(content, dict):
            print(f"  {slot}:")
            for key, value in content.items():
                print(f"    {key}: {value}")
        else:
            print(f"  {slot}: {content}")
    
    # Step18ã¨ã®æ¯”è¼ƒ
    expected = {
        "S": {
            "sub-s": "the experienced manager who",
            "sub-aux": "had",
            "sub-m2": "recently",
            "sub-v": "taken", 
            "sub-o1": "charge"
        },
        "V": {"v": "completed"}
    }
    
    print("\nğŸ” Step18ã¨ã®æ¯”è¼ƒ:")
    print("æœŸå¾…å€¤:")
    for slot, content in expected.items():
        print(f"  {slot}: {content}")
    
    print("\nçµæœ:")
    for slot, content in result.items():
        print(f"  {slot}: {content}")

if __name__ == "__main__":
    test_rephrase_compliant_engine()
