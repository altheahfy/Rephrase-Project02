#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨ã‚¹ãƒ­ãƒƒãƒˆå…±é€šèª²é¡Œãƒ‡ãƒãƒƒã‚°ãƒ„ãƒ¼ãƒ«
spaCyä¾å­˜æ§‹é€ è§£æã®è©³ç´°åˆ†æ

èª²é¡Œ:
1. "home" æœªé…ç½®å•é¡Œ: "to go home" ã® "home" ãŒèªè­˜ã•ã‚Œãªã„
2. ç–‘å•è©ç¯€æœªå‡¦ç†å•é¡Œ: "what you said" ãŒå‡¦ç†ã•ã‚Œãªã„
"""

import spacy
from typing import List, Dict, Any

class CommonIssueDebugger:
    """å…¨ã‚¹ãƒ­ãƒƒãƒˆå…±é€šèª²é¡Œã®ãƒ‡ãƒãƒƒã‚°ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.nlp = spacy.load("en_core_web_sm")
    
    def analyze_phrase(self, text: str) -> None:
        """ãƒ•ãƒ¬ãƒ¼ã‚ºã®è©³ç´°åˆ†æ"""
        doc = self.nlp(text)
        
        print(f"=== '{text}' ã®è©³ç´°åˆ†æ ===")
        print(f"ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(doc)}")
        print()
        
        # å„ãƒˆãƒ¼ã‚¯ãƒ³ã®è©³ç´°æƒ…å ±
        print("ã€ãƒˆãƒ¼ã‚¯ãƒ³è©³ç´°ã€‘")
        for i, token in enumerate(doc):
            print(f"[{i}] '{token.text}':")
            print(f"    POS: {token.pos_}")
            print(f"    TAG: {token.tag_}")
            print(f"    DEP: {token.dep_}")
            print(f"    HEAD: '{token.head.text}' (index: {token.head.i})")
            print(f"    CHILDREN: {[child.text for child in token.children]}")
            print()
        
        # ä¾å­˜é–¢ä¿‚ã®è¦–è¦šåŒ–
        print("ã€ä¾å­˜é–¢ä¿‚ãƒ„ãƒªãƒ¼ã€‘")
        for token in doc:
            indent = "  " * self._get_depth(token, doc)
            print(f"{indent}{token.text} ({token.dep_}) -> {token.head.text}")
        
        print()
        print("=" * 50)
        print()
    
    def _get_depth(self, token, doc) -> int:
        """ãƒˆãƒ¼ã‚¯ãƒ³ã®éšå±¤æ·±åº¦ã‚’è¨ˆç®—"""
        depth = 0
        current = token
        while current.head != current and current.i != current.head.i:
            depth += 1
            current = current.head
            if depth > 10:  # ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢
                break
        return depth
    
    def find_objects_and_modifiers(self, text: str) -> Dict[str, List[str]]:
        """ç›®çš„èªã¨ä¿®é£¾èªã®æ¤œå‡º"""
        doc = self.nlp(text)
        
        result = {
            'direct_objects': [],
            'indirect_objects': [],
            'prepositional_objects': [],
            'adverbial_modifiers': [],
            'nominal_modifiers': [],
            'unassigned_tokens': []
        }
        
        for token in doc:
            if token.dep_ == "dobj":
                result['direct_objects'].append(token.text)
            elif token.dep_ == "iobj":
                result['indirect_objects'].append(token.text)
            elif token.dep_ == "pobj":
                result['prepositional_objects'].append(token.text)
            elif token.dep_ in ["advmod", "amod"]:
                result['adverbial_modifiers'].append(token.text)
            elif token.dep_ in ["nmod", "compound"]:
                result['nominal_modifiers'].append(token.text)
            elif token.dep_ in ["ROOT", "aux", "mark", "cc", "det"]:
                # æ§‹é€ çš„ãªèªã¯é™¤å¤–
                continue
            else:
                result['unassigned_tokens'].append(f"{token.text}({token.dep_})")
        
        print(f"=== '{text}' ã®èªå½™åˆ†é¡ ===")
        for category, tokens in result.items():
            if tokens:
                print(f"{category}: {tokens}")
        print()
        
        return result

def debug_common_issues():
    """å…±é€šèª²é¡Œã®ãƒ‡ãƒãƒƒã‚°å®Ÿè¡Œ"""
    debugger = CommonIssueDebugger()
    
    # å•é¡Œã‚±ãƒ¼ã‚¹ã®åˆ†æ
    problem_cases = [
        "to go home",           # homeæœªé…ç½®å•é¡Œ
        "To learn English",     # æ­£å¸¸ã‚±ãƒ¼ã‚¹ï¼ˆæ¯”è¼ƒç”¨ï¼‰
        "eager to go home",     # C2ã§ã®åŒæ§˜å•é¡Œ
        "what you said",        # ç–‘å•è©ç¯€æœªå‡¦ç†
        "where he went",        # ç–‘å•è©ç¯€æœªå‡¦ç†
        "reading books"         # æ­£å¸¸ã‚±ãƒ¼ã‚¹ï¼ˆæ¯”è¼ƒç”¨ï¼‰
    ]
    
    print("ğŸ” å…¨ã‚¹ãƒ­ãƒƒãƒˆå…±é€šèª²é¡Œãƒ‡ãƒãƒƒã‚°é–‹å§‹\n")
    
    for case in problem_cases:
        debugger.analyze_phrase(case)
        debugger.find_objects_and_modifiers(case)
        print("\n" + "="*80 + "\n")

if __name__ == "__main__":
    debug_common_issues()
