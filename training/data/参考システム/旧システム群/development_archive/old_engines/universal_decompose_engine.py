import spacy
import pandas as pd
import re
from collections import defaultdict

class UniversalDecomposeEngine:
    """
    Rephraseçµ±ä¸€åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³
    M1, S, M2, C1, O1, O2, C2, M3ã®8ã‚¹ãƒ­ãƒƒãƒˆå…±é€š
    ãƒ•ãƒ«ã‚»ãƒƒãƒˆè§£æã§ç™ºè¦‹ã—ãŸå…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œ
    """
    
    def __init__(self):
        self.nlp = spacy.load("en_core_web_sm")
        
        # ãƒ•ãƒ«ã‚»ãƒƒãƒˆè§£æã‹ã‚‰æŠ½å‡ºã—ãŸå…¨ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.patterns = {
            # Pattern A: sub-c1 + sub-m1 + sub-m2 + sub-s + sub-v (2ä¾‹)
            'subordinate_clause_with_complement': {
                'triggers': ['although', 'even though', 'while'],
                'structure': 'CONJ + SUBJ + VERB + ADV + COMP'
            },
            
            # Pattern B: sub-aux + sub-m1 + sub-o1 + sub-s + sub-v (3ä¾‹) 
            'subordinate_clause_with_object': {
                'triggers': ['because', 'while', 'even though'],
                'structure': 'CONJ + SUBJ + AUX + VERB + OBJ'
            },
            
            # Pattern C: sub-aux + sub-m2 + sub-o1 + sub-s + sub-v (4ä¾‹)
            'relative_clause_with_object': {
                'triggers': ['who', 'that', 'which'],
                'structure': 'REL + SUBJ + AUX + ADV + VERB + OBJ'
            },
            
            # Pattern D: sub-c1 + sub-s + sub-v (2ä¾‹)
            'simple_complement': {
                'triggers': ['seemed', 'appeared', 'looked'],
                'structure': 'SUBJ + VERB + COMP'
            },
            
            # Pattern E: sub-aux + sub-o2 + sub-s + sub-v (1ä¾‹) - æœ€é‡è¦
            'that_clause_with_infinitive': {
                'triggers': ['that'],
                'structure': 'THAT + SUBJ + AUX + VERB + TO_INFINITIVE'
            },
            
            # Pattern F: sub-o1 + sub-s + sub-v (1ä¾‹)
            'that_clause_simple': {
                'triggers': ['that'],
                'structure': 'THAT + SUBJ + VERB + TO_INFINITIVE'
            }
        }
    
    def decompose(self, phrase):
        """
        çµ±ä¸€åˆ†è§£ãƒ¡ã‚½ãƒƒãƒ‰
        ã©ã®ã‚¹ãƒ­ãƒƒãƒˆã‹ã‚‰ã§ã‚‚å‘¼ã³å‡ºã—å¯èƒ½
        """
        if not phrase or phrase.strip() == "":
            return self._empty_subslots()
            
        phrase = phrase.strip()
        doc = self.nlp(phrase)
        
        # å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°è©¦è¡Œ
        result = self._try_all_patterns(phrase, doc)
        
        # 100%å˜èªä¿å…¨ãƒã‚§ãƒƒã‚¯
        if not self._verify_word_coverage(phrase, result):
            print(f"âš ï¸  Word coverage incomplete for: '{phrase}'")
            result = self._fallback_decompose(phrase, doc)
        
        return result
    
    def _try_all_patterns(self, phrase, doc):
        """å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é †æ¬¡è©¦è¡Œ"""
        
        # Pattern E: thatç¯€+ä¸å®šè©ï¼ˆæœ€é‡è¦ï¼‰
        if phrase.startswith('that '):
            return self._decompose_that_clause_infinitive(phrase, doc)
        
        # Pattern A: å¾“å±ç¯€+è£œèª
        for trigger in ['although', 'even though', 'while']:
            if phrase.startswith(trigger):
                return self._decompose_subordinate_complement(phrase, doc, trigger)
        
        # Pattern B: å¾“å±ç¯€+ç›®çš„èª
        for trigger in ['because', 'while', 'even though']:
            if phrase.startswith(trigger):
                return self._decompose_subordinate_object(phrase, doc, trigger)
        
        # Pattern C: é–¢ä¿‚ç¯€+ç›®çš„èª
        if ' who ' in phrase or ' that ' in phrase or ' which ' in phrase:
            return self._decompose_relative_clause(phrase, doc)
        
        # Pattern D: å˜ç´”è£œèª
        for verb in ['seemed', 'appeared', 'looked']:
            if f' {verb} ' in phrase:
                return self._decompose_simple_complement(phrase, doc, verb)
        
        # ãã®ä»–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³è©¦è¡Œ
        return self._decompose_general_structure(phrase, doc)
    
    def _decompose_that_clause_infinitive(self, phrase, doc):
        """Pattern E: thatç¯€+ä¸å®šè©æ§‹é€  - spaCyä¾å­˜è§£æãƒ™ãƒ¼ã‚¹"""
        result = self._empty_subslots()
        
        # spaCyè§£æã«ã‚ˆã‚‹æ–‡æ³•æ§‹é€ èªè­˜
        that_token = None
        subject_tokens = []
        aux_tokens = []
        verb_tokens = []
        infinitive_tokens = []
        
        for token in doc:
            # "that"ã®æ¤œå‡º
            if token.text.lower() == "that" and token.dep_ in ["mark", "nsubj"]:
                that_token = token
            
            # ä¸»èªã®æ¤œå‡ºï¼ˆthatä»¥é™ã®åè©å¥ï¼‰
            elif token.dep_ in ["nsubj", "nsubjpass"] and token.pos_ in ["PRON", "NOUN"]:
                subject_tokens.append(token)
            
            # åŠ©å‹•è©ã®æ¤œå‡º
            elif token.pos_ == "AUX" or (token.dep_ == "aux" and token.pos_ == "VERB"):
                aux_tokens.append(token)
            
            # ãƒ¡ã‚¤ãƒ³å‹•è©ã®æ¤œå‡º
            elif token.dep_ == "ROOT" or (token.pos_ == "VERB" and token.dep_ in ["xcomp", "ccomp"]):
                verb_tokens.append(token)
            
            # ä¸å®šè©å¥ã®æ¤œå‡ºï¼ˆto + å‹•è©ï¼‰
            elif token.text.lower() == "to" or (token.dep_ in ["xcomp", "dobj"] and "to" in phrase):
                # ä¸å®šè©å¥ã®é–‹å§‹ä½ç½®ã‹ã‚‰çµ‚ã‚ã‚Šã¾ã§å–å¾—
                infinitive_start = None
                for i, t in enumerate(doc):
                    if t.text.lower() == "to" and i < len(doc) - 1:
                        infinitive_start = i
                        break
                
                if infinitive_start:
                    infinitive_tokens = doc[infinitive_start:]
                break
        
        # Rephraseåˆ†è§£ãƒ«ãƒ¼ãƒ«é©ç”¨
        if that_token and subject_tokens:
            # sub-s: "that" + ä¸»èª
            if subject_tokens:
                result['sub-s'] = f"that {subject_tokens[0].text}"
        
        if aux_tokens:
            # sub-aux: åŠ©å‹•è©
            result['sub-aux'] = aux_tokens[0].text
        
        if verb_tokens:
            # sub-v: ãƒ¡ã‚¤ãƒ³å‹•è©ï¼ˆè¤‡åˆå‹•è©å«ã‚€ï¼‰
            verb_phrase = []
            for v in verb_tokens:
                verb_phrase.append(v.text)
                # ä»˜éšã™ã‚‹åˆ†è©ç­‰ã‚‚å«ã‚ã‚‹
                for child in v.children:
                    if child.dep_ in ["aux", "auxpass", "neg"] or child.pos_ in ["VERB", "ADP"]:
                        verb_phrase.append(child.text)
            result['sub-v'] = " ".join(verb_phrase[:2])  # æœ€å¤§2èªã¾ã§
        
        if infinitive_tokens:
            # sub-o2: ä¸å®šè©å¥
            result['sub-o2'] = infinitive_tokens.text
        
        return result
    
    def _decompose_subordinate_complement(self, phrase, doc, trigger):
        """Pattern A: å¾“å±ç¯€+è£œèªæ§‹é€  - spaCyä¾å­˜è§£æãƒ™ãƒ¼ã‚¹"""
        result = self._empty_subslots()
        
        # spaCyè§£æã«ã‚ˆã‚‹æ§‹é€ èªè­˜
        conj_token = None
        subj_token = None
        verb_token = None
        adv_tokens = []
        comp_tokens = []
        
        for token in doc:
            # å¾“å±æ¥ç¶šè©
            if token.text.lower() == trigger.lower():
                conj_token = token
                result['sub-m1'] = token.text
            
            # ä¸»èª
            elif token.dep_ in ["nsubj", "nsubjpass"]:
                subj_token = token
                result['sub-s'] = token.text
            
            # å‹•è©ï¼ˆROOT or copï¼‰
            elif token.dep_ in ["ROOT", "cop"] and token.pos_ == "VERB":
                verb_token = token
                result['sub-v'] = token.text
            
            # å‰¯è©ï¼ˆä¿®é£¾èªï¼‰
            elif token.pos_ == "ADV" and token.dep_ in ["advmod"]:
                adv_tokens.append(token)
            
            # è£œèª
            elif token.dep_ in ["attr", "acomp", "pcomp"] or (token.pos_ == "ADJ" and token.dep_ != "amod"):
                comp_tokens.append(token)
        
        # å‰¯è©ã‚’sub-m2ã«é…ç½®
        if adv_tokens:
            result['sub-m2'] = adv_tokens[0].text
        
        # è£œèªã‚’sub-c1ã«é…ç½®
        if comp_tokens:
            result['sub-c1'] = comp_tokens[0].text
        
        return result
    
    def _decompose_subordinate_object(self, phrase, doc, trigger):
        """Pattern B: å¾“å±ç¯€+ç›®çš„èªæ§‹é€  - spaCyä¾å­˜è§£æãƒ™ãƒ¼ã‚¹"""
        result = self._empty_subslots()
        
        # spaCyè§£æã«ã‚ˆã‚‹æ§‹é€ èªè­˜
        for token in doc:
            # å¾“å±æ¥ç¶šè©
            if token.text.lower() == trigger.lower():
                result['sub-m1'] = token.text
            
            # ä¸»èªï¼ˆå¥å…¨ä½“ï¼‰
            elif token.dep_ in ["nsubj", "nsubjpass"]:
                subj_span = self._get_extended_span(token, doc)
                result['sub-s'] = subj_span
            
            # åŠ©å‹•è©
            elif token.pos_ == "AUX" or token.dep_ == "aux":
                result['sub-aux'] = token.text
            
            # ãƒ¡ã‚¤ãƒ³å‹•è©
            elif token.dep_ == "ROOT" and token.pos_ == "VERB":
                result['sub-v'] = token.text
            
            # ç›®çš„èª
            elif token.dep_ in ["dobj", "pobj"]:
                obj_span = self._get_extended_span(token, doc)
                result['sub-o1'] = obj_span
        
        return result
    
    def _decompose_relative_clause(self, phrase, doc):
        """Pattern C/D: é–¢ä¿‚ç¯€æ§‹é€  - spaCyä¾å­˜è§£æãƒ™ãƒ¼ã‚¹"""
        result = self._empty_subslots()
        
        # é–¢ä¿‚ä»£åè©ã®æ¤œå‡ºã¨å‡¦ç†
        rel_pronoun = None
        main_subj = None
        rel_clause_tokens = []
        
        for token in doc:
            # é–¢ä¿‚ä»£åè©ã®æ¤œå‡º
            if token.text.lower() in ['who', 'that', 'which'] and token.dep_ in ['nsubj', 'dobj', 'nsubjpass']:
                rel_pronoun = token
                
                # ãƒ¡ã‚¤ãƒ³ä¸»èªéƒ¨åˆ†ï¼ˆé–¢ä¿‚ä»£åè©ã‚ˆã‚Šå‰ï¼‰
                main_part = []
                for t in doc:
                    if t.i < token.i:
                        main_part.append(t.text)
                    elif t.i == token.i:
                        main_part.append(t.text)
                        break
                
                result['sub-s'] = " ".join(main_part)
                
                # é–¢ä¿‚ç¯€éƒ¨åˆ†ã®å‡¦ç†
                rel_clause_start = token.i + 1
                rel_clause_tokens = doc[rel_clause_start:]
                break
        
        # é–¢ä¿‚ç¯€å†…ã®æ–‡æ³•è¦ç´ å‡¦ç†
        if rel_clause_tokens:
            for token in rel_clause_tokens:
                # åŠ©å‹•è©
                if token.pos_ == "AUX" or token.dep_ == "aux":
                    result['sub-aux'] = token.text
                
                # å‰¯è©
                elif token.pos_ == "ADV" and token.dep_ == "advmod":
                    result['sub-m2'] = token.text
                
                # å‹•è©
                elif token.pos_ == "VERB" and token.dep_ in ["ROOT", "relcl"]:
                    result['sub-v'] = token.text
                
                # è£œèª
                elif token.dep_ in ["attr", "acomp"] or token.pos_ == "ADJ":
                    result['sub-c1'] = token.text
                
                # ç›®çš„èª
                elif token.dep_ in ["dobj", "pobj"]:
                    obj_span = self._get_extended_span(token, doc)
                    result['sub-o1'] = obj_span
        
        return result
    
    def _decompose_complex_relative(self, phrase, doc):
        """è¤‡é›‘ãªé–¢ä¿‚ç¯€: the manager who had recently taken charge of the project"""
        result = self._empty_subslots()
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³: SUBJ who AUX ADV VERB OBJ
        parts = phrase.split(' who ')
        if len(parts) == 2:
            result['sub-s'] = f"{parts[0]} who"
            
            remainder = parts[1].split()
            if len(remainder) >= 4:
                result['sub-aux'] = remainder[0]  # had
                result['sub-m2'] = remainder[1]   # recently  
                result['sub-v'] = remainder[2]    # taken
                result['sub-o1'] = " ".join(remainder[3:])  # charge of the project
        
        return result
    
    def _decompose_simple_complement(self, phrase, doc, verb):
        """Pattern D: simple complement structure"""
        result = self._empty_subslots()
        
        parts = phrase.split(f' {verb} ')
        if len(parts) == 2:
            result['sub-s'] = parts[0]
            result['sub-v'] = verb
            result['sub-c1'] = parts[1]
        
        return result
    
    def _decompose_general_structure(self, phrase, doc):
        """ä¸€èˆ¬çš„ãªæ§‹é€ åˆ†æãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        result = self._empty_subslots()
        
        # spaCyã«ã‚ˆã‚‹æ–‡æ³•è§£æ
        for token in doc:
            if token.dep_ == "nsubj" or token.dep_ == "nsubjpass":
                # ä¸»èªã®å–å¾—ï¼ˆä¿®é£¾èªå«ã‚€ï¼‰
                subj_span = self._get_extended_span(token, doc)
                result['sub-s'] = subj_span
            elif token.pos_ == "AUX":
                result['sub-aux'] = token.text
            elif token.dep_ == "ROOT" and token.pos_ == "VERB":
                result['sub-v'] = token.text
            elif token.dep_ == "dobj":
                obj_span = self._get_extended_span(token, doc)
                result['sub-o1'] = obj_span
            elif token.dep_ == "attr" or token.dep_ == "acomp":
                result['sub-c1'] = token.text
        
        # å˜ä¸€è¦ç´ ã®å ´åˆ
        if not any(result.values()):
            result['sub-s'] = phrase  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        return result
    
    def _get_extended_span(self, token, doc):
        """ãƒˆãƒ¼ã‚¯ãƒ³ã®ä¿®é£¾èªã‚’å«ã‚€æ‹¡å¼µã‚¹ãƒ‘ãƒ³ã‚’å–å¾—"""
        left = token.i
        right = token.i + 1
        
        # å·¦å´ã®ä¿®é£¾èª
        for child in token.children:
            if child.i < token.i:
                left = min(left, child.i)
        
        # å³å´ã®ä¿®é£¾èª
        for child in token.children:
            if child.i > token.i:
                right = max(right, child.i + 1)
        
        return doc[left:right].text
    
    def _fallback_decompose(self, phrase, doc):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå˜ç´”åˆ†è§£"""
        result = self._empty_subslots()
        result['sub-s'] = phrase  # æœ€ä½é™ã®ä¿å…¨
        return result
    
    def _verify_word_coverage(self, original, subslots):
        """100%å˜èªä¿å…¨ãƒã‚§ãƒƒã‚¯"""
        original_words = set(original.lower().split())
        covered_words = set()
        
        for subslot_value in subslots.values():
            if subslot_value:
                covered_words.update(subslot_value.lower().split())
        
        missing_words = original_words - covered_words
        return len(missing_words) == 0
    
    def _empty_subslots(self):
        """ç©ºã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ """
        return {
            'sub-m1': '',
            'sub-s': '',  
            'sub-aux': '',
            'sub-m2': '',
            'sub-v': '',
            'sub-c1': '',
            'sub-o1': '',
            'sub-o2': '',
            'sub-c2': '',
            'sub-m3': ''
        }


# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
if __name__ == "__main__":
    engine = UniversalDecomposeEngine()
    
    print('ğŸ¯ çµ±ä¸€åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³ï¼šãƒ•ãƒ«ã‚»ãƒƒãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œãƒ†ã‚¹ãƒˆ')
    print('=' * 80)
    
    # ãƒ•ãƒ«ã‚»ãƒƒãƒˆã®é‡è¦ä¾‹æ–‡ã§ãƒ†ã‚¹ãƒˆ
    test_cases = [
        "although it was emotionally hard",
        "that he had been trying to avoid Tom", 
        "the woman who seemed indecisive",
        "because he was afraid of hurting her feelings",
        "the manager who had recently taken charge of the project"
    ]
    
    for phrase in test_cases:
        print(f'\nğŸ“‹ å…¥åŠ›: "{phrase}"')
        result = engine.decompose(phrase)
        
        print('åˆ†è§£çµæœ:')
        for key, value in result.items():
            if value:
                print(f'  {key}: "{value}"')
        
        # å˜èªä¿å…¨ãƒã‚§ãƒƒã‚¯
        original_words = phrase.split()
        covered_words = []
        for value in result.values():
            if value:
                covered_words.extend(value.split())
        
        print(f'å˜èªä¿å…¨: {len(original_words)}èª -> {len(covered_words)}èª')
        if len(original_words) == len(covered_words):
            print('âœ… 100%ä¿å…¨é”æˆ')
        else:
            print('âŒ å˜èªæ¬ è½ã‚ã‚Š')
