#!/usr/bin/env python3
"""
spaCyè¾æ›¸ + äººé–“æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ 
ç¾åœ¨ã®ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ã®ç§»æ¤ç‰ˆ
"""

import spacy
import logging
from typing import Dict, List, Any, Optional, Tuple

class SpacyHumanGrammarMapper:
    """
    spaCyè¾æ›¸ + äººé–“æ–‡æ³•èªè­˜ã«ã‚ˆã‚‹è‹±èª5æ–‡å‹ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ã‚·ã‚¹ãƒ†ãƒ 
    """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        # spaCyè¾æ›¸ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        try:
            self.nlp = spacy.load("en_core_web_sm")
            print("âœ… spaCyè¾æ›¸ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except OSError:
            print("âŒ spaCyè‹±èªãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            raise
        
        # ãƒ­ã‚°è¨­å®š
        self.logger = logging.getLogger(__name__)
        
        # äººé–“æ–‡æ³•èªè­˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.confidence_threshold = 0.8
        
    def analyze_sentence(self, sentence: str) -> Dict[str, Any]:
        """
        æ–‡ç« ã‚’è§£æã—ã¦Rephraseã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ ã‚’ç”Ÿæˆ
        
        Args:
            sentence (str): è§£æå¯¾è±¡ã®æ–‡ç« 
            
        Returns:
            Dict[str, Any]: Rephraseã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ 
        """
        try:
            # 1. spaCyèªå½™è§£æï¼ˆè¾æ›¸æ©Ÿèƒ½ï¼‰
            lexical_info = self._extract_lexical_knowledge(sentence)
            
            # 2. äººé–“æ–‡æ³•èªè­˜ï¼ˆæ§‹é€ èªè­˜ï¼‰
            grammar_pattern = self._human_grammar_recognition(lexical_info)
            
            # 3. Rephraseã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ
            rephrase_slots = self._generate_rephrase_slots(lexical_info, grammar_pattern)
            
            return rephrase_slots
            
        except Exception as e:
            self.logger.error(f"æ–‡ç« è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_error_result(sentence, str(e))
    
    def _extract_lexical_knowledge(self, sentence: str) -> Dict[str, Any]:
        """
        spaCyè¾æ›¸ã‹ã‚‰èªå½™çŸ¥è­˜ã‚’æŠ½å‡º
        """
        doc = self.nlp(sentence)
        
        lexical_info = {
            'tokens': [],
            'sentence': sentence,
            'spacy_doc': doc
        }
        
        for token in doc:
            token_info = {
                'text': token.text,
                'pos': token.pos_,           # ä¸»è¦å“è© (NOUN, VERB, etc.)
                'tag': token.tag_,           # è©³ç´°å“è© (NNS, VBZ, etc.)
                'lemma': token.lemma_,       # åŸå½¢
                'morph': str(token.morph),   # å½¢æ…‹æƒ…å ±
                'is_stop': token.is_stop,    # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰
                'is_alpha': token.is_alpha,  # ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆ
                'index': token.i             # ä½ç½®
            }
            lexical_info['tokens'].append(token_info)
        
        self.logger.info(f"èªå½™è§£æå®Œäº†: {len(lexical_info['tokens'])}èªå½™")
        return lexical_info
    
    def _human_grammar_recognition(self, lexical_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        äººé–“æ–‡æ³•èªè­˜ã«ã‚ˆã‚‹æ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        å„ªå…ˆé †ä½: è¤‡é›‘ãªæ–‡å‹ï¼ˆSVOO, SVOCï¼‰â†’ å˜ç´”ãªæ–‡å‹ï¼ˆSVO, SVC, SVï¼‰
        """
        tokens = lexical_info['tokens']
        
        # å„ªå…ˆé †ä½é †ã«æ¤œå‡ºï¼ˆè¤‡é›‘â†’å˜ç´”ï¼‰
        
        # ç¬¬5æ–‡å‹ï¼ˆSVOCï¼‰æ¤œå‡º - æœ€å„ªå…ˆ
        svoc_result = self._detect_svoc_pattern_human(tokens)
        if svoc_result['detected']:
            return svoc_result
        
        # ç¬¬4æ–‡å‹ï¼ˆSVOOï¼‰æ¤œå‡º
        svoo_result = self._detect_svoo_pattern_human(tokens)
        if svoo_result['detected']:
            return svoo_result
        
        # ç¬¬3æ–‡å‹ï¼ˆSVOï¼‰æ¤œå‡º
        svo_result = self._detect_svo_pattern_human(tokens)
        if svo_result['detected']:
            return svo_result
        
        # ç¬¬2æ–‡å‹ï¼ˆSVCï¼‰æ¤œå‡º
        svc_result = self._detect_svc_pattern_human(tokens)
        if svc_result['detected']:
            return svc_result
        
        # ç¬¬1æ–‡å‹ï¼ˆSVï¼‰æ¤œå‡º - æœ€å¾Œ
        sv_result = self._detect_sv_pattern_human(tokens)
        if sv_result['detected']:
            return sv_result
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆçµæœ
        return {
            'detected': False,
            'pattern': 'UNKNOWN',
            'confidence': 0.0,
            'elements': {},
            'error': 'æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸ'
        }
    
    def _detect_sv_pattern_human(self, tokens: List[Dict]) -> Dict[str, Any]:
        """
        ç¬¬1æ–‡å‹ï¼ˆSVï¼‰äººé–“æ–‡æ³•èªè­˜
        
        äººé–“ã®èªè­˜: [ä¸»èª] + [è‡ªå‹•è©] = ç¬¬1æ–‡å‹
        """
        if len(tokens) < 2:
            return {'detected': False, 'pattern': 'SV', 'confidence': 0.0}
        
        # åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³: 2èªï¼ˆä¸»èª + å‹•è©ï¼‰
        if len(tokens) == 2:
            token1, token2 = tokens[0], tokens[1]
            
            # ä¸»èªå€™è£œåˆ¤å®šï¼ˆäººé–“åŸºæº–ï¼‰
            is_subject = self._is_subject_human(token1)
            # è‡ªå‹•è©å€™è£œåˆ¤å®šï¼ˆäººé–“åŸºæº–ï¼‰
            is_intransitive_verb = self._is_intransitive_verb_human(token2)
            
            if is_subject and is_intransitive_verb:
                return {
                    'detected': True,
                    'pattern': 'SV',
                    'confidence': 0.95,
                    'elements': {
                        'subject': token1,
                        'verb': token2
                    },
                    'slots': ['S', 'V'],
                    'slot_phrases': [token1['text'], token2['text']]
                }
        
        # 3èªãƒ‘ã‚¿ãƒ¼ãƒ³: å† è© + åè© + å‹•è©
        elif len(tokens) == 3:
            token1, token2, token3 = tokens[0], tokens[1], tokens[2]
            
            # The dog runs ãƒ‘ã‚¿ãƒ¼ãƒ³
            if (self._is_determiner_human(token1) and 
                self._is_noun_human(token2) and 
                self._is_intransitive_verb_human(token3)):
                
                return {
                    'detected': True,
                    'pattern': 'SV',
                    'confidence': 0.90,
                    'elements': {
                        'subject': f"{token1['text']} {token2['text']}",
                        'verb': token3
                    },
                    'slots': ['S', 'V'],
                    'slot_phrases': [f"{token1['text']} {token2['text']}", token3['text']]
                }
        
        return {'detected': False, 'pattern': 'SV', 'confidence': 0.0}
    
    def _detect_svc_pattern_human(self, tokens: List[Dict]) -> Dict[str, Any]:
        """
        ç¬¬2æ–‡å‹ï¼ˆSVCï¼‰äººé–“æ–‡æ³•èªè­˜
        
        äººé–“ã®èªè­˜: [ä¸»èª] + [é€£çµå‹•è©] + [è£œèª] = ç¬¬2æ–‡å‹
        """
        # å¥èª­ç‚¹ã‚’é™¤å¤–ã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        content_tokens = [t for t in tokens if t['pos'] != 'PUNCT']
        
        if len(content_tokens) < 3:
            return {'detected': False, 'pattern': 'SVC', 'confidence': 0.0}
        
        # 3èªãƒ‘ã‚¿ãƒ¼ãƒ³: S + é€£çµå‹•è© + C
        if len(content_tokens) == 3:
            subject, verb, complement = content_tokens[0], content_tokens[1], content_tokens[2]
            
            if (self._is_subject_human(subject) and 
                self._is_linking_verb_human(verb) and 
                self._is_complement_human(complement)):
                
                return {
                    'detected': True,
                    'pattern': 'SVC',
                    'confidence': 0.95,
                    'elements': {
                        'subject': subject,
                        'verb': verb,
                        'complement': complement
                    },
                    'slots': ['S', 'V', 'C'],
                    'slot_phrases': [subject['text'], verb['text'], complement['text']]
                }
        
        # 4èªãƒ‘ã‚¿ãƒ¼ãƒ³: The + åè© + é€£çµå‹•è© + è£œèª
        elif len(content_tokens) == 4:
            det, noun, verb, complement = content_tokens[0], content_tokens[1], content_tokens[2], content_tokens[3]
            
            if (self._is_determiner_human(det) and 
                self._is_noun_human(noun) and 
                self._is_linking_verb_human(verb) and 
                self._is_complement_human(complement)):
                
                subject_text = f"{det['text']} {noun['text']}"
                
                return {
                    'detected': True,
                    'pattern': 'SVC',
                    'confidence': 0.90,
                    'elements': {
                        'subject': subject_text,
                        'verb': verb,
                        'complement': complement
                    },
                    'slots': ['S', 'V', 'C'],
                    'slot_phrases': [subject_text, verb['text'], complement['text']]
                }
        
        return {'detected': False, 'pattern': 'SVC', 'confidence': 0.0}
    
    def _detect_svo_pattern_human(self, tokens: List[Dict]) -> Dict[str, Any]:
        """ç¬¬3æ–‡å‹ï¼ˆSVOï¼‰äººé–“æ–‡æ³•èªè­˜ - ä¸»èª + ä»–å‹•è© + ç›®çš„èª"""
        if len(tokens) < 3:
            return {'detected': False, 'pattern': 'SVO', 'confidence': 0.0}
        
        # åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³: S + V + O
        for i in range(len(tokens) - 2):
            if (self._is_subject_human(tokens[i]) and 
                self._is_transitive_verb_human(tokens[i + 1]) and 
                self._is_object_human(tokens[i + 2])):
                
                return {
                    'detected': True,
                    'pattern': 'SVO',
                    'confidence': 0.90,
                    'elements': {
                        'subject': tokens[i],
                        'verb': tokens[i + 1],
                        'object': tokens[i + 2]
                    },
                    'slots': ['S', 'V', 'O'],
                    'slot_phrases': [tokens[i]['text'], tokens[i + 1]['text'], tokens[i + 2]['text']]
                }
        
        return {'detected': False, 'pattern': 'SVO', 'confidence': 0.0}
    
    def _detect_svoo_pattern_human(self, tokens: List[Dict]) -> Dict[str, Any]:
        """ç¬¬4æ–‡å‹ï¼ˆSVOOï¼‰äººé–“æ–‡æ³•èªè­˜ - ä¸»èª + å‹•è© + é–“æ¥ç›®çš„èª + ç›´æ¥ç›®çš„èª"""
        if len(tokens) < 4:
            return {'detected': False, 'pattern': 'SVOO', 'confidence': 0.0}
        
        # åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³: S + V + O1 + O2
        for i in range(len(tokens) - 3):
            if (self._is_subject_human(tokens[i]) and 
                self._is_ditransitive_verb_human(tokens[i + 1]) and 
                self._is_object_human(tokens[i + 2]) and 
                self._is_object_human(tokens[i + 3])):
                
                return {
                    'detected': True,
                    'pattern': 'SVOO',
                    'confidence': 0.85,
                    'elements': {
                        'subject': tokens[i],
                        'verb': tokens[i + 1],
                        'indirect_object': tokens[i + 2],
                        'direct_object': tokens[i + 3]
                    },
                    'slots': ['S', 'V', 'O', 'O'],
                    'slot_phrases': [tokens[i]['text'], tokens[i + 1]['text'], tokens[i + 2]['text'], tokens[i + 3]['text']]
                }
        
        return {'detected': False, 'pattern': 'SVOO', 'confidence': 0.0}
    
    def _detect_svoc_pattern_human(self, tokens: List[Dict]) -> Dict[str, Any]:
        """
        ç¬¬5æ–‡å‹ï¼ˆSVOCï¼‰äººé–“æ–‡æ³•èªè­˜
        
        äººé–“ã®èªè­˜: [ä¸»èª] + [ä½¿å½¹å‹•è©] + [ç›®çš„èª] + [è£œèª] = ç¬¬5æ–‡å‹
        """
        if len(tokens) < 4:
            return {'detected': False, 'pattern': 'SVOC', 'confidence': 0.0}
        
        # 4èªãƒ‘ã‚¿ãƒ¼ãƒ³: S + V + O + C
        if len(tokens) == 4:
            subject, verb, obj, complement = tokens[0], tokens[1], tokens[2], tokens[3]
            
            if (self._is_subject_human(subject) and 
                self._is_factitive_verb_human(verb) and 
                self._is_object_human(obj) and 
                self._is_complement_human(complement)):
                
                return {
                    'detected': True,
                    'pattern': 'SVOC',
                    'confidence': 0.90,
                    'elements': {
                        'subject': subject,
                        'verb': verb,
                        'object': obj,
                        'complement': complement
                    },
                    'slots': ['S', 'V', 'O', 'C'],
                    'slot_phrases': [subject['text'], verb['text'], obj['text'], complement['text']]
                }
        
        # 5èªãƒ‘ã‚¿ãƒ¼ãƒ³: S + V + the + O + C
        elif len(tokens) == 5:
            subject, verb, det, obj, complement = tokens[0], tokens[1], tokens[2], tokens[3], tokens[4]
            
            if (self._is_subject_human(subject) and 
                self._is_factitive_verb_human(verb) and 
                self._is_determiner_human(det) and 
                self._is_noun_human(obj) and 
                self._is_complement_human(complement)):
                
                object_text = f"{det['text']} {obj['text']}"
                
                return {
                    'detected': True,
                    'pattern': 'SVOC',
                    'confidence': 0.85,
                    'elements': {
                        'subject': subject,
                        'verb': verb,
                        'object': object_text,
                        'complement': complement
                    },
                    'slots': ['S', 'V', 'O', 'C'],
                    'slot_phrases': [subject['text'], verb['text'], object_text, complement['text']]
                }
        
        return {'detected': False, 'pattern': 'SVOC', 'confidence': 0.0}
    
    # äººé–“åŸºæº–åˆ¤å®šé–¢æ•°ç¾¤
    def _is_subject_human(self, token: Dict) -> bool:
        """äººé–“åŸºæº–ã§ã®ä¸»èªåˆ¤å®š"""
        return token['pos'] in ['NOUN', 'PRON', 'PROPN']
    
    def _is_noun_human(self, token: Dict) -> bool:
        """äººé–“åŸºæº–ã§ã®åè©åˆ¤å®š"""
        return token['pos'] in ['NOUN', 'PROPN']
    
    def _is_determiner_human(self, token: Dict) -> bool:
        """äººé–“åŸºæº–ã§ã®å† è©ãƒ»é™å®šè©åˆ¤å®š"""
        return token['pos'] == 'DET'
    
    def _is_intransitive_verb_human(self, token: Dict) -> bool:
        """äººé–“åŸºæº–ã§ã®è‡ªå‹•è©åˆ¤å®š"""
        return token['pos'] == 'VERB' and token['tag'] in ['VBP', 'VBZ', 'VB']
    
    def _is_be_verb_human(self, token: Dict) -> bool:
        """äººé–“åŸºæº–ã§ã®beå‹•è©åˆ¤å®š"""
        return (token['pos'] in ['AUX', 'VERB'] and 
                token['lemma'].lower() in ['be', 'am', 'is', 'are', 'was', 'were'])
    
    def _is_linking_verb_human(self, token: Dict) -> bool:
        """äººé–“åŸºæº–ã§ã®é€£çµå‹•è©åˆ¤å®šï¼ˆbeå‹•è© + ãã®ä»–é€£çµå‹•è©ï¼‰"""
        linking_verbs = {
            'be', 'am', 'is', 'are', 'was', 'were',  # beå‹•è©
            'become', 'became', 'get', 'seem', 'appear', 'look', 'sound', 
            'feel', 'taste', 'smell', 'remain', 'stay', 'turn', 'grow'
        }
        return (token['pos'] in ['AUX', 'VERB'] and 
                token['lemma'].lower() in linking_verbs)
    
    def _is_transitive_verb_human(self, token: Dict) -> bool:
        """äººé–“åŸºæº–ã§ã®ä»–å‹•è©åˆ¤å®š"""
        return token['pos'] == 'VERB' and token['tag'] in ['VBP', 'VBZ', 'VB', 'VBD']
    
    def _is_ditransitive_verb_human(self, token: Dict) -> bool:
        """äººé–“åŸºæº–ã§ã®æˆä¸å‹•è©åˆ¤å®šï¼ˆgive, tell, showç­‰ï¼‰"""
        ditransitive_verbs = {
            'give', 'tell', 'show', 'send', 'teach', 'offer', 'bring',
            'buy', 'bought', 'get', 'pass', 'hand', 'lend', 'sell', 'pay'
        }
        return (token['pos'] == 'VERB' and 
                token['lemma'].lower() in ditransitive_verbs)
    
    def _is_factitive_verb_human(self, token: Dict) -> bool:
        """äººé–“åŸºæº–ã§ã®ä½¿å½¹å‹•è©åˆ¤å®šï¼ˆmake, call, considerç­‰ï¼‰"""
        factitive_verbs = {
            'make', 'call', 'consider', 'find', 'keep', 'leave',
            'paint', 'color', 'dye', 'name', 'elect', 'choose'
        }
        return (token['pos'] == 'VERB' and 
                token['lemma'].lower() in factitive_verbs)
    
    def _is_object_human(self, token: Dict) -> bool:
        """äººé–“åŸºæº–ã§ã®ç›®çš„èªåˆ¤å®š"""
        return token['pos'] in ['NOUN', 'PRON', 'PROPN']
    
    def _is_complement_human(self, token: Dict) -> bool:
        """äººé–“åŸºæº–ã§ã®è£œèªåˆ¤å®š"""
        # åŸºæœ¬çš„ãªè£œèªå“è©
        if token['pos'] in ['ADJ', 'NOUN', 'PROPN']:
            return True
        
        # è‰²åã¯åè©ã§ã‚‚è£œèªã¨ã—ã¦æ©Ÿèƒ½
        color_words = {
            'red', 'blue', 'green', 'yellow', 'black', 'white', 
            'brown', 'pink', 'purple', 'orange', 'gray', 'grey'
        }
        if token['lemma'].lower() in color_words:
            return True
        
        return False
    
    def _generate_rephrase_slots(self, lexical_info: Dict, grammar_pattern: Dict) -> Dict[str, Any]:
        """
        Rephraseã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ ç”Ÿæˆï¼ˆç¾åœ¨ã‚·ã‚¹ãƒ†ãƒ äº’æ›ï¼‰
        """
        if not grammar_pattern['detected']:
            return self._create_error_result(lexical_info['sentence'], 
                                           grammar_pattern.get('error', 'æ–‡å‹èªè­˜å¤±æ•—'))
        
        # Rephraseå®Œå…¨äº’æ›ã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ 
        slots = grammar_pattern['slots']
        slot_phrases = grammar_pattern['slot_phrases']
        
        result = {
            'Slot': slots,
            'SlotPhrase': slot_phrases,
            'Slot_display_order': list(range(1, len(slots) + 1)),
            'display_order': list(range(1, len(slots) + 1)),
            'PhraseType': self._determine_phrase_types(slots),
            'SubslotID': list(range(len(slots))),
            
            # è§£ææƒ…å ±
            'pattern_detected': grammar_pattern['pattern'],
            'confidence': grammar_pattern['confidence'],
            'analysis_method': 'spacy_human_grammar',
            'lexical_tokens': len(lexical_info['tokens'])
        }
        
        return result
    
    def _determine_phrase_types(self, slots: List[str]) -> List[str]:
        """ã‚¹ãƒ­ãƒƒãƒˆã«å¯¾å¿œã™ã‚‹å¥å‹ã‚’æ±ºå®š"""
        phrase_types = []
        for slot in slots:
            if slot == 'S':
                phrase_types.append('åè©å¥')
            elif slot == 'V':
                phrase_types.append('å‹•è©å¥')
            elif slot == 'O':
                phrase_types.append('åè©å¥')
            elif slot == 'C':
                phrase_types.append('è£œèªå¥')
            else:
                phrase_types.append('æœªåˆ†é¡')
        return phrase_types
    
    def _create_error_result(self, sentence: str, error_msg: str) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼çµæœç”Ÿæˆ"""
        return {
            'Slot': [],
            'SlotPhrase': [],
            'Slot_display_order': [],
            'display_order': [],
            'PhraseType': [],
            'SubslotID': [],
            'error': error_msg,
            'sentence': sentence,
            'analysis_method': 'spacy_human_grammar'
        }

def test_spacy_human_grammar_system():
    """spaCyäººé–“æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    print("=== spaCyè¾æ›¸ + äººé–“æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ  5æ–‡å‹ãƒ†ã‚¹ãƒˆ ===\n")
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    mapper = SpacyHumanGrammarMapper()
    
    # 5æ–‡å‹ãƒ†ã‚¹ãƒˆæ–‡ç« 
    test_sentences = [
        # ç¬¬1æ–‡å‹ï¼ˆSVï¼‰
        "Children play",
        "The dog runs", 
        "Birds fly",
        
        # ç¬¬2æ–‡å‹ï¼ˆSVCï¼‰
        "She is happy",
        "The book is interesting",
        "He became a doctor",
        
        # ç¬¬3æ–‡å‹ï¼ˆSVOï¼‰
        "I like apples",
        "She reads books",
        "They watch movies",
        
        # ç¬¬4æ–‡å‹ï¼ˆSVOOï¼‰
        "I give him a book",
        "She told me the truth",
        "He showed us the way",
        
        # ç¬¬5æ–‡å‹ï¼ˆSVOCï¼‰
        "We call him Tom",
        "I consider her smart",
        "They made me happy"
    ]
    
    results = {}
    total_success = 0
    
    for sentence in test_sentences:
        print(f"--- '{sentence}' ---")
        result = mapper.analyze_sentence(sentence)
        
        if 'error' in result:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
            results[sentence] = 'FAILED'
        else:
            print(f"âœ… ãƒ‘ã‚¿ãƒ¼ãƒ³: {result['pattern_detected']}")
            print(f"   ç¢ºä¿¡åº¦: {result['confidence']:.1%}")
            print(f"   ã‚¹ãƒ­ãƒƒãƒˆ: {result['Slot']}")
            print(f"   å¥: {result['SlotPhrase']}")
            print(f"   å¥å‹: {result['PhraseType']}")
            results[sentence] = 'SUCCESS'
            total_success += 1
        print()
    
    # çµ±è¨ˆè¡¨ç¤º
    success_rate = total_success / len(test_sentences) * 100
    print(f"=== 5æ–‡å‹èªè­˜çµæœçµ±è¨ˆ ===")
    print(f"æˆåŠŸç‡: {success_rate:.1f}% ({total_success}/{len(test_sentences)})")
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥çµ±è¨ˆ
    patterns = {'SV': 0, 'SVC': 0, 'SVO': 0, 'SVOO': 0, 'SVOC': 0}
    for sentence, status in results.items():
        if status == 'SUCCESS':
            result = mapper.analyze_sentence(sentence)
            pattern = result['pattern_detected']
            patterns[pattern] += 1
    
    print("\nãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥æˆåŠŸæ•°:")
    for pattern, count in patterns.items():
        print(f"  {pattern}: {count}å€‹")
    
    return results

def interactive_test():
    """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ†ã‚¹ãƒˆæ©Ÿèƒ½"""
    print("=== spaCyäººé–“æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ  - ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ†ã‚¹ãƒˆ ===")
    print("æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ã‚¨ãƒ³ã‚¿ãƒ¼ã‚’æŠ¼ã—ã¦ãã ã•ã„ï¼ˆ'quit'ã§çµ‚äº†ï¼‰")
    print()
    
    mapper = SpacyHumanGrammarMapper()
    
    while True:
        try:
            sentence = input("ğŸ“ æ–‡ç« ã‚’å…¥åŠ›: ").strip()
            
            if sentence.lower() in ['quit', 'exit', 'q']:
                print("ãƒ†ã‚¹ãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                break
            
            if not sentence:
                print("âŒ æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                continue
            
            print(f"\n--- è§£æä¸­: '{sentence}' ---")
            
            # è§£æå®Ÿè¡Œ
            result = mapper.analyze_sentence(sentence)
            
            if 'error' in result:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
            else:
                print("âœ… è§£ææˆåŠŸ!")
                print(f"   ğŸ” æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³: {result['pattern_detected']}")
                print(f"   ğŸ“Š ç¢ºä¿¡åº¦: {result['confidence']:.1%}")
                print(f"   ğŸ·ï¸  ã‚¹ãƒ­ãƒƒãƒˆ: {result['Slot']}")
                print(f"   ğŸ“ ã‚¹ãƒ­ãƒƒãƒˆå¥: {result['SlotPhrase']}")
                print(f"   ğŸ—ï¸  å¥å‹: {result['PhraseType']}")
                
                # èªå½™è§£æè©³ç´°
                print("\n   ğŸ“š èªå½™è§£æè©³ç´°:")
                lexical_info = mapper._extract_lexical_knowledge(sentence)
                for token in lexical_info['tokens']:
                    print(f"      {token['text']}: {token['pos']} ({token['tag']}) [lemma: {token['lemma']}]")
            
            print()
            
        except KeyboardInterrupt:
            print("\n\nãƒ†ã‚¹ãƒˆã‚’ä¸­æ–­ã—ã¾ã—ãŸã€‚")
            break
        except Exception as e:
            print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == '__main__':
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == '--interactive':
        interactive_test()
    else:
        test_spacy_human_grammar_system()
