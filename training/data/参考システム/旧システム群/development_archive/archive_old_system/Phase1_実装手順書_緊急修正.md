# Phase 1 å®Ÿè£…æ‰‹é †æ›¸ - ç·Šæ€¥ä¿®æ­£
**å®Ÿè¡ŒæœŸé–“**: 2025å¹´8æœˆ16æ—¥-8æœˆ17æ—¥  
**ç›®æ¨™**: 90%ç²¾åº¦é”æˆ

---

## ğŸ”§ Task 1.1: é–¢ä¿‚è©ç¯€ä¸»èªæŠ½å‡ºã®ä¿®æ­£

### å•é¡Œã®è©³ç´°
```
Input: "The car which we saw was red"
Current: S:"we", V:"", C1:"red" âŒ
Expected: S:"The car which we saw", V:"was", C1:"red" âœ…
```

### ä¿®æ­£ç®‡æ‰€
**ãƒ•ã‚¡ã‚¤ãƒ«**: `simple_unified_rephrase_integrator.py`  
**ãƒ¡ã‚½ãƒƒãƒ‰**: `_extract_basic_elements()`  
**è¡Œæ•°**: ç´„130-150è¡Œç›®

### å…·ä½“çš„ä¿®æ­£ã‚³ãƒ¼ãƒ‰
```python
def _extract_basic_elements(self, doc) -> Dict[str, str]:
    # ç¾åœ¨ã®subtreeå‡¦ç†ã‚’æ”¹å–„
    if root_verb:
        for child in root_verb.children:
            if child.dep_ in ['nsubj', 'nsubjpass']:
                # ğŸš¨ ä¿®æ­£: whichç¯€ã®ä¸»èªè§£é‡ˆã‚¨ãƒ©ãƒ¼å¯¾å¿œ
                subject_tokens = list(child.subtree)
                
                # ğŸ”§ è¿½åŠ : é–¢ä¿‚è©ç¯€ã®æ­£ã—ã„ç¯„å›²æ¤œå‡º
                if any(token.text.lower() in ['which', 'that', 'who'] for token in subject_tokens):
                    # é–¢ä¿‚è©ç¯€ã‚’å«ã‚€å®Œå…¨ãªä¸»èªã‚’æ§‹ç¯‰
                    full_subject = self._build_complete_subject_with_relative(child, doc)
                    slots['S'] = full_subject
                else:
                    # é€šå¸¸ã®ä¸»èªå‡¦ç†
                    subject_tokens.sort(key=lambda x: x.i)
                    slots['S'] = ' '.join([token.text for token in subject_tokens])
```

### ãƒ†ã‚¹ãƒˆæ–¹æ³•
```python
# ä¿®æ­£å¾Œã®ãƒ†ã‚¹ãƒˆ
test_cases = [
    "The car which we saw was red",
    "The book that I bought is good", 
    "The person who called me was John"
]
# æœŸå¾…çµæœ: ã™ã¹ã¦ã§æ­£ã—ã„ä¸»èªæŠ½å‡º
```

---

## ğŸ”§ Task 1.2: ä½¿å½¹å‹•è©"had"ã®æ¤œå‡ºè¿½åŠ 

### å•é¡Œã®è©³ç´°
```
Input: "He had me clean the room"
Current: S:"He", V:"had", O1:"the room" (ä½¿å½¹å‹•è©ã¨ã—ã¦æœªæ¤œå‡º) âŒ
Expected: ä½¿å½¹å‹•è©æ§‹æ–‡ã¨ã—ã¦æ¤œå‡ºã€é©åˆ‡ãªã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ âœ…
```

### ä¿®æ­£ç®‡æ‰€
**ãƒ•ã‚¡ã‚¤ãƒ«**: `simple_unified_rephrase_integrator.py`  
**ãƒ¡ã‚½ãƒƒãƒ‰**: `_process_causative_construction()`

### å…·ä½“çš„ä¿®æ­£ã‚³ãƒ¼ãƒ‰
```python
def _has_causative_verb(self, doc) -> Optional[any]:
    causative_lemmas = ['make', 'let', 'have', 'help', 'get', 'force', 'cause']
    
    for token in doc:
        # ğŸ”§ ä¿®æ­£: "had"ã®ç‰¹åˆ¥å‡¦ç†è¿½åŠ 
        if token.lemma_ in causative_lemmas:
            # "have"ã®å ´åˆã¯æ–‡è„ˆç¢ºèª
            if token.lemma_ == 'have':
                if self._is_causative_have_construction(token, doc):
                    return token
            else:
                return token
    return None

def _is_causative_have_construction(self, have_token, doc):
    """ä½¿å½¹å‹•è©ã®haveæ§‹æ–‡åˆ¤å®š"""
    # 1. have + äºº + å‹•è©åŸå½¢ãƒ‘ã‚¿ãƒ¼ãƒ³
    # 2. æ‰€æœ‰ã®haveã¨åŒºåˆ¥
    for child in have_token.children:
        if child.dep_ == 'dobj':  # ç›®çš„èªï¼ˆäººï¼‰
            for grandchild in child.children:
                if grandchild.pos_ == 'VERB' and grandchild.dep_ in ['xcomp', 'ccomp']:
                    return True
    return False
```

### ãƒ†ã‚¹ãƒˆæ–¹æ³•
```python
test_cases = [
    "He had me clean the room",  # ä½¿å½¹å‹•è©
    "I had a car",  # æ‰€æœ‰å‹•è©ï¼ˆåŒºåˆ¥ã™ã‚‹ã“ã¨ï¼‰
    "She made him study"  # æ—¢å­˜å‹•ä½œç¢ºèª
]
```

---

## ğŸ”§ Task 1.3: æ™‚é–“ãƒ»æ¡ä»¶ç¯€ã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£å¼·åŒ–

### å•é¡Œã®è©³ç´°
```
Input: "When I arrived, he was sleeping"
Current: M1:"When", sub_slots:{} (ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ãªã—) âŒ  
Expected: M1ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ ã« sub_M1:"When", sub_S:"I", sub_V:"arrived" âœ…
```

### ä¿®æ­£ç®‡æ‰€
**ãƒ•ã‚¡ã‚¤ãƒ«**: `sub_slot_decomposer.py`  
**ãƒ¡ã‚½ãƒƒãƒ‰**: `_decompose_adverbial_clause()`

### å…·ä½“çš„ä¿®æ­£ã‚³ãƒ¼ãƒ‰
```python
def _decompose_adverbial_clause(self, text: str) -> SubSlotResult:
    if not text.strip():
        return SubSlotResult("adverbial_clause", text, {}, 0.9)
    
    # ğŸ”§ ä¿®æ­£: æ™‚é–“ãƒ»æ¡ä»¶ç¯€ã®è©³ç´°åˆ†è§£è¿½åŠ 
    temporal_markers = ['when', 'while', 'before', 'after', 'since', 'until']
    conditional_markers = ['if', 'unless', 'as long as', 'provided that']
    
    # ãƒãƒ¼ã‚«ãƒ¼æ¤œå‡º
    doc = self.nlp(text)
    marker_token = None
    
    for token in doc:
        if token.text.lower() in temporal_markers + conditional_markers:
            marker_token = token
            break
    
    if marker_token:
        # ğŸš¨ è¿½åŠ : ãƒãƒ¼ã‚«ãƒ¼ä»¥é™ã®éƒ¨åˆ†ã‚’è©³ç´°åˆ†è§£
        remaining_text = text[marker_token.idx + len(marker_token.text):].strip()
        if remaining_text:
            # ç°¡æ˜“SVOåˆ†è§£
            sub_slots = self._simple_svo_decomposition(remaining_text)
            sub_slots['sub_M1'] = marker_token.text
            
            return SubSlotResult("adverbial_clause", text, sub_slots, 0.95)
    
    return SubSlotResult("adverbial_clause", text, {}, 0.9)

def _simple_svo_decomposition(self, text: str) -> Dict[str, str]:
    """ç°¡æ˜“SVOåˆ†è§£"""
    doc = self.nlp(text)
    result = {}
    
    for token in doc:
        if token.dep_ in ['nsubj', 'nsubjpass']:
            result['sub_S'] = token.text
        elif token.dep_ == 'ROOT' and token.pos_ == 'VERB':
            result['sub_V'] = token.text
        elif token.dep_ in ['dobj', 'attr', 'acomp']:
            if 'sub_O1' not in result:
                result['sub_O1'] = token.text
    
    return result
```

---

## ğŸ“Š Phase 1 å®Œäº†æ¤œè¨¼

### æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
```bash
python honest_system_evaluation.py
```

### æˆåŠŸåŸºæº–
- **å…¨ä½“ç²¾åº¦**: 90%ä»¥ä¸Š
- **é–¢ä¿‚è©ç¯€**: 100%æ­£ç¢º
- **ä½¿å½¹å‹•è©**: 95%ä»¥ä¸Š
- **æ™‚é–“ãƒ»æ¡ä»¶ç¯€**: 90%ä»¥ä¸Š

### å•é¡Œç™ºç”Ÿæ™‚ã®å¯¾å‡¦
1. **å€‹åˆ¥ãƒ†ã‚¹ãƒˆä½œæˆ**: å•é¡Œã‚±ãƒ¼ã‚¹ã®è©³ç´°åˆ†æ
2. **spaCyè§£æç¢ºèª**: `debug_dependency.py`ã§æ§‹é€ ç¢ºèª
3. **æ®µéšçš„ä¿®æ­£**: å°ã•ãªå¤‰æ›´ã§å½±éŸ¿ç¯„å›²é™å®š

---

## ğŸ“ Phase 1 å®Œäº†å ±å‘Šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

```markdown
## Phase 1 å®Œäº†å ±å‘Š
**å®Ÿè¡Œæ—¥**: 2025å¹´8æœˆXXæ—¥
**ä½œæ¥­æ™‚é–“**: XXæ™‚é–“

### ä¿®æ­£å†…å®¹
- [ ] Task 1.1: é–¢ä¿‚è©ç¯€ä¸»èªæŠ½å‡ºä¿®æ­£
- [ ] Task 1.2: ä½¿å½¹å‹•è©"had"æ¤œå‡ºè¿½åŠ   
- [ ] Task 1.3: æ™‚é–“ãƒ»æ¡ä»¶ç¯€ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå¼·åŒ–

### ç²¾åº¦æ¸¬å®šçµæœ
- å…¨ä½“ç²¾åº¦: XX% (ç›®æ¨™90%)
- é–¢ä¿‚è©ç¯€: XX% (ç›®æ¨™100%)
- ä½¿å½¹å‹•è©: XX% (ç›®æ¨™95%)

### ç™ºè¦‹ã—ãŸè¿½åŠ èª²é¡Œ
- [ ] èª²é¡Œ1: è©³ç´°
- [ ] èª²é¡Œ2: è©³ç´°

### Phase 2 æº–å‚™çŠ¶æ³
- [ ] æº–å‚™å®Œäº† / [ ] è¿½åŠ ä½œæ¥­å¿…è¦
```

---

*Phase 1å®Œäº†å¾Œã€Phase 2ã®è©³ç´°æ‰‹é †æ›¸ã‚’ä½œæˆã—ã¾ã™ã€‚*
