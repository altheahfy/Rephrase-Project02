"""
Phase A2: çœŸã®BasicFivePatternHandlerå®Ÿè£…
ãƒ¬ã‚¬ã‚·ãƒ¼åˆ†è§£æ©Ÿèƒ½ã‚’ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å†…ã«å®Œå…¨ç§»è¡Œ

ä½œæˆæ—¥: 2025å¹´8æœˆ25æ—¥
ç›®çš„: Central Control            # 1. ã‚³ã‚¢è¦ç´ ç‰¹å®šï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œï¼‰
            self.logger.debug("ğŸ” Step 1: Starting core elements identification")
            core_elements = self.identify_core_elements_enhanced(filtered_tokens)
            self.logger.debug(f"ğŸ” Step 1 complete: {core_elements}")
            
            # 2. æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¤å®šï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œï¼‰
            self.logger.debug("ğŸ” Step 2: Starting pattern determination")
            sentence_pattern = self.determine_pattern_enhanced(filtered_tokens, core_elements)
            self.logger.debug(f"ğŸ” Step 2 complete: {sentence_pattern}")
            
            # 3. æ–‡æ³•å½¹å‰²å‰²ã‚Šå½“ã¦ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œï¼‰
            self.logger.debug("ğŸ” Step 3: Starting roles assignment")
            grammar_elements = self.assign_roles_enhanced(filtered_tokens, sentence_pattern, core_elements, relative_clause_info)
            self.logger.debug(f"ğŸ” Step 3 complete: {len(grammar_elements) if grammar_elements else 0} elements")èƒ½ã‚’ç§»è¡Œã—ã€ç´”ç²‹ä¸­å¤®ç®¡ç†ã‚’å®Ÿç¾
"""

from typing import Dict, List, Any, Optional, Tuple
import logging
from collections import namedtuple

# GrammarElementå®šç¾©ï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¨ã®äº’æ›æ€§ï¼‰
GrammarElement = namedtuple('GrammarElement', [
    'text', 'tokens', 'role', 'start_idx', 'end_idx', 'confidence'
])

class BasicFivePatternHandler:
    """
    ğŸ¯ æ‹¡å¼µç‰ˆåŸºæœ¬5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    
    Phase A2å®Ÿè£…:
    â”œâ”€ ãƒ¬ã‚¬ã‚·ãƒ¼åˆ†è§£æ©Ÿèƒ½ã®å®Œå…¨çµ±åˆ
    â”œâ”€ ä¸»èªãƒ»å‹•è©ãƒ»ç›®çš„èªãƒ»è£œèªã®ç²¾å¯†ç‰¹å®š
    â”œâ”€ æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³è‡ªå‹•åˆ¤å®š  
    â””â”€ ã‚¹ãƒ­ãƒƒãƒˆé…ç½®ã®å®Œå…¨è‡ªå‹•åŒ–
    
    ç§»è¡Œå¯¾è±¡æ©Ÿèƒ½:
    â”œâ”€ _identify_core_elements() â†’ identify_core_elements_enhanced()
    â”œâ”€ _determine_sentence_pattern() â†’ determine_pattern_enhanced()
    â””â”€ _assign_grammar_roles() â†’ assign_roles_enhanced()
    """
    
    def __init__(self, logger=None):
        self.logger = logger or logging.getLogger(__name__)
        
        # ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œ: èªå½™ãƒªã‚¹ãƒˆ
        self.linking_verbs = {
            'be', 'is', 'am', 'are', 'was', 'were', 'been', 'being',
            'become', 'became', 'get', 'got', 'seem', 'seemed', 'appear', 'appeared',
            'look', 'looked', 'sound', 'sounded', 'feel', 'felt', 'taste', 'tasted',
            'smell', 'smelled', 'remain', 'remained', 'stay', 'stayed', 'keep', 'kept',
            'turn', 'turned', 'grow', 'grew', 'prove', 'proved'
        }
        
        self.ditransitive_verbs = {
            'give', 'gave', 'given', 'tell', 'told', 'show', 'showed', 'shown',
            'send', 'sent', 'bring', 'brought', 'teach', 'taught', 'offer', 'offered',
            'sell', 'sold', 'buy', 'bought', 'lend', 'lent', 'hand', 'handed',
            'pass', 'passed', 'throw', 'threw', 'thrown'
        }
        
        self.objective_complement_verbs = {
            'make', 'made', 'making', 'makes',
            'call', 'called', 'calling', 'calls',
            'consider', 'considered', 'considering', 'considers',
            'find', 'found', 'finding', 'finds',
            'keep', 'kept', 'keeping', 'keeps',
            'leave', 'left', 'leaving', 'leaves',
            'elect', 'elected', 'electing', 'elects',
            'name', 'named', 'naming', 'names',
            'choose', 'chose', 'chosen', 'choosing', 'chooses'
        }
        
        # ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œ: æ›–æ˜§èªè¾æ›¸
        self.ambiguous_words = {
            'lives': ['NOUN', 'VERB'],    # lifeè¤‡æ•°å½¢ vs liveä¸‰äººç§°å˜æ•°
            'works': ['NOUN', 'VERB'],    # workè¤‡æ•°å½¢ vs workä¸‰äººç§°å˜æ•°  
            'runs': ['NOUN', 'VERB'],     # runè¤‡æ•°å½¢ vs runä¸‰äººç§°å˜æ•°
            'goes': ['NOUN', 'VERB'],     # goè¤‡æ•°å½¢ vs goä¸‰äººç§°å˜æ•°
            'comes': ['NOUN', 'VERB'],    # comeè¤‡æ•°å½¢ vs comeä¸‰äººç§°å˜æ•°
            'stays': ['NOUN', 'VERB'],    # stayè¤‡æ•°å½¢ vs stayä¸‰äººç§°å˜æ•°
            'plays': ['NOUN', 'VERB'],    # playè¤‡æ•°å½¢ vs playä¸‰äººç§°å˜æ•°
            'looks': ['NOUN', 'VERB'],    # lookè¤‡æ•°å½¢ vs lookä¸‰äººç§°å˜æ•°
            'walks': ['NOUN', 'VERB'],    # walkè¤‡æ•°å½¢ vs walkä¸‰äººç§°å˜æ•°
            'talks': ['NOUN', 'VERB'],    # talkè¤‡æ•°å½¢ vs talkä¸‰äººç§°å˜æ•°
            'moves': ['NOUN', 'VERB'],    # moveè¤‡æ•°å½¢ vs moveä¸‰äººç§°å˜æ•°
            'drives': ['NOUN', 'VERB'],   # driveè¤‡æ•°å½¢ vs driveä¸‰äººç§°å˜æ•°
            'flies': ['NOUN', 'VERB'],    # flyè¤‡æ•°å½¢ vs flyä¸‰äººç§°å˜æ•°
            'rides': ['NOUN', 'VERB'],    # rideè¤‡æ•°å½¢ vs rideä¸‰äººç§°å˜æ•°
            'sits': ['NOUN', 'VERB']      # sitè¤‡æ•°å½¢ vs sitä¸‰äººç§°å˜æ•°
        }
    
    def analyze_basic_pattern(self, filtered_tokens: List[Dict], relative_clause_info: Dict) -> Dict[str, Any]:
        """
        ğŸ¯ Phase A3: çµ±åˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
        ãƒ¬ã‚¬ã‚·ãƒ¼åˆ†è§£æ©Ÿèƒ½ã‚’çµ±åˆã—ãŸæ–‡å‹è§£æ
        
        Args:
            filtered_tokens: é–¢ä¿‚ç¯€è¦ç´ ã‚’é™¤å¤–ã—ãŸè§£æå¯¾è±¡ãƒˆãƒ¼ã‚¯ãƒ³
            relative_clause_info: é–¢ä¿‚ç¯€æƒ…å ±
            
        Returns:
            Dict containing:
            - core_elements: ç‰¹å®šã•ã‚ŒãŸã‚³ã‚¢è¦ç´ 
            - sentence_pattern: åˆ¤å®šã•ã‚ŒãŸæ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³  
            - grammar_elements: æ–‡æ³•å½¹å‰²ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸè¦ç´ 
        """
        try:
            # 1. ã‚³ã‚¢è¦ç´ ç‰¹å®šï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œï¼‰
            core_elements = self.identify_core_elements_enhanced(filtered_tokens)
            
            # 2. æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¤å®šï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œï¼‰
            sentence_pattern = self.determine_pattern_enhanced(filtered_tokens, core_elements)
            
            # 3. æ–‡æ³•å½¹å‰²å‰²ã‚Šå½“ã¦ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œï¼‰
            grammar_elements = self.assign_roles_enhanced(filtered_tokens, sentence_pattern, core_elements)
            
            return {
                'core_elements': core_elements,
                'sentence_pattern': sentence_pattern,
                'grammar_elements': grammar_elements,
                'handler_success': True,
                'analysis_method': 'basic_five_pattern_enhanced'
            }
            
        except Exception as e:
            self.logger.error(f"BasicFivePatternHandler.analyze_basic_pattern error: {e}")
            return {
                'core_elements': {},
                'sentence_pattern': 'unknown',
                'grammar_elements': [],
                'handler_success': False,
                'error': str(e)
            }
        
        # åŒå½¢èªå‡¦ç†ï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ç¶™æ‰¿ï¼‰
        self.ambiguous_words = {
            'works': ['NOUN', 'VERB'],   # workè¤‡æ•°å½¢ vs workä¸‰äººç§°å˜æ•°
            'runs': ['NOUN', 'VERB'],    # runè¤‡æ•°å½¢ vs runä¸‰äººç§°å˜æ•°
            'calls': ['NOUN', 'VERB'],   # callè¤‡æ•°å½¢ vs callä¸‰äººç§°å˜æ•°
            'studies': ['NOUN', 'VERB'], # studyè¤‡æ•°å½¢ vs studyä¸‰äººç§°å˜æ•°
            'rides': ['NOUN', 'VERB'],   # rideè¤‡æ•°å½¢ vs rideä¸‰äººç§°å˜æ•°
            'sits': ['NOUN', 'VERB']     # sitè¤‡æ•°å½¢ vs sitä¸‰äººç§°å˜æ•°
        }
    
    def handle(self, tokens: List[Dict], context: Dict) -> Dict[str, Any]:
        """
        ğŸ¯ ãƒ¡ã‚¤ãƒ³ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å‡¦ç†
        
        çµ±åˆã•ã‚ŒãŸãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ã‚’é †æ¬¡å®Ÿè¡Œ:
        Step 1: åŸºæœ¬è¦ç´ ç‰¹å®š
        Step 2: æ–‡å‹åˆ¤å®š
        Step 3: å½¹å‰²å‰²ã‚Šå½“ã¦
        Step 4: çµæœç”Ÿæˆ
        """
        try:
            # Step 1: åŸºæœ¬è¦ç´ ç‰¹å®šï¼ˆæ—§_identify_core_elementsçµ±åˆï¼‰
            core_elements = self.identify_core_elements_enhanced(tokens)
            
            # Step 2: æ–‡å‹åˆ¤å®šï¼ˆæ—§_determine_sentence_patternçµ±åˆï¼‰
            pattern = self.determine_pattern_enhanced(tokens, core_elements)
            
            # Step 3: å½¹å‰²å‰²ã‚Šå½“ã¦ï¼ˆæ—§_assign_grammar_rolesçµ±åˆï¼‰
            grammar_elements = self.assign_roles_enhanced(tokens, pattern, core_elements)
            
            # Step 4: çµæœç”Ÿæˆ
            result = self._convert_to_handler_result(grammar_elements, pattern, tokens)
            
            return result
            
        except Exception as e:
            self.logger.error(f"BasicFivePatternHandler error: {e}")
            return {'success': False, 'error': str(e)}
    
    def identify_core_elements_enhanced(self, tokens: List[Dict]) -> Dict[str, Any]:
        """
        ğŸ¯ ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½çµ±åˆç‰ˆ: åŸºæœ¬è¦ç´ ç‰¹å®š
        
        æ—§_identify_core_elements()ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Œå…¨ç§»è¡Œãƒ»å¼·åŒ–
        """
        try:
            # ãƒ‡ãƒãƒƒã‚°ï¼šãƒˆãƒ¼ã‚¯ãƒ³ã®æ¤œè¨¼
            if not isinstance(tokens, list):
                raise TypeError(f"Expected list of tokens, got {type(tokens)}")
                
            for i, token in enumerate(tokens):
                if not isinstance(token, dict):
                    raise TypeError(f"Token at index {i} is not a dict: {type(token)} = {token}")
                if 'text' not in token:
                    self.logger.warning(f"Token at index {i} missing 'text' field: {token}")
                    
            core = {
                'subject': None,
                'verb': None,
                'subject_indices': [],
                'verb_indices': [],
                'auxiliary': None,
                'auxiliary_indices': []
            }
            
            # å‹•è©ã‚’æ¢ã™ï¼ˆæœ€ã‚‚é‡è¦ï¼‰
            main_verb_idx = self._find_main_verb_enhanced(tokens)
            if main_verb_idx is not None:
                core['verb'] = tokens[main_verb_idx]
                core['verb_indices'] = [main_verb_idx]
                
                # åŠ©å‹•è©ã‚’æ¢ã™
                aux_idx = self._find_auxiliary_enhanced(tokens, main_verb_idx)
                if aux_idx is not None:
                    core['auxiliary'] = tokens[aux_idx]
                    core['auxiliary_indices'] = [aux_idx]
            
            # ä¸»èªã‚’æ¢ã™ï¼ˆå‹•è©ã®å‰ã§æœ€ã‚‚é©åˆ‡ãªåè©å¥ï¼‰
            if main_verb_idx is not None:
                subject_indices = self._find_subject_enhanced(tokens, main_verb_idx)
                if subject_indices:
                    core['subject_indices'] = subject_indices
                    core['subject'] = ' '.join([tokens[i]['text'] for i in subject_indices])
            
            return core
            
        except Exception as e:
            self.logger.error(f"identify_core_elements_enhanced error: {e}")
            raise
    
    def determine_pattern_enhanced(self, tokens: List[Dict], core_elements: Dict) -> str:
        """
        ğŸ¯ ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½çµ±åˆç‰ˆ: æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¤å®š
        
        æ—§_determine_sentence_pattern()ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Œå…¨ç§»è¡Œãƒ»å¼·åŒ–
        """
        if not core_elements['verb']:
            return 'UNKNOWN'
        
        verb = core_elements['verb']
        verb_lemma = verb['lemma'].lower()
        verb_indices = core_elements['verb_indices'] + core_elements.get('auxiliary_indices', [])
        subject_indices = core_elements['subject_indices']
        
        # è‡ªå‹•è©ãƒªã‚¹ãƒˆï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼ç¶™æ‰¿ï¼‰
        intransitive_verbs = {
            'arrive', 'arrived', 'come', 'came', 'go', 'went', 'sleep', 'slept',
            'walk', 'walked', 'run', 'ran', 'happen', 'happened', 'occur', 'occurred',
            'exist', 'existed', 'fall', 'fell', 'rise', 'rose', 'sit', 'sat',
            'stand', 'stood', 'lie', 'lay', 'work', 'worked', 'laugh', 'laughed',
            'cry', 'cried', 'smile', 'smiled', 'die', 'died'
        }
        
        # ä½¿ç”¨æ¸ˆã¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        used_indices = set(verb_indices + subject_indices)
        
        # æ®‹ã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åˆ†æ
        remaining_tokens = [
            (i, token) for i, token in enumerate(tokens) 
            if i not in used_indices and token['pos'] != 'PUNCT'
        ]
        
        # è‡ªå‹•è©ã®å ´åˆã¯å¼·åˆ¶çš„ã«SVãƒ‘ã‚¿ãƒ¼ãƒ³
        if verb_lemma in intransitive_verbs or verb['text'].lower() in intransitive_verbs:
            return 'SV'
        
        # é€£çµå‹•è©ã®å ´åˆ â†’ SVCå€™è£œ
        if verb_lemma in self.linking_verbs:
            if remaining_tokens:
                # è£œèªãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                for i, token in remaining_tokens:
                    if self._can_be_complement_enhanced(token):
                        return 'SVC'
            return 'SV'  # è£œèªãŒãªã„å ´åˆ
        
        # æˆä¸å‹•è©ã®å ´åˆ â†’ SVOOå€™è£œ
        if verb_lemma in self.ditransitive_verbs:
            if len(remaining_tokens) >= 2:
                return 'SVOO'
            elif len(remaining_tokens) == 1:
                return 'SVO'
        
        # ãã®ä»–ã®å ´åˆï¼šæ®‹ã‚Šãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§åˆ¤å®š
        if len(remaining_tokens) >= 2:
            return 'SVOC'  # SVOCå€™è£œ
        elif len(remaining_tokens) == 1:
            return 'SVO'   # SVOå€™è£œ
        else:
            return 'SV'    # SV
    
    def assign_roles_enhanced(self, tokens: List[Dict], pattern: str, core_elements: Dict) -> List[GrammarElement]:
        """
        ğŸ¯ ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½çµ±åˆç‰ˆ: æ–‡æ³•çš„å½¹å‰²å‰²ã‚Šå½“ã¦
        
        æ—§_assign_grammar_roles()ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Œå…¨ç§»è¡Œãƒ»å¼·åŒ–
        """
        elements = []
        used_indices = set()
        
        # ä¸»èªå‡¦ç†
        if core_elements['subject_indices']:
            subject_element = GrammarElement(
                text=core_elements['subject'],
                tokens=[tokens[i] for i in core_elements['subject_indices']],
                role='S',
                start_idx=min(core_elements['subject_indices']),
                end_idx=max(core_elements['subject_indices']),
                confidence=0.95
            )
            elements.append(subject_element)
            used_indices.update(core_elements['subject_indices'])
        
        # åŠ©å‹•è©å‡¦ç†
        if core_elements['auxiliary_indices']:
            aux_element = GrammarElement(
                text=core_elements['auxiliary']['text'],
                tokens=[core_elements['auxiliary']],
                role='Aux',
                start_idx=core_elements['auxiliary_indices'][0],
                end_idx=core_elements['auxiliary_indices'][0],
                confidence=0.9
            )
            elements.append(aux_element)
            used_indices.update(core_elements['auxiliary_indices'])
        
        # å‹•è©å‡¦ç†
        if core_elements['verb_indices']:
            verb_element = GrammarElement(
                text=core_elements['verb']['text'],
                tokens=[core_elements['verb']],
                role='V',
                start_idx=core_elements['verb_indices'][0],
                end_idx=core_elements['verb_indices'][0],
                confidence=0.95
            )
            elements.append(verb_element)
            used_indices.update(core_elements['verb_indices'])
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥å‡¦ç†
        remaining_tokens = [
            (i, token) for i, token in enumerate(tokens)
            if i not in used_indices and token['pos'] != 'PUNCT'
        ]
        
        if pattern == 'SVO' and len(remaining_tokens) >= 1:
            # ç›®çš„èª
            obj_idx, obj_token = remaining_tokens[0]
            obj_element = GrammarElement(
                text=obj_token['text'],
                tokens=[obj_token],
                role='O1',
                start_idx=obj_idx,
                end_idx=obj_idx,
                confidence=0.8
            )
            elements.append(obj_element)
        
        elif pattern == 'SVC' and len(remaining_tokens) >= 1:
            # è£œèªï¼ˆãƒ•ãƒ¬ãƒ¼ã‚ºå…¨ä½“ã‚’å–å¾—ï¼‰
            comp_tokens = []
            comp_indices = []
            
            # è£œèªã¨ã—ã¦é©åˆ‡ãªã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
            for i, (idx, token) in enumerate(remaining_tokens):
                if self._can_be_complement_enhanced(token):
                    comp_tokens.append(token)
                    comp_indices.append(idx)
                elif comp_tokens:  # æ—¢ã«è£œèªãŒã‚ã‚Šã€è£œèªã§ãªã„å ´åˆã¯åœæ­¢
                    break
            
            if comp_tokens:
                comp_text = ' '.join(t['text'] for t in comp_tokens)
                comp_element = GrammarElement(
                    text=comp_text,
                    tokens=comp_tokens,
                    role='C1',
                    start_idx=comp_indices[0],
                    end_idx=comp_indices[-1],
                    confidence=0.8
                )
                elements.append(comp_element)
        
        elif pattern == 'SVOO' and len(remaining_tokens) >= 2:
            # é–“æ¥ç›®çš„èª
            obj1_idx, obj1_token = remaining_tokens[0]
            obj1_element = GrammarElement(
                text=obj1_token['text'],
                tokens=[obj1_token],
                role='O1',
                start_idx=obj1_idx,
                end_idx=obj1_idx,
                confidence=0.8
            )
            elements.append(obj1_element)
            
            # ç›´æ¥ç›®çš„èª
            obj2_idx, obj2_token = remaining_tokens[1]
            obj2_element = GrammarElement(
                text=obj2_token['text'],
                tokens=[obj2_token],
                role='O2',
                start_idx=obj2_idx,
                end_idx=obj2_idx,
                confidence=0.8
            )
            elements.append(obj2_element)
        
        return elements
    
    def _find_main_verb_enhanced(self, tokens: List[Dict]) -> Optional[int]:
        """
        ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œ: ãƒ¡ã‚¤ãƒ³å‹•è©ç‰¹å®š
        æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶™æ‰¿ãƒ»å¼·åŒ–
        æ›–æ˜§èªè§£æ±º4æ®µéšãƒ—ãƒ­ã‚»ã‚¹å®Ÿè£…
        """
        try:
            # ğŸ¯ Step 1: ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒªã‚¹ãƒˆåŒ–ã«ã‚ˆã‚‹æ›–æ˜§èªèªè­˜
            self.logger.debug(f"ğŸ” [æ›–æ˜§èªè§£æ±º] Step 1: æ›–æ˜§èªå€™è£œæ¤œç´¢é–‹å§‹")
            ambiguous_candidates = []
            for i, token in enumerate(tokens):
                if token['text'].lower() in self.ambiguous_words:
                    ambiguous_candidates.append((i, token))
                    self.logger.debug(f"ğŸ” [æ›–æ˜§èªè§£æ±º] æ›–æ˜§èªç™ºè¦‹: {token['text']} (ä½ç½®{i})")
            
            # POSãƒ™ãƒ¼ã‚¹ã¨æ–‡è„ˆãƒ™ãƒ¼ã‚¹ã®ä¸¡æ–¹ã‚’å–å¾—
            pos_candidates = []
            for i, token in enumerate(tokens):
                # å‹•è©ã®å“è©ã‚¿ã‚°
                if (token['tag'].startswith('VB') and token['pos'] == 'VERB') or token['pos'] == 'AUX':
                    pos_candidates.append((i, token))
                    
            self.logger.debug(f"ğŸ” [æ›–æ˜§èªè§£æ±º] POSå‹•è©å€™è£œ: {[(i, t['text'], t['pos'], t['tag']) for i, t in pos_candidates]}")
            
            # æ–‡è„ˆçš„å‹•è©è­˜åˆ¥ï¼ˆPOSèª¤èªè­˜å¯¾ç­–ï¼‰
            contextual_candidates = self._find_contextual_verbs_enhanced(tokens)
            
            # ä¸¡æ–¹ã‚’çµ±åˆï¼ˆé‡è¤‡é™¤å»ï¼‰
            verb_candidates = pos_candidates.copy()
            for i, token in contextual_candidates:
                # æ—¢ã«å­˜åœ¨ã—ãªã„å ´åˆã®ã¿è¿½åŠ 
                if not any(existing_i == i for existing_i, _ in verb_candidates):
                    verb_candidates.append((i, token))
            
            self.logger.debug(f"ğŸ” [æ›–æ˜§èªè§£æ±º] å…¨å‹•è©å€™è£œ: {[(i, t['text']) for i, t in verb_candidates]}")
            
            if not verb_candidates:
                return None
            
            # ğŸ¯ Step 2-4: æ›–æ˜§èªã®ä¸¡ã‚±ãƒ¼ã‚¹æ¤œè¨¼ãƒ—ãƒ­ã‚»ã‚¹
            if ambiguous_candidates:
                resolved_verb = self._resolve_ambiguous_words(tokens, verb_candidates, ambiguous_candidates)
                if resolved_verb is not None:
                    self.logger.debug(f"ğŸ”¥ [æ›–æ˜§èªè§£æ±º] æ–‡æ³•çš„å®Œæ•´æ€§ã«ã‚ˆã‚Šè§£æ±º: ä½ç½®{resolved_verb} = '{tokens[resolved_verb]['text']}'")
                    return resolved_verb
            
            # ğŸ”¥ Phase A3: spaCy POSã‚¿ã‚°ã‚’å„ªå…ˆï¼ˆæ›–æ˜§èªè§£æ±ºå¾Œï¼‰
            # VERBã‚¿ã‚°ã®å‹•è©ã‚’å„ªå…ˆé¸æŠ
            verb_tagged_candidates = [(i, token) for i, token in verb_candidates if token['pos'] == 'VERB']
            if verb_tagged_candidates:
                # æœ€å¾Œã®VERBã‚¿ã‚°å‹•è©ã‚’é¸æŠï¼ˆãƒ¡ã‚¤ãƒ³å‹•è©ã¨ã—ã¦ï¼‰
                return verb_tagged_candidates[-1][0]
            
            # äººé–“çš„åˆ¤å®šï¼šé–¢ä¿‚ç¯€ã‚’é™¤å¤–ã—ã¦ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’ç‰¹å®š
            non_relative_verbs = []
            
            for i, token in verb_candidates:
                # é–¢ä¿‚ä»£åè©ã®ç›´å¾Œã®å‹•è©ã¯é–¢ä¿‚ç¯€å†…å‹•è©ã¨ã—ã¦é™¤å¤–
                is_in_relative_clause = False
                
                # ğŸ”¥ Phase A3: filtered_tokensã§ã¯é–¢ä¿‚ç¯€ã¯æ—¢ã«é™¤å¤–æ¸ˆã¿
                # è¿½åŠ ã®é–¢ä¿‚ç¯€æ¤œå‡ºã¯ä¸è¦ï¼ˆé‡è¤‡å‡¦ç†å›é¿ï¼‰
                
                # å‰ã®å˜èªã‚’ç¢ºèªï¼ˆfiltered_tokensã§ãªã„å ´åˆã®ã¿ï¼‰
                # Phase A3ã§ã¯é–¢ä¿‚ç¯€å‡¦ç†ã¯ä¸­å¤®ç®¡ç†ã§å®Ÿè¡Œæ¸ˆã¿
                
                if not is_in_relative_clause:
                    non_relative_verbs.append((i, token))
            
            if non_relative_verbs:
                # ãƒ¡ã‚¤ãƒ³å‹•è©å€™è£œã‹ã‚‰åŠ©å‹•è©ã§ãªã„ã‚‚ã®ã‚’å„ªå…ˆ
                main_verbs = [(i, token) for i, token in non_relative_verbs if not self._is_auxiliary_verb_enhanced(token)]
                if main_verbs:
                    # æ–‡ã®å¾ŒåŠã«ã‚ã‚‹ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’å„ªå…ˆï¼ˆé–¢ä¿‚ç¯€ã®å¾Œï¼‰
                    return main_verbs[-1][0]
                return non_relative_verbs[-1][0]
            
            # æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦ã€ã©ã®å‹•è©ã§ã‚‚é¸æŠ
            return verb_candidates[-1][0]
            
        except Exception as e:
            self.logger.error(f"_find_main_verb_enhanced error: {e}")
            import traceback
            self.logger.error(f"Traceback: {traceback.format_exc()}")
            raise
    
    def _resolve_ambiguous_words(self, tokens: List[Dict], verb_candidates: List[Tuple[int, Dict]], 
                                ambiguous_candidates: List[Tuple[int, Dict]]) -> Optional[int]:
        """
        æ›–æ˜§èªè§£æ±º4æ®µéšãƒ—ãƒ­ã‚»ã‚¹å®Ÿè£…
        æ–‡æ³•çš„å®Œæ•´æ€§ã‚’ã‚±ãƒ¼ã‚¹é¸æŠã®æœ€çµ‚åˆ¤å®šåŸºæº–ã¨ã™ã‚‹
        """
        self.logger.debug(f"ğŸ” [æ›–æ˜§èªè§£æ±º] Step 2: ä¸¡ã‚±ãƒ¼ã‚¹å¯èƒ½æ€§ä»˜ä¸é–‹å§‹")
        
        for amb_idx, amb_token in ambiguous_candidates:
            word = amb_token['text'].lower()
            self.logger.debug(f"ğŸ” [æ›–æ˜§èªè§£æ±º] æ¤œè¨¼å¯¾è±¡: '{word}' (ä½ç½®{amb_idx})")
            
            # Step 3: ã‚±ãƒ¼ã‚¹1æ¤œè¨¼ï¼ˆåè©è§£é‡ˆï¼‰
            noun_case_valid = self._validate_noun_case(tokens, amb_idx, word)
            self.logger.debug(f"ğŸ” [æ›–æ˜§èªè§£æ±º] ã‚±ãƒ¼ã‚¹1ï¼ˆåè©è§£é‡ˆï¼‰: {noun_case_valid}")
            
            # Step 4: ã‚±ãƒ¼ã‚¹2æ¤œè¨¼ï¼ˆå‹•è©è§£é‡ˆï¼‰  
            verb_case_valid = self._validate_verb_case(tokens, amb_idx, word)
            self.logger.debug(f"ğŸ” [æ›–æ˜§èªè§£æ±º] ã‚±ãƒ¼ã‚¹2ï¼ˆå‹•è©è§£é‡ˆï¼‰: {verb_case_valid}")
            
            # æ–‡æ³•çš„å®Œæ•´æ€§ã«ã‚ˆã‚‹æœ€çµ‚åˆ¤å®š
            if verb_case_valid and not noun_case_valid:
                # å‹•è©ã¨ã—ã¦è§£é‡ˆã™ã‚‹ã“ã¨ã§æ–‡ãŒå®Œæˆã™ã‚‹å ´åˆ
                return amb_idx
            elif not verb_case_valid and noun_case_valid:
                # åè©ã¨ã—ã¦è§£é‡ˆã™ã¹ãå ´åˆã€ä»–ã®å‹•è©ã‚’æ¢ã™
                continue
                
        # æ›–æ˜§èªè§£æ±ºã«å¤±æ•—ã—ãŸå ´åˆã€å¾“æ¥ãƒ­ã‚¸ãƒƒã‚¯ã«æˆ»ã‚‹
        return None
    
    def _validate_noun_case(self, tokens: List[Dict], amb_idx: int, word: str) -> bool:
        """
        Step 3: åè©è§£é‡ˆã§ã®æ–‡æ³•çš„å®Œæ•´æ€§æ¤œè¨¼
        ä¾‹ï¼šlives â†’ lifeè¤‡æ•°å½¢ï¼ˆåè©ï¼‰ã¨ã—ã¦æ‰±ã£ãŸå ´åˆã®æ–‡ã®å®Œæ•´æ€§
        """
        # spaCy POSè§£æçµæœã‚’é‡è¦–ï¼šNOUNã¨ã‚¿ã‚°ä»˜ã‘ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã¯åè©ã¨ã—ã¦æ‰±ã†
        actual_pos = tokens[amb_idx]['pos']
        if actual_pos == 'NOUN':
            self.logger.debug(f"ğŸ” [æ›–æ˜§èªè§£æ±º] spaCyè§£æ: '{word}' ã¯ NOUN ã‚¿ã‚° â†’ åè©è§£é‡ˆæœ‰åŠ¹")
            return True
            
        # åè©ã¨ã—ã¦æ‰±ã£ãŸå ´åˆã€ä¸»èªãƒ»å‹•è©ãƒ»ç›®çš„èªãŒé©åˆ‡ã«é…ç½®ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        sentence_tokens = [t['text'] for t in tokens]
        
        # é–¢ä¿‚ç¯€ã®å¢ƒç•Œã‚’èªè­˜
        relative_pronouns = ['who', 'whom', 'which', 'that', 'whose', 'where', 'when']
        relative_start = None
        
        for i, token in enumerate(tokens):
            if token['text'].lower() in relative_pronouns:
                relative_start = i
                break
        
        if relative_start is not None and amb_idx > relative_start:
            # é–¢ä¿‚ç¯€å†…ã®èªã¨ã—ã¦åè©è§£é‡ˆã®å ´åˆ
            # é–¢ä¿‚ç¯€ã ã‘ã§å®Œçµã—ãŸæ–‡ã«ãªã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            relative_clause_tokens = tokens[relative_start:amb_idx+1]
            
            # é–¢ä¿‚ç¯€å†…ã«å‹•è©ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            has_relative_verb = any(t['pos'] == 'VERB' and t['tag'].startswith('VB') 
                                  for t in relative_clause_tokens if t != tokens[amb_idx])
            
            if has_relative_verb:
                # é–¢ä¿‚ç¯€å†…ã«ä»–ã®å‹•è©ãŒã‚ã‚Šã€ã“ã®èªã‚’åè©ã¨ã™ã‚‹ã¨é–¢ä¿‚ç¯€ãŒå®Œçµ
                # ã—ã‹ã—ä¸»æ–‡ã®å‹•è©ãŒãªã„çŠ¶æ…‹ â†’ æ–‡æ³•çš„ã«ä¸å®Œå…¨
                main_clause_tokens = tokens[amb_idx+1:]
                has_main_verb = any(t['pos'] == 'VERB' and t['tag'].startswith('VB') 
                                  for t in main_clause_tokens)
                
                if not has_main_verb:
                    self.logger.debug(f"ğŸ” [æ›–æ˜§èªè§£æ±º] åè©è§£é‡ˆâ†’é–¢ä¿‚ç¯€å®Œçµã™ã‚‹ãŒä¸»æ–‡ã«å‹•è©ãªã— â†’ ä¸å®Œæ•´")
                    return False
                    
        return True
    
    def _validate_verb_case(self, tokens: List[Dict], amb_idx: int, word: str) -> bool:
        """
        Step 4: å‹•è©è§£é‡ˆã§ã®æ–‡æ³•çš„å®Œæ•´æ€§æ¤œè¨¼
        ä¾‹ï¼šlives â†’ å‹•è©ã¨ã—ã¦æ‰±ã£ãŸå ´åˆã®æ–‡ã®å®Œæ•´æ€§
        """
        # spaCy POSè§£æçµæœã‚’é‡è¦–ï¼šNOUNã¨ã‚¿ã‚°ä»˜ã‘ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã¯å‹•è©è§£é‡ˆç„¡åŠ¹
        actual_pos = tokens[amb_idx]['pos']
        if actual_pos == 'NOUN':
            self.logger.debug(f"ğŸ” [æ›–æ˜§èªè§£æ±º] spaCyè§£æ: '{word}' ã¯ NOUN ã‚¿ã‚° â†’ å‹•è©è§£é‡ˆç„¡åŠ¹")
            return False
            
        # å‹•è©ã¨ã—ã¦æ‰±ã£ãŸå ´åˆã®æ–‡ã®æ§‹é€ ãƒã‚§ãƒƒã‚¯
        sentence_tokens = [t['text'] for t in tokens]
        
        # é–¢ä¿‚ç¯€ã®å¢ƒç•Œã‚’èªè­˜
        relative_pronouns = ['who', 'whom', 'which', 'that', 'whose', 'where', 'when']
        relative_start = None
        
        for i, token in enumerate(tokens):
            if token['text'].lower() in relative_pronouns:
                relative_start = i
                break
                
        if relative_start is not None and amb_idx > relative_start:
            # é–¢ä¿‚ç¯€ã®å¾Œã®ä½ç½®ã§ã®å‹•è©è§£é‡ˆ
            # ä¸»æ–‡ã®å‹•è©ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            
            # ä¸»èªã®å­˜åœ¨ç¢ºèªï¼ˆé–¢ä¿‚ç¯€ã‚ˆã‚Šå‰ï¼‰
            has_subject = any(t['dep'] in ['nsubj', 'nsubj:pass'] 
                            for t in tokens[:relative_start])
            
            if has_subject:
                self.logger.debug(f"ğŸ” [æ›–æ˜§èªè§£æ±º] å‹•è©è§£é‡ˆâ†’ä¸»èªã‚ã‚Šã€å®Œå…¨ãªæ–‡ã¨ã—ã¦æˆç«‹")
                return True
                
        return True  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ‰åŠ¹ã¨ã™ã‚‹
    
    def _find_contextual_verbs_enhanced(self, tokens: List[Dict]) -> List[Tuple[int, Dict]]:
        """
        ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œ: æ–‡è„ˆçš„å‹•è©è­˜åˆ¥
        """
        contextual_verbs = []
        sentence_text = ' '.join([token['text'] for token in tokens])
        
        for i, token in enumerate(tokens):
            # æ—¢ã«å‹•è©ã¨ã—ã¦èªè­˜ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®
            if token['pos'] in ['VERB', 'AUX']:
                continue
            
            # åŒå½¢èªãƒã‚§ãƒƒã‚¯
            if token['text'].lower() in self.ambiguous_words:
                if 'VERB' in self.ambiguous_words[token['text'].lower()]:
                    # æ–‡è„ˆã‹ã‚‰å‹•è©ã¨ã—ã¦åˆ¤å®š
                    if self._contextual_verb_check_enhanced(tokens, i, sentence_text):
                        # ãƒ•ãƒ©ã‚°ã‚’è¿½åŠ ã—ã¦å‹•è©ã¨ã—ã¦æ‰±ã†
                        enhanced_token = token.copy()
                        enhanced_token['contextual_override'] = True
                        enhanced_token['pos'] = 'VERB'
                        contextual_verbs.append((i, enhanced_token))
        
        return contextual_verbs
    
    def _contextual_verb_check_enhanced(self, tokens: List[Dict], token_idx: int, sentence: str) -> bool:
        """
        ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œ: æ–‡è„ˆçš„å‹•è©åˆ¤å®š
        """
        token = tokens[token_idx]
        
        # å¾Œç¶šã«ç›®çš„èªã‚‰ã—ãè¦ç´ ãŒã‚ã‚‹ã‹
        for i in range(token_idx + 1, min(len(tokens), token_idx + 3)):
            if i < len(tokens) and tokens[i]['pos'] in ['NOUN', 'PRON']:
                return True
        
        # å‰ã«ä¸»èªã‚‰ã—ãè¦ç´ ãŒã‚ã‚‹ã‹
        for i in range(max(0, token_idx - 3), token_idx):
            if tokens[i]['pos'] in ['NOUN', 'PRON']:
                return True
        
        return False
    
    def _find_auxiliary_enhanced(self, tokens: List[Dict], main_verb_idx: int) -> Optional[int]:
        """
        ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œ: åŠ©å‹•è©ç‰¹å®š
        """
        # ãƒ¡ã‚¤ãƒ³å‹•è©ã®å‰ã‚’æ¢ç´¢
        for i in range(main_verb_idx - 1, -1, -1):
            token = tokens[i]
            if self._is_auxiliary_verb_enhanced(token):
                return i
            # åè©ãŒæ¥ãŸã‚‰æ¢ç´¢çµ‚äº†
            if token['pos'] in ['NOUN', 'PRON']:
                break
        return None
    
    def _find_subject_enhanced(self, tokens: List[Dict], verb_idx: int) -> List[int]:
        """
        ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œ: ä¸»èªç‰¹å®š
        """
        subject_indices = []
        
        # å‹•è©ã®å‰ã§æœ€åˆã«è¦‹ã¤ã‹ã‚‹åè©å¥ã‚’ä¸»èªã¨ã™ã‚‹
        for i in range(verb_idx - 1, -1, -1):
            token = tokens[i]
            if token['pos'] in ['NOUN', 'PRON']:
                subject_indices.insert(0, i)  # èªé †ã‚’ä¿æŒ
            elif token['pos'] in ['DET', 'ADJ']:
                subject_indices.insert(0, i)  # ä¿®é£¾èªã‚‚å«ã‚ã‚‹
            elif subject_indices:  # åè©å¥ãŒé–‹å§‹ã•ã‚ŒãŸã‚‰ç¶™ç¶š
                break
        
        return subject_indices
    
    def _is_auxiliary_verb_enhanced(self, token: Dict) -> bool:
        """
        ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œ: åŠ©å‹•è©åˆ¤å®š
        """
        aux_verbs = {
            'am', 'is', 'are', 'was', 'were', 'be', 'being', 'been',
            'have', 'has', 'had', 'having',
            'do', 'does', 'did',
            'will', 'would', 'shall', 'should',
            'can', 'could', 'may', 'might',
            'must', 'ought'
        }
        return token['lemma'].lower() in aux_verbs or token['pos'] == 'AUX'
    
    def _can_be_complement_enhanced(self, token: Dict) -> bool:
        """
        ãƒ¬ã‚¬ã‚·ãƒ¼æ©Ÿèƒ½ç§»è¡Œ: è£œèªåˆ¤å®š
        """
        return token['pos'] in ['NOUN', 'ADJ', 'PRON']
    
    def _convert_to_handler_result(self, grammar_elements: List[GrammarElement], pattern: str, tokens: List[Dict]) -> Dict[str, Any]:
        """
        GrammarElementsã‚’çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼å½¢å¼ã«å¤‰æ›
        """
        slots = {}
        
        for element in grammar_elements:
            if element.text and element.text.strip():
                slots[element.role] = element.text.strip()
        
        return {
            'success': True,
            'slots': slots,
            'pattern_detected': pattern,
            'confidence': 0.9,
            'handler_name': 'basic_five_pattern_enhanced',
            'processing_notes': f'Enhanced pattern: {pattern}, elements: {len(grammar_elements)}'
        }
