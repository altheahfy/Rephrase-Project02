#!/usr/bin/env python3
"""
Unified Boundary Expansion Library v1.0
Pure Stanza V3.1ã‹ã‚‰æŠ½å‡ºã—ãŸçµ±ä¸€å¢ƒç•Œæ‹¡å¼µãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

ç‰¹å¾´:
- ã‚¹ãƒ­ãƒƒãƒˆåˆ¥ã‚«ã‚¹ã‚¿ãƒ å¢ƒç•Œæ‹¡å¼µ
- é–¢ä¿‚ä»£åè©å¯¾å¿œ
- spaCyçµ±åˆå‡¦ç†
- å®Œå…¨ã«ç‹¬ç«‹ãƒ»æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã«å½±éŸ¿ãªã—
"""

import spacy
from typing import Dict, List, Optional, Any

class BoundaryExpansionLib:
    """çµ±ä¸€å¢ƒç•Œæ‹¡å¼µãƒ©ã‚¤ãƒ–ãƒ©ãƒª"""
    
    def __init__(self):
        """å¢ƒç•Œæ‹¡å¼µãƒ©ã‚¤ãƒ–ãƒ©ãƒªåˆæœŸåŒ–"""
        print("ğŸš€ çµ±ä¸€å¢ƒç•Œæ‹¡å¼µãƒ©ã‚¤ãƒ–ãƒ©ãƒª v1.0 åˆæœŸåŒ–ä¸­...")
        
        # spaCy NLP ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆå¢ƒç•Œèª¿æ•´ç”¨ï¼‰
        try:
            self.spacy_nlp = spacy.load("en_core_web_sm")
            print("âœ… spaCyæº–å‚™å®Œäº†")
        except OSError:
            print("âš ï¸ spaCyè‹±èªãƒ¢ãƒ‡ãƒ«æœªæ¤œå‡ºãƒ»åŸºæœ¬æ©Ÿèƒ½ã®ã¿ä½¿ç”¨")
            self.spacy_nlp = None
        
        # Pure Stanza V3.1ã‹ã‚‰æŠ½å‡º: step18æ±ç”¨å¢ƒç•Œæ‹¡å¼µè¨­å®š
        self.span_expand_deps = ['det', 'poss', 'compound', 'amod', 'nummod', 'case']
        self.relative_pronoun_deps = ['nsubj', 'dobj', 'pobj']  # é–¢ä¿‚ä»£åè©ã®å½¹å‰²
        
        # ã‚¹ãƒ­ãƒƒãƒˆç‰¹åŒ–æ‹¡å¼µãƒ«ãƒ¼ãƒ«ï¼ˆPure Stanza V3.1å®Œå…¨æŠ½å‡ºç‰ˆï¼‰
        self.slot_specific_expansion_map = {
            # === ä¸»èªç³»ï¼ˆå®Œå…¨æ‹¡å¼µï¼‰===
            'S': ['det', 'amod', 'compound', 'nmod', 'acl', 'acl:relcl', 'nummod', 'poss', 
                  'case', 'mark'],  # é–¢ä¿‚ç¯€å«ã‚€å®Œå…¨ä¸»èªæ‹¡å¼µ
            
            # === å‹•è©ç³»ï¼ˆåŠ©å‹•è©ãƒ»ä¿®é£¾å®Œå…¨å¯¾å¿œï¼‰===
            'V': ['aux', 'aux:pass', 'auxpass', 'neg', 'advmod', 'compound:prt', 'prt'],
            'Aux': ['neg', 'advmod'],  # åŠ©å‹•è©å°‚ç”¨
            
            # === ç›®çš„èªç³»ï¼ˆä¸»èªã¨åŒç­‰æ‹¡å¼µï¼‰===
            'O1': ['det', 'amod', 'compound', 'nmod', 'acl', 'acl:relcl', 'nummod', 'poss'],
            'O2': ['det', 'amod', 'compound', 'nmod', 'nummod', 'poss'],
            
            # === è£œèªç³»ï¼ˆå½¢å®¹è©ãƒ»åè©è£œèªç‰¹åŒ–ï¼‰===
            'C1': ['det', 'amod', 'compound', 'advmod', 'case', 'mark'],  # æ¯”è¼ƒæ§‹æ–‡å¯¾å¿œå¼·åŒ–
            'C2': ['det', 'amod', 'compound', 'to'],  # ä¸å®šè©è£œèªå¯¾å¿œ
            
            # === ä¿®é£¾èªç³»ï¼ˆå„ã‚¿ã‚¤ãƒ—ç‰¹åŒ–ï¼‰===
            'M1': ['advmod', 'prep', 'pobj', 'case', 'mark', 'cc', 'conj'],  # å‰ç½®è©å¥ãƒ»æ¥ç¶šè©å®Œå…¨å¯¾å¿œ
            'M2': ['advmod', 'prep', 'pobj', 'compound'],  # å‰¯è©ä¿®é£¾å¼·åŒ–
            'M3': ['advmod', 'prep', 'pobj', 'tmod', 'npadvmod']  # æ™‚é–“ãƒ»å ´æ‰€ä¿®é£¾ç‰¹åŒ–
        }
        
        # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ—¢å­˜ãƒãƒƒãƒ—ã‚‚ä¿æŒ
        self.slot_expansion_map = self.slot_specific_expansion_map
        
        print("ğŸ—ï¸ çµ±ä¸€å¢ƒç•Œæ‹¡å¼µãƒ©ã‚¤ãƒ–ãƒ©ãƒªæº–å‚™å®Œäº†")
    
    def expand_span_generic(self, text: str, expansion_context: Optional[Dict] = None) -> str:
        """
        æ±ç”¨ã‚¹ãƒ‘ãƒ³æ‹¡å¼µå‡¦ç†ï¼ˆPure Stanza V3.1 step18ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ï¼‰
        
        Args:
            text: æ‹¡å¼µå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            expansion_context: æ‹¡å¼µã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¨­å®š
            
        Returns:
            å¢ƒç•Œæ‹¡å¼µã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not self.spacy_nlp:
            return text  # spaCyæœªåˆ©ç”¨æ™‚ã¯å…ƒãƒ†ã‚­ã‚¹ãƒˆãã®ã¾ã¾
        
        try:
            spacy_doc = self.spacy_nlp(text)
            
            if len(spacy_doc) <= 1:
                return text
                
            # æ‹¡å¼µè¨­å®šï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ï¼‰
            expand_deps = expansion_context.get('expand_deps', self.span_expand_deps) if expansion_context else self.span_expand_deps
            
            # å„ãƒˆãƒ¼ã‚¯ãƒ³ã®å¢ƒç•Œæ‹¡å¼µ
            expanded_spans = []
            
            for token in spacy_doc:
                span_start = token.i
                span_end = token.i
                
                # ä¾å­˜èªã«ã‚ˆã‚‹æ‹¡å¼µ
                for child in token.children:
                    if child.dep_ in expand_deps:
                        span_start = min(span_start, child.i)
                        span_end = max(span_end, child.i)
                
                # é–¢ä¿‚ä»£åè©ã®å¢ƒç•Œæ‹¡å¼µ
                if token.dep_ in ['relcl', 'acl']:
                    rel_pronouns = self._find_relative_pronouns_in_span(token, spacy_doc)
                    for rel_idx in rel_pronouns:
                        span_start = min(span_start, rel_idx)
                        span_end = max(span_end, rel_idx)
                
                if span_start <= span_end:
                    span_text = ' '.join(spacy_doc[i].text for i in range(span_start, span_end + 1))
                    expanded_spans.append(span_text)
            
            # é‡è¤‡é™¤å»ã¨çµåˆ
            unique_spans = list(dict.fromkeys(expanded_spans))  # é †åºä¿æŒã§é‡è¤‡é™¤å»
            return ' '.join(unique_spans) if unique_spans else text
            
        except Exception as e:
            print(f"âš ï¸ æ±ç”¨ã‚¹ãƒ‘ãƒ³æ‹¡å¼µã‚¨ãƒ©ãƒ¼: {e}")
            return text
    
    def expand_span_for_slot(self, text: str, slot_key: str) -> str:
        """
        ã‚¹ãƒ­ãƒƒãƒˆç‰¹åŒ–å¢ƒç•Œæ‹¡å¼µï¼ˆPure Stanza V3.1å®Œå…¨ç‰ˆï¼‰
        
        Args:
            text: æ‹¡å¼µå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            slot_key: ã‚¹ãƒ­ãƒƒãƒˆåï¼ˆS, V, O1, O2, C1, C2, M1, M2, M3, Auxï¼‰
            
        Returns:
            ã‚¹ãƒ­ãƒƒãƒˆåˆ¥æœ€é©åŒ–ã•ã‚ŒãŸå¢ƒç•Œæ‹¡å¼µãƒ†ã‚­ã‚¹ãƒˆ
        """
        # ã‚¹ãƒ­ãƒƒãƒˆç‰¹åŒ–æ‹¡å¼µä¾å­˜èªè¨­å®šå–å¾—
        expand_deps = self.slot_specific_expansion_map.get(slot_key, self.span_expand_deps)
        
        # ã‚¹ãƒ­ãƒƒãƒˆç‰¹åŒ–æ‹¡å¼µã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
        expansion_context = {
            'expand_deps': expand_deps,
            'slot_type': slot_key,
            'slot_specific': True  # ã‚¹ãƒ­ãƒƒãƒˆç‰¹åŒ–ãƒ¢ãƒ¼ãƒ‰ãƒ•ãƒ©ã‚°
        }
        
        # ã‚¹ãƒ­ãƒƒãƒˆåˆ¥ç‰¹åˆ¥å‡¦ç†
        if slot_key in ['S', 'O1'] and self._contains_relative_clause(text):
            # ä¸»èªãƒ»ç›®çš„èªã®é–¢ä¿‚ç¯€ç‰¹åŒ–å‡¦ç†
            return self._expand_with_relative_clause_optimization(text, expansion_context)
        elif slot_key == 'V' and self._contains_modal_verb(text):
            # å‹•è©ã®ãƒ¢ãƒ¼ãƒ€ãƒ«ç‰¹åŒ–å‡¦ç†
            return self._expand_with_modal_optimization(text, expansion_context)
        elif slot_key in ['M1', 'M2', 'M3'] and self._contains_prepositional_phrase(text):
            # ä¿®é£¾èªã®å‰ç½®è©å¥ç‰¹åŒ–å‡¦ç†
            return self._expand_with_prepositional_optimization(text, expansion_context)
        else:
            # æ±ç”¨æ‹¡å¼µå‡¦ç†
            return self.expand_span_generic(text, expansion_context)
    
    def _contains_relative_clause(self, text: str) -> bool:
        """é–¢ä¿‚ç¯€å«æœ‰åˆ¤å®š"""
        if not self.spacy_nlp:
            return False
        try:
            doc = self.spacy_nlp(text)
            return any(token.dep_ in ['acl:relcl', 'relcl'] for token in doc)
        except:
            return False
    
    def _contains_modal_verb(self, text: str) -> bool:
        """ãƒ¢ãƒ¼ãƒ€ãƒ«å‹•è©å«æœ‰åˆ¤å®š"""
        modal_verbs = {'can', 'could', 'may', 'might', 'will', 'would', 'shall', 'should', 'must'}
        return any(word.lower() in modal_verbs for word in text.split())
    
    def _contains_prepositional_phrase(self, text: str) -> bool:
        """å‰ç½®è©å¥å«æœ‰åˆ¤å®š"""
        if not self.spacy_nlp:
            return False
        try:
            doc = self.spacy_nlp(text)
            return any(token.pos_ == 'ADP' for token in doc)
        except:
            return False
    
    def _expand_with_relative_clause_optimization(self, text: str, context: Dict) -> str:
        """é–¢ä¿‚ç¯€æœ€é©åŒ–æ‹¡å¼µ"""
        # é–¢ä¿‚ç¯€ç‰¹åŒ–ã®æ‹¡å¼µå‡¦ç†
        enhanced_deps = context['expand_deps'] + ['mark', 'nsubj:relcl', 'obj:relcl']
        enhanced_context = {**context, 'expand_deps': enhanced_deps}
        return self.expand_span_generic(text, enhanced_context)
    
    def _expand_with_modal_optimization(self, text: str, context: Dict) -> str:
        """ãƒ¢ãƒ¼ãƒ€ãƒ«å‹•è©æœ€é©åŒ–æ‹¡å¼µ"""
        # ãƒ¢ãƒ¼ãƒ€ãƒ«å‹•è©ç‰¹åŒ–ã®æ‹¡å¼µå‡¦ç†
        enhanced_deps = context['expand_deps'] + ['ccomp', 'xcomp', 'advcl']
        enhanced_context = {**context, 'expand_deps': enhanced_deps}
        return self.expand_span_generic(text, enhanced_context)
    
    def _expand_with_prepositional_optimization(self, text: str, context: Dict) -> str:
        """å‰ç½®è©å¥æœ€é©åŒ–æ‹¡å¼µ"""
        # å‰ç½®è©å¥ç‰¹åŒ–ã®æ‹¡å¼µå‡¦ç†
        enhanced_deps = context['expand_deps'] + ['pcomp', 'pobj', 'agent']
        enhanced_context = {**context, 'expand_deps': enhanced_deps}
        return self.expand_span_generic(text, enhanced_context)
    
    def _find_relative_pronouns_in_span(self, rel_token, spacy_doc) -> List[int]:
        """ã‚¹ãƒ‘ãƒ³å†…é–¢ä¿‚ä»£åè©ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¤œå‡ºï¼ˆæ±ç”¨ï¼‰"""
        rel_indices = []
        
        for child in rel_token.children:
            if (child.pos_ == 'PRON' and 
                child.dep_ in self.relative_pronoun_deps and
                child.text.lower() in ['who', 'whom', 'whose', 'which', 'that']):
                rel_indices.append(child.i)
        
        return rel_indices
    
    def get_expansion_deps_for_slot(self, slot_key: str) -> List[str]:
        """
        ã‚¹ãƒ­ãƒƒãƒˆã‚¿ã‚¤ãƒ—åˆ¥æ‹¡å¼µä¾å­˜èªè¨­å®šå–å¾—ï¼ˆPure Stanza V3.1å®Œå…¨ç‰ˆï¼‰
        
        Args:
            slot_key: ã‚¹ãƒ­ãƒƒãƒˆå
            
        Returns:
            ã‚¹ãƒ­ãƒƒãƒˆç‰¹åŒ–æ‹¡å¼µä¾å­˜èªãƒªã‚¹ãƒˆ
        """
        return self.slot_specific_expansion_map.get(slot_key, self.span_expand_deps)
    
    def check_requires_expansion(self, text: str) -> bool:
        """
        å¢ƒç•Œæ‹¡å¼µãŒå¿…è¦ã‹ã©ã†ã‹åˆ¤å®š
        
        Args:
            text: åˆ¤å®šå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            True: æ‹¡å¼µå¿…è¦, False: ä¸è¦
        """
        if not self.spacy_nlp or not text or len(text.strip()) == 0:
            return False
        
        try:
            spacy_doc = self.spacy_nlp(text)
            
            # ä¿®é£¾èªã‚«ã‚¦ãƒ³ãƒˆ
            modifier_count = sum(1 for token in spacy_doc if token.dep_ in self.span_expand_deps)
            
            # ä¿®é£¾èªãŒå­˜åœ¨ã™ã‚Œã°æ‹¡å¼µå¯¾è±¡
            return modifier_count > 0
            
        except Exception:
            return False  # spaCyå‡¦ç†å¤±æ•—æ™‚ã¯åŸºæœ¬åˆ¤å®šã®ã¿

# === ãƒ†ã‚¹ãƒˆãƒ»æ¤œè¨¼ç”¨é–¢æ•° ===

def test_boundary_expansion():
    """å¢ƒç•Œæ‹¡å¼µãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ†ã‚¹ãƒˆ"""
    lib = BoundaryExpansionLib()
    
    test_cases = [
        {
            "text": "the tall man",
            "slot": "S",
            "expected_improved": True,
            "description": "é™å®šè©+å½¢å®¹è©+åè©"
        },
        {
            "text": "very carefully",
            "slot": "M2", 
            "expected_improved": True,
            "description": "å‰¯è©ä¿®é£¾"
        },
        {
            "text": "New York City",
            "slot": "O1",
            "expected_improved": True,
            "description": "è¤‡åˆåè©"
        },
        {
            "text": "run",
            "slot": "V",
            "expected_improved": False,
            "description": "å˜ä¸€èªï¼ˆæ‹¡å¼µä¸è¦ï¼‰"
        }
    ]
    
    print("ğŸ§ª å¢ƒç•Œæ‹¡å¼µãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 50)
    
    for i, case in enumerate(test_cases, 1):
        print(f"\n{i}. {case['description']}: '{case['text']}'")
        
        # æ‹¡å¼µå‰
        original = case['text']
        
        # æ±ç”¨æ‹¡å¼µ
        generic_expanded = lib.expand_span_generic(original)
        
        # ã‚¹ãƒ­ãƒƒãƒˆåˆ¥æ‹¡å¼µ
        slot_expanded = lib.expand_span_for_slot(original, case['slot'])
        
        # æ‹¡å¼µå¿…è¦æ€§åˆ¤å®š
        requires_expansion = lib.check_requires_expansion(original)
        
        print(f"   å…ƒãƒ†ã‚­ã‚¹ãƒˆ: '{original}'")
        print(f"   æ±ç”¨æ‹¡å¼µ: '{generic_expanded}'")
        print(f"   {case['slot']}æ‹¡å¼µ: '{slot_expanded}'")
        print(f"   æ‹¡å¼µå¿…è¦: {requires_expansion}")
        print(f"   æœŸå¾…çµæœ: {'æ‹¡å¼µã‚ã‚Š' if case['expected_improved'] else 'æ‹¡å¼µãªã—'}")
        
        # çµæœåˆ¤å®š
        improved = (generic_expanded != original) or (slot_expanded != original)
        result = "âœ… æˆåŠŸ" if improved == case['expected_improved'] else "âŒ å¤±æ•—"
        print(f"   {result}")
    
    print(f"\nâœ… å¢ƒç•Œæ‹¡å¼µãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ†ã‚¹ãƒˆå®Œäº†")

if __name__ == "__main__":
    test_boundary_expansion()
