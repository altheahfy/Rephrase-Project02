#!/usr/bin/env python3
"""
Pure Stanza Engine v3.1 - Unified Recursive Engine
Rephraseã®çµ±ä¸€å…¥ã‚Œå­æ§‹é€ å®Ÿè£…ç‰ˆ

çµ±ä¸€åˆ†è§£åŸå‰‡:
1. Aux, Vä»¥å¤–ã®8ã‚¹ãƒ­ãƒƒãƒˆï¼ˆM1,S,O1,O2,C1,C2,M2,M3ï¼‰ã¯å†å¸°æ§‹é€ 
2. å„ã‚¹ãƒ­ãƒƒãƒˆãŒåŒã˜10ã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ ã‚’å†…åŒ…
3. çµ±ä¸€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§å…¨éšå±¤ã«å¯¾å¿œ
4. phrase/clauseæ™‚ã®ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆç©ºåŒ–
"""

import stanza
import spacy
import json
from typing import Dict, List, Optional, Tuple, Any

class PureStanzaEngineV31:
    """
    Pure Stanza Engine v3.1 - Unified Recursive Engine
    
    Rephraseã®çµ±ä¸€å…¥ã‚Œå­æ§‹é€ å®Ÿè£…:
    - 8ã¤ã®å†å¸°å¯èƒ½ã‚¹ãƒ­ãƒƒãƒˆ: M1,S,O1,O2,C1,C2,M2,M3
    - 2ã¤ã®èªå½™å°‚ç”¨ã‚¹ãƒ­ãƒƒãƒˆ: Aux,V
    - çµ±ä¸€åˆ†è§£ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹ç„¡é™éšå±¤å¯¾å¿œ
    """
    
    def __init__(self):
        """çµ±ä¸€å†å¸°åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–"""
        print("ğŸš€ çµ±ä¸€å†å¸°åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³ v3.1 åˆæœŸåŒ–ä¸­...")
        
        # Stanza NLP ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        self.nlp = stanza.Pipeline('en', verbose=False)
        print("âœ… Stanzaæº–å‚™å®Œäº†")
        
        # spaCyï¼ˆå¢ƒç•Œèª¿æ•´ç”¨ï¼‰- step18æ±ç”¨ãƒ¡ã‚«ãƒ‹ã‚ºãƒ çµ±åˆ
        self.spacy_nlp = spacy.load("en_core_web_sm")
        print("âœ… spaCyæº–å‚™å®Œäº†")
        
        # step18æ±ç”¨å¢ƒç•Œæ‹¡å¼µè¨­å®š
        self.span_expand_deps = ['det', 'poss', 'compound', 'amod', 'nummod', 'case']
        self.relative_pronoun_deps = ['nsubj', 'dobj', 'pobj']  # é–¢ä¿‚ä»£åè©ã®ä¸€èˆ¬çš„å½¹å‰²
        
        # Rephraseã®çµ±ä¸€ã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ å®šç¾©
        self.RECURSIVE_SLOTS = ['M1', 'S', 'O1', 'O2', 'C1', 'C2', 'M2', 'M3']
        self.WORD_ONLY_SLOTS = ['Aux', 'V']
        self.ALL_SLOTS = self.RECURSIVE_SLOTS + self.WORD_ONLY_SLOTS
        
        # æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«ï¼ˆä¸Šä½ãƒ¬ãƒ™ãƒ«ç”¨ï¼‰
        self.sentence_patterns = self._load_sentence_patterns()
        
        # ã‚µãƒ–ãƒ¬ãƒ™ãƒ«å°‚ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«
        self.sublevel_patterns = self._load_sublevel_patterns()
        
        # ä¿®é£¾èªãƒãƒƒãƒ”ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«ï¼ˆä¸Šä½ãƒ¬ãƒ™ãƒ«ç”¨ï¼‰
        self.modifier_mappings = self._load_modifier_mappings()
        
        # ã‚µãƒ–ãƒ¬ãƒ™ãƒ«å°‚ç”¨ä¿®é£¾èªãƒãƒƒãƒ”ãƒ³ã‚°
        self.sublevel_modifiers = self._load_sublevel_modifiers()
        
        # å…¥ã‚Œå­åˆ¤å®šãƒ«ãƒ¼ãƒ«
        self.nested_triggers = self._load_nested_triggers()
        
        print("ğŸ—ï¸ çµ±ä¸€å†å¸°åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³æº–å‚™å®Œäº† (v3.1)")
        
    def _load_sentence_patterns(self) -> Dict[str, Any]:
        """çµ±ä¸€æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«ï¼ˆä¸Šä½ãƒ¬ãƒ™ãƒ«ç”¨ï¼‰"""
        return {
            # åŸºæœ¬5æ–‡å‹
            "SV": {
                "required_relations": ["nsubj", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "root": "V"}
            },
            "SVC": {
                "required_relations": ["nsubj", "cop", "root"],
                "root_pos": ["ADJ", "NOUN"],
                "mapping": {"nsubj": "S", "cop": "V", "root": "C1"}
            },
            "SVO": {
                "required_relations": ["nsubj", "obj", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "obj": "O1", "root": "V"}
            },
            "SVOO": {
                "required_relations": ["nsubj", "iobj", "obj", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "iobj": "O1", "obj": "O2", "root": "V"}
            },
            "SVOC": {
                "required_relations": ["nsubj", "obj", "xcomp", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "obj": "O1", "xcomp": "C2", "root": "V"}
            },
            # åŠ©å‹•è©æ§‹æ–‡
            "S_AUX_V": {
                "required_relations": ["nsubj", "aux", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "aux": "Aux", "root": "V"}
            },
            # å—å‹•æ…‹
            "PASSIVE": {
                "required_relations": ["nsubj:pass", "aux:pass", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj:pass": "S", "aux:pass": "Aux", "root": "V"}
            }
        }
    
    def _load_sublevel_patterns(self) -> Dict[str, Any]:
        """ã‚µãƒ–ãƒ¬ãƒ™ãƒ«å°‚ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ« - ä¸Šä½ãƒ¬ãƒ™ãƒ«5æ–‡å‹ã«å¯¾å¿œ"""
        return {
            # === åŸºæœ¬å¥æ§‹é€  ===
            "NOUN_PHRASE": {
                "required_relations": ["root"],
                "root_pos": ["NOUN", "PRON"],
                "mapping": {"root": "C1"}
            },
            "ADJ_PHRASE": {
                "required_relations": ["root"],
                "root_pos": ["ADJ"],
                "mapping": {"root": "C1"}
            },
            "ADV_PHRASE": {
                "required_relations": ["root"],
                "root_pos": ["ADV"],
                "mapping": {"root": "M2"}
            },
            "PREP_PHRASE": {
                "required_relations": ["root", "case"],
                "root_pos": ["NOUN"],
                "mapping": {"root": "C1", "case": "M1"}
            },
            
            # === ã‚µãƒ–ãƒ¬ãƒ™ãƒ«5æ–‡å‹ï¼ˆé–¢ä¿‚ç¯€ãƒ»å¾“å±ç¯€å¯¾å¿œï¼‰ ===
            "SUB_SV": {
                "required_relations": ["nsubj", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "root": "V"}
            },
            "SUB_SVC": {
                "required_relations": ["nsubj", "cop", "root"],
                "root_pos": ["ADJ", "NOUN"],
                "mapping": {"nsubj": "S", "cop": "V", "root": "C1"}
            },
            "SUB_SVO": {
                "required_relations": ["nsubj", "obj", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "obj": "O1", "root": "V"}
            },
            "SUB_SVOO": {
                "required_relations": ["nsubj", "iobj", "obj", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "iobj": "O1", "obj": "O2", "root": "V"}
            },
            "SUB_SVOC": {
                "required_relations": ["nsubj", "obj", "xcomp", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "obj": "O1", "xcomp": "C2", "root": "V"}
            },
            
            # === é–¢ä¿‚ä»£åè©ç¯€å°‚ç”¨ ===
            "REL_SUBJ": {
                # "who runs" - é–¢ä¿‚ä»£åè©ãŒä¸»èª
                "required_relations": ["root"],
                "root_pos": ["VERB"],
                "special": "relative_subject",
                "mapping": {"root": "V"}
            },
            "REL_OBJ": {
                # "that he bought" - é–¢ä¿‚ä»£åè©ãŒç›®çš„èª
                "required_relations": ["nsubj", "root"],
                "root_pos": ["VERB"],
                "special": "relative_object", 
                "mapping": {"nsubj": "S", "root": "V"}
            },
            
            # === å¾“å±ç¯€ï¼ˆå‰¯è©ç¯€ï¼‰ ===
            "ADV_CLAUSE": {
                # "although he runs", "when she arrives"
                "required_relations": ["mark", "nsubj", "root"],
                "root_pos": ["VERB"],
                "mapping": {"mark": "M1", "nsubj": "S", "root": "V"}
            },
            
            # === åˆ†è©æ§‹æ–‡ ===
            "PARTICIPLE": {
                # "running fast", "built yesterday"
                "required_relations": ["root"],
                "root_pos": ["VERB"],
                "special": "participial",
                "mapping": {"root": "V"}
            },
            
            # === æ¯”è¼ƒæ§‹æ–‡ ===
            "COMPARATIVE": {
                # "taller than John"
                "required_relations": ["root", "case"],
                "root_pos": ["ADJ"],
                "special": "comparison",
                "mapping": {"root": "C1", "case": "M1"}
            }
        }
    
    def _load_sublevel_modifiers(self) -> Dict[str, str]:
        """ã‚µãƒ–ãƒ¬ãƒ™ãƒ«å°‚ç”¨ä¿®é£¾èªãƒãƒƒãƒ”ãƒ³ã‚° - ä¸Šä½ãƒ¬ãƒ™ãƒ«ç¶²ç¾…å¯¾å¿œ"""
        return {
            # === åŸºæœ¬ä¿®é£¾èª ===
            "det": "M1",        # é™å®šè©: a, the, this, that
            "amod": "M2",       # å½¢å®¹è©ä¿®é£¾: tall, beautiful, red
            "advmod": "M3",     # å‰¯è©ä¿®é£¾: very, quite, extremely  
            "case": "M1",       # å‰ç½®è©: in, on, at, with, by
            "compound": "M2",   # è¤‡åˆèª: New York, high school
            "nummod": "M1",     # æ•°é‡è©: two, many, several
            
            # === é«˜æ¬¡ä¿®é£¾èª ===
            "mark": "M1",       # å¾“å±ç¯€ãƒãƒ¼ã‚«ãƒ¼: that, which, who, because, although
            "cc": "M1",         # ç­‰ä½æ¥ç¶šè©: and, or, but
            "conj": "M2",       # ç­‰ä½è¦ç´ : books and pens ã® "pens"
            "appos": "M2",      # åŒæ ¼: John, the teacher
            "acl": "M2",        # ä¿®é£¾ç¯€: book that I bought
            "acl:relcl": "M2",  # é–¢ä¿‚ä»£åè©ç¯€: man who runs
            
            # === æ™‚åˆ¶ãƒ»ç›¸é–¢é€£ ===
            "aux": "Aux",       # åŠ©å‹•è©: have, be, will, can
            "aux:pass": "Aux",  # å—å‹•åŠ©å‹•è©: be (in "be built")
            "auxpass": "Aux",   # å—å‹•åŠ©å‹•è©ï¼ˆåˆ¥è¡¨è¨˜ï¼‰
            "tmod": "M3",       # æ™‚é–“ä¿®é£¾: yesterday, tomorrow
            
            # === å‰ç½®è©å¥é–¢é€£ ===
            "pobj": "C1",       # å‰ç½®è©ã®ç›®çš„èª: in the garden ã® "garden"
            "pcomp": "C1",      # å‰ç½®è©è£œèª
            "agent": "M1",      # å‹•ä½œä¸»: by him (å—å‹•æ…‹)
            
            # === æ¯”è¼ƒãƒ»ç¨‹åº¦ ===
            "neg": "M3",        # å¦å®š: not, never
            "expl": "M3",       # è™šè¾: there (in "there is")
            "dep": "M3",        # ãã®ä»–ä¾å­˜èª
            "parataxis": "M2",  # ä¸¦åˆ—æ–‡: He said, "Hello"
            
            # === ç–‘å•ãƒ»é–¢ä¿‚èª ===
            "nsubj:xsubj": "S", # ç–‘å•ä¸»èª
            "ccomp": "O1",      # è£œæ–‡: think that he runs
            "xcomp": "C2",      # åˆ¶å¾¡è£œæ–‡: want to go
        }
    
    def _load_modifier_mappings(self) -> Dict[str, str]:
        """ä¿®é£¾èªã‚¹ãƒ­ãƒƒãƒˆãƒãƒƒãƒ”ãƒ³ã‚°"""
        return {
            "advmod": "M2",     # å‰¯è©ä¿®é£¾èª
            "amod": "nested",   # å½¢å®¹è©ä¿®é£¾èª â†’ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ
            "det": "nested",    # é™å®šè© â†’ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ
            "case": "nested",   # å‰ç½®è© â†’ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ
            "nmod": "M1",       # åè©ä¿®é£¾èª
            "mark": "nested",   # å¾“å±ç¯€ãƒãƒ¼ã‚«ãƒ¼ â†’ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ
        }
    
    def _load_nested_triggers(self) -> Dict[str, str]:
        """å…¥ã‚Œå­åˆ†è§£ãƒˆãƒªã‚¬ãƒ¼æ¡ä»¶"""
        return {
            "phrase": "multi_word",      # è¤‡æ•°èªå¥
            "clause": "has_verb",        # å‹•è©ã‚’å«ã‚€å¥
            "complex": "has_modifiers"   # ä¿®é£¾èªã‚’å«ã‚€å¥
        }
    
    def decompose_unified(self, text: str, depth: int = 0) -> Dict[str, Any]:
        """
        çµ±ä¸€åˆ†è§£ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  - Rephraseã®æ ¸å¿ƒå®Ÿè£…
        éšå±¤ã«å¿œã˜ã¦ä¸Šä½ãƒ¬ãƒ™ãƒ«/ã‚µãƒ–ãƒ¬ãƒ™ãƒ«å°‚ç”¨å‡¦ç†ã‚’é©ç”¨
        
        Args:
            text: åˆ†è§£å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            depth: å†å¸°æ·±åº¦ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            
        Returns:
            çµ±ä¸€10ã‚¹ãƒ­ãƒƒãƒˆæ§‹é€  + ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ
        """
        print(f"{'  ' * depth}ğŸ” çµ±ä¸€åˆ†è§£é–‹å§‹: '{text[:30]}...' (depth={depth})")
        
        # Stanzaæ§‹æ–‡è§£æ
        doc = self.nlp(text)
        sent = doc.sentences[0]
        
        # ROOTèªæ¤œå‡ºï¼ˆéšå±¤ã«é–¢ä¿‚ãªãå®Ÿè¡Œï¼‰
        root_word = self._find_root_word(sent)
        if not root_word:
            print(f"{'  ' * depth}âš ï¸ ROOTèªæœªæ¤œå‡º")
            return {"error": "No root word found"}
        
        print(f"{'  ' * depth}ğŸ¯ ROOT: '{root_word.text}' ({root_word.pos})")
        
        # é–¢ä¿‚ç¯€ã‚’å«ã‚€åè©å¥ã®ç‰¹åˆ¥å‡¦ç†
        has_relative_clause = any(w.deprel == 'acl:relcl' for w in sent.words)
        
        # éšå±¤åˆ¤å®šï¼šä¸Šä½ãƒ¬ãƒ™ãƒ« vs ã‚µãƒ–ãƒ¬ãƒ™ãƒ«
        is_sublevel = depth > 0
        
        if has_relative_clause and root_word.pos == 'NOUN' and depth == 0:
            print(f"{'  ' * depth}ğŸ“– é–¢ä¿‚ç¯€ã‚’å«ã‚€åè©å¥ã¨ã—ã¦å‡¦ç†")
            basic_slots = self._extract_noun_phrase_with_relative_clause(sent, root_word, depth)
        elif is_sublevel:
            # ã‚µãƒ–ãƒ¬ãƒ™ãƒ«å°‚ç”¨å‡¦ç†
            print(f"{'  ' * depth}ğŸ“Š ã‚µãƒ–ãƒ¬ãƒ™ãƒ«å‡¦ç†é©ç”¨")
            basic_slots = self._extract_sublevel_slots(sent, root_word, depth)
        else:
            # ä¸Šä½ãƒ¬ãƒ™ãƒ«å‡¦ç†ï¼ˆå¾“æ¥é€šã‚Šï¼‰
            print(f"{'  ' * depth}ğŸ“Š ä¸Šä½ãƒ¬ãƒ™ãƒ«å‡¦ç†é©ç”¨")
            basic_slots = self._extract_basic_slots(sent, root_word, depth)
        
        # çµ±ä¸€å…¥ã‚Œå­å‡¦ç†ï¼š8ã¤ã®å†å¸°å¯èƒ½ã‚¹ãƒ­ãƒƒãƒˆã§å†å¸°é©ç”¨
        unified_result = self._apply_unified_nesting(basic_slots, depth)
        
        print(f"{'  ' * depth}ğŸ“‹ çµ±ä¸€åˆ†è§£å®Œäº†: {len([k for k, v in unified_result.items() if k != 'metadata'])}ã‚¹ãƒ­ãƒƒãƒˆ")
        
        return unified_result
    
    def _apply_unified_nesting(self, slots: Dict[str, Any], depth: int) -> Dict[str, Any]:
        """çµ±ä¸€å…¥ã‚Œå­é©ç”¨ï¼ˆRephraseåŸå‰‡ï¼šä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆç©ºåŒ– + ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆé…ç½®ï¼‰"""
        unified_result = {}
        
        for slot_key, slot_data in slots.items():
            if slot_key.startswith('_') or slot_key == 'metadata':
                unified_result[slot_key] = slot_data
                continue
            
            if not slot_data or not slot_data.get('content'):
                continue
            
            content = slot_data['content']
            
            # å¢ƒç•Œæ‹¡å¼µé©ç”¨ï¼ˆstep18æ±ç”¨ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ï¼‰
            expanded_content = self._expand_span_generic(content, {
                'slot_type': slot_key,
                'depth': depth
            })
            
            # å…¥ã‚Œå­åˆ¤å®šï¼ˆspaCyå¼·åŒ–ç‰ˆï¼‰
            if self._needs_nesting(expanded_content, slot_data):
                # ã€RephraseåŸå‰‡ã€‘ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã‚’ç©ºã«è¨­å®š
                unified_result[slot_key] = ''
                
                print(f"{'  ' * depth}ğŸ”„ '{slot_key}' â†’ ã‚µãƒ–ãƒ¬ãƒ™ãƒ«åˆ†è§£: '{expanded_content[:20]}...'")
                
                # ã‚µãƒ–ãƒ¬ãƒ™ãƒ«è§£æï¼ˆå†å¸°å®Ÿè¡Œï¼‰
                sublevel_data = self.decompose_unified(expanded_content, depth + 1)
                
                # ã€RephraseåŸå‰‡ã€‘å…¨å†…å®¹ã‚’ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®
                for sub_key, sub_value in sublevel_data.items():
                    if sub_key == 'metadata' or sub_key.startswith('_'):
                        continue  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯ã‚¹ã‚­ãƒƒãƒ—
                    
                    # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã¨ã—ã¦é…ç½®ï¼ˆsub-ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ãï¼‰
                    final_key = f"sub-{sub_key.lower()}"
                    # å€¤ã‚’æ–‡å­—åˆ—ã¨ã—ã¦æ ¼ç´ï¼ˆRephraseåŸå‰‡ï¼‰
                    if isinstance(sub_value, dict) and 'content' in sub_value:
                        unified_result[final_key] = sub_value['content']
                    elif isinstance(sub_value, dict):
                        # è¾æ›¸ã®å ´åˆã€ä¸»è¦ãªå€¤ã‚’æŠ½å‡º
                        main_value = next(iter(sub_value.values())) if sub_value else ''
                        unified_result[final_key] = main_value
                    else:
                        unified_result[final_key] = sub_value
                    
                    print(f"{'  ' * depth}  â†³ {final_key}: '{unified_result[final_key]}'")
            else:
                # å˜èªãƒ¬ãƒ™ãƒ«ï¼šãã®ã¾ã¾é…ç½®
                unified_result[slot_key] = expanded_content
                print(f"{'  ' * depth}ğŸ“ '{slot_key}': '{expanded_content}' (å˜èªãƒ¬ãƒ™ãƒ«)")
        
        return unified_result
    
    def _find_root_word(self, sent) -> Optional[Any]:
        """ROOTèªæ¤œå‡ºï¼ˆéšå±¤å…±é€šï¼‰- å‹•è©ä»¥å¤–ã‚‚å¯¾å¿œ"""
        for word in sent.words:
            if word.deprel == 'root':
                return word
        return None
    
    def _find_root_verb(self, sent) -> Optional[Any]:
        """ROOTå‹•è©æ¤œå‡ºï¼ˆä¸Šä½ãƒ¬ãƒ™ãƒ«äº’æ›ç”¨ï¼‰"""
        return self._find_root_word(sent)
    
    def _extract_sublevel_slots(self, sent, root_word, depth: int) -> Dict[str, Any]:
        """ã‚µãƒ–ãƒ¬ãƒ™ãƒ«å°‚ç”¨ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º - Stanzaå…¥ã‚Œå­æ§‹é€ å¯¾å¿œ"""
        # å…¨ä¾å­˜é–¢ä¿‚ã‚’åé›†
        all_relations = {}
        for word in sent.words:
            all_relations[word.deprel] = word
        
        # ã‚µãƒ–ãƒ¬ãƒ™ãƒ«å°‚ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        matched_pattern = self._match_sublevel_pattern(sent, root_word, depth)
        
        if not matched_pattern:
            print(f"{'  ' * depth}âŒ ã‚µãƒ–ãƒ¬ãƒ™ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³æœªæ¤œå‡º")
            return {}
        
        print(f"{'  ' * depth}âœ… ã‚µãƒ–ãƒãƒƒãƒãƒ‘ã‚¿ãƒ¼ãƒ³: {matched_pattern}")
        
        # ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º
        slots = {}
        
        # ã‚µãƒ–ãƒ¬ãƒ™ãƒ«å°‚ç”¨ã‚³ã‚¢ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º
        self._extract_sublevel_core_slots(sent, root_word, slots, depth)
        
        # ã‚µãƒ–ãƒ¬ãƒ™ãƒ«å°‚ç”¨ä¿®é£¾èªå‡¦ç†
        self._process_sublevel_modifiers(sent, slots, depth)
        
        return slots
    
    def _match_sublevel_pattern(self, sent, root_word, depth: int) -> Optional[str]:
        """ã‚µãƒ–ãƒ¬ãƒ™ãƒ«å°‚ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚° - 5æ–‡å‹å¯¾å¿œå¼·åŒ–ç‰ˆ"""
        relations = {word.deprel: word for word in sent.words}
        root_pos = root_word.pos
        
        # === 1. å®Œå…¨5æ–‡å‹åˆ¤å®šï¼ˆé–¢ä¿‚ç¯€ãƒ»å¾“å±ç¯€ï¼‰ ===
        if 'nsubj' in relations and root_pos == 'VERB':
            # é–¢ä¿‚ä»£åè©ç¯€åˆ¤å®š
            if 'mark' in relations:
                mark_word = relations['mark'].text.lower()
                if mark_word in ['that', 'which', 'who', 'whom', 'whose']:
                    if 'obj' in relations and 'iobj' in relations:
                        return "SUB_SVOO"  # é–¢ä¿‚ç¯€å†…ã®ç¬¬4æ–‡å‹
                    elif 'obj' in relations and 'xcomp' in relations:
                        return "SUB_SVOC"  # é–¢ä¿‚ç¯€å†…ã®ç¬¬5æ–‡å‹
                    elif 'obj' in relations:
                        return "SUB_SVO"   # é–¢ä¿‚ç¯€å†…ã®ç¬¬3æ–‡å‹
                    else:
                        return "SUB_SV"    # é–¢ä¿‚ç¯€å†…ã®ç¬¬1æ–‡å‹
            
            # å¾“å±ç¯€åˆ¤å®š
            elif 'mark' in relations:
                mark_word = relations['mark'].text.lower()
                if mark_word in ['because', 'although', 'when', 'if', 'since', 'while']:
                    return "ADV_CLAUSE"  # å‰¯è©ç¯€
            
            # é€šå¸¸ã®ã‚µãƒ–ãƒ¬ãƒ™ãƒ«5æ–‡å‹
            elif 'obj' in relations and 'iobj' in relations:
                return "SUB_SVOO"
            elif 'obj' in relations and 'xcomp' in relations:
                return "SUB_SVOC"
            elif 'obj' in relations:
                return "SUB_SVO"
            else:
                return "SUB_SV"
        
        # === 2. SVCç³»ï¼ˆé–¢ä¿‚ç¯€å¯¾å¿œï¼‰ ===
        if 'nsubj' in relations and 'cop' in relations:
            return "SUB_SVC"
        
        # === 3. å‰ç½®è©å¥åˆ¤å®š ===
        if 'case' in relations and root_pos == 'NOUN':
            case_word = relations['case'].text.lower()
            if case_word in ['than', 'as']:  # æ¯”è¼ƒæ§‹æ–‡
                return "COMPARATIVE"
            else:
                return "PREP_PHRASE"
        
        # === 4. é–¢ä¿‚ä»£åè©ç‰¹æ®Šå½¢ï¼ˆä¸»èªçœç•¥ï¼‰ ===
        if root_pos == 'VERB' and 'nsubj' not in relations:
            # "who runs" ã®ã‚ˆã†ãªé–¢ä¿‚ä»£åè©ãŒä¸»èªã®å ´åˆ
            return "REL_SUBJ"
        
        # === 5. åˆ†è©æ§‹æ–‡åˆ¤å®š ===
        if root_pos == 'VERB' and root_word.text.endswith(('ing', 'ed', 'en')):
            return "PARTICIPLE"
        
        # === 6. åŸºæœ¬å¥åˆ¤å®šï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰ ===
        if root_pos in ["NOUN", "PRON"]:
            return "NOUN_PHRASE"
        elif root_pos == "ADJ":
            # æ¯”è¼ƒç´šãƒ»æœ€ä¸Šç´šåˆ¤å®š
            if any(word.deprel == 'case' for word in sent.words):
                return "COMPARATIVE"
            else:
                return "ADJ_PHRASE"
        elif root_pos == "ADV":
            return "ADV_PHRASE"
        
        return "NOUN_PHRASE"  # æœ€çµ‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def _extract_sublevel_core_slots(self, sent, root_word, slots: Dict[str, Any], depth: int):
        """ã‚µãƒ–ãƒ¬ãƒ™ãƒ«å°‚ç”¨ã‚³ã‚¢ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        # ROOTèªã‚’é©åˆ‡ãªã‚¹ãƒ­ãƒƒãƒˆã«ãƒãƒƒãƒ”ãƒ³ã‚°
        root_pos = root_word.pos
        
        if root_pos in ["NOUN", "PRON"]:
            slot_name = "C1"  # è£œèªã¨ã—ã¦æ‰±ã†
        elif root_pos == "ADJ":
            slot_name = "C1"  # å½¢å®¹è©è£œèª
        elif root_pos == "ADV":
            slot_name = "M2"  # å‰¯è©ä¿®é£¾èª
        elif root_pos == "VERB":
            slot_name = "V"   # å‹•è©
        else:
            slot_name = "C1"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        slots[slot_name] = {
            "content": root_word.text,
            "pos": root_word.pos,
            "deprel": root_word.deprel,
            "word_obj": root_word
        }
        
        print(f"{'  ' * depth}ğŸ“ {slot_name}: '{root_word.text}' (ã‚µãƒ–ãƒ¬ãƒ™ãƒ«ROOT)")
    
    def _process_sublevel_modifiers(self, sent, slots: Dict[str, Any], depth: int):
        """ã‚µãƒ–ãƒ¬ãƒ™ãƒ«å°‚ç”¨ä¿®é£¾èªå‡¦ç†"""
        for word in sent.words:
            if word.deprel in self.sublevel_modifiers:
                slot_mapping = self.sublevel_modifiers[word.deprel]
                
                # ã‚µãƒ–ãƒ¬ãƒ™ãƒ«ã§ã¯ç›´æ¥ã‚¹ãƒ­ãƒƒãƒˆãƒãƒƒãƒ”ãƒ³ã‚°
                slots[slot_mapping] = {
                    "content": word.text,
                    "pos": word.pos,
                    "deprel": word.deprel,
                    "word_obj": word
                }
                print(f"{'  ' * depth}ğŸ“ {slot_mapping}: '{word.text}' (ã‚µãƒ–ãƒ¬ãƒ™ãƒ«ä¿®é£¾èª: {word.deprel})")
    
    def _extract_basic_slots(self, sent, root_verb, depth: int) -> Dict[str, Any]:
        """åŸºæœ¬10ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡ºï¼ˆé–¢ä¿‚ä»£åè©ç¯€åˆ†é›¢å¯¾å¿œç‰ˆï¼‰"""
        # é–¢ä¿‚ä»£åè©ç¯€ã‚’äº‹å‰ã«åˆ†é›¢
        main_words, relative_clauses = self._separate_relative_clauses(sent, depth)
        
        # å…¨ä¾å­˜é–¢ä¿‚ã‚’åé›†ï¼ˆãƒ¡ã‚¤ãƒ³ç¯€ã®ã¿ï¼‰
        all_relations = {}
        for word in main_words:
            all_relations[word.deprel] = word
        
        # æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼ˆãƒ¡ã‚¤ãƒ³ç¯€ã§å®Ÿè¡Œï¼‰
        matched_pattern = self._match_sentence_pattern_enhanced(sent, root_verb)
        
        if not matched_pattern:
            print(f"{'  ' * depth}âŒ æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³æœªæ¤œå‡º")
            return {}
        
        print(f"{'  ' * depth}âœ… ãƒãƒƒãƒã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³: {matched_pattern}")
        
        # ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º
        slots = {}
        
        # åŸºæœ¬ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡ºï¼ˆãƒ¡ã‚¤ãƒ³ç¯€ã®ã¿ï¼‰
        self._extract_core_slots(sent, root_verb, slots, depth, main_words)
        
        # ä¿®é£¾èªå‡¦ç†ï¼ˆãƒ¡ã‚¤ãƒ³ç¯€ï¼‰
        self._process_modifiers(sent, slots, depth, main_words)
        
        # é–¢ä¿‚ä»£åè©ç¯€ã‚’é©åˆ‡ãªã‚¹ãƒ­ãƒƒãƒˆã«çµ±åˆ
        self._integrate_relative_clauses(relative_clauses, slots, depth)
        
        return slots
    
    def _separate_relative_clauses(self, sent, depth: int) -> Tuple[List, Dict]:
        """é–¢ä¿‚ä»£åè©ç¯€ã‚’åˆ†é›¢ã—ã¦ãƒ¡ã‚¤ãƒ³ç¯€ã¨é–¢ä¿‚ç¯€ã«åˆ†å‰²"""
        main_words = []
        relative_clauses = {}
        
        # é–¢ä¿‚ä»£åè©ç¯€ã®æ¤œå‡º
        rel_verbs = [w for w in sent.words if w.deprel == 'acl:relcl']
        
        if not rel_verbs:
            # é–¢ä¿‚ç¯€ãŒãªã„å ´åˆã€å…¨ã¦ã®èªã‚’ãƒ¡ã‚¤ãƒ³ç¯€ã¨ã—ã¦è¿”ã™
            return list(sent.words), {}
        
        for rel_verb in rel_verbs:
            # ã“ã®é–¢ä¿‚ç¯€ã«å±ã™ã‚‹èªã‚’åé›†
            rel_words = [rel_verb]  # é–¢ä¿‚ç¯€å‹•è©
            
            # é–¢ä¿‚ç¯€å‹•è©ã«ä¾å­˜ã™ã‚‹èªã‚’åé›†
            for word in sent.words:
                if word.head == rel_verb.id:
                    rel_words.append(word)
            
            # ä¿®é£¾ã•ã‚Œã‚‹åè©ã‚’ç‰¹å®š
            modified_noun_id = rel_verb.head
            modified_noun = next((w for w in sent.words if w.id == modified_noun_id), None)
            
            if modified_noun:
                relative_clauses[modified_noun.id] = {
                    'modified_noun': modified_noun,
                    'rel_verb': rel_verb,
                    'rel_words': rel_words,
                    'clause_text': ' '.join(w.text for w in sorted(rel_words, key=lambda x: x.id))
                }
                
                print(f"{'  ' * depth}ğŸ”— é–¢ä¿‚ç¯€æ¤œå‡º: '{modified_noun.text}' â† '{relative_clauses[modified_noun.id]['clause_text']}'")
        
        # ãƒ¡ã‚¤ãƒ³ç¯€ã®èªã‚’æŠ½å‡ºï¼ˆé–¢ä¿‚ç¯€ã«å±ã•ãªã„èªï¼‰
        rel_word_ids = set()
        for rel_info in relative_clauses.values():
            rel_word_ids.update(w.id for w in rel_info['rel_words'])
        
        main_words = [w for w in sent.words if w.id not in rel_word_ids]
        
        return main_words, relative_clauses
    
    def _integrate_relative_clauses(self, relative_clauses: Dict, slots: Dict[str, Any], depth: int):
        """é–¢ä¿‚ä»£åè©ç¯€ã‚’é©åˆ‡ãªã‚¹ãƒ­ãƒƒãƒˆã®ã‚µãƒ–æ§‹é€ ã¨ã—ã¦çµ±åˆ"""
        for modified_noun_id, rel_info in relative_clauses.items():
            modified_noun = rel_info['modified_noun']
            clause_text = rel_info['clause_text']
            
            # ä¿®é£¾ã•ã‚Œã‚‹åè©ãŒã©ã®ã‚¹ãƒ­ãƒƒãƒˆã«å«ã¾ã‚Œã‚‹ã‹ã‚’ç‰¹å®š
            target_slot = self._find_slot_containing_noun(modified_noun, slots)
            
            if target_slot:
                print(f"{'  ' * depth}ğŸ¯ é–¢ä¿‚ç¯€çµ±åˆ: {target_slot}ã‚¹ãƒ­ãƒƒãƒˆå†…ã®'{modified_noun.text}'ã«'{clause_text}'ã‚’è¿½åŠ ")
                
                # ãã®ã‚¹ãƒ­ãƒƒãƒˆã«é–¢ä¿‚ç¯€ãƒãƒ¼ã‚«ãƒ¼ã‚’è¿½åŠ 
                if 'relative_clauses' not in slots[target_slot]:
                    slots[target_slot]['relative_clauses'] = []
                
                slots[target_slot]['relative_clauses'].append({
                    'clause_text': clause_text,
                    'modified_noun': modified_noun.text
                })
    
    def _find_slot_containing_noun(self, noun, slots: Dict[str, Any]) -> Optional[str]:
        """æŒ‡å®šã—ãŸåè©ã‚’å«ã‚€ã‚¹ãƒ­ãƒƒãƒˆã‚’ç‰¹å®š"""
        for slot_name, slot_data in slots.items():
            if isinstance(slot_data, dict) and 'content' in slot_data:
                if noun.text in slot_data['content']:
                    return slot_name
        return None
    
    def _match_sentence_pattern_enhanced(self, sent, root_verb) -> Optional[str]:
        """å¼·åŒ–ã•ã‚ŒãŸæ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°"""
        relations = {word.deprel: word for word in sent.words}
        
        # SVOç³»ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è©³ç´°åˆ¤å®š
        if 'nsubj' in relations and root_verb.pos == 'VERB':
            if 'obj' in relations and 'iobj' in relations:
                return "SVOO"  # ç¬¬4æ–‡å‹
            elif 'obj' in relations and 'xcomp' in relations:
                return "SVOC"  # ç¬¬5æ–‡å‹ 
            elif 'obj' in relations:
                return "SVO"   # ç¬¬3æ–‡å‹
            elif 'aux' in relations:
                return "S_AUX_V"  # åŠ©å‹•è©æ§‹æ–‡
            else:
                return "SV"    # ç¬¬1æ–‡å‹
        
        # SVCç³»ãƒ‘ã‚¿ãƒ¼ãƒ³
        if 'nsubj' in relations and 'cop' in relations:
            return "SVC"  # ç¬¬2æ–‡å‹
        
        # å—å‹•æ…‹
        if 'nsubj:pass' in relations and 'aux:pass' in relations:
            return "PASSIVE"
        
        return "SV"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def _extract_core_slots(self, sent, root_verb, slots: Dict[str, Any], depth: int, target_words: List = None):
        """ã‚³ã‚¢ã‚¹ãƒ­ãƒƒãƒˆã®æŠ½å‡ºï¼ˆSVOOé‡è¤‡å•é¡Œä¿®æ­£ç‰ˆ + é–¢ä¿‚ç¯€åˆ†é›¢å¯¾å¿œï¼‰"""
        # å‡¦ç†å¯¾è±¡ã®èªã‚’æ±ºå®š
        words_to_process = target_words if target_words is not None else sent.words
        
        # æ–‡å‹åˆ¤å®š
        relations = {word.deprel: word for word in words_to_process}
        is_svoo = 'iobj' in relations and 'obj' in relations
        is_svo = 'obj' in relations and 'iobj' not in relations
        
        for word in words_to_process:
            slot_name = None
            
            # ä¸»èªç³»
            if word.deprel == 'nsubj' or word.deprel == 'nsubj:pass':
                slot_name = 'S'
                # è¤‡åˆä¸»èªã®å ´åˆã®å‡¦ç†
                subject_text = self._extract_compound_phrase(sent, word, target_words)
                word_text = subject_text if subject_text != word.text else word.text
            # ç›®çš„èªç³»ï¼ˆæ–‡å‹ã«å¿œã˜ã¦é©åˆ‡ã«åˆ†é¡ï¼‰
            elif word.deprel == 'iobj':
                slot_name = 'O1'  # é–“æ¥ç›®çš„èª â†’ å¸¸ã«O1
                word_text = word.text
            elif word.deprel == 'obj':
                if is_svoo:
                    slot_name = 'O2'  # SVOOæ§‹æ–‡ï¼šç›´æ¥ç›®çš„èª â†’ O2
                else:
                    slot_name = 'O1'  # SVOæ§‹æ–‡ï¼šç›®çš„èª â†’ O1
                word_text = self._extract_compound_phrase(sent, word, target_words)
            # è£œèªç³»
            elif word.deprel == 'xcomp':
                slot_name = 'C2'
                word_text = word.text
            elif word.deprel == 'root' and word.pos in ['ADJ', 'NOUN']:
                slot_name = 'C1'
                word_text = word.text
            # å‹•è©ç³»
            elif word.deprel == 'root' and word.pos == 'VERB':
                slot_name = 'V'
                word_text = word.text
            elif word.deprel == 'cop':
                slot_name = 'V'
                word_text = word.text
            elif word.deprel == 'aux' or word.deprel == 'aux:pass':
                slot_name = 'Aux'
                word_text = word.text
            else:
                continue
            
            if slot_name:
                slots[slot_name] = {
                    "content": word_text,
                    "pos": word.pos,
                    "deprel": word.deprel,
                    "word_obj": word
                }
                
                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã«æ–‡å‹æƒ…å ±ã‚’è¿½åŠ 
                pattern_info = ""
                if is_svoo and word.deprel in ['iobj', 'obj']:
                    pattern_info = f" [SVOO: {'é–“æ¥' if word.deprel == 'iobj' else 'ç›´æ¥'}ç›®çš„èª]"
                elif is_svo and word.deprel == 'obj':
                    pattern_info = " [SVO: ç›®çš„èª]"
                
                print(f"{'  ' * depth}ğŸ“ {slot_name}: '{word_text}' (deprel: {word.deprel}){pattern_info}")
    
    def _extract_compound_phrase(self, sent, head_word, target_words: List = None) -> str:
        """è¤‡åˆå¥ã®æŠ½å‡ºï¼ˆspaCyå¢ƒç•Œèª¿æ•´çµ±åˆç‰ˆï¼‰- step18æ±ç”¨ãƒ¡ã‚«ãƒ‹ã‚ºãƒ é©ç”¨"""
        # å‡¦ç†å¯¾è±¡ã®èªã‚’æ±ºå®š
        words_to_check = target_words if target_words is not None else sent.words
        
        # Stanzaãƒ™ãƒ¼ã‚¹ã®åŸºæœ¬æŠ½å‡º
        phrase_words = [head_word]
        
        # ä¾å­˜èªã‚’åé›†ï¼ˆå‡¦ç†å¯¾è±¡ã®èªã®ã¿ã‹ã‚‰ï¼‰
        for word in words_to_check:
            if word.head == head_word.id and word.deprel in ['det', 'amod', 'compound', 'case']:
                phrase_words.append(word)
        
        # spaCyå¢ƒç•Œèª¿æ•´ã‚’é©ç”¨
        basic_phrase = ' '.join(w.text for w in sorted(phrase_words, key=lambda x: x.id))
        
        # spaCyã§ã®ç²¾å¯†å¢ƒç•Œèª¿æ•´
        adjusted_phrase = self._apply_spacy_boundary_adjustment(basic_phrase, sent)
        
        return adjusted_phrase
    
    def _apply_spacy_boundary_adjustment(self, phrase_text: str, stanza_sent) -> str:
        """spaCyå¢ƒç•Œèª¿æ•´ï¼ˆstep18æ±ç”¨ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ï¼‰"""
        try:
            # spaCyã§è§£æ
            spacy_doc = self.spacy_nlp(phrase_text)
            
            # å¢ƒç•Œèª¿æ•´ãƒ­ã‚¸ãƒƒã‚¯é©ç”¨
            adjusted_tokens = []
            for token in spacy_doc:
                # åŸºæœ¬ãƒˆãƒ¼ã‚¯ãƒ³è¿½åŠ 
                adjusted_tokens.append(token.text)
                
                # é–¢ä¿‚ä»£åè©ã®å¢ƒç•Œèª¿æ•´
                if token.dep_ in ['relcl', 'acl'] and token.pos_ == 'VERB':
                    # é–¢ä¿‚ä»£åè©ã‚’å«ã‚€å¢ƒç•Œæ‹¡å¼µ
                    rel_pronouns = self._find_relative_pronouns(token, spacy_doc)
                    adjusted_tokens.extend(rel_pronouns)
            
            return ' '.join(adjusted_tokens)
            
        except Exception as e:
            # spaCyå‡¦ç†å¤±æ•—æ™‚ã¯Stanzaãƒ™ãƒ¼ã‚¹ã®çµæœã‚’è¿”ã™
            print(f"âš ï¸ spaCyå¢ƒç•Œèª¿æ•´å¤±æ•—: {e}")
            return phrase_text
    
    def _find_relative_pronouns(self, rel_verb_token, spacy_doc) -> List[str]:
        """é–¢ä¿‚ä»£åè©æ¤œå‡ºï¼ˆæ±ç”¨ï¼‰- step18ãƒ¡ã‚«ãƒ‹ã‚ºãƒ é©ç”¨"""
        rel_pronouns = []
        
        # é–¢ä¿‚å‹•è©ã®å­è¦ç´ ã‹ã‚‰é–¢ä¿‚ä»£åè©ã‚’æ¢ã™
        for child in rel_verb_token.children:
            if child.pos_ == 'PRON' and child.dep_ in self.relative_pronoun_deps:
                # é–¢ä¿‚ä»£åè©ã®ä¸€èˆ¬çš„ãƒ‘ã‚¿ãƒ¼ãƒ³
                if child.text.lower() in ['who', 'whom', 'whose', 'which', 'that']:
                    rel_pronouns.append(child.text)
        
        return rel_pronouns
    
    def _match_sentence_pattern(self, relations: List[str], root_pos: str) -> Optional[str]:
        """æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°"""
        for pattern_name, pattern_info in self.sentence_patterns.items():
            required = set(pattern_info["required_relations"])
            available = set(relations)
            
            if required.issubset(available) and root_pos in pattern_info["root_pos"]:
                return pattern_name
        
        return None
    
    def _process_modifiers(self, sent, slots: Dict[str, Any], depth: int, target_words: List = None):
        """ä¿®é£¾èªå‡¦ç†ï¼ˆçµ±ä¸€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰é–¢ä¿‚ç¯€åˆ†é›¢å¯¾å¿œç‰ˆ"""
        # å‡¦ç†å¯¾è±¡ã®èªã‚’æ±ºå®š
        words_to_process = target_words if target_words is not None else sent.words
        
        for word in words_to_process:
            if word.deprel in self.modifier_mappings:
                slot_mapping = self.modifier_mappings[word.deprel]
                
                if slot_mapping == "nested":
                    # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå€™è£œã¨ã—ã¦è¨˜éŒ²
                    if "modifiers" not in slots:
                        slots["modifiers"] = []
                    slots["modifiers"].append({
                        "word": word.text,
                        "deprel": word.deprel,
                        "head_id": word.head
                    })
                elif slot_mapping in self.ALL_SLOTS:
                    # ç›´æ¥ã‚¹ãƒ­ãƒƒãƒˆãƒãƒƒãƒ”ãƒ³ã‚°
                    slots[slot_mapping] = {
                        "content": word.text,
                        "pos": word.pos,
                        "deprel": word.deprel
                    }
                    print(f"{'  ' * depth}ğŸ“ {slot_mapping}: '{word.text}' (ä¿®é£¾èª: {word.deprel})")
    
    def _apply_unified_nesting(self, basic_slots: Dict[str, Any], depth: int) -> Dict[str, Any]:
        """
        çµ±ä¸€å…¥ã‚Œå­å‡¦ç† - Rephraseã®æ ¸å¿ƒå®Ÿè£…
        8ã¤ã®å†å¸°å¯èƒ½ã‚¹ãƒ­ãƒƒãƒˆã«çµ±ä¸€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é©ç”¨
        """
        result = {}
        
        for slot_name in self.ALL_SLOTS:
            if slot_name in basic_slots:
                slot_data = basic_slots[slot_name]
                
                if slot_name in self.RECURSIVE_SLOTS:
                    # å†å¸°å¯èƒ½ã‚¹ãƒ­ãƒƒãƒˆï¼šå…¥ã‚Œå­åˆ¤å®šã—ã¦å†å¸°é©ç”¨
                    result[slot_name] = self._process_recursive_slot(slot_data, slot_name, depth)
                else:
                    # èªå½™å°‚ç”¨ã‚¹ãƒ­ãƒƒãƒˆï¼šãã®ã¾ã¾ä¿æŒ
                    result[slot_name] = {slot_name.lower(): slot_data["content"]}
        
        return result
    
    def _process_recursive_slot(self, slot_data: Dict[str, Any], slot_name: str, depth: int) -> Dict[str, Any]:
        """
        å†å¸°å¯èƒ½ã‚¹ãƒ­ãƒƒãƒˆå‡¦ç†
        å…¥ã‚Œå­æ¡ä»¶ã‚’æº€ãŸã™å ´åˆã¯çµ±ä¸€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å†å¸°é©ç”¨
        """
        content = slot_data["content"]
        
        # å…¥ã‚Œå­åˆ¤å®š
        if self._needs_nesting(content, slot_data):
            print(f"{'  ' * depth}ğŸ“ {slot_name}ã‚¹ãƒ­ãƒƒãƒˆ: '{content}' â†’ çµ±ä¸€å†å¸°åˆ†è§£")
            
            # çµ±ä¸€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å†å¸°é©ç”¨
            subslot_result = self.decompose_unified(content, depth + 1)
            
            if "error" not in subslot_result:
                # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆç©ºåŒ–ï¼ˆRephraseãƒ«ãƒ¼ãƒ«ï¼‰
                return {
                    "content": "",  # ä¸Šä½ã‚’ç©ºåŒ–
                    "subslots": subslot_result  # çµ±ä¸€åˆ†è§£çµæœã‚’æ ¼ç´
                }
        
        # å˜èªãƒ¬ãƒ™ãƒ«ï¼šãã®ã¾ã¾ä¿æŒ
        return {slot_name.lower(): content}
    
    def _needs_nesting(self, content: str, slot_data: Dict[str, Any]) -> bool:
        """å…¥ã‚Œå­åˆ†è§£åˆ¤å®šï¼ˆspaCyå¼·åŒ–ç‰ˆï¼‰"""
        # è¤‡æ•°èªå¥åˆ¤å®š
        if len(content.split()) > 1:
            return True
        
        # spaCyã§ã®è©³ç´°åˆ†æ
        try:
            spacy_doc = self.spacy_nlp(content)
            
            # é–¢ä¿‚ç¯€ãƒ»å¾“å±ç¯€æ¤œå‡º
            for token in spacy_doc:
                if token.dep_ in ['relcl', 'acl', 'advcl', 'ccomp', 'xcomp']:
                    return True
                    
            # è¤‡åˆä¿®é£¾èªæ¤œå‡º
            modifier_count = sum(1 for token in spacy_doc if token.dep_ in self.span_expand_deps)
            if modifier_count > 1:
                return True
                
        except Exception:
            pass  # spaCyå‡¦ç†å¤±æ•—æ™‚ã¯åŸºæœ¬åˆ¤å®šã®ã¿
        
        return False
    
    def _expand_span_generic(self, text: str, expansion_context: Dict = None) -> str:
        """æ±ç”¨ã‚¹ãƒ‘ãƒ³æ‹¡å¼µå‡¦ç†ï¼ˆstep18ãƒ¡ã‚«ãƒ‹ã‚ºãƒ çµ±åˆï¼‰"""
        try:
            spacy_doc = self.spacy_nlp(text)
            
            if len(spacy_doc) <= 1:
                return text
                
            # æ‹¡å¼µè¨­å®šï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ï¼‰
            expand_deps = expansion_context.get('expand_deps', self.span_expand_deps) if expansion_context else self.span_expand_deps
            
            # å„ãƒˆãƒ¼ã‚¯ãƒ³ã®å¢ƒç•Œæ‹¡å¼µ
            expanded_spans = []
            
            for token in spacy_doc:
                span_start = token.i
                span_end = token.i
                
                # ä¾å­˜èªã«ã‚ˆã‚‹æ‹¡å¼µ
                for child in token.children:
                    if child.dep_ in expand_deps:
                        span_start = min(span_start, child.i)
                        span_end = max(span_end, child.i)
                
                # é–¢ä¿‚ä»£åè©ã®å¢ƒç•Œæ‹¡å¼µ
                if token.dep_ in ['relcl', 'acl']:
                    rel_pronouns = self._find_relative_pronouns_in_span(token, spacy_doc)
                    for rel_idx in rel_pronouns:
                        span_start = min(span_start, rel_idx)
                        span_end = max(span_end, rel_idx)
                
                if span_start <= span_end:
                    span_text = ' '.join(spacy_doc[i].text for i in range(span_start, span_end + 1))
                    expanded_spans.append(span_text)
            
            # é‡è¤‡é™¤å»ã¨çµåˆ
            unique_spans = list(dict.fromkeys(expanded_spans))  # é †åºä¿æŒã§é‡è¤‡é™¤å»
            return ' '.join(unique_spans) if unique_spans else text
            
        except Exception as e:
            print(f"âš ï¸ æ±ç”¨ã‚¹ãƒ‘ãƒ³æ‹¡å¼µã‚¨ãƒ©ãƒ¼: {e}")
            return text
    
    def _find_relative_pronouns_in_span(self, rel_token, spacy_doc) -> List[int]:
        """ã‚¹ãƒ‘ãƒ³å†…é–¢ä¿‚ä»£åè©ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¤œå‡ºï¼ˆæ±ç”¨ï¼‰"""
        rel_indices = []
        
        for child in rel_token.children:
            if (child.pos_ == 'PRON' and 
                child.dep_ in self.relative_pronoun_deps and
                child.text.lower() in ['who', 'whom', 'whose', 'which', 'that']):
                rel_indices.append(child.i)
        
        return rel_indices
    
    def _analyze_sublevel_structure(self, content: str, slot_data: Dict[str, Any], context: Dict = None) -> Dict[str, Any]:
        """ã‚µãƒ–ãƒ¬ãƒ™ãƒ«æ§‹é€ è§£æï¼ˆæ±ç”¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰"""
        try:
            print(f"ğŸ”¬ ã‚µãƒ–ãƒ¬ãƒ™ãƒ«è§£æ: '{content[:30]}...'")
            
            # å†å¸°å®Ÿè¡Œã§ã‚µãƒ–ãƒ¬ãƒ™ãƒ«åˆ†è§£
            sublevel_result = self.decompose_unified(content, context.get('depth', 1) if context else 1)
            
            # ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹èª¿æ•´
            adjusted_result = {}
            level_prefix = context.get('level_prefix', 'sub-') if context else 'sub-'
            
            for key, value in sublevel_result.items():
                if key in ['metadata', '_pattern'] or key.startswith('_'):
                    continue
                
                # sub-ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹é©ç”¨
                final_key = f"{level_prefix}{key.lower()}"
                adjusted_result[final_key] = value
            
            return adjusted_result
            
        except Exception as e:
            print(f"âš ï¸ ã‚µãƒ–ãƒ¬ãƒ™ãƒ«è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def _get_expansion_deps_for_slot(self, slot_key: str) -> List[str]:
        """ã‚¹ãƒ­ãƒƒãƒˆã‚¿ã‚¤ãƒ—åˆ¥æ‹¡å¼µä¾å­˜èªè¨­å®š"""
        slot_expansion_map = {
            'S': ['det', 'amod', 'compound', 'nmod', 'acl', 'relcl'],
            'V': ['aux', 'auxpass', 'neg', 'advmod'],
            'O1': ['det', 'amod', 'compound', 'nmod', 'acl', 'relcl'],
            'O2': ['det', 'amod', 'compound', 'nmod'],
            'M1': ['advmod', 'prep', 'pobj', 'case'],
            'C1': ['det', 'amod', 'compound'],
            'C2': ['det', 'amod', 'compound'],
            'M2': ['advmod', 'prep', 'pobj'],
            'M3': ['advmod', 'prep', 'pobj']
        }
        
        return slot_expansion_map.get(slot_key, self.span_expand_deps)
    
    def _is_noun_phrase_with_relative_clause(self, sent, root_word) -> bool:
        """é–¢ä¿‚ç¯€ã‚’å«ã‚€åè©å¥ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        if root_word.pos != 'NOUN':
            return False
        
        # é–¢ä¿‚ç¯€ï¼ˆacl:relclï¼‰ã‚’å«ã‚€ã‹ãƒã‚§ãƒƒã‚¯
        for word in sent.words:
            if word.deprel == 'acl:relcl' and word.head == root_word.id:
                return True
        
        return False
    
    def _extract_noun_phrase_with_relative_clause(self, sent, root_word, depth: int) -> Dict[str, Any]:
        """é–¢ä¿‚ç¯€ä»˜ãåè©å¥ã®å°‚ç”¨å‡¦ç†ï¼ˆRephraseåŸå‰‡æº–æ‹ ï¼‰- æ§‹é€ è§£æãƒ™ãƒ¼ã‚¹"""
        slots = {}
        
        # === æ§‹é€ åˆ†æã«åŸºã¥ãæ­£ã—ã„åˆ†é›¢ ===
        # 1. ãƒ¡ã‚¤ãƒ³åè©å¥: root_noun + ãã®ä¿®é£¾èª + é–¢ä¿‚ä»£åè©
        # 2. é–¢ä¿‚ç¯€: é–¢ä¿‚ä»£åè©ä»¥å¤–ã®é–¢ä¿‚ç¯€æ§‹æˆè¦ç´ 
        
        main_phrase_words = [root_word]  # "book"
        rel_pronoun = None               # "that"
        rel_verb = None                  # "bought" 
        rel_subject = None               # "he"
        
        # èªã®åˆ†é¡
        for word in sent.words:
            if word.head == root_word.id:
                if word.deprel == 'det' or word.deprel == 'amod':
                    # åè©ã®ä¿®é£¾èª: "The"
                    main_phrase_words.append(word)
                elif word.deprel == 'acl:relcl':
                    # é–¢ä¿‚ç¯€å‹•è©: "bought"
                    rel_verb = word
            elif word.deprel == 'obj' and any(w.id == word.head and w.deprel == 'acl:relcl' for w in sent.words):
                # é–¢ä¿‚ä»£åè©ï¼ˆé–¢ä¿‚ç¯€å‹•è©ã®ç›®çš„èªï¼‰: "that"
                rel_pronoun = word
                main_phrase_words.append(word)  # åè©å¥ã«å«ã‚ã‚‹
            elif word.deprel == 'nsubj' and any(w.id == word.head and w.deprel == 'acl:relcl' for w in sent.words):
                # é–¢ä¿‚ç¯€ã®ä¸»èª: "he"
                rel_subject = word
        
        # èªé †ã§ã‚½ãƒ¼ãƒˆ
        main_phrase_words.sort(key=lambda w: w.id)
        
        # ãƒ¡ã‚¤ãƒ³åè©å¥æ§‹ç¯‰: "The book that"
        main_phrase = ' '.join(w.text for w in main_phrase_words)
        
        # RephraseåŸå‰‡ï¼šå„è¦ç´ ã‚’ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®
        slots['O1'] = {
            'content': main_phrase,  # "The book that"
            'deprel': 'noun_phrase_with_rel',
            'pos': 'NOUN_PHRASE'
        }
        
        if rel_subject:
            slots['S'] = {
                'content': rel_subject.text,  # "he"
                'deprel': 'nsubj',
                'pos': rel_subject.pos
            }
        
        if rel_verb:
            slots['V'] = {
                'content': rel_verb.text,  # "bought"
                'deprel': 'acl:relcl', 
                'pos': rel_verb.pos
            }
        
        print(f"{'  ' * depth}ğŸ¯ æ§‹é€ åˆ†æ: '{main_phrase}' | {rel_subject.text if rel_subject else '?'} | {rel_verb.text if rel_verb else '?'}")
        
        return slots

# === ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ ===
if __name__ == "__main__":
    print("="*60)
    print("ğŸš€ Pure Stanza Engine v3.1 - RephraseåŸå‰‡æº–æ‹ ç‰ˆãƒ†ã‚¹ãƒˆ")
    print("="*60)
    
    engine = PureStanzaEngineV31()
    
    # é–¢ä¿‚ç¯€ãƒ†ã‚¹ãƒˆ - The book that he bought
    print("\nğŸ“– é–¢ä¿‚ç¯€ãƒ†ã‚¹ãƒˆ: 'The book that he bought'")
    print("-" * 50)
    result1 = engine.decompose_unified("The book that he bought")
    print("\nğŸ“Š çµæœ:")
    for k, v in result1.items():
        if not k.startswith('_') and not k == 'metadata':
            if isinstance(v, dict) and len(v) == 1:
                # å˜ä¸€å€¤ã®è¾æ›¸ã¯å€¤ã®ã¿è¡¨ç¤º
                value = next(iter(v.values()))
                print(f"  {k}: '{value}'")
            else:
                print(f"  {k}: '{v}'")
    
    print(f"\næœŸå¾…çµæœ:")
    print(f"  O1: '' (ç©º)")
    print(f"  sub-o1: 'The book that'")
    print(f"  sub-s: 'he'")
    print(f"  sub-v: 'bought'")
    
    print("\n" + "="*60)
    print("ğŸ¯ RephraseåŸå‰‡æº–æ‹ ã®é–¢ä¿‚ç¯€å‡¦ç†å®Œæˆï¼")
    print("="*60)

# ãƒ†ã‚¹ãƒˆç”¨ã®ç°¡å˜ãªå®Ÿè¡Œé–¢æ•°
def test_unified_engine():
    """çµ±ä¸€å†å¸°åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³ã®ãƒ†ã‚¹ãƒˆ"""
    engine = PureStanzaEngineV31()
    
    test_cases = [
        "I sleep.",
        "She is happy.", 
        "He plays tennis.",
        "I gave him a book.",
        "The tall man runs fast.",  # å…¥ã‚Œå­ãƒ†ã‚¹ãƒˆç”¨
    ]
    
    for sentence in test_cases:
        print(f"\n{'='*50}")
        print(f"ãƒ†ã‚¹ãƒˆæ–‡: {sentence}")
        print('='*50)
        
        result = engine.decompose_unified(sentence)
        print(f"çµæœ: {json.dumps(result, indent=2, ensure_ascii=False)}")

if __name__ == "__main__":
    test_unified_engine()
