#!/usr/bin/env python3
"""
Pure Stanza Engine v3.1 - Unified Recursive Engine
Rephraseã®çµ±ä¸€å…¥ã‚Œå­æ§‹é€ å®Ÿè£…ç‰ˆ

çµ±ä¸€åˆ†è§£åŸå‰‡:
1. Aux, Vä»¥å¤–ã®8ã‚¹ãƒ­ãƒƒãƒˆï¼ˆM1,S,O1,O2,C1,C2,M2,M3ï¼‰ã¯å†å¸°æ§‹é€ 
2. å„ã‚¹ãƒ­ãƒƒãƒˆãŒåŒã˜10ã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ ã‚’å†…åŒ…
3. çµ±ä¸€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§å…¨éšå±¤ã«å¯¾å¿œ
4. phrase/clauseæ™‚ã®ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆç©ºåŒ–
"""

import stanza
import spacy
import json
from typing import Dict, List, Optional, Tuple, Any

class PureStanzaEngineV31:
    """
    Pure Stanza Engine v3.1 - Unified Recursive Engine
    
    Rephraseã®çµ±ä¸€å…¥ã‚Œå­æ§‹é€ å®Ÿè£…:
    - 8ã¤ã®å†å¸°å¯èƒ½ã‚¹ãƒ­ãƒƒãƒˆ: M1,S,O1,O2,C1,C2,M2,M3
    - 2ã¤ã®èªå½™å°‚ç”¨ã‚¹ãƒ­ãƒƒãƒˆ: Aux,V
    - çµ±ä¸€åˆ†è§£ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹ç„¡é™éšå±¤å¯¾å¿œ
    """
    
    def __init__(self):
        """çµ±ä¸€å†å¸°åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–"""
        print("ğŸš€ çµ±ä¸€å†å¸°åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³ v3.1 åˆæœŸåŒ–ä¸­...")
        
        # Stanza NLP ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        self.nlp = stanza.Pipeline('en', verbose=False)
        print("âœ… Stanzaæº–å‚™å®Œäº†")
        
        # spaCyï¼ˆå¢ƒç•Œèª¿æ•´ç”¨ï¼‰
        self.spacy_nlp = spacy.load("en_core_web_sm")
        print("âœ… spaCyæº–å‚™å®Œäº†")
        
        # Rephraseã®çµ±ä¸€ã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ å®šç¾©
        self.RECURSIVE_SLOTS = ['M1', 'S', 'O1', 'O2', 'C1', 'C2', 'M2', 'M3']
        self.WORD_ONLY_SLOTS = ['Aux', 'V']
        self.ALL_SLOTS = self.RECURSIVE_SLOTS + self.WORD_ONLY_SLOTS
        
        # æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«
        self.sentence_patterns = self._load_sentence_patterns()
        
        # ä¿®é£¾èªãƒãƒƒãƒ”ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«
        self.modifier_mappings = self._load_modifier_mappings()
        
        # å…¥ã‚Œå­åˆ¤å®šãƒ«ãƒ¼ãƒ«
        self.nested_triggers = self._load_nested_triggers()
        
        print("ğŸ—ï¸ çµ±ä¸€å†å¸°åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³æº–å‚™å®Œäº† (v3.1)")
        
    def _load_sentence_patterns(self) -> Dict[str, Any]:
        """çµ±ä¸€æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«"""
        return {
            # åŸºæœ¬5æ–‡å‹
            "SV": {
                "required_relations": ["nsubj", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "root": "V"}
            },
            "SVC": {
                "required_relations": ["nsubj", "cop", "root"],
                "root_pos": ["ADJ", "NOUN"],
                "mapping": {"nsubj": "S", "cop": "V", "root": "C1"}
            },
            "SVO": {
                "required_relations": ["nsubj", "obj", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "obj": "O1", "root": "V"}
            },
            "SVOO": {
                "required_relations": ["nsubj", "iobj", "obj", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "iobj": "O1", "obj": "O2", "root": "V"}
            },
            "SVOC": {
                "required_relations": ["nsubj", "obj", "xcomp", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "obj": "O1", "xcomp": "C2", "root": "V"}
            },
            # åŠ©å‹•è©æ§‹æ–‡
            "S_AUX_V": {
                "required_relations": ["nsubj", "aux", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "aux": "Aux", "root": "V"}
            },
            # å—å‹•æ…‹
            "PASSIVE": {
                "required_relations": ["nsubj:pass", "aux:pass", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj:pass": "S", "aux:pass": "Aux", "root": "V"}
            }
        }
    
    def _load_modifier_mappings(self) -> Dict[str, str]:
        """ä¿®é£¾èªã‚¹ãƒ­ãƒƒãƒˆãƒãƒƒãƒ”ãƒ³ã‚°"""
        return {
            "advmod": "M2",     # å‰¯è©ä¿®é£¾èª
            "amod": "nested",   # å½¢å®¹è©ä¿®é£¾èª â†’ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ
            "det": "nested",    # é™å®šè© â†’ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ
            "case": "nested",   # å‰ç½®è© â†’ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ
            "nmod": "M1",       # åè©ä¿®é£¾èª
            "mark": "nested",   # å¾“å±ç¯€ãƒãƒ¼ã‚«ãƒ¼ â†’ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ
        }
    
    def _load_nested_triggers(self) -> Dict[str, str]:
        """å…¥ã‚Œå­åˆ†è§£ãƒˆãƒªã‚¬ãƒ¼æ¡ä»¶"""
        return {
            "phrase": "multi_word",      # è¤‡æ•°èªå¥
            "clause": "has_verb",        # å‹•è©ã‚’å«ã‚€å¥
            "complex": "has_modifiers"   # ä¿®é£¾èªã‚’å«ã‚€å¥
        }
    
    def decompose_unified(self, text: str, depth: int = 0) -> Dict[str, Any]:
        """
        çµ±ä¸€åˆ†è§£ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  - Rephraseã®æ ¸å¿ƒå®Ÿè£…
        
        Args:
            text: åˆ†è§£å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            depth: å†å¸°æ·±åº¦ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            
        Returns:
            çµ±ä¸€10ã‚¹ãƒ­ãƒƒãƒˆæ§‹é€  + ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ
        """
        print(f"{'  ' * depth}ğŸ” çµ±ä¸€åˆ†è§£é–‹å§‹: '{text[:30]}...' (depth={depth})")
        
        # Stanzaæ§‹æ–‡è§£æ
        doc = self.nlp(text)
        sent = doc.sentences[0]
        
        # ROOTå‹•è©æ¤œå‡º
        root_verb = self._find_root_verb(sent)
        if not root_verb:
            print(f"{'  ' * depth}âš ï¸ ROOTå‹•è©æœªæ¤œå‡º")
            return {"error": "No root verb found"}
        
        print(f"{'  ' * depth}ğŸ¯ ROOT: '{root_verb.text}' ({root_verb.pos})")
        
        # åŸºæœ¬10ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£
        basic_slots = self._extract_basic_slots(sent, root_verb, depth)
        
        # çµ±ä¸€å…¥ã‚Œå­å‡¦ç†ï¼š8ã¤ã®å†å¸°å¯èƒ½ã‚¹ãƒ­ãƒƒãƒˆã§å†å¸°é©ç”¨
        unified_result = self._apply_unified_nesting(basic_slots, depth)
        
        print(f"{'  ' * depth}ğŸ“‹ çµ±ä¸€åˆ†è§£å®Œäº†: {len([k for k, v in unified_result.items() if k != 'metadata'])}ã‚¹ãƒ­ãƒƒãƒˆ")
        
        return unified_result
    
    def _find_root_verb(self, sent) -> Optional[Any]:
        """ROOTå‹•è©æ¤œå‡ºï¼ˆçµ±ä¸€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ç”¨ï¼‰"""
        for word in sent.words:
            if word.deprel == 'root':
                return word
        return None
    
    def _extract_basic_slots(self, sent, root_verb, depth: int) -> Dict[str, Any]:
        """åŸºæœ¬10ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡ºï¼ˆçµ±ä¸€ãƒ™ãƒ¼ã‚¹å‡¦ç†ï¼‰"""
        # å…¨ä¾å­˜é–¢ä¿‚ã‚’åé›†
        all_relations = {}
        for word in sent.words:
            all_relations[word.deprel] = word
        
        # æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        matched_pattern = self._match_sentence_pattern_enhanced(sent, root_verb)
        
        if not matched_pattern:
            print(f"{'  ' * depth}âŒ æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³æœªæ¤œå‡º")
            return {}
        
        print(f"{'  ' * depth}âœ… ãƒãƒƒãƒã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³: {matched_pattern}")
        
        # ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º
        slots = {}
        
        # åŸºæœ¬ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º
        self._extract_core_slots(sent, root_verb, slots, depth)
        
        # ä¿®é£¾èªå‡¦ç†
        self._process_modifiers(sent, slots, depth)
        
        return slots
    
    def _match_sentence_pattern_enhanced(self, sent, root_verb) -> Optional[str]:
        """å¼·åŒ–ã•ã‚ŒãŸæ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°"""
        relations = {word.deprel: word for word in sent.words}
        
        # SVOç³»ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è©³ç´°åˆ¤å®š
        if 'nsubj' in relations and root_verb.pos == 'VERB':
            if 'obj' in relations and 'iobj' in relations:
                return "SVOO"  # ç¬¬4æ–‡å‹
            elif 'obj' in relations and 'xcomp' in relations:
                return "SVOC"  # ç¬¬5æ–‡å‹ 
            elif 'obj' in relations:
                return "SVO"   # ç¬¬3æ–‡å‹
            elif 'aux' in relations:
                return "S_AUX_V"  # åŠ©å‹•è©æ§‹æ–‡
            else:
                return "SV"    # ç¬¬1æ–‡å‹
        
        # SVCç³»ãƒ‘ã‚¿ãƒ¼ãƒ³
        if 'nsubj' in relations and 'cop' in relations:
            return "SVC"  # ç¬¬2æ–‡å‹
        
        # å—å‹•æ…‹
        if 'nsubj:pass' in relations and 'aux:pass' in relations:
            return "PASSIVE"
        
        return "SV"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def _extract_core_slots(self, sent, root_verb, slots: Dict[str, Any], depth: int):
        """ã‚³ã‚¢ã‚¹ãƒ­ãƒƒãƒˆã®æŠ½å‡º"""
        for word in sent.words:
            slot_name = None
            
            # ä¸»èªç³»
            if word.deprel == 'nsubj' or word.deprel == 'nsubj:pass':
                slot_name = 'S'
                # è¤‡åˆä¸»èªã®å ´åˆã®å‡¦ç†
                subject_text = self._extract_compound_phrase(sent, word)
                word_text = subject_text if subject_text != word.text else word.text
            # ç›®çš„èªç³»  
            elif word.deprel == 'obj':
                slot_name = 'O1'
                word_text = self._extract_compound_phrase(sent, word)
            elif word.deprel == 'iobj':
                slot_name = 'O1' 
                word_text = word.text
            # è£œèªç³»
            elif word.deprel == 'xcomp':
                slot_name = 'C2'
                word_text = word.text
            elif word.deprel == 'root' and word.pos in ['ADJ', 'NOUN']:
                slot_name = 'C1'
                word_text = word.text
            # å‹•è©ç³»
            elif word.deprel == 'root' and word.pos == 'VERB':
                slot_name = 'V'
                word_text = word.text
            elif word.deprel == 'cop':
                slot_name = 'V'
                word_text = word.text
            elif word.deprel == 'aux' or word.deprel == 'aux:pass':
                slot_name = 'Aux'
                word_text = word.text
            else:
                continue
            
            if slot_name:
                slots[slot_name] = {
                    "content": word_text,
                    "pos": word.pos,
                    "deprel": word.deprel,
                    "word_obj": word
                }
                print(f"{'  ' * depth}ğŸ“ {slot_name}: '{word_text}' (deprel: {word.deprel})")
    
    def _extract_compound_phrase(self, sent, head_word) -> str:
        """è¤‡åˆå¥ã®æŠ½å‡ºï¼ˆé™å®šè©ãƒ»å½¢å®¹è©ãªã©ã‚’å«ã‚€ï¼‰"""
        phrase_words = [head_word]
        
        # ä¾å­˜èªã‚’åé›†
        for word in sent.words:
            if word.head == head_word.id and word.deprel in ['det', 'amod', 'compound', 'case']:
                phrase_words.append(word)
        
        # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
        phrase_words.sort(key=lambda w: w.id)
        
        return ' '.join(word.text for word in phrase_words)
    
    def _match_sentence_pattern(self, relations: List[str], root_pos: str) -> Optional[str]:
        """æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°"""
        for pattern_name, pattern_info in self.sentence_patterns.items():
            required = set(pattern_info["required_relations"])
            available = set(relations)
            
            if required.issubset(available) and root_pos in pattern_info["root_pos"]:
                return pattern_name
        
        return None
    
    def _process_modifiers(self, sent, slots: Dict[str, Any], depth: int):
        """ä¿®é£¾èªå‡¦ç†ï¼ˆçµ±ä¸€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰"""
        for word in sent.words:
            if word.deprel in self.modifier_mappings:
                slot_mapping = self.modifier_mappings[word.deprel]
                
                if slot_mapping == "nested":
                    # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå€™è£œã¨ã—ã¦è¨˜éŒ²
                    if "modifiers" not in slots:
                        slots["modifiers"] = []
                    slots["modifiers"].append({
                        "word": word.text,
                        "deprel": word.deprel,
                        "head_id": word.head
                    })
                elif slot_mapping in self.ALL_SLOTS:
                    # ç›´æ¥ã‚¹ãƒ­ãƒƒãƒˆãƒãƒƒãƒ”ãƒ³ã‚°
                    slots[slot_mapping] = {
                        "content": word.text,
                        "pos": word.pos,
                        "deprel": word.deprel
                    }
                    print(f"{'  ' * depth}ğŸ“ {slot_mapping}: '{word.text}' (ä¿®é£¾èª: {word.deprel})")
    
    def _apply_unified_nesting(self, basic_slots: Dict[str, Any], depth: int) -> Dict[str, Any]:
        """
        çµ±ä¸€å…¥ã‚Œå­å‡¦ç† - Rephraseã®æ ¸å¿ƒå®Ÿè£…
        8ã¤ã®å†å¸°å¯èƒ½ã‚¹ãƒ­ãƒƒãƒˆã«çµ±ä¸€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é©ç”¨
        """
        result = {}
        
        for slot_name in self.ALL_SLOTS:
            if slot_name in basic_slots:
                slot_data = basic_slots[slot_name]
                
                if slot_name in self.RECURSIVE_SLOTS:
                    # å†å¸°å¯èƒ½ã‚¹ãƒ­ãƒƒãƒˆï¼šå…¥ã‚Œå­åˆ¤å®šã—ã¦å†å¸°é©ç”¨
                    result[slot_name] = self._process_recursive_slot(slot_data, slot_name, depth)
                else:
                    # èªå½™å°‚ç”¨ã‚¹ãƒ­ãƒƒãƒˆï¼šãã®ã¾ã¾ä¿æŒ
                    result[slot_name] = {slot_name.lower(): slot_data["content"]}
        
        return result
    
    def _process_recursive_slot(self, slot_data: Dict[str, Any], slot_name: str, depth: int) -> Dict[str, Any]:
        """
        å†å¸°å¯èƒ½ã‚¹ãƒ­ãƒƒãƒˆå‡¦ç†
        å…¥ã‚Œå­æ¡ä»¶ã‚’æº€ãŸã™å ´åˆã¯çµ±ä¸€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å†å¸°é©ç”¨
        """
        content = slot_data["content"]
        
        # å…¥ã‚Œå­åˆ¤å®š
        if self._needs_nesting(content, slot_data):
            print(f"{'  ' * depth}ğŸ“ {slot_name}ã‚¹ãƒ­ãƒƒãƒˆ: '{content}' â†’ çµ±ä¸€å†å¸°åˆ†è§£")
            
            # çµ±ä¸€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å†å¸°é©ç”¨
            subslot_result = self.decompose_unified(content, depth + 1)
            
            if "error" not in subslot_result:
                # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆç©ºåŒ–ï¼ˆRephraseãƒ«ãƒ¼ãƒ«ï¼‰
                return {
                    "content": "",  # ä¸Šä½ã‚’ç©ºåŒ–
                    "subslots": subslot_result  # çµ±ä¸€åˆ†è§£çµæœã‚’æ ¼ç´
                }
        
        # å˜èªãƒ¬ãƒ™ãƒ«ï¼šãã®ã¾ã¾ä¿æŒ
        return {slot_name.lower(): content}
    
    def _needs_nesting(self, content: str, slot_data: Dict[str, Any]) -> bool:
        """å…¥ã‚Œå­åˆ†è§£åˆ¤å®š"""
        # è¤‡æ•°èªå¥åˆ¤å®š
        if len(content.split()) > 1:
            return True
        
        # å‹•è©ã‚’å«ã‚€åˆ¤å®šï¼ˆå°†æ¥ã®å¥ãƒ»ç¯€å¯¾å¿œï¼‰
        # TODO: ã‚ˆã‚Šè©³ç´°ãªåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯å®Ÿè£…
        
        return False

# ãƒ†ã‚¹ãƒˆç”¨ã®ç°¡å˜ãªå®Ÿè¡Œé–¢æ•°
def test_unified_engine():
    """çµ±ä¸€å†å¸°åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³ã®ãƒ†ã‚¹ãƒˆ"""
    engine = PureStanzaEngineV31()
    
    test_cases = [
        "I sleep.",
        "She is happy.", 
        "He plays tennis.",
        "I gave him a book.",
        "The tall man runs fast.",  # å…¥ã‚Œå­ãƒ†ã‚¹ãƒˆç”¨
    ]
    
    for sentence in test_cases:
        print(f"\n{'='*50}")
        print(f"ãƒ†ã‚¹ãƒˆæ–‡: {sentence}")
        print('='*50)
        
        result = engine.decompose_unified(sentence)
        print(f"çµæœ: {json.dumps(result, indent=2, ensure_ascii=False)}")

if __name__ == "__main__":
    test_unified_engine()
