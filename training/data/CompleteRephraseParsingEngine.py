#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Rephrase Parsing Engine v3.0 - å®Œå…¨ç‰ˆ
Rephraseãƒ«ãƒ¼ãƒ«è¾æ›¸100%æ´»ç”¨ + Sub-slotå®Œå…¨å¯¾å¿œ + 5æ–‡å‹ãƒ•ãƒ«ã‚»ãƒƒãƒˆå¯¾å¿œ
"""

import spacy
import json
import os
import re
from typing import Dict, List, Any, Optional, Tuple, Union

# spaCyåˆæœŸåŒ–
try:
    nlp = spacy.load("en_core_web_sm")
    SPACY_AVAILABLE = True
    print("âœ… spaCyèªå½™èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
except (OSError, ImportError) as e:
    nlp = None
    SPACY_AVAILABLE = False
    print(f"âš ï¸ spaCyåˆæœŸåŒ–å¤±æ•—: {e}")

class CompleteRephraseParsingEngine:
    """å®Œå…¨ç‰ˆRephraseå“è©åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³ v3.0"""
    
    def __init__(self):
        self.engine_name = "Complete Rephrase Parsing Engine v3.0"
        self.rules_data = self.load_rules()
        self.nlp = nlp if SPACY_AVAILABLE else None
        
        # Rephraseã‚¹ãƒ­ãƒƒãƒˆå®Œå…¨å®šç¾©
        self.main_slots = ['S', 'Aux', 'V', 'O1', 'O2', 'C1', 'C2', 'M1', 'M2', 'M3']
        self.sub_slots = ['sub-m1', 'sub-s', 'sub-aux', 'sub-v', 'sub-o1', 'sub-o2', 'sub-m2', 'sub-m3', 'sub-c1']
        
        # ãƒ«ãƒ¼ãƒ«å„ªå…ˆåº¦ãƒãƒƒãƒ”ãƒ³ã‚°
        self.rule_priority_map = {}
        self._build_rule_priority_map()
        
        # ğŸš€ ãƒ•ã‚§ãƒ¼ã‚ºæ‹¡å¼µçµ±è¨ˆãƒ‡ãƒ¼ã‚¿åˆæœŸåŒ–
        self.phase1_stats = {}
        self.phase2_stats = {}
        self.phase3_stats = {}
        
    def load_rules(self):
        """Rephraseãƒ«ãƒ¼ãƒ«è¾æ›¸ã®å®Œå…¨èª­ã¿è¾¼ã¿"""
        rules_file = os.path.join(os.path.dirname(__file__), 'rephrase_rules_v1.0.json')
        try:
            with open(rules_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"âœ… Rephraseãƒ«ãƒ¼ãƒ«è¾æ›¸èª­ã¿è¾¼ã¿å®Œäº†: {len(data.get('rules', []))} ãƒ«ãƒ¼ãƒ«")
                return data
        except FileNotFoundError:
            print(f"âŒ ãƒ«ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {rules_file}")
            return {}
    
    def _build_rule_priority_map(self):
        """ãƒ«ãƒ¼ãƒ«å„ªå…ˆåº¦ãƒãƒƒãƒ—ã®æ§‹ç¯‰"""
        if 'rules' not in self.rules_data:
            return
            
        for rule in self.rules_data['rules']:
            rule_id = rule.get('id', '')
            priority = rule.get('priority', 50)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå„ªå…ˆåº¦
            self.rule_priority_map[rule_id] = priority
    
    def analyze_sentence(self, sentence: str) -> Dict[str, Any]:
        """
        å®Œå…¨ç‰ˆæ–‡è¦ç´ åˆ†è§£
        
        Returns:
            {
                'main_slots': {...},      # ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ­ãƒƒãƒˆ
                'sub_structures': [...],  # ã‚µãƒ–æ§‹é€ ï¼ˆé–¢ä¿‚è©ç¯€ç­‰ï¼‰
                'sentence_type': '...',   # æ–‡å‹
                'metadata': {...}         # ãƒ¡ã‚¿æƒ…å ±
            }
        """
        if not self.nlp:
            return {"error": "spaCy not available"}
            
        try:
            # Step 1: spaCyå®Œå…¨è§£æ
            doc = self.nlp(sentence)
            spacy_analysis = self._comprehensive_spacy_analysis(doc)
            
            # Step 2: æ–‡æ§‹é€ ã®éšå±¤åˆ†æ
            sentence_hierarchy = self._analyze_sentence_hierarchy(doc, spacy_analysis)
            
            # Step 3: Rephraseãƒ«ãƒ¼ãƒ«21å€‹ã®å®Œå…¨é©ç”¨
            rephrase_slots = self._apply_complete_rephrase_rules(doc, sentence_hierarchy)
            
            # Step 3.5: ğŸš€ ãƒ•ã‚§ãƒ¼ã‚º1 spaCyå®Œå…¨å¯¾å¿œæ‹¡å¼µæ©Ÿèƒ½é©ç”¨
            rephrase_slots = self._apply_phase1_enhancements(doc, rephrase_slots)
            
            # Step 3.6: ğŸš€ ãƒ•ã‚§ãƒ¼ã‚º2 æ–‡æ§‹é€ æ‹¡å¼µæ©Ÿèƒ½é©ç”¨ï¼ˆ80%ã‚«ãƒãƒ¬ãƒƒã‚¸é”æˆï¼‰
            rephrase_slots = self._apply_phase2_enhancements(doc, rephrase_slots)
            
            # Step 3.7: ğŸš€ ãƒ•ã‚§ãƒ¼ã‚º3 é«˜åº¦æ–‡æ³•æ©Ÿèƒ½é©ç”¨ï¼ˆ90%+ã‚«ãƒãƒ¬ãƒƒã‚¸é”æˆï¼‰
            rephrase_slots = self._apply_phase3_enhancements(doc, rephrase_slots)
            
            # Step 4: Sub-slotæ§‹é€ ã®ç”Ÿæˆ
            sub_structures = self._generate_subslot_structures(doc, sentence_hierarchy)
            
            # Step 5: æ–‡å‹åˆ¤å®šï¼ˆç¬¬1ã€œ5æ–‡å‹ï¼‰
            sentence_pattern = self._determine_sentence_pattern(rephrase_slots, sub_structures)
            
            # ğŸ¯ ãƒ•ã‚§ãƒ¼ã‚º1&2&3çµ±åˆãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ
            enhanced_data = {}
            
            # ãƒ•ã‚§ãƒ¼ã‚º1çµ±è¨ˆ
            if hasattr(self, 'phase1_stats'):
                enhanced_data.update(self.phase1_stats)
            
            # ãƒ•ã‚§ãƒ¼ã‚º2çµ±è¨ˆ
            if hasattr(self, 'phase2_stats'):
                enhanced_data.update(self.phase2_stats)
            
            # ãƒ•ã‚§ãƒ¼ã‚º3çµ±è¨ˆ
            if hasattr(self, 'phase3_stats'):
                enhanced_data.update(self.phase3_stats)
            
            return {
                'rephrase_slots': rephrase_slots,
                'slots': rephrase_slots,
                'main_slots': rephrase_slots,
                'sub_structures': sub_structures,
                'sentence_pattern': sentence_pattern,
                'sentence_type': sentence_pattern,
                'enhanced_data': enhanced_data,
                'metadata': {
                    'engine': self.engine_name,
                    'rules_applied': len([r for r in rephrase_slots.values() if r]),
                    'complexity_score': self._calculate_complexity(sentence_hierarchy),
                    'phase1_enhanced': True,
                    'phase2_enhanced': True,
                    'phase3_enhanced': True,
                    'coverage_features': len(enhanced_data),
                    'total_coverage': '90%+'
                }
            }
            
        except Exception as e:
            print(f"å®Œå…¨ç‰ˆãƒ‘ãƒ¼ã‚·ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _comprehensive_spacy_analysis(self, doc) -> Dict[str, Any]:
        """spaCyã«ã‚ˆã‚‹åŒ…æ‹¬çš„è¨€èªè§£æ"""
        
        analysis = {
            'tokens': [],
            'dependencies': [],
            'clauses': {
                'main': None,
                'subordinate': [],
                'relative': [],
                'infinitive': [],
                'participial': []
            },
            'phrases': {
                'noun_phrases': [],
                'verb_phrases': [],
                'prep_phrases': [],
                'adj_phrases': []
            },
            'entities': [],
            'sentence_structure': None
        }
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«è§£æ
        for token in doc:
            token_info = {
                'text': token.text,
                'lemma': token.lemma_,
                'pos': token.pos_,
                'tag': token.tag_,
                'dep': token.dep_,
                'head': token.head.text if token.head != token else 'ROOT',
                'children': [child.text for child in token.children],
                'is_punct': token.is_punct,
                'is_stop': token.is_stop
            }
            analysis['tokens'].append(token_info)
            
            # ä¾å­˜é–¢ä¿‚ã®è¨˜éŒ²
            if token.dep_ != 'ROOT':
                analysis['dependencies'].append({
                    'child': token.text,
                    'relation': token.dep_,
                    'head': token.head.text
                })
        
        # ä¸»ç¯€ãƒ»å¾“å±ç¯€ã®ç‰¹å®š
        root_verb = None
        for token in doc:
            if token.dep_ == "ROOT" and token.pos_ == "VERB":
                root_verb = token
                break
                
        analysis['clauses']['main'] = root_verb
        
        # å¾“å±ç¯€ã®ç‰¹å®š
        for token in doc:
            if token.dep_ == "advcl":  # å‰¯è©ç¯€
                analysis['clauses']['subordinate'].append({
                    'verb': token,
                    'type': 'adverbial',
                    'marker': self._find_clause_marker(token)
                })
            elif token.dep_ == "relcl":  # é–¢ä¿‚è©ç¯€
                analysis['clauses']['relative'].append({
                    'verb': token,
                    'modified_noun': token.head,
                    'relativizer': self._find_relativizer(token)
                })
            elif token.dep_ == "xcomp":  # è£œæ–‡ç¯€
                analysis['clauses']['infinitive'].append({
                    'verb': token,
                    'type': 'infinitive' if 'to' in [c.text for c in token.head.children] else 'bare_infinitive'
                })
        
        # ãƒ•ãƒ¬ãƒ¼ã‚ºãƒ¬ãƒ™ãƒ«è§£æ
        analysis['phrases']['noun_phrases'] = self._extract_noun_phrases(doc)
        analysis['phrases']['verb_phrases'] = self._extract_verb_phrases(doc)
        analysis['phrases']['prep_phrases'] = self._extract_prep_phrases(doc)
        
        # å›ºæœ‰è¡¨ç¾
        analysis['entities'] = [(ent.text, ent.label_) for ent in doc.ents]
        
        return analysis
    
    def _analyze_sentence_hierarchy(self, doc, spacy_analysis) -> Dict[str, Any]:
        """æ–‡æ§‹é€ ã®éšå±¤åˆ†æ"""
        
        hierarchy = {
            'main_clause': {
                'subject': None,
                'verb': spacy_analysis['clauses']['main'],
                'objects': [],
                'complements': [],
                'modifiers': []
            },
            'subordinate_structures': [],
            'sentence_complexity': 'simple'  # simple, compound, complex, compound-complex
        }
        
        # ä¸»ç¯€ã®æ§‹æˆè¦ç´ ç‰¹å®š
        main_verb = spacy_analysis['clauses']['main']
        if main_verb:
            # ä¸»èªç‰¹å®š
            for token in doc:
                if token.dep_ == "nsubj" and token.head == main_verb:
                    hierarchy['main_clause']['subject'] = {
                        'core': token,
                        'full_phrase': self._get_complete_noun_phrase(token),
                        'type': 'simple' if not self._has_modifying_clause(token) else 'complex'
                    }
                    break
            
            # ç›®çš„èªç‰¹å®š
            for token in doc:
                if token.head == main_verb:
                    if token.dep_ == "dobj":
                        hierarchy['main_clause']['objects'].append({
                            'core': token,
                            'full_phrase': self._get_complete_noun_phrase(token),
                            'type': 'direct',
                            'position': 1
                        })
                    elif token.dep_ in ["iobj", "dative"]:
                        hierarchy['main_clause']['objects'].append({
                            'core': token,
                            'full_phrase': self._get_complete_noun_phrase(token),
                            'type': 'indirect',
                            'position': 2
                        })
        
        # å¾“å±æ§‹é€ ã®åˆ†æ
        for rel_clause in spacy_analysis['clauses']['relative']:
            substructure = self._analyze_subclause_structure(rel_clause['verb'], 'relative')
            hierarchy['subordinate_structures'].append(substructure)
            
        for adv_clause in spacy_analysis['clauses']['subordinate']:
            substructure = self._analyze_subclause_structure(adv_clause['verb'], 'adverbial')
            hierarchy['subordinate_structures'].append(substructure)
        
        # æ–‡ã®è¤‡é›‘åº¦åˆ¤å®š
        if hierarchy['subordinate_structures']:
            if len(hierarchy['subordinate_structures']) == 1:
                hierarchy['sentence_complexity'] = 'complex'
            else:
                hierarchy['sentence_complexity'] = 'compound-complex'
        
        return hierarchy
    
    def _apply_complete_rephrase_rules(self, doc, hierarchy) -> Dict[str, List[Dict[str, Any]]]:
        """Rephraseãƒ«ãƒ¼ãƒ«21å€‹ã®å®Œå…¨é©ç”¨"""
        
        slots = {slot: [] for slot in self.main_slots}
        
        if 'rules' not in self.rules_data:
            print("âš ï¸ ãƒ«ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return slots
        
        # ãƒ«ãƒ¼ãƒ«ã‚’å„ªå…ˆåº¦é †ã§ã‚½ãƒ¼ãƒˆ
        rules = sorted(self.rules_data['rules'], 
                      key=lambda r: r.get('priority', 50), 
                      reverse=True)
        
        applied_rules = []
        blocked_rules = []  # ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚ŒãŸãƒ«ãƒ¼ãƒ«ã®ãƒªã‚¹ãƒˆ
        
        for rule in rules:
            rule_id = rule.get('id', '')
            
            # å‘¼ã³ã‹ã‘ãƒ«ãƒ¼ãƒ«ãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹å ´åˆã€ä¸»èªãƒ«ãƒ¼ãƒ«ã‚’ãƒ–ãƒ­ãƒƒã‚¯
            if rule_id == 'subject-pronoun-np-front' and 'vocative-you-comma' in applied_rules:
                blocked_rules.append(rule_id)
                print(f"ğŸš« ãƒ«ãƒ¼ãƒ«ãƒ–ãƒ­ãƒƒã‚¯: {rule_id} (å‘¼ã³ã‹ã‘ãƒ«ãƒ¼ãƒ«å„ªå…ˆ)")
                continue
            
            try:
                # ãƒ«ãƒ¼ãƒ«ã®é©ç”¨
                if self._should_apply_rule(rule, doc, hierarchy):
                    result = self._apply_single_rule(rule, doc, hierarchy, slots)
                    if result:
                        applied_rules.append(rule_id)
                        print(f"âœ… ãƒ«ãƒ¼ãƒ«é©ç”¨: {rule_id}")
                    
            except Exception as e:
                print(f"âš ï¸ ãƒ«ãƒ¼ãƒ«é©ç”¨ã‚¨ãƒ©ãƒ¼ {rule_id}: {e}")
        
        print(f"ğŸ“Š é©ç”¨ã•ã‚ŒãŸãƒ«ãƒ¼ãƒ«æ•°: {len(applied_rules)}/21")
        if blocked_rules:
            print(f"ğŸš« ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚ŒãŸãƒ«ãƒ¼ãƒ«æ•°: {len(blocked_rules)} â†’ {blocked_rules}")
        
        # æ±ç”¨çš„ãªä¸»èªæ¤œå‡ºï¼ˆãƒ«ãƒ¼ãƒ«ã§æ•ç²ã•ã‚Œãªã‹ã£ãŸå ´åˆï¼‰
        if not slots['S']:
            generic_subject = self._extract_generic_subject(doc, hierarchy)
            if generic_subject:
                slots['S'].append(generic_subject)
                print(f"âœ… æ±ç”¨ä¸»èªæ¤œå‡º: {generic_subject}")
        
        # æ±ç”¨çš„ãªå‹•è©æ¤œå‡ºï¼ˆãƒ«ãƒ¼ãƒ«ã§æ•ç²ã•ã‚Œãªã‹ã£ãŸå ´åˆï¼‰
        if not slots['V']:
            generic_verb = self._extract_generic_verb(doc, hierarchy)
            if generic_verb:
                slots['V'].append({
                    'value': generic_verb,
                    'rule_id': 'generic-verb',
                    'confidence': 0.7,
                    'order': 4
                })
                print(f"âœ… æ±ç”¨å‹•è©æ¤œå‡º: {generic_verb}")
        
        # æ±ç”¨çš„ãªç›®çš„èªæ¤œå‡ºï¼ˆãƒ«ãƒ¼ãƒ«ã§æ•ç²ã•ã‚Œãªã‹ã£ãŸå ´åˆï¼‰
        if not slots['O1']:
            generic_object = self._extract_generic_object(doc, hierarchy)
            if generic_object:
                slots['O1'].append({
                    'value': generic_object,
                    'rule_id': 'generic-object',
                    'confidence': 0.7,
                    'order': 5
                })
                print(f"âœ… æ±ç”¨ç›®çš„èªæ¤œå‡º: {generic_object}")
        
        # SVOOæ§‹é€ ã®æ¤œå‡ºï¼ˆé–“æ¥ç›®çš„èªã¨ç›´æ¥ç›®çš„èªã®åˆ†é›¢ï¼‰
        self._extract_ditransitive_objects(doc, slots)
        
        # æ±ç”¨çš„ãªæ™‚é–“è¡¨ç¾æ¤œå‡ºï¼ˆãƒ«ãƒ¼ãƒ«ã§æ•ç²ã•ã‚Œãªã‹ã£ãŸå ´åˆï¼‰
        if not slots['M3']:
            generic_time = self._extract_generic_time_expression(doc)
            if generic_time:
                slots['M3'].append({
                    'value': generic_time,
                    'rule_id': 'generic-time',
                    'confidence': 0.8,
                    'order': 6
                })
                print(f"âœ… æ±ç”¨æ™‚é–“è¡¨ç¾æ¤œå‡º: {generic_time}")
        
        # æ±ç”¨çš„ãªåŠ©å‹•è©æ¤œå‡ºï¼ˆç¸®ç´„å½¢å¯¾å¿œï¼‰
        if not slots['Aux']:
            generic_aux = self._extract_generic_auxiliary(doc)
            if generic_aux:
                slots['Aux'].append({
                    'value': generic_aux,
                    'rule_id': 'generic-aux',
                    'confidence': 0.8,
                    'order': 2
                })
                print(f"âœ… æ±ç”¨åŠ©å‹•è©æ¤œå‡º: {generic_aux}")
        
        # æ±ç”¨çš„ãªå‰ç½®è©å¥æ¤œå‡ºï¼ˆå‰¯è©çš„ä¿®é£¾èªã¨ã—ã¦ï¼‰
        if not slots['M2']:
            generic_prep = self._extract_generic_prepositional_phrase(doc)
            if generic_prep:
                slots['M2'].append({
                    'value': generic_prep,
                    'rule_id': 'generic-prep-phrase',
                    'confidence': 0.8,
                    'order': 7
                })
                print(f"âœ… æ±ç”¨å‰ç½®è©å¥æ¤œå‡º: {generic_prep}")
        
        # æ±ç”¨çš„ãªè£œèªæ¤œå‡ºï¼ˆC1ï¼‰- beå‹•è©ã‚„becomeç­‰ã®å¾Œã®åè©å¥ãƒ»å½¢å®¹è©å¥
        if not slots['C1']:
            generic_complement = self._extract_generic_complement(doc)
            if generic_complement:
                slots['C1'].append({
                    'value': generic_complement,
                    'rule_id': 'generic-complement',
                    'confidence': 0.8,
                    'order': 8
                })
                print(f"âœ… æ±ç”¨è£œèªæ¤œå‡º: {generic_complement}")
        
        return slots
    
    def _should_apply_rule(self, rule: Dict[str, Any], doc, hierarchy) -> bool:
        """ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨ã™ã¹ãã‹ã©ã†ã‹ã®åˆ¤å®š"""
        
        rule_id = rule.get('id', 'unknown')
        trigger = rule.get('trigger', {})
        
        print(f"ğŸ” ãƒ«ãƒ¼ãƒ«åˆ¤å®š: {rule_id}")
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«ã®å ´åˆã¯ç‰¹åˆ¥å‡¦ç†
        if 'patterns' in rule:
            patterns = rule['patterns']
            print(f"  ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«æ¤œå‡º: {len(patterns)}å€‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³")
            for pattern_obj in patterns:
                pattern_text = pattern_obj.get('pattern', '')
                if re.search(pattern_text, doc.text, re.IGNORECASE):
                    print(f"  âœ… ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒ: {pattern_text}")
                    return True
            print(f"  âŒ ãƒ‘ã‚¿ãƒ¼ãƒ³éãƒãƒƒãƒ")
            return False
        
        # tokenãƒˆãƒªã‚¬ãƒ¼ã®ç¢ºèª
        if 'token' in trigger:
            target_token = trigger['token']
            doc_tokens = [token.text for token in doc]
            token_match = target_token in doc_tokens
            print(f"  tokenãƒˆãƒªã‚¬ãƒ¼: '{target_token}' â†’ æ–‡æ›¸å†…: {doc_tokens} â†’ ãƒãƒƒãƒ: {token_match}")
            if not token_match:
                return False
        
        # lemmaãƒˆãƒªã‚¬ãƒ¼ã®ç¢ºèª
        if 'lemma' in trigger:
            lemmas = trigger['lemma'] if isinstance(trigger['lemma'], list) else [trigger['lemma']]
            doc_lemmas = [token.lemma_ for token in doc]
            lemma_match = any(token.lemma_ in lemmas for token in doc)
            print(f"  lemmaãƒˆãƒªã‚¬ãƒ¼: {lemmas} â†’ æ–‡æ›¸å†…: {doc_lemmas} â†’ ãƒãƒƒãƒ: {lemma_match}")
            if not lemma_match:
                return False
        
        # posãƒˆãƒªã‚¬ãƒ¼ã®ç¢ºèª
        if 'pos' in trigger:
            pos_tags = trigger['pos'] if isinstance(trigger['pos'], list) else [trigger['pos']]
            doc_pos = [token.pos_ for token in doc]
            pos_match = any(token.pos_ in pos_tags for token in doc)
            print(f"  POSãƒˆãƒªã‚¬ãƒ¼: {pos_tags} â†’ æ–‡æ›¸å†…: {doc_pos} â†’ ãƒãƒƒãƒ: {pos_match}")
            if not pos_match:
                return False
        
        # depãƒˆãƒªã‚¬ãƒ¼ã®ç¢ºèªï¼ˆä¾å­˜é–¢ä¿‚ãƒ©ãƒ™ãƒ«ï¼‰
        if 'dep' in trigger:
            dep_tags = trigger['dep'] if isinstance(trigger['dep'], list) else [trigger['dep']]
            doc_deps = [token.dep_ for token in doc]
            dep_match = any(token.dep_ in dep_tags for token in doc)
            print(f"  depãƒˆãƒªã‚¬ãƒ¼: {dep_tags} â†’ æ–‡æ›¸å†…: {doc_deps} â†’ ãƒãƒƒãƒ: {dep_match}")
            if not dep_match:
                return False
        
        # patternãƒˆãƒªã‚¬ãƒ¼ã®ç¢ºèª
        if 'pattern' in trigger:
            pattern = trigger['pattern']
            pattern_match = bool(re.search(pattern, doc.text))
            print(f"  ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒˆãƒªã‚¬ãƒ¼: {pattern} â†’ ãƒãƒƒãƒ: {pattern_match}")
            if not pattern_match:
                return False
        
        # positionãƒˆãƒªã‚¬ãƒ¼ã®ç¢ºèªï¼ˆé«˜åº¦ãªæ¡ä»¶ï¼‰
        if 'position' in trigger:
            position = trigger['position']
            print(f"  ä½ç½®æ¡ä»¶: {position}")
            # å®Ÿè£…ä¾‹ï¼š'before_first_main_verb' ãªã©
            if position == 'before_first_main_verb':
                main_verbs = [token for token in doc if token.pos_ == 'VERB' and token.dep_ in ['ROOT', 'ccomp']]
                if main_verbs:
                    first_verb_idx = main_verbs[0].i
                    # ã“ã“ã§ä½ç½®é–¢ä¿‚ã‚’ç¢ºèªã™ã‚‹å…·ä½“çš„ãªãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
                    print(f"    ä¸»å‹•è©ä½ç½®: {first_verb_idx}")
            elif position == 'after_V':
                # å‹•è©ã®å¾Œã«æ¥ã‚‹å‰¯è©ã‚’ãƒã‚§ãƒƒã‚¯
                main_verbs = [token for token in doc if token.pos_ == 'VERB' and token.dep_ in ['ROOT', 'ccomp']]
                if main_verbs:
                    main_verb = main_verbs[0]
                    # ä¸»å‹•è©ã®å¾Œã«ã‚ã‚‹å‰¯è©ã‚’æ¢ã™
                    pos_check_passed = False
                    if 'pos' in trigger:
                        target_pos = trigger['pos']
                        adverbs_after_verb = [token for token in doc if token.pos_ in target_pos and token.i > main_verb.i]
                        if adverbs_after_verb:
                            pos_check_passed = True
                            print(f"    å‹•è©å¾Œã®{target_pos}: {[token.text for token in adverbs_after_verb]}")
                        else:
                            print(f"    å‹•è©å¾Œã«{target_pos}ãŒè¦‹ã¤ã‹ã‚‰ãªã„")
                            return False
                    else:
                        pos_check_passed = True
                    
                    if not pos_check_passed:
                        return False
        
        # senseãƒˆãƒªã‚¬ãƒ¼ã®ç¢ºèªï¼ˆæ„å‘³çš„æ¡ä»¶ï¼‰
        if 'sense' in trigger:
            sense = trigger['sense']
            print(f"  æ„å‘³æ¡ä»¶: {sense}")
            # å®Ÿè£…ä¾‹ï¼š'exist_locative' ãªã©
            if sense == 'exist_locative':
                # å ´æ‰€çš„å­˜åœ¨ã‚’è¡¨ã™æ–‡è„ˆã‹ã©ã†ã‹ã‚’åˆ¤å®š
                prep_tokens = [token for token in doc if token.pos_ == 'ADP']
                location_preps = any(token.text.lower() in ['in', 'on', 'at', 'by'] for token in prep_tokens)
                print(f"    å ´æ‰€çš„å‰ç½®è©: {location_preps}")
        
        # conditionsãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèªï¼ˆè¿½åŠ æ¡ä»¶ï¼‰
        conditions = rule.get('conditions', {})
        if conditions:
            position = conditions.get('position', '')
            if position == 'sentence_final':
                # æ–‡æœ«ä½ç½®ã®ç¢ºèª
                lemmas = trigger.get('lemma', [])
                if lemmas:
                    target_lemma = lemmas[0] if isinstance(lemmas, list) else lemmas
                    # å¯¾è±¡lemmaãŒæ–‡æœ«è¿‘ãã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆå¥èª­ç‚¹é™¤ãï¼‰
                    content_tokens = [token for token in doc if not token.is_punct]
                    if content_tokens:
                        last_content_token = content_tokens[-1]
                        if last_content_token.lemma_ == target_lemma:
                            print(f"    æ–‡æœ«ä½ç½®ç¢ºèª: '{target_lemma}' ãŒæ–‡æœ«ã«é…ç½®")
                        else:
                            print(f"    æ–‡æœ«ä½ç½®ç¢ºèªå¤±æ•—: æœ€çµ‚èªã¯ '{last_content_token.lemma_}' (æœŸå¾…: '{target_lemma}')")
                            return False
        
        # å¥å‹•è©ç²’å­ã®ç‰¹åˆ¥ãªæ¡ä»¶ãƒã‚§ãƒƒã‚¯
        if 'conditions' in trigger:
            phrasal_conditions = trigger['conditions']
            if 'follows_verb' in phrasal_conditions and phrasal_conditions['follows_verb']:
                # å‹•è©ã®å¾Œã«ç¶šãç²’å­ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
                verb_found = False
                particle_found = False
                
                verbs = [token for token in doc if token.pos_ == 'VERB']
                if verbs:
                    main_verb = verbs[0]  # æœ€åˆã®å‹•è©ã‚’ä¸»å‹•è©ã¨ã—ã¦æ‰±ã†
                    
                    # å‹•è©ã®ç›´å¾Œã¾ãŸã¯è¿‘ãã«ç²’å­ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    for token in doc:
                        if (token.pos_ == 'ADP' and 
                            token.dep_ in ['prt', 'prep'] and
                            token.i > main_verb.i and 
                            token.i - main_verb.i <= 3):  # å‹•è©ã‹ã‚‰3ãƒˆãƒ¼ã‚¯ãƒ³ä»¥å†…
                            
                            # å¥å‹•è©ç²’å­ãƒªã‚¹ãƒˆã®ãƒã‚§ãƒƒã‚¯
                            if 'phrasal_verb_particle' in phrasal_conditions:
                                particle_list = phrasal_conditions['phrasal_verb_particle']
                                if token.text.lower() in particle_list:
                                    particle_found = True
                                    print(f"    å¥å‹•è©ç²’å­ç¢ºèª: '{main_verb.text}' + '{token.text}' (dep: {token.dep_})")
                                    break
                
                if not particle_found:
                    print(f"    å¥å‹•è©ç²’å­æ¡ä»¶å¤±æ•—: å‹•è©å¾Œã®é©åˆ‡ãªç²’å­ãŒè¦‹ã¤ã‹ã‚‰ãªã„")
                    return False

        print(f"  â†’ ãƒ«ãƒ¼ãƒ«é©ç”¨å¯¾è±¡: {rule_id}")
        return True
    
    def _apply_single_rule(self, rule: Dict[str, Any], doc, hierarchy, slots: Dict[str, List]) -> bool:
        """å˜ä¸€ãƒ«ãƒ¼ãƒ«ã®é©ç”¨"""
        
        rule_id = rule.get('id', '')
        assignment = rule.get('assign', {})
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒˆãƒªã‚¬ãƒ¼ã®å ´åˆã¯ç‰¹åˆ¥å‡¦ç†
        trigger = rule.get('trigger', {})
        if 'pattern' in trigger:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã®å€¤æŠ½å‡º
            pattern = trigger['pattern']
            match = re.search(pattern, doc.text, re.IGNORECASE)
            if match:
                # assignãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§valueãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’å„ªå…ˆ
                assignment = rule.get('assign', {})
                if 'value' in assignment:
                    value = assignment['value']
                # å®Ÿéš›ã®å€¤ã‚’æ±ºå®š
                elif rule_id == 'place-M3':
                    value = self._extract_place_prepositional_phrase(doc)
                elif rule_id == 'to-direction-M2':
                    value = self._extract_direction_prepositional_phrase(doc, rule_id)
                elif rule_id == 'for-purpose-M2':
                    value = self._extract_direction_prepositional_phrase(doc, rule_id)
                elif rule_id == 'from-source-M3':
                    value = self._extract_from_prepositional_phrase(doc)
                else:
                    value = match.group()
                
                if value:
                    slot = assignment.get('slot', '')
                    if slot in slots:
                        # ä¸å®šè©ã®ç”¨æ³•åˆ¥å‡¦ç†
                        if rule_id == 'to-direction-M2' and self._contains_verb(value, doc):
                            # ä¸å®šè©ã®åè©çš„ç”¨æ³•ã‚’ãƒã‚§ãƒƒã‚¯
                            if self._is_infinitive_as_noun(value, doc):
                                # ä¸å®šè©ã®åè©çš„ç”¨æ³•ã®å ´åˆã¯O1ã®phraseå€™è£œã¨ã—ã¦è¿½åŠ 
                                print(f"ğŸ”„ ä¸å®šè©ã®åè©çš„ç”¨æ³•æ¤œå‡º: '{value}' â†’ O1 phraseã«å¤‰æ›´")
                                if 'O1' not in slots:
                                    slots['O1'] = []
                                slots['O1'].append({
                                    'value': value,
                                    'rule_id': rule_id,
                                    'confidence': 0.95,
                                    'is_phrase': True,
                                    'label': 'phrase'
                                })
                                print(f"    âœ… O1ã«'{value}'ã‚’phraseã¨ã—ã¦è¨­å®š")
                                return True
                            
                            # ä¸å®šè©ã®å½¢å®¹è©çš„ç”¨æ³•ã‚’ãƒã‚§ãƒƒã‚¯
                            elif self._is_infinitive_as_adjective(value, doc):
                                # å½¢å®¹è©çš„ç”¨æ³•ã®å ´åˆã¯å…ƒã®åè©å¥ã«çµ±åˆï¼ˆM2ã«è¿½åŠ ã—ãªã„ï¼‰
                                print(f"ğŸ”„ ä¸å®šè©ã®å½¢å®¹è©çš„ç”¨æ³•æ¤œå‡º: '{value}' â†’ åè©å¥ã«çµ±åˆï¼ˆM2ã‹ã‚‰é™¤å¤–ï¼‰")
                                return True  # å‡¦ç†æ¸ˆã¿ã¨ã—ã¦M2ã«ã¯è¿½åŠ ã—ãªã„
                            
                            # ä¸å®šè©ã®å‰¯è©çš„ç”¨æ³•ã‚’ãƒã‚§ãƒƒã‚¯
                            elif self._is_infinitive_as_adverb(value, doc):
                                # å‰¯è©çš„ç”¨æ³•ã®å ´åˆã¯M2ã®phraseã¨ã—ã¦å‡¦ç†
                                print(f"ğŸ”„ ä¸å®šè©ã®å‰¯è©çš„ç”¨æ³•æ¤œå‡º: '{value}' â†’ M2 phraseã¨ã—ã¦å‡¦ç†")
                                slots[slot].append({
                                    'value': value,
                                    'rule_id': rule_id,
                                    'confidence': 0.9,
                                    'is_phrase': True,
                                    'label': 'phrase'
                                })
                                print(f"    âœ… M2ã«'{value}'ã‚’phraseã¨ã—ã¦è¨­å®š")
                                return True
                        
                        # å‹•è©ã‚’å«ã‚€å¥ã®ã¿ã‚’ã€Œphraseã€ã¨ã—ã¦æ‰±ã†
                        is_verb_phrase = self._contains_verb(value, doc)
                        
                        candidate_data = {
                            'value': value,
                            'rule_id': rule_id,
                            'confidence': 0.9
                        }
                        
                        # assignãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã‚‰orderæƒ…å ±ã‚’å–å¾—
                        assignment = rule.get('assign', {})
                        if 'order' in assignment:
                            candidate_data['order'] = assignment['order']
                        
                        # å‹•è©ã‚’å«ã‚€å¥ã®å ´åˆã®ã¿phraseãƒ•ãƒ©ã‚°ã‚’è¨­å®š
                        if is_verb_phrase:
                            candidate_data['is_phrase'] = True
                            print(f"ğŸ“ å‹•è©å¥ãƒ«ãƒ¼ãƒ«é©ç”¨: {rule_id} â†’ {slot}: '{value}' (phrase)")
                        else:
                            print(f"ğŸ“ ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«é©ç”¨: {rule_id} â†’ {slot}: '{value}' (word)")
                        
                        slots[slot].append(candidate_data)
                        return True
            return False
        
        # é€šå¸¸ã®ãƒ«ãƒ¼ãƒ«å‡¦ç†
        if isinstance(assignment, list):
            # è¤‡æ•°å‰²ã‚Šå½“ã¦ã®å ´åˆ
            for assign_item in assignment:
                self._execute_assignment(assign_item, doc, hierarchy, slots, rule_id)
            return True
        else:
            # å˜ä¸€å‰²ã‚Šå½“ã¦ã®å ´åˆ
            return self._execute_assignment(assignment, doc, hierarchy, slots, rule_id)
    
    def _apply_pattern_rule(self, rule: Dict[str, Any], doc, hierarchy, slots: Dict[str, List]) -> bool:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«ã®é©ç”¨"""
        
        rule_id = rule.get('id', '')
        patterns = rule.get('patterns', [])
        
        print(f"ğŸ“ ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«é©ç”¨: {rule_id}")
        
        for pattern_obj in patterns:
            pattern_text = pattern_obj.get('pattern', '')
            assign_data = pattern_obj.get('assign', {})
            
            match = re.search(pattern_text, doc.text, re.IGNORECASE)
            if match:
                print(f"  âœ… ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒ: {pattern_text}")
                print(f"  ğŸ“Œ ãƒãƒƒãƒéƒ¨åˆ†: '{match.group()}'")
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãå‰²ã‚Šå½“ã¦å®Ÿè¡Œ
                if isinstance(assign_data, list):
                    for assign_item in assign_data:
                        self._execute_pattern_assignment(assign_item, match, doc, slots, rule_id)
                else:
                    self._execute_pattern_assignment(assign_data, match, doc, slots, rule_id)
                return True
        
        return False
    
    def _execute_pattern_assignment(self, assignment: Dict[str, Any], match, doc, slots: Dict[str, List], rule_id: str):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã®å‰²ã‚Šå½“ã¦å®Ÿè¡Œ"""
        
        slot = assignment.get('slot', '')
        value_type = assignment.get('type', 'word')
        value_spec = assignment.get('value', '')
        
        print(f"    ğŸ¯ ã‚¹ãƒ­ãƒƒãƒˆ: {slot}, ã‚¿ã‚¤ãƒ—: {value_type}, å€¤æŒ‡å®š: {value_spec}")
        
        # å®Ÿéš›ã®å€¤ã‚’æ±ºå®š
        if value_type == 'group':
            # æ­£è¦è¡¨ç¾ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰å€¤ã‚’å–å¾—
            group_num = assignment.get('group', 1)
            if group_num <= len(match.groups()):
                value = match.group(group_num)
            else:
                value = match.group()
        elif value_type == 'word':
            # æŒ‡å®šã•ã‚ŒãŸå˜èªã‹ã‚‰å€¤ã‚’å–å¾—
            value = self._find_word_in_sentence(value_spec, doc)
        elif value_type == 'phrase':
            # æŒ‡å®šã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ã‚ºã‹ã‚‰å€¤ã‚’å–å¾—
            value = value_spec
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãå‰ç½®è©å¥æŠ½å‡º
            if rule_id == 'place-M3':
                value = self._extract_place_prepositional_phrase(doc)
            elif rule_id in ['to-direction-M2', 'for-purpose-M2']:
                value = self._extract_direction_prepositional_phrase(doc, rule_id)
            else:
                value = match.group()
        
        if value and slot in slots:
            # ä¸å®šè©ã®åè©çš„ç”¨æ³•ã‚’ãƒã‚§ãƒƒã‚¯
            if rule_id == 'to-direction-M2' and self._is_infinitive_as_noun(value, doc):
                # ä¸å®šè©ã®åè©çš„ç”¨æ³•ã®å ´åˆã¯O1ã®phraseå€™è£œã¨ã—ã¦è¿½åŠ 
                print(f"ğŸ”„ ä¸å®šè©ã®åè©çš„ç”¨æ³•æ¤œå‡º: '{value}' â†’ O1 phraseã«å¤‰æ›´")
                if 'O1' not in slots:
                    slots['O1'] = []
                slots['O1'].append({
                    'value': value,
                    'rule_id': rule_id,
                    'confidence': 0.95,
                    'pattern_based': True,
                    'is_phrase': True,
                    'label': 'phrase'
                })
                print(f"    âœ… O1ã«'{value}'ã‚’phraseã¨ã—ã¦è¨­å®š")
            else:
                # é€šå¸¸ã®å‰ç½®è©å¥ã¨ã—ã¦å‡¦ç†
                is_phrase = self._contains_verb(value, doc)
                slots[slot].append({
                    'value': value,
                    'rule_id': rule_id,
                    'confidence': 0.9,
                    'pattern_based': True,
                    'is_phrase': is_phrase,
                    'label': 'phrase' if is_phrase else 'word'
                })
                print(f"    âœ… {slot}ã«'{value}'ã‚’è¨­å®š")
    
    def _find_word_in_sentence(self, target_word: str, doc) -> str:
        """æ–‡ä¸­ã‹ã‚‰æŒ‡å®šã•ã‚ŒãŸå˜èªã‚’æ¤œç´¢"""
        for token in doc:
            if token.text.lower() == target_word.lower() or token.lemma_.lower() == target_word.lower():
                return token.text
        return target_word
    
    def _execute_assignment(self, assignment: Dict[str, Any], doc, hierarchy, slots: Dict[str, List], rule_id: str) -> bool:
        """å‰²ã‚Šå½“ã¦ã®å®Ÿè¡Œ"""
        
        slot = assignment.get('slot', '')
        assign_type = assignment.get('type', 'word')
        
        if slot not in slots:
            return False
        
        # å€¤ã®æ±ºå®š
        value = self._determine_assignment_value(assignment, doc, hierarchy, rule_id)
        
        if value:
            candidate_data = {
                'value': value,
                'type': assign_type,
                'rule_id': rule_id
            }
            
            # assignãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã‚‰orderæƒ…å ±ã‚’å–å¾—
            if 'order' in assignment:
                candidate_data['order'] = assignment['order']
                
            slots[slot].append(candidate_data)
            return True
            
        return False
    
    def _determine_assignment_value(self, assignment: Dict[str, Any], doc, hierarchy, rule_id: str) -> Optional[str]:
        """å‰²ã‚Šå½“ã¦å€¤ã®æ±ºå®š"""
        
        # ç‰¹å®šã®ãƒ«ãƒ¼ãƒ«ã«åŸºã¥ãå€¤æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯
        if rule_id.startswith('aux-'):
            return self._extract_auxiliary_value(doc, hierarchy)
        elif rule_id.startswith('V-'):
            return self._extract_verb_value(doc, hierarchy, rule_id)
        elif rule_id.startswith('time-'):
            return self._extract_temporal_value(doc, hierarchy)
        elif rule_id.startswith('subject-'):
            return self._extract_subject_value(doc, hierarchy)
        elif rule_id == 'manner-degree-M2':
            return self._extract_adverb_value(doc)
        elif rule_id == 'phrasal-verb-particle-M2':
            return self._extract_phrasal_verb_particle(doc)
        elif rule_id == 'wh-where-front':
            return self._extract_wh_where_value(doc)
        elif rule_id == 'please-interjection-M3':
            return self._extract_please_interjection_value(doc)
        else:
            # æ±ç”¨çš„ãªå€¤æŠ½å‡º
            return self._extract_generic_value(assignment, doc, hierarchy)
    
    def _generate_subslot_structures(self, doc, hierarchy) -> List[Dict[str, Any]]:
        """Sub-slotæ§‹é€ ã®ç”Ÿæˆï¼ˆé–¢ä¿‚è©ç¯€ãƒ»å¾“å±ç¯€ç”¨ï¼‰"""
        
        sub_structures = []
        
        for sub_structure in hierarchy['subordinate_structures']:
            sub_slots = {slot: [] for slot in self.sub_slots}
            
            # Sub-slotã®å‰²ã‚Šå½“ã¦
            clause_type = sub_structure['type']
            clause_verb = sub_structure['verb']
            
            if clause_type == 'relative':
                # é–¢ä¿‚è©ç¯€ã®sub-slotå‡¦ç†
                self._process_relative_clause_subslots(clause_verb, sub_slots, doc)
            elif clause_type == 'adverbial':
                # å‰¯è©ç¯€ã®sub-slotå‡¦ç†
                self._process_adverbial_clause_subslots(clause_verb, sub_slots, doc)
            
            sub_structures.append({
                'type': clause_type,
                'verb': clause_verb.text,
                'sub_slots': sub_slots,
                'parent_element': sub_structure.get('modified_element', '')
            })
        
        return sub_structures
    
    def _determine_sentence_pattern(self, main_slots: Dict[str, List], sub_structures: List) -> str:
        """5æ–‡å‹ã®æ­£ç¢ºãªåˆ¤å®š"""
        
        has_s = bool(main_slots.get('S'))
        has_v = bool(main_slots.get('V'))
        has_o1 = bool(main_slots.get('O1'))
        has_o2 = bool(main_slots.get('O2'))
        has_c1 = bool(main_slots.get('C1'))
        has_aux = bool(main_slots.get('Aux'))
        
        # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
        print(f"ğŸ” æ–‡å‹åˆ¤å®š: S={has_s}, V={has_v}, O1={has_o1}, O2={has_o2}, C1={has_c1}, Aux={has_aux}")
        
        if has_s and has_v:
            if has_o1 and has_o2:
                return "ç¬¬4æ–‡å‹ (SVOO)"
            elif has_o1 and has_c1:
                return "ç¬¬5æ–‡å‹ (SVOC)"
            elif has_o1:
                return "ç¬¬3æ–‡å‹ (SVO)"
            elif has_c1:
                return "ç¬¬2æ–‡å‹ (SVC)"
            else:
                return "ç¬¬1æ–‡å‹ (SV)"
        elif has_v:
            return "å‘½ä»¤æ–‡ã¾ãŸã¯ç‰¹æ®Šæ§‹é€ "
        
        return "ä¸å®Œå…¨ãªæ–‡æ§‹é€ "
    
    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤
    def _find_clause_marker(self, verb_token) -> Optional[str]:
        """ç¯€ã®ãƒãƒ¼ã‚«ãƒ¼ã‚’æ¢ã™"""
        for child in verb_token.children:
            if child.dep_ == "mark":
                return child.text
        return None
    
    def _find_relativizer(self, verb_token) -> Optional[str]:
        """é–¢ä¿‚ä»£åè©ã‚’æ¢ã™"""
        for token in verb_token.doc:
            if token.dep_ == "nsubj" and token.head == verb_token and token.pos_ == "PRON":
                if token.text.lower() in ['who', 'which', 'that', 'whom', 'whose']:
                    return token.text
        return None
    
    def _get_complete_noun_phrase(self, token) -> str:
        """å®Œå…¨ãªåè©å¥ã‚’å–å¾—ï¼ˆé–¢ä¿‚è©ç¯€ã‚’å«ã‚€ï¼‰"""
        # åŸºæœ¬çš„ãªåè©å¥ã®ç¯„å›²ã‚’æ±ºå®š
        start_i = token.i
        end_i = token.i + 1
        
        # å·¦å´ã®ä¿®é£¾èªã‚’æ¢ç´¢ï¼ˆå† è©ã€å½¢å®¹è©ã€æ‰€æœ‰æ ¼ãªã©ï¼‰
        for child in token.children:
            if child.dep_ in ['det', 'amod', 'nmod', 'compound', 'nummod', 'poss'] and child.i < token.i:
                start_i = min(start_i, child.i)
        
        # å³å´ã®é–¢ä¿‚è©ç¯€ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ - é–¢ä¿‚è©ç¯€å…¨ä½“ã‚’å«ã‚ã‚‹
        for child in token.children:
            if child.dep_ == 'relcl':
                # é–¢ä¿‚è©ç¯€ã®çµ‚ç«¯ã¾ã§å«ã‚ã‚‹
                end_i = max(end_i, child.right_edge.i + 1)
        
        # å‰ç½®è©å¥ã®å ´åˆã®å‡¦ç†
        if token.head and token.head.pos_ == 'ADP' and token.head.i < start_i:
            start_i = token.head.i
        
        return token.doc[start_i:end_i].text
    
    def _get_relative_clause_phrase(self, rel_verb) -> str:
        """é–¢ä¿‚è©ç¯€ã®å®Œå…¨ãªãƒ•ãƒ¬ãƒ¼ã‚ºã‚’å–å¾— - Rephraseã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå½¢å¼"""
        # é–¢ä¿‚ä»£åè©ã®ç‰¹å®š
        relativizer = self._find_relativizer(rel_verb)
        if not relativizer:
            # thatã®å ´åˆï¼ˆç›®çš„æ ¼é–¢ä¿‚ä»£åè©ï¼‰
            for token in rel_verb.doc:
                if (token.text.lower() == 'that' and 
                    token.i < rel_verb.i and
                    any(child.dep_ == 'relcl' and child == rel_verb for child in token.head.children)):
                    relativizer = "that"
                    break
            if not relativizer:
                relativizer = "that"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            
        # é–¢ä¿‚è©ç¯€å†…ã®è¦ç´ ã‚’åé›†ï¼ˆé–¢ä¿‚ä»£åè©ã‚’é™¤ãï¼‰
        clause_tokens = []
        
        # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®è¦ç´ ã‚’ç‰¹å®š
        sub_elements = {}
        
        # ä¸»èª (é–¢ä¿‚ä»£åè© or é€šå¸¸ã®ä¸»èª)
        if relativizer.lower() in ['who', 'which']:
            sub_elements['s'] = f"{relativizer}_sub-s"
        else:
            # ç›®çš„æ ¼é–¢ä¿‚ä»£åè©ã®å ´åˆ
            if relativizer.lower() in ['that', 'whom', 'which']:
                sub_elements['o1'] = f"{relativizer}_sub-o1"
            
            # é–¢ä¿‚è©ç¯€å†…ã®ä¸»èªã‚’æ¢ã™
            for child in rel_verb.children:
                if child.dep_ == 'nsubj':
                    sub_elements['s'] = f"{child.text}_sub-s"
                    break
        
        # å‹•è©
        sub_elements['v'] = f"{rel_verb.text}_sub-v"
        
        # ç›®çš„èª
        for child in rel_verb.children:
            if (child.dep_ == 'dobj' and 
                child.text.lower() not in ['who', 'which', 'that', 'whom']):
                sub_elements['o1'] = f"{child.text}_sub-o1"
                break
        
        # å‰¯è©ãƒ»ä¿®é£¾èª
        for child in rel_verb.children:
            if child.dep_ == 'advmod' and child.pos_ == 'ADV':
                sub_elements['m2'] = f"{child.text}_sub-m2"
            elif child.dep_ == 'prep':
                prep_phrase = self._get_prepositional_phrase(child)
                if self._is_temporal_or_locative(child):
                    sub_elements['m3'] = f"{prep_phrase}_sub-m3"
        
        # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå½¢å¼ã§çµåˆ
        parts = []
        for slot in ['s', 'v', 'o1', 'o2', 'aux', 'm1', 'm2', 'm3', 'c1']:
            if slot in sub_elements:
                parts.append(sub_elements[slot])
        
        return ', '.join(parts) if parts else f"{relativizer} {rel_verb.text}"
    
    def _has_modifying_clause(self, token) -> bool:
        """ä¿®é£¾ç¯€ã‚’æŒã¤ã‹ã©ã†ã‹ã®ç¢ºèª"""
        for child in token.children:
            if child.dep_ in ['relcl', 'acl']:
                return True
        return False
    
    def _analyze_subclause_structure(self, verb_token, clause_type: str) -> Dict[str, Any]:
        """å¾“å±ç¯€æ§‹é€ ã®åˆ†æ"""
        return {
            'type': clause_type,
            'verb': verb_token,
            'modified_element': verb_token.head.text if clause_type == 'relative' else None,
            'complexity': 'simple'  # ç°¡ç•¥åŒ–
        }
    
    def _calculate_complexity(self, hierarchy) -> int:
        """æ–‡ã®è¤‡é›‘åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        base_score = 1
        if hierarchy['subordinate_structures']:
            base_score += len(hierarchy['subordinate_structures']) * 2
        
        return base_score
    
    # === Step 1: åŸºæœ¬æŠ½å‡ºãƒ¡ã‚½ãƒƒãƒ‰ã®å®Ÿè£… ===
    
    def _extract_subject_value(self, doc, hierarchy) -> Optional[str]:
        """ä¸»èªã®æ­£ç¢ºãªæŠ½å‡º - Rephraseãƒ«ãƒ¼ãƒ«å¯¾å¿œ"""
        
        # ä¸»ç¯€ã®ä¸»èªã‚’å„ªå…ˆ
        main_subject = hierarchy.get('main_clause', {}).get('subject')
        
        if main_subject and main_subject['type'] == 'complex':
            # è¤‡é›‘ãªä¸»èªï¼ˆé–¢ä¿‚è©ç¯€ä»˜ãï¼‰ã®å‡¦ç†
            return main_subject['full_phrase']
        elif main_subject:
            # å˜ç´”ãªä¸»èª
            return main_subject['full_phrase']
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: spaCyã‹ã‚‰ç›´æ¥æŠ½å‡º
        for token in doc:
            if token.dep_ == "nsubj" and token.head.dep_ == "ROOT":
                return self._get_complete_noun_phrase(token)
        
        return None
    
    def _extract_verb_value(self, doc, hierarchy, rule_id: str) -> Optional[str]:
        """å‹•è©ã®æ­£ç¢ºãªæŠ½å‡º - ãƒ«ãƒ¼ãƒ«åˆ¥å‡¦ç†"""
        
        print(f"ğŸ” å‹•è©æŠ½å‡ºé–‹å§‹ - ãƒ«ãƒ¼ãƒ«: {rule_id}")
        print(f"  éšå±¤ãƒ‡ãƒ¼ã‚¿: {hierarchy.keys()}")
        
        main_verb = hierarchy.get('main_clause', {}).get('verb')
        print(f"  éšå±¤ã‹ã‚‰å–å¾—ã—ãŸå‹•è©: {main_verb}")
        
        if main_verb:
            # åŸºæœ¬å‹•è©ã®æŠ½å‡º
            verb_text = main_verb.text
            print(f"  â†’ å‹•è©ãƒ†ã‚­ã‚¹ãƒˆ: '{verb_text}'")
            
            # ç‰¹å®šã®ãƒ«ãƒ¼ãƒ«ã«åŸºã¥ãèª¿æ•´
            if 'progressive' in rule_id:
                # é€²è¡Œå½¢ã®å‡¦ç†
                for child in main_verb.children:
                    if child.dep_ == "aux" and child.lemma_ == "be":
                        return f"{child.text} {verb_text}"
            
            elif 'perfect' in rule_id:
                # å®Œäº†å½¢ã®å‡¦ç†
                for child in main_verb.children:
                    if child.dep_ == "aux" and child.lemma_ == "have":
                        return f"{child.text} {verb_text}"
            
            return verb_text
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ROOTå‹•è©ã‚’æ¢ã™
        print(f"  ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ROOTå‹•è©ã‚’æ¤œç´¢")
        for token in doc:
            if token.dep_ == "ROOT" and token.pos_ == "VERB":
                print(f"  â†’ ROOTå‹•è©ç™ºè¦‹: '{token.text}' (pos: {token.pos_}, dep: {token.dep_})")
                return token.text
        
        print(f"  â†’ å‹•è©è¦‹ã¤ã‹ã‚‰ãš")
        return None
    
    def _extract_generic_subject(self, doc, hierarchy) -> Optional[str]:
        """æ±ç”¨çš„ãªä¸»èªæ¤œå‡ºï¼ˆãƒ«ãƒ¼ãƒ«ã§æ•ç²ã•ã‚Œãªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        print(f"ğŸ” æ±ç”¨ä¸»èªæ¤œå‡ºé–‹å§‹")
        
        # éšå±¤ã‹ã‚‰ä¸»èªã‚’å–å¾—
        main_subject = hierarchy.get('main_clause', {}).get('subject')
        if main_subject and 'full_phrase' in main_subject:
            subject_phrase = main_subject['full_phrase']
            print(f"  â†’ éšå±¤ã‹ã‚‰ä¸»èªæŠ½å‡º: '{subject_phrase}'")
            return subject_phrase
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: spaCyã‹ã‚‰ç›´æ¥æŠ½å‡ºï¼ˆé–¢ä¿‚è©ç¯€ã‚’å«ã‚€å®Œå…¨ãªåè©å¥ï¼‰
        for token in doc:
            if token.dep_ == "nsubj" and token.head.dep_ == "ROOT":
                subject_phrase = self._get_complete_noun_phrase(token)
                print(f"  â†’ spaCyã‹ã‚‰ä¸»èªæŠ½å‡º: '{subject_phrase}' (token: {token.text})")
                return subject_phrase
        
        print(f"  â†’ ä¸»èªè¦‹ã¤ã‹ã‚‰ãš")
        return None
    
    def _extract_generic_verb(self, doc, hierarchy) -> Optional[str]:
        """æ±ç”¨çš„ãªå‹•è©æ¤œå‡ºï¼ˆãƒ«ãƒ¼ãƒ«ã§æ•ç²ã•ã‚Œãªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        
        # ROOTå‹•è©ã‚’æœ€å„ªå…ˆ
        for token in doc:
            if token.dep_ == "ROOT" and token.pos_ == "VERB":
                return token.text
        
        # ä»–ã®å‹•è©ã‚’æ¤œç´¢
        for token in doc:
            if token.pos_ == "VERB":
                return token.text
                
        return None
    
    def _extract_generic_object(self, doc, hierarchy) -> Optional[str]:
        """æ±ç”¨çš„ãªç›®çš„èªæ¤œå‡ºï¼ˆãƒ«ãƒ¼ãƒ«ã§æ•ç²ã•ã‚Œãªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        
        # ç›´æ¥ç›®çš„èªï¼ˆdobjï¼‰ã‚’æœ€å„ªå…ˆ
        for token in doc:
            if token.dep_ == "dobj":
                return self._get_complete_noun_phrase(token)
        
        # é–“æ¥ç›®çš„èªï¼ˆiobjï¼‰
        for token in doc:
            if token.dep_ == "iobj":
                return self._get_complete_noun_phrase(token)
                
        # è£œèªï¼ˆattr, pcompï¼‰
        for token in doc:
            if token.dep_ in ["attr", "pcomp"]:
                return self._get_complete_noun_phrase(token)
                
        return None
    
    def _extract_generic_time_expression(self, doc) -> Optional[str]:
        """æ±ç”¨çš„ãªæ™‚é–“è¡¨ç¾æ¤œå‡º"""
        
        # æ˜ç¢ºãªæ™‚é–“ãƒ‘ã‚¿ãƒ¼ãƒ³
        time_patterns = [
            r'\b(today|tomorrow|yesterday)\b',
            r'\b(last|this|next)\s+(week|month|year|morning|afternoon|evening|night)\b',
            r'\b(every|each)\s+(day|week|month|morning|afternoon|evening)\b',
            r'\b(a\s+few|several|many)\s+(days?|weeks?|months?|years?)\s+ago\b',
            r'\b(that|this)\s+(morning|afternoon|evening|night)\b',
            r'\b\d+\s*(am|pm)\b',
            r'\b(at|on|in)\s+(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\b'
        ]
        
        text = doc.text
        for pattern in time_patterns:
            import re
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(0)
        
        # spaCyã®æ™‚é–“ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
        for ent in doc.ents:
            if ent.label_ in ["TIME", "DATE"]:
                return ent.text
        
        # å‰¯è©å¥ï¼ˆæ™‚é–“è¡¨ç¾ã®å¯èƒ½æ€§ï¼‰
        time_advs = []
        for token in doc:
            if token.pos_ == "ADV" and token.dep_ in ["advmod", "npadvmod"]:
                # æ™‚é–“ã‚’ç¤ºå”†ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
                if any(keyword in token.text.lower() for keyword in ['day', 'morning', 'afternoon', 'evening', 'night', 'today', 'ago']):
                    return self._get_complete_adverb_phrase(token)
        
        return None
    
    def _extract_phrasal_verb_particle(self, doc) -> Optional[str]:
        """å¥å‹•è©ç²’å­ã®æŠ½å‡º"""
        
        # å¥å‹•è©ã®ç²’å­ãƒªã‚¹ãƒˆ
        phrasal_particles = [
            "off", "on", "up", "down", "out", "in", "away", "back", 
            "over", "after", "through", "into", "onto", "across", 
            "along", "around", "aside", "apart", "ahead", "behind", 
            "beyond", "beneath", "between", "toward", "towards", "underneath"
        ]
        
        # å‹•è©ã‚’æœ€åˆã«è¦‹ã¤ã‘ã‚‹
        verbs = [token for token in doc if token.pos_ == 'VERB']
        if not verbs:
            return None
        
        main_verb = verbs[0]  # æœ€åˆã®å‹•è©ã‚’ä¸»å‹•è©ã¨ã¿ãªã™
        
        # å‹•è©ã®å¾Œã«ç¶šãç²’å­ã‚’æ¢ã™
        for token in doc:
            if (token.pos_ == 'ADP' and 
                token.dep_ in ['prt', 'prep'] and
                token.i > main_verb.i and 
                token.i - main_verb.i <= 3 and  # å‹•è©ã‹ã‚‰3ãƒˆãƒ¼ã‚¯ãƒ³ä»¥å†…
                token.text.lower() in phrasal_particles):
                
                print(f"  âœ… å¥å‹•è©ç²’å­æ¤œå‡º: '{main_verb.text}' + '{token.text}' (ä¾å­˜é–¢ä¿‚: {token.dep_})")
                return token.text
        
        return None
    
    def _extract_generic_auxiliary(self, doc) -> Optional[str]:
        """æ±ç”¨çš„ãªåŠ©å‹•è©æ¤œå‡ºï¼ˆç¸®ç´„å½¢å¯¾å¿œï¼‰"""
        
        # spaCyãŒè§£æã—ãŸç¸®ç´„å½¢ã‚’ç¢ºèª
        for token in doc:
            if token.text.lower() == "ca" and token.pos_ == "AUX":
                # "ca"ã¯"can"ã®ç¸®ç´„å½¢ã®ä¸€éƒ¨
                for next_token in doc:
                    if next_token.i == token.i + 1 and next_token.text in ["n't", "not"]:
                        return "cannot"
                        
        # å®Œå…¨ãªç¸®ç´„å½¢ãƒãƒƒãƒ”ãƒ³ã‚°
        contractions = {
            "can't": "cannot",
            "won't": "will not", 
            "shouldn't": "should not",
            "wouldn't": "would not",
            "couldn't": "could not",
            "mustn't": "must not",
            "needn't": "need not"
        }
        
        # å…ƒã®æ–‡ã‹ã‚‰ç¸®ç´„å½¢ã®æ¤œå‡º
        text_lower = doc.text.lower()
        for contraction, expansion in contractions.items():
            if contraction in text_lower:
                return expansion
        
        # AUXå“è©ã®æ¤œå‡º
        for token in doc:
            if token.pos_ == "AUX" and token.dep_ == "aux":
                # å¦å®šã®å ´åˆ
                for child in token.children:
                    if child.dep_ == "neg" or child.text in ["n't", "not"]:
                        return f"{token.lemma_} not"
                return token.text
                
        return None
    
    def _extract_generic_prepositional_phrase(self, doc) -> Optional[str]:
        """æ±ç”¨çš„ãªå‰ç½®è©å¥æ¤œå‡ºï¼ˆæ–¹å‘ãƒ»å¯¾è±¡ã®M2ã®ã¿ï¼‰"""
        
        # M2ã«å±ã™ã‚‹æ–¹å‘ãƒ»å¯¾è±¡ã‚’ç¤ºã™å‰ç½®è©ã®ã¿
        m2_preps = ['to', 'for', 'with', 'about']  # 'on', 'in', 'by', 'from' ã¯é™¤å¤–ï¼ˆM3ã‚„ãã®ä»–ã®å¯èƒ½æ€§ï¼‰
        
        for token in doc:
            if token.pos_ == "ADP" and token.text.lower() in m2_preps:
                # å‰ç½®è©ã®ç›®çš„èªã‚’å–å¾—
                prep_object = None
                for child in token.children:
                    if child.dep_ == "pobj":
                        prep_object = self._get_complete_noun_phrase(child)
                        break
                
                if prep_object:
                    # å‹•è©ã«ç›´æ¥ä¾å­˜ã—ã¦ã„ã‚‹å‰ç½®è©å¥ã®ã¿ã‚’M2ã¨ã™ã‚‹
                    if token.dep_ == "prep" and token.head.pos_ == "VERB":
                        return f"{token.text} {prep_object}"
        
        return None
    
    def _extract_place_prepositional_phrase(self, doc) -> Optional[str]:
        """å ´æ‰€ã‚’è¡¨ã™å‰ç½®è©å¥ã®æŠ½å‡ºï¼ˆM3ç”¨ï¼‰"""
        
        # å ´æ‰€ã‚’è¡¨ã™å‰ç½®è©
        place_preps = ['on', 'in', 'under', 'by', 'at']
        
        for token in doc:
            if (token.pos_ == "ADP" and 
                token.text.lower() in place_preps and
                token.dep_ == "prep"):
                
                # å‰ç½®è©ã®ç›®çš„èªã‚’å–å¾—
                for child in token.children:
                    if child.dep_ == "pobj":
                        prep_object = self._get_complete_noun_phrase(child)
                        return f"{token.text} {prep_object}"
        
        return None
    
    def _extract_direction_prepositional_phrase(self, doc, rule_id: str) -> Optional[str]:
        """æ–¹å‘ãƒ»ç›®çš„ã‚’è¡¨ã™å‰ç½®è©å¥ã®æŠ½å‡ºï¼ˆM2ç”¨ï¼‰"""
        
        # ãƒ«ãƒ¼ãƒ«ã«å¿œã˜ãŸå‰ç½®è©ã‚’ç‰¹å®š
        if rule_id == 'to-direction-M2':
            target_prep = 'to'
        elif rule_id == 'for-purpose-M2':
            target_prep = 'for'
        else:
            return None
        
        for token in doc:
            # å‰ç½®è©å¥ã®å‡¦ç†ï¼ˆé€šå¸¸ã®å‰ç½®è©å¥ + äºŒé‡ç›®çš„èªã®é–“æ¥ç›®çš„èªãƒãƒ¼ã‚«ãƒ¼ï¼‰
            if (token.pos_ == "ADP" and 
                token.text.lower() == target_prep and
                token.dep_ in ["prep", "dative"]):  # dativeã‚‚è¿½åŠ 
                
                # å‰ç½®è©ã®ç›®çš„èªã‚’å–å¾—
                for child in token.children:
                    if child.dep_ == "pobj":
                        prep_object = self._get_complete_noun_phrase(child)
                        return f"{token.text} {prep_object}"
            
            # ä¸å®šè©å¥ã®å‡¦ç†ï¼ˆto ã®å ´åˆã®ã¿ï¼‰
            elif (rule_id == 'to-direction-M2' and 
                  token.pos_ == "PART" and 
                  token.text.lower() == "to" and
                  token.head and token.head.pos_ == "VERB"):
                
                # ä¸å®šè©å¥å…¨ä½“ã‚’æ§‹ç¯‰
                infinitive_verb = token.head
                infinitive_phrase = f"to {infinitive_verb.text}"
                
                # å‹•è©ã®ç›®çš„èªã‚„ä¿®é£¾èªãŒã‚ã‚Œã°è¿½åŠ 
                objects = []
                for child in infinitive_verb.children:
                    if child.dep_ in ["dobj", "pobj"]:
                        objects.append(self._get_complete_noun_phrase(child))
                    elif child.dep_ in ["prep"]:
                        # å‰ç½®è©å¥ã‚‚å«ã‚ã‚‹
                        for prep_child in child.children:
                            if prep_child.dep_ == "pobj":
                                prep_phrase = f"{child.text} {self._get_complete_noun_phrase(prep_child)}"
                                objects.append(prep_phrase)
                
                if objects:
                    infinitive_phrase += " " + " ".join(objects)
                
                return infinitive_phrase
        
        return None
    
    def _extract_from_prepositional_phrase(self, doc) -> Optional[str]:
        """fromå¥ã®æŠ½å‡ºï¼ˆM3ç”¨ï¼‰"""
        
        for token in doc:
            if (token.pos_ == "ADP" and 
                token.text.lower() == 'from' and
                token.dep_ == "prep"):
                
                # å‰ç½®è©ã®ç›®çš„èªã‚’å–å¾—
                for child in token.children:
                    if child.dep_ == "pobj":
                        prep_object = self._get_complete_noun_phrase(child)
                        return f"{token.text} {prep_object}"
        
        return None
    
    def _contains_verb(self, phrase: str, doc) -> bool:
        """ãƒ•ãƒ¬ãƒ¼ã‚ºãŒå‹•è©ã‚’å«ã‚€ã‹ã©ã†ã‹ã‚’åˆ¤å®šï¼ˆçœŸã®phraseã‹ã©ã†ã‹ï¼‰"""
        
        # to + å‹•è©ã®ä¸å®šè©å¥ã‚’ãƒã‚§ãƒƒã‚¯
        if phrase.lower().startswith('to '):
            words = phrase.split()
            if len(words) >= 2:
                # spaCyã§å‹•è©ã‹ã©ã†ã‹ã‚’ç¢ºèª
                verb_word = words[1]
                for token in doc:
                    if token.text.lower() == verb_word.lower() and token.pos_ == 'VERB':
                        return True
        
        # ä»–ã®å‹•è©å¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        words = phrase.split()
        for word in words:
            for token in doc:
                if (token.text.lower() == word.lower() and 
                    token.pos_ in ['VERB', 'AUX'] and 
                    token.dep_ not in ['aux', 'auxpass']):  # åŠ©å‹•è©ã¯é™¤å¤–
                    return True
        
        return False
    
    def _is_infinitive_as_noun(self, phrase: str, doc) -> bool:
        """ä¸å®šè©ã®åè©çš„ç”¨æ³•ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        
        # "to + å‹•è©" ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒã‚§ãƒƒã‚¯
        if not phrase.lower().startswith('to '):
            return False
            
        words = phrase.split()
        if len(words) < 2:
            return False
            
        # æ–‡ä¸­ã§ã“ã®ä¸å®šè©å¥ã®æ–‡æ³•çš„å½¹å‰²ã‚’ãƒã‚§ãƒƒã‚¯
        for token in doc:
            if (token.pos_ == 'PART' and token.text.lower() == 'to' and 
                token.head and token.head.pos_ == 'VERB'):
                
                # ä¸å®šè©å¥ã®ä¾å­˜é–¢ä¿‚ã‚’ãƒã‚§ãƒƒã‚¯
                infinitive_verb = token.head
                
                # åè©çš„ç”¨æ³•ã®å…¸å‹çš„ãªä¾å­˜é–¢ä¿‚
                if infinitive_verb.dep_ in ['dobj', 'nsubj', 'pcomp', 'ccomp']:
                    return True
                    
                # ä¸»èªã¨ã—ã¦æ©Ÿèƒ½ã—ã¦ã„ã‚‹å ´åˆ
                if (infinitive_verb.dep_ == 'csubj' or 
                    (infinitive_verb.dep_ == 'acl' and infinitive_verb.head.dep_ == 'nsubj')):
                    return True
                    
                # want, like, need ãªã©ã®å‹•è©ã®ç›®çš„èªã¨ã—ã¦æ©Ÿèƒ½
                if (infinitive_verb.dep_ == 'xcomp' and 
                    infinitive_verb.head.lemma_ in ['want', 'like', 'need', 'plan', 'try', 'decide']):
                    return True
        
        return False
    
    def _is_infinitive_as_adjective(self, phrase: str, doc) -> bool:
        """ä¸å®šè©ã®å½¢å®¹è©çš„ç”¨æ³•ã‹ã©ã†ã‹ã‚’åˆ¤å®šï¼ˆåè©ã‚’ä¿®é£¾ï¼‰"""
        
        # "to + å‹•è©" ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒã‚§ãƒƒã‚¯
        if not phrase.lower().startswith('to '):
            return False
            
        words = phrase.split()
        if len(words) < 2:
            return False
            
        # æ–‡ä¸­ã§ã“ã®ä¸å®šè©å¥ã®æ–‡æ³•çš„å½¹å‰²ã‚’ãƒã‚§ãƒƒã‚¯
        for token in doc:
            if (token.pos_ == 'PART' and token.text.lower() == 'to' and 
                token.head and token.head.pos_ == 'VERB'):
                
                infinitive_verb = token.head
                
                # å½¢å®¹è©çš„ç”¨æ³•ã®å…¸å‹çš„ãªä¾å­˜é–¢ä¿‚
                if infinitive_verb.dep_ in ['relcl', 'acl']:
                    # åè©ã‚’ä¿®é£¾ã—ã¦ã„ã‚‹å ´åˆ
                    if infinitive_verb.head.pos_ in ['NOUN', 'PRON']:
                        return True
        
        return False
    
    def _is_infinitive_as_adverb(self, phrase: str, doc) -> bool:
        """ä¸å®šè©ã®å‰¯è©çš„ç”¨æ³•ã‹ã©ã†ã‹ã‚’åˆ¤å®šï¼ˆç›®çš„ãƒ»çµæœï¼‰"""
        
        # "to + å‹•è©" ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒã‚§ãƒƒã‚¯
        if not phrase.lower().startswith('to '):
            return False
            
        words = phrase.split()
        if len(words) < 2:
            return False
            
        # æ–‡ä¸­ã§ã“ã®ä¸å®šè©å¥ã®æ–‡æ³•çš„å½¹å‰²ã‚’ãƒã‚§ãƒƒã‚¯
        for token in doc:
            if (token.pos_ == 'PART' and token.text.lower() == 'to' and 
                token.head and token.head.pos_ == 'VERB'):
                
                infinitive_verb = token.head
                
                # å‰¯è©çš„ç”¨æ³•ã®å…¸å‹çš„ãªä¾å­˜é–¢ä¿‚
                if infinitive_verb.dep_ in ['advcl', 'purpcl']:
                    return True
                    
                # go, come ãªã©ã®ç§»å‹•å‹•è©ã®ç›®çš„ã¨ã—ã¦æ©Ÿèƒ½
                if (infinitive_verb.dep_ == 'xcomp' and 
                    infinitive_verb.head.lemma_ in ['go', 'come', 'run', 'walk', 'drive']):
                    return True
        
        return False
    
    def _get_complete_adverb_phrase(self, token) -> str:
        """å®Œå…¨ãªå‰¯è©å¥ã®å–å¾—"""
        phrase_tokens = []
        
        # ä¾å­˜é–¢ä¿‚ã‚’æŒã¤å­è¦ç´ ã‚’å«ã‚ã‚‹
        def collect_phrase(tok):
            phrase_tokens.append(tok)
            for child in tok.children:
                if child.dep_ in ["det", "amod", "compound", "nummod"]:
                    collect_phrase(child)
        
        collect_phrase(token)
        
        # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
        phrase_tokens.sort(key=lambda t: t.i)
        return " ".join([t.text for t in phrase_tokens])
    
    def _extract_ditransitive_objects(self, doc, slots: Dict[str, List]):
        """çœŸã®SVOOæ§‹é€ ï¼ˆåè©+åè©ã®ã¿ï¼‰ã®æ¤œå‡º"""
        
        # ã™ã§ã«O1ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿å‡¦ç†
        if not slots.get('O1'):
            return
            
        # çœŸã®SVOO: ä¸¡æ–¹ã¨ã‚‚åè©å¥ã§ã‚ã‚‹å ´åˆã®ã¿
        indirect_obj = None
        direct_obj = None
        
        # ä¾å­˜é–¢ä¿‚ã«ã‚ˆã‚‹ç›®çš„èªã®ç‰¹å®š
        for token in doc:
            if token.dep_ == "iobj" and token.pos_ in ["PRON", "NOUN", "PROPN"]:
                indirect_obj = self._get_complete_noun_phrase(token)
                print(f"ğŸ“ é–“æ¥ç›®çš„èªæ¤œå‡º: {indirect_obj} (iobj)")
            elif token.dep_ == "dative" and token.pos_ in ["PRON", "NOUN", "PROPN"]:
                indirect_obj = self._get_complete_noun_phrase(token)
                print(f"ğŸ“ é–“æ¥ç›®çš„èªæ¤œå‡º: {indirect_obj} (dative)")
            elif token.dep_ == "dobj" and token.pos_ in ["NOUN", "PROPN"]:
                direct_obj = self._get_complete_noun_phrase(token)
                print(f"ğŸ“ ç›´æ¥ç›®çš„èªæ¤œå‡º: {direct_obj} (dobj)")
        
        # çœŸã®SVOOæ§‹é€ ã®å ´åˆã®ã¿ã‚¹ãƒ­ãƒƒãƒˆå‰²ã‚Šå½“ã¦
        if indirect_obj and direct_obj:
            slots['O1'] = [{
                'value': indirect_obj,
                'rule_id': 'svoo-indirect',
                'confidence': 0.95
            }]
            slots['O2'] = [{
                'value': direct_obj,
                'rule_id': 'svoo-direct', 
                'confidence': 0.95
            }]
            print(f"âœ… çœŸã®SVOOæ§‹é€ æ¤œå‡º: O1={indirect_obj}, O2={direct_obj}")
        else:
            print(f"ğŸ” çœŸã®SVOOæ§‹é€ ãªã—: indirect={indirect_obj}, direct={direct_obj}")
            # å‰ç½®è©å¥ã¯å‰¯è©ã¨ã—ã¦åˆ¥é€”å‡¦ç†ã•ã‚Œã‚‹
        """äºŒé‡ç›®çš„èªå‹•è©ã®ç‰¹åˆ¥å‡¦ç† (give, tell, show, teachç­‰)"""
        
        # äºŒé‡ç›®çš„èªå‹•è©ã®ãƒªã‚¹ãƒˆ
        ditransitive_verbs = {
            'give', 'gave', 'given',
            'tell', 'told', 
            'show', 'showed', 'shown',
            'teach', 'taught',
            'send', 'sent',
            'buy', 'bought',
            'make', 'made',
            'get', 'got'
        }
        
        # ç¾åœ¨ã®å‹•è©ã‚’ãƒã‚§ãƒƒã‚¯
        main_verb = None
        for token in doc:
            if token.dep_ == "ROOT" and token.pos_ == "VERB":
                main_verb = token
                break
        
        if not main_verb or main_verb.lemma_.lower() not in ditransitive_verbs:
            return
            
        print(f"ğŸ“ äºŒé‡ç›®çš„èªå‹•è©æ¤œå‡º: {main_verb.text} ({main_verb.lemma_})")
        
        # å‹•è©ã®å‘¨è¾ºã«ã‚ã‚‹ç›®çš„èªå€™è£œã‚’åé›†
        objects = []
        for token in doc:
            if token.dep_ in ["dobj", "iobj"] or (token.dep_ == "pobj" and token.head.dep_ == "prep"):
                objects.append({
                    'token': token,
                    'phrase': self._get_complete_noun_phrase(token),
                    'position': token.i,
                    'dep': token.dep_
                })
        
        # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆï¼ˆæ–‡ä¸­ã®é †åºï¼‰
        objects.sort(key=lambda x: x['position'])
        
        if len(objects) >= 2:
            # 2ã¤ã®ç›®çš„èªãŒã‚ã‚‹å ´åˆ
            obj1, obj2 = objects[0], objects[1]
            
            # giveå‹ã®å ´åˆ: æœ€åˆãŒé–“æ¥ç›®çš„èªã€æ¬¡ãŒç›´æ¥ç›®çš„èª
            if main_verb.lemma_ in ['give', 'tell', 'show', 'send']:
                slots['O1'] = [{
                    'value': obj1['phrase'],
                    'rule_id': f'ditrans-{main_verb.lemma_}-iobj',
                    'confidence': 0.95
                }]
                slots['O2'] = [{
                    'value': obj2['phrase'], 
                    'rule_id': f'ditrans-{main_verb.lemma_}-dobj',
                    'confidence': 0.95
                }]
                print(f"âœ… {main_verb.lemma_}å‹SVOO: O1={obj1['phrase']}, O2={obj2['phrase']}")
            
            # teachå‹ã®å ´åˆ: æœ€åˆãŒç›´æ¥ç›®çš„èªã€æ¬¡ãŒé–“æ¥ç›®çš„èªï¼ˆå‰ç½®è©å¥ï¼‰
            elif main_verb.lemma_ in ['teach', 'buy']:
                slots['O1'] = [{
                    'value': obj1['phrase'],
                    'rule_id': f'ditrans-{main_verb.lemma_}-dobj',
                    'confidence': 0.95
                }]
                if obj2['dep'] == 'pobj':  # å‰ç½®è©å¥ã®å ´åˆ
                    prep = obj2['token'].head.text if obj2['token'].head.dep_ == 'prep' else 'to'
                    slots['O2'] = [{
                        'value': f"{prep} {obj2['phrase']}",
                        'rule_id': f'ditrans-{main_verb.lemma_}-prep',
                        'confidence': 0.95
                    }]
                    print(f"âœ… {main_verb.lemma_}å‹SVOO: O1={obj1['phrase']}, O2={prep} {obj2['phrase']}")
        
        elif len(objects) == 1 and not slots.get('O2'):
            # 1ã¤ã®ç›®çš„èªã—ã‹ãªã„å ´åˆã€å‰ç½®è©å¥ã‚’æ¢ã™
            obj = objects[0]
            for token in doc:
                if token.dep_ == "prep" and token.text.lower() in ["to", "for"]:
                    for child in token.children:
                        if child.dep_ == "pobj":
                            prep_phrase = self._get_complete_noun_phrase(child)
                            slots['O2'] = [{
                                'value': f"{token.text} {prep_phrase}",
                                'rule_id': f'ditrans-{main_verb.lemma_}-prep-supplement',
                                'confidence': 0.9
                            }]
                            print(f"âœ… {main_verb.lemma_}å‹å‰ç½®è©å¥è£œå®Œ: O2={token.text} {prep_phrase}")
                            break
    
    def _extract_auxiliary_value(self, doc, hierarchy) -> Optional[str]:
        """åŠ©å‹•è©ã®æ­£ç¢ºãªæŠ½å‡º - ç¸®ç´„å½¢å¯¾å¿œ"""
        
        main_verb = hierarchy.get('main_clause', {}).get('verb')
        if not main_verb:
            return None
        
        auxiliaries = []
        
        # åŠ©å‹•è©ã®åé›†
        for child in main_verb.children:
            if child.dep_ == "aux":
                aux_text = child.text
                
                # ç¸®ç´„å½¢ã®ä¿®æ­£
                if aux_text == "ca" and child.i < len(doc) - 1:
                    next_token = doc[child.i + 1]
                    if next_token.text == "n't":
                        aux_text = "cannot"  # can't -> cannot
                elif aux_text == "wo" and child.i < len(doc) - 1:
                    next_token = doc[child.i + 1]
                    if next_token.text == "n't":
                        aux_text = "will not"  # won't -> will not
                
                auxiliaries.append({
                    'text': aux_text,
                    'position': child.i
                })
        
        # ä½ç½®é †ã§ã‚½ãƒ¼ãƒˆ
        auxiliaries.sort(key=lambda x: x['position'])
        
        if auxiliaries:
            return ' '.join([aux['text'] for aux in auxiliaries])
        
        return None
    
    def _extract_temporal_value(self, doc, hierarchy) -> Optional[str]:
        """æ™‚é–“è¡¨ç¾ã®æŠ½å‡º - Rephraseãƒ«ãƒ¼ãƒ«å¯¾å¿œ"""
        
        temporal_expressions = []
        
        # npadvmodæ™‚é–“è¡¨ç¾
        for token in doc:
            if token.dep_ == "npadvmod" and self._is_temporal_word(token.text):
                temporal_expressions.append({
                    'text': self._get_complete_noun_phrase(token),
                    'position': token.i,
                    'type': 'npadvmod'
                })
        
        # å›ºæœ‰è¡¨ç¾ï¼ˆæ™‚é–“ï¼‰
        for ent in doc.ents:
            if ent.label_ in ['TIME', 'DATE']:
                temporal_expressions.append({
                    'text': ent.text,
                    'position': ent.start,
                    'type': 'named_entity'
                })
        
        # "ago"æ§‹é€ ã®ç‰¹åˆ¥å‡¦ç†
        for i, token in enumerate(doc):
            if token.text.lower() == "ago" and i >= 2:
                # "a few days ago" ã®ã‚ˆã†ãªæ§‹é€ 
                phrase_tokens = []
                j = i - 1
                while j >= 0 and doc[j].dep_ in ['det', 'amod', 'nummod', 'noun']:
                    phrase_tokens.insert(0, doc[j])
                    j -= 1
                    if len(phrase_tokens) >= 4:  # å®‰å…¨åˆ¶é™
                        break
                
                if phrase_tokens:
                    phrase_tokens.append(token)  # "ago"ã‚’è¿½åŠ 
                    full_phrase = ' '.join([t.text for t in phrase_tokens])
                    temporal_expressions.append({
                        'text': full_phrase,
                        'position': phrase_tokens[0].i,
                        'type': 'ago_structure'
                    })
        
        # æœ€ã‚‚é©åˆ‡ãªæ™‚é–“è¡¨ç¾ã‚’é¸æŠï¼ˆæ–‡é ­ã«è¿‘ã„ã‚‚ã®ã‚’å„ªå…ˆï¼‰
        if temporal_expressions:
            temporal_expressions.sort(key=lambda x: x['position'])
            return temporal_expressions[0]['text']
        
        return None
    
    def _is_temporal_word(self, word: str) -> bool:
        """æ™‚é–“ã‚’è¡¨ã™å˜èªã‹ã©ã†ã‹åˆ¤å®š - æ‹¡å¼µç‰ˆ"""
        temporal_words = {
            # æ™‚é–“å¸¯
            'morning', 'afternoon', 'evening', 'night', 'midnight', 'noon',
            # æ—¥
            'today', 'yesterday', 'tomorrow',
            'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday',
            # æœˆ
            'january', 'february', 'march', 'april', 'may', 'june',
            'july', 'august', 'september', 'october', 'november', 'december',
            # æœŸé–“
            'week', 'month', 'year', 'day', 'hour', 'minute', 'second',
            'weekend', 'weekday',
            # é »åº¦
            'always', 'never', 'often', 'sometimes', 'usually', 'rarely',
            'daily', 'weekly', 'monthly', 'yearly',
            # ãã®ä»–
            'now', 'then', 'soon', 'late', 'early', 'recently', 'lately'
        }
        
        return word.lower() in temporal_words
    
    # å®Ÿè£…äºˆå®šã®ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤
    def _extract_noun_phrases(self, doc): return []
    def _extract_verb_phrases(self, doc): return []
    def _extract_prep_phrases(self, doc): return []
    def _extract_generic_value(self, assignment: Dict[str, Any], doc, hierarchy) -> Optional[str]:
        """æ±ç”¨çš„ãªå€¤æŠ½å‡º - ãƒ«ãƒ¼ãƒ«è¾æ›¸å¯¾å¿œ"""
        
        slot = assignment.get('slot', '')
        
        # ã‚¹ãƒ­ãƒƒãƒˆåˆ¥ã®æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯
        if slot == 'S':
            return self._extract_subject_value(doc, hierarchy)
        elif slot == 'V':
            return self._extract_verb_value(doc, hierarchy, 'generic')
        elif slot == 'Aux':
            return self._extract_auxiliary_value(doc, hierarchy)
        elif slot == 'O1':
            return self._extract_direct_object_value(doc, hierarchy)
        elif slot == 'O2':
            return self._extract_indirect_object_value(doc, hierarchy)
        elif slot == 'M3':
            return self._extract_temporal_value(doc, hierarchy)
        elif slot in ['M1', 'M2']:
            return self._extract_modifier_value(doc, hierarchy, slot)
        
        return None
    
    def _extract_direct_object_value(self, doc, hierarchy) -> Optional[str]:
        """ç›´æ¥ç›®çš„èªã®æŠ½å‡º"""
        
        main_verb = hierarchy.get('main_clause', {}).get('verb')
        if not main_verb:
            return None
        
        # ç›´æ¥ç›®çš„èªã‚’æ¢ã™
        for token in doc:
            if token.dep_ == "dobj" and token.head == main_verb:
                return self._get_complete_noun_phrase(token)
        
        return None
    
    def _extract_indirect_object_value(self, doc, hierarchy) -> Optional[str]:
        """é–“æ¥ç›®çš„èªã®æŠ½å‡º"""
        
        main_verb = hierarchy.get('main_clause', {}).get('verb')
        if not main_verb:
            return None
        
        # é–“æ¥ç›®çš„èªã‚’æ¢ã™
        for token in doc:
            if token.dep_ in ["iobj", "dative"] and token.head == main_verb:
                return self._get_complete_noun_phrase(token)
        
        return None
    
    def _extract_modifier_value(self, doc, hierarchy, slot: str) -> Optional[str]:
        """ä¿®é£¾èªã®æŠ½å‡º - M1/M2/M3åˆ†é¡"""
        
        if slot == 'M1':
            # å ´æ‰€ãƒ»çŠ¶æ³ä¿®é£¾èª
            return self._extract_locative_modifier(doc)
        elif slot == 'M2':
            # æ–¹æ³•ãƒ»æ‰‹æ®µä¿®é£¾èª
            return self._extract_manner_modifier(doc)
        elif slot == 'M3':
            # æ™‚é–“ãƒ»é »åº¦ä¿®é£¾èª
            return self._extract_temporal_value(doc, hierarchy)
        
        return None
    
    def _extract_locative_modifier(self, doc) -> Optional[str]:
        """å ´æ‰€ä¿®é£¾èªã®æŠ½å‡º"""
        
        for token in doc:
            if token.pos_ == "ADP" and token.text.lower() in ['at', 'in', 'on', 'near', 'by']:
                # å‰ç½®è©å¥ã®å ´åˆ
                for child in token.children:
                    if child.dep_ == "pobj":
                        # æ™‚é–“è¡¨ç¾ã§ãªã„ã“ã¨ã‚’ç¢ºèª
                        if not self._is_temporal_word(child.text):
                            phrase = f"{token.text} {self._get_complete_noun_phrase(child)}"
                            return phrase
        
        return None
    
    def _extract_manner_modifier(self, doc) -> Optional[str]:
        """æ–¹æ³•ãƒ»æ‰‹æ®µä¿®é£¾èªã®æŠ½å‡º"""
        
        for token in doc:
            if token.pos_ == "ADP" and token.text.lower() in ['with', 'by', 'through']:
                for child in token.children:
                    if child.dep_ == "pobj":
                        phrase = f"{token.text} {self._get_complete_noun_phrase(child)}"
                        return phrase
        
        return None
    
    def _extract_generic_complement(self, doc) -> Optional[str]:
        """æ±ç”¨è£œèªæŠ½å‡ºï¼ˆbeå‹•è©ã€becomeã€seemç­‰ã®å¾Œã®è£œèªï¼‰"""
        
        # è£œèªã‚’ã¨ã‚‹å‹•è©ã®ãƒªã‚¹ãƒˆ
        complement_verbs = ['be', 'become', 'seem', 'appear', 'remain', 'stay', 'turn', 'get', 'grow', 'feel', 'look', 'sound', 'smell', 'taste']
        
        for token in doc:
            # è£œèªã‚’ã¨ã‚‹å‹•è©ã‚’æ¢ã™
            if token.lemma_ in complement_verbs and token.pos_ == "VERB":
                # attrï¼ˆå±æ€§è£œèªï¼‰ã‚’æ¢ã™
                for child in token.children:
                    if child.dep_ == "attr":
                        return self._get_complete_noun_phrase(child)
                
                # acompï¼ˆå½¢å®¹è©è£œèªï¼‰ã‚’æ¢ã™  
                for child in token.children:
                    if child.dep_ == "acomp":
                        return child.text
                
                # pcompï¼ˆå‰ç½®è©è£œèªï¼‰ã‚’æ¢ã™
                for child in token.children:
                    if child.dep_ == "prep":
                        for grandchild in child.children:
                            if grandchild.dep_ == "pobj":
                                return f"{child.text} {self._get_complete_noun_phrase(grandchild)}"
        
        return None
    
    def _extract_adverb_value(self, doc) -> Optional[str]:
        """å‰¯è©ã‚’æŠ½å‡ºï¼ˆM2ã‚¹ãƒ­ãƒƒãƒˆç”¨ï¼‰"""
        adverbs = []
        
        for token in doc:
            # å‰¯è©ï¼ˆADVï¼‰ã§ã€å‹•è©ã‚’ä¿®é£¾ã—ã¦ã„ã‚‹å ´åˆ
            if token.pos_ == "ADV" and token.dep_ in ["advmod", "prep"]:
                # å‰ç½®è©å¥ã§ãªã„å˜èªã®å‰¯è©ã‚’å„ªå…ˆ
                if not any(child.dep_ == "pobj" for child in token.children):
                    adverbs.append(token.text)
        
        # æœ€åˆã®å‰¯è©ã‚’è¿”ã™ï¼ˆè¤‡æ•°ã‚ã‚‹å ´åˆã¯æœ€åˆã®ã‚‚ã®ï¼‰
        return adverbs[0] if adverbs else None
    
    def _extract_wh_where_value(self, doc) -> Optional[str]:
        """Whç–‘å•è©whereã‚’æŠ½å‡ºï¼ˆM3ã‚¹ãƒ­ãƒƒãƒˆç”¨ï¼‰"""
        for token in doc:
            if token.text.lower() == "where":
                return "where_M3_1"
        return None
    
    def _extract_please_interjection_value(self, doc) -> Optional[str]:
        """æ–‡æœ«ã®ã€Œpleaseã€æ„Ÿå˜†è©ã‚’æŠ½å‡ºï¼ˆM3ã‚¹ãƒ­ãƒƒãƒˆç”¨ï¼‰"""
        for token in doc:
            if (token.lemma_.lower() == "please" and 
                token.pos_ == "INTJ" and 
                token.dep_ == "intj"):
                return token.text
        return None
    
    # =============================================================================
    # ãƒ•ã‚§ãƒ¼ã‚º1: spaCyå®Œå…¨å¯¾å¿œ - æ–°ä¾å­˜é–¢ä¿‚å‡¦ç†æ©Ÿèƒ½
    # =============================================================================
    
    def _extract_compound_phrase(self, token) -> str:
        """è¤‡åˆèªãƒ»è¤‡åˆåè©ã®å®Œå…¨æŠ½å‡º (compoundä¾å­˜é–¢ä¿‚)"""
        compound_tokens = [token]
        
        # è¤‡åˆèªã®æ§‹æˆè¦ç´ ã‚’åé›†
        for child in token.children:
            if child.dep_ == 'compound':
                compound_tokens.append(child)
        
        # èªé †ã§ã‚½ãƒ¼ãƒˆ
        compound_tokens.sort(key=lambda x: x.i)
        return ' '.join([t.text for t in compound_tokens])
    
    def _extract_conjunction_phrase(self, token) -> str:
        """ä¸¦åˆ—æ§‹é€ ã®å®Œå…¨æŠ½å‡º (conj + ccä¾å­˜é–¢ä¿‚)"""
        conj_elements = [token]
        coordinator = None
        
        # ä¸¦åˆ—è¦ç´ ã¨ç­‰ä½æ¥ç¶šè©ã‚’åé›†
        for child in token.children:
            if child.dep_ == 'conj':
                conj_elements.append(child)
            elif child.dep_ == 'cc':
                coordinator = child
        
        # èªé †ã§ã‚½ãƒ¼ãƒˆ
        conj_elements.sort(key=lambda x: x.i)
        
        if coordinator and len(conj_elements) > 1:
            # "A and B" å½¢å¼
            result = []
            for i, elem in enumerate(conj_elements):
                result.append(elem.text)
                if i == len(conj_elements) - 2:  # æœ€å¾Œã‹ã‚‰2ç•ªç›®ã®è¦ç´ ã®å¾Œ
                    result.append(coordinator.text)
            return ' '.join(result)
        else:
            return ' '.join([elem.text for elem in conj_elements])
    
    def _extract_negation_scope(self, token) -> str:
        """å¦å®šè¡¨ç¾ã®ã‚¹ã‚³ãƒ¼ãƒ—ä»˜ãæŠ½å‡º (negä¾å­˜é–¢ä¿‚)"""
        neg_token = None
        
        # å¦å®šèªã‚’æ¤œç´¢
        for child in token.children:
            if child.dep_ == 'neg':
                neg_token = child
                break
        
        if neg_token:
            # å¦å®šèª + å‹•è©/å½¢å®¹è©
            if token.pos_ in ['VERB', 'AUX']:
                # åŠ©å‹•è©ãŒã‚ã‚‹å ´åˆã®å‡¦ç†
                aux_tokens = [child for child in token.children if child.dep_ == 'aux']
                if aux_tokens:
                    aux = aux_tokens[0]
                    return f"{aux.text} {neg_token.text} {token.text}"
                else:
                    return f"{neg_token.text} {token.text}"
            else:
                return f"{neg_token.text} {token.text}"
        
        return token.text
    
    def _extract_numeric_phrase(self, token) -> str:
        """æ•°è©ä¿®é£¾ã®å®Œå…¨æŠ½å‡º (nummodä¾å­˜é–¢ä¿‚)"""
        numeric_parts = [token]
        
        # æ•°è©ä¿®é£¾èªã‚’åé›†
        for child in token.children:
            if child.dep_ == 'nummod':
                numeric_parts.append(child)
        
        # èªé †ã§ã‚½ãƒ¼ãƒˆ
        numeric_parts.sort(key=lambda x: x.i)
        return ' '.join([part.text for part in numeric_parts])
    
    def _detect_compound_dependencies(self, doc) -> List[Dict[str, Any]]:
        """compoundä¾å­˜é–¢ä¿‚ã®æ¤œå‡º"""
        compounds = []
        for token in doc:
            if token.dep_ == 'compound':
                compounds.append({
                    'head': token.head,
                    'compound': token,
                    'phrase': self._extract_compound_phrase(token.head)
                })
        return compounds
    
    def _detect_conjunction_dependencies(self, doc) -> List[Dict[str, Any]]:
        """conj + ccä¾å­˜é–¢ä¿‚ã®æ¤œå‡º"""
        conjunctions = []
        for token in doc:
            if token.dep_ == 'conj':
                conjunctions.append({
                    'head': token.head,
                    'conj_element': token,
                    'phrase': self._extract_conjunction_phrase(token.head)
                })
        return conjunctions
    
    def _detect_negation_dependencies(self, doc) -> List[Dict[str, Any]]:
        """negä¾å­˜é–¢ä¿‚ã®æ¤œå‡º"""
        negations = []
        for token in doc:
            if token.dep_ == 'neg':
                negations.append({
                    'negated_element': token.head,
                    'negation': token,
                    'phrase': self._extract_negation_scope(token.head)
                })
        return negations
    
    def _detect_nummod_dependencies(self, doc) -> List[Dict[str, Any]]:
        """nummodä¾å­˜é–¢ä¿‚ã®æ¤œå‡º"""
        nummods = []
        for token in doc:
            if token.dep_ == 'nummod':
                nummods.append({
                    'modified_noun': token.head,
                    'number': token,
                    'phrase': self._extract_numeric_phrase(token.head)
                })
        return nummods
    
    def _apply_phase1_enhancements(self, doc, slots: Dict[str, List]) -> Dict[str, List]:
        """ãƒ•ã‚§ãƒ¼ã‚º1æ‹¡å¼µæ©Ÿèƒ½ã®é©ç”¨"""
        
        # æ—¢å­˜ã‚¹ãƒ­ãƒƒãƒˆã®å€¤ã‚’å–å¾—ï¼ˆè¾æ›¸ã¨æ–‡å­—åˆ—ã®ä¸¡æ–¹ã«å¯¾å¿œï¼‰
        def get_slot_values(slot_list):
            values = []
            for item in slot_list:
                if isinstance(item, dict) and 'value' in item:
                    values.append(item['value'])
                elif isinstance(item, str):
                    values.append(item)
            return values
        
        all_existing_values = []
        for slot_name, slot_items in slots.items():
            all_existing_values.extend(get_slot_values(slot_items))
        
        # 1. è¤‡åˆèªå‡¦ç†
        compounds = self._detect_compound_dependencies(doc)
        for compound in compounds:
            phrase = compound['phrase']
            if phrase and phrase not in all_existing_values:
                slots['M1'].append(phrase)
        
        # 2. ä¸¦åˆ—æ§‹é€ å‡¦ç†  
        conjunctions = self._detect_conjunction_dependencies(doc)
        for conj in conjunctions:
            phrase = conj['phrase']
            if phrase and phrase not in all_existing_values:
                slots['M2'].append(phrase)
        
        # 3. å¦å®šè¡¨ç¾å‡¦ç†
        negations = self._detect_negation_dependencies(doc)
        for neg in negations:
            phrase = neg['phrase']
            if phrase and phrase not in all_existing_values:
                slots['M1'].append(phrase)
        
        # 4. æ•°è©ä¿®é£¾å‡¦ç†
        nummods = self._detect_nummod_dependencies(doc)
        for nummod in nummods:
            phrase = nummod['phrase']
            if phrase and phrase not in all_existing_values:
                slots['M1'].append(phrase)
        
        return slots
    
    # ============================================================================
    # ãƒ•ã‚§ãƒ¼ã‚º2: æ–‡æ§‹é€ æ‹¡å¼µæ©Ÿèƒ½ (nmod, xcomp, ccomp, auxpass, agent, pcomp, dative)
    # ============================================================================
    
    def _extract_nmod_phrase(self, token) -> str:
        """åè©ä¿®é£¾é–¢ä¿‚ã®å®Œå…¨æŠ½å‡º (nmodä¾å­˜é–¢ä¿‚)"""
        nmod_parts = [token]
        
        # nmodä¿®é£¾èªã‚’åé›†
        for child in token.children:
            if child.dep_ == 'nmod':
                nmod_parts.append(child)
                # å‰ç½®è©ã‚‚å«ã‚ã‚‹
                for grandchild in child.children:
                    if grandchild.dep_ == 'case':
                        nmod_parts.append(grandchild)
        
        # èªé †ã§ã‚½ãƒ¼ãƒˆ
        nmod_parts.sort(key=lambda x: x.i)
        return ' '.join([part.text for part in nmod_parts])
    
    def _extract_xcomp_clause(self, token) -> str:
        """ã‚ªãƒ¼ãƒ—ãƒ³ç¯€è£œèªã®å®Œå…¨æŠ½å‡º (xcompä¾å­˜é–¢ä¿‚)"""
        xcomp_parts = []
        
        # xcompç¯€ã‚’æ¤œç´¢
        for child in token.children:
            if child.dep_ == 'xcomp':
                clause_tokens = [child]
                # xcompç¯€ã®å…¨ã¦ã®å­è¦ç´ ã‚’åé›†
                for grandchild in child.subtree:
                    if grandchild != child:
                        clause_tokens.append(grandchild)
                
                # èªé †ã§ã‚½ãƒ¼ãƒˆ
                clause_tokens.sort(key=lambda x: x.i)
                # toä¸å®šè©ãƒãƒ¼ã‚«ãƒ¼ã‚‚è¿½åŠ 
                if any(t.text.lower() == 'to' and t.dep_ == 'aux' for t in clause_tokens):
                    return 'to ' + ' '.join([t.text for t in clause_tokens if t.text.lower() != 'to'])
                else:
                    return ' '.join([t.text for t in clause_tokens])
        
        return token.text
    
    def _extract_ccomp_clause(self, token) -> str:
        """ç¯€è£œèªã®å®Œå…¨æŠ½å‡º (ccompä¾å­˜é–¢ä¿‚)"""
        ccomp_parts = []
        
        # ccompç¯€ã‚’æ¤œç´¢
        for child in token.children:
            if child.dep_ == 'ccomp':
                clause_tokens = list(child.subtree)
                # èªé †ã§ã‚½ãƒ¼ãƒˆ
                clause_tokens.sort(key=lambda x: x.i)
                # thatè£œå®Œè©ã‚‚å«ã‚ã‚‹
                complementizer = None
                for sibling in token.children:
                    if sibling.dep_ == 'mark' and sibling.text.lower() == 'that':
                        complementizer = sibling
                        break
                
                if complementizer:
                    return f"that {' '.join([t.text for t in clause_tokens])}"
                else:
                    return ' '.join([t.text for t in clause_tokens])
        
        return token.text
    
    def _extract_auxpass_auxiliary(self, token) -> str:
        """å—å‹•æ…‹åŠ©å‹•è©ã®æŠ½å‡º (auxpassä¾å­˜é–¢ä¿‚)"""
        auxpass_tokens = []
        
        # å—å‹•æ…‹åŠ©å‹•è©ã‚’æ¤œç´¢
        for child in token.children:
            if child.dep_ == 'auxpass':
                auxpass_tokens.append(child)
        
        # èªé †ã§ã‚½ãƒ¼ãƒˆ
        auxpass_tokens.sort(key=lambda x: x.i)
        if auxpass_tokens:
            return ' '.join([aux.text for aux in auxpass_tokens])
        
        return token.text
    
    def _extract_agent_phrase(self, token) -> str:
        """å—å‹•æ…‹ã®å‹•ä½œä¸»æŠ½å‡º (agentä¾å­˜é–¢ä¿‚)"""
        agent_parts = []
        
        # agentå¥ã‚’æ¤œç´¢
        for child in token.children:
            if child.dep_ == 'agent':
                # å‰ç½®è©byã‚’å«ã‚ã‚‹
                prep_tokens = []
                for grandchild in child.children:
                    if grandchild.dep_ == 'case' and grandchild.text.lower() == 'by':
                        prep_tokens.append(grandchild)
                
                agent_phrase = list(child.subtree)
                agent_phrase.sort(key=lambda x: x.i)
                
                if prep_tokens:
                    return f"by {' '.join([t.text for t in agent_phrase])}"
                else:
                    return ' '.join([t.text for t in agent_phrase])
        
        return token.text
    
    def _extract_pcomp_complement(self, token) -> str:
        """å‰ç½®è©è£œèªã®æŠ½å‡º (pcompä¾å­˜é–¢ä¿‚)"""
        pcomp_parts = []
        
        # pcompè£œèªã‚’æ¤œç´¢
        for child in token.children:
            if child.dep_ == 'pcomp':
                comp_tokens = list(child.subtree)
                comp_tokens.sort(key=lambda x: x.i)
                return ' '.join([t.text for t in comp_tokens])
        
        return token.text
    
    def _extract_dative_object(self, token) -> str:
        """ä¸æ ¼ãƒ»é–“æ¥ç›®çš„èªã®æŠ½å‡º (dativeä¾å­˜é–¢ä¿‚)"""
        dative_parts = []
        
        # ä¸æ ¼ç›®çš„èªã‚’æ¤œç´¢
        for child in token.children:
            if child.dep_ == 'dative':
                dative_phrase = self._get_complete_noun_phrase(child)
                return dative_phrase
        
        return token.text
    
    def _detect_nmod_dependencies(self, doc) -> List[Dict[str, Any]]:
        """nmodä¾å­˜é–¢ä¿‚ã®æ¤œå‡º"""
        nmods = []
        for token in doc:
            if token.dep_ == 'nmod':
                nmods.append({
                    'modified_noun': token.head,
                    'modifier': token,
                    'phrase': self._extract_nmod_phrase(token.head)
                })
        return nmods
    
    def _detect_xcomp_dependencies(self, doc) -> List[Dict[str, Any]]:
        """xcompä¾å­˜é–¢ä¿‚ã®æ¤œå‡º"""
        xcomps = []
        for token in doc:
            if token.dep_ == 'xcomp':
                xcomps.append({
                    'governing_verb': token.head,
                    'xcomp_clause': token,
                    'phrase': self._extract_xcomp_clause(token.head)
                })
        return xcomps
    
    def _detect_ccomp_dependencies(self, doc) -> List[Dict[str, Any]]:
        """ccompä¾å­˜é–¢ä¿‚ã®æ¤œå‡º"""
        ccomps = []
        for token in doc:
            if token.dep_ == 'ccomp':
                ccomps.append({
                    'governing_verb': token.head,
                    'ccomp_clause': token,
                    'phrase': self._extract_ccomp_clause(token.head)
                })
        return ccomps
    
    def _detect_auxpass_dependencies(self, doc) -> List[Dict[str, Any]]:
        """auxpassä¾å­˜é–¢ä¿‚ã®æ¤œå‡º"""
        auxpasses = []
        for token in doc:
            if token.dep_ == 'auxpass':
                auxpasses.append({
                    'main_verb': token.head,
                    'auxiliary': token,
                    'phrase': self._extract_auxpass_auxiliary(token.head)
                })
        return auxpasses
    
    def _detect_agent_dependencies(self, doc) -> List[Dict[str, Any]]:
        """agentä¾å­˜é–¢ä¿‚ã®æ¤œå‡º"""
        agents = []
        for token in doc:
            if token.dep_ == 'agent':
                agents.append({
                    'passive_verb': token.head,
                    'agent_phrase': token,
                    'phrase': self._extract_agent_phrase(token.head)
                })
        return agents
    
    def _detect_pcomp_dependencies(self, doc) -> List[Dict[str, Any]]:
        """pcompä¾å­˜é–¢ä¿‚ã®æ¤œå‡º"""
        pcomps = []
        for token in doc:
            if token.dep_ == 'pcomp':
                pcomps.append({
                    'preposition': token.head,
                    'complement': token,
                    'phrase': self._extract_pcomp_complement(token.head)
                })
        return pcomps
    
    def _detect_dative_dependencies(self, doc) -> List[Dict[str, Any]]:
        """dativeä¾å­˜é–¢ä¿‚ã®æ¤œå‡º"""
        datives = []
        for token in doc:
            if token.dep_ == 'dative':
                datives.append({
                    'governing_verb': token.head,
                    'dative_object': token,
                    'phrase': self._extract_dative_object(token.head)
                })
        return datives
    
    def _apply_phase2_enhancements(self, doc, slots: Dict[str, List]) -> Dict[str, List]:
        """ãƒ•ã‚§ãƒ¼ã‚º2æ‹¡å¼µæ©Ÿèƒ½ã®é©ç”¨"""
        
        # æ—¢å­˜ã‚¹ãƒ­ãƒƒãƒˆã®å€¤ã‚’å–å¾—
        def get_slot_values(slot_list):
            values = []
            for item in slot_list:
                if isinstance(item, dict) and 'value' in item:
                    values.append(item['value'])
                elif isinstance(item, str):
                    values.append(item)
            return values
        
        all_existing_values = []
        for slot_name, slot_items in slots.items():
            all_existing_values.extend(get_slot_values(slot_items))
        
        # 1. åè©ä¿®é£¾å‡¦ç† (nmod)
        nmods = self._detect_nmod_dependencies(doc)
        for nmod in nmods:
            phrase = nmod['phrase']
            if phrase and phrase not in all_existing_values:
                slots['M1'].append(phrase)
        
        # 2. ã‚ªãƒ¼ãƒ—ãƒ³ç¯€è£œèªå‡¦ç† (xcomp)
        xcomps = self._detect_xcomp_dependencies(doc)
        for xcomp in xcomps:
            phrase = xcomp['phrase']
            if phrase and phrase not in all_existing_values:
                slots['O2'].append(phrase)
        
        # 3. ç¯€è£œèªå‡¦ç† (ccomp)
        ccomps = self._detect_ccomp_dependencies(doc)
        for ccomp in ccomps:
            phrase = ccomp['phrase']
            if phrase and phrase not in all_existing_values:
                slots['O2'].append(phrase)
        
        # 4. å—å‹•æ…‹åŠ©å‹•è©å‡¦ç† (auxpass)
        auxpasses = self._detect_auxpass_dependencies(doc)
        for auxpass in auxpasses:
            phrase = auxpass['phrase']
            if phrase and phrase not in all_existing_values:
                slots['Aux'].append(phrase)
        
        # 5. å—å‹•æ…‹å‹•ä½œä¸»å‡¦ç† (agent)
        agents = self._detect_agent_dependencies(doc)
        for agent in agents:
            phrase = agent['phrase']
            if phrase and phrase not in all_existing_values:
                slots['M3'].append(phrase)
        
        # 6. å‰ç½®è©è£œèªå‡¦ç† (pcomp)
        pcomps = self._detect_pcomp_dependencies(doc)
        for pcomp in pcomps:
            phrase = pcomp['phrase']
            if phrase and phrase not in all_existing_values:
                slots['M2'].append(phrase)
        
        # 7. ä¸æ ¼å‡¦ç† (dative)
        datives = self._detect_dative_dependencies(doc)
        for dative in datives:
            phrase = dative['phrase']
            if phrase and phrase not in all_existing_values:
                slots['O2'].append(phrase)
        
        # ğŸ¯ ãƒ•ã‚§ãƒ¼ã‚º2çµ±è¨ˆæ›´æ–°
        self.phase2_stats = {
            'nmod_phrases': [nmod['phrase'] for nmod in nmods if nmod['phrase']],
            'xcomp_clauses': [xcomp['phrase'] for xcomp in xcomps if xcomp['phrase']],
            'ccomp_clauses': [ccomp['phrase'] for ccomp in ccomps if ccomp['phrase']],
            'auxpass_auxiliaries': [auxpass['phrase'] for auxpass in auxpasses if auxpass['phrase']],
            'agent_phrases': [agent['phrase'] for agent in agents if agent['phrase']],
            'pcomp_complements': [pcomp['phrase'] for pcomp in pcomps if pcomp['phrase']],
            'dative_objects': [dative['phrase'] for dative in datives if dative['phrase']]
        }
        
        return slots
    
    # ========================================
    # ğŸš€ ãƒ•ã‚§ãƒ¼ã‚º3: é«˜åº¦æ–‡æ³•æ©Ÿèƒ½å®Ÿè£… (90%+ã‚«ãƒãƒ¬ãƒƒã‚¸)
    # ========================================
    
    def _extract_prep_phrase(self, doc, prep_dep) -> Dict[str, Any]:
        """å‰ç½®è©å¥ã®é«˜ç²¾åº¦æŠ½å‡º"""
        try:
            prep_token = prep_dep['token']
            pobj_tokens = [child for child in prep_token.children if child.dep_ == 'pobj']
            
            if pobj_tokens:
                pobj = pobj_tokens[0]
                # å‰ç½®è©å¥å…¨ä½“ã‚’æ§‹ç¯‰
                phrase_tokens = [prep_token] + list(pobj.subtree)
                phrase_text = ' '.join([t.text for t in phrase_tokens])
                
                return {
                    'phrase': phrase_text,
                    'prep': prep_token.text,
                    'object': pobj.text,
                    'type': 'prepositional_phrase',
                    'semantic_role': self._classify_prep_semantic_role(prep_token.text)
                }
        except Exception as e:
            print(f"å‰ç½®è©å¥æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return {'phrase': None}
    
    def _extract_amod_phrase(self, doc, amod_dep) -> Dict[str, Any]:
        """å½¢å®¹è©ä¿®é£¾èªã®é«˜ç²¾åº¦æŠ½å‡º"""
        try:
            amod_token = amod_dep['token']
            head_noun = amod_token.head
            
            # è¤‡æ•°å½¢å®¹è©ã®åé›†
            all_amods = [child for child in head_noun.children if child.dep_ == 'amod']
            all_amods.sort(key=lambda x: x.i)  # ä½ç½®é †ã‚½ãƒ¼ãƒˆ
            
            # å½¢å®¹è©ä»˜ãåè©å¥ã®æ§‹ç¯‰
            phrase_tokens = all_amods + [head_noun]
            phrase_text = ' '.join([t.text for t in phrase_tokens])
            
            return {
                'phrase': phrase_text,
                'adjectives': [adj.text for adj in all_amods],
                'noun': head_noun.text,
                'type': 'adjective_modified_noun'
            }
        except Exception as e:
            print(f"å½¢å®¹è©ä¿®é£¾èªæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return {'phrase': None}
    
    def _extract_advmod_phrase(self, doc, advmod_dep) -> Dict[str, Any]:
        """å‰¯è©ä¿®é£¾èªã®æ–‡è„ˆåˆ¥æŠ½å‡º"""
        try:
            advmod_token = advmod_dep['token']
            head_token = advmod_token.head
            
            # å‰¯è©ä¿®é£¾ã®ç¨®é¡ã‚’åˆ¤å®š
            if head_token.pos_ == 'VERB':
                mod_type = 'verb_modifier'
                target_slot = 'M2'
            elif head_token.pos_ == 'ADJ':
                mod_type = 'adjective_intensifier'
                target_slot = 'embedded'
            elif head_token.pos_ == 'ADV':
                mod_type = 'adverb_modifier'
                target_slot = 'M2'
            else:
                mod_type = 'general_modifier'
                target_slot = 'M1'
            
            return {
                'phrase': f"{advmod_token.text} {head_token.text}",
                'adverb': advmod_token.text,
                'modified_word': head_token.text,
                'type': mod_type,
                'target_slot': target_slot
            }
        except Exception as e:
            print(f"å‰¯è©ä¿®é£¾èªæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return {'phrase': None}
    
    def _extract_det_phrase(self, doc, det_dep) -> Dict[str, Any]:
        """é™å®šè©ã®åŒ…æ‹¬çš„å‡¦ç†"""
        try:
            det_token = det_dep['token']
            head_noun = det_token.head
            
            # é™å®šè©ã®ç¨®é¡ã‚’åˆ†é¡
            det_type = 'definite' if det_token.text.lower() in ['the'] else \
                      'indefinite' if det_token.text.lower() in ['a', 'an'] else \
                      'demonstrative' if det_token.text.lower() in ['this', 'that', 'these', 'those'] else \
                      'quantifier' if det_token.text.lower() in ['some', 'many', 'few', 'several'] else \
                      'possessive' if det_token.text.lower() in ['my', 'your', 'his', 'her', 'our', 'their'] else \
                      'general'
            
            return {
                'phrase': f"{det_token.text} {head_noun.text}",
                'determiner': det_token.text,
                'noun': head_noun.text,
                'type': det_type,
                'embedded': True  # é€šå¸¸ã¯åè©å¥ã«åŸ‹ã‚è¾¼ã¿
            }
        except Exception as e:
            print(f"é™å®šè©æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return {'phrase': None}
    
    def _extract_attr_phrase(self, doc, attr_dep) -> Dict[str, Any]:
        """å±æ€§è£œèªã®é«˜ç²¾åº¦æŠ½å‡º"""
        try:
            attr_token = attr_dep['token']
            head_verb = attr_token.head
            
            # å±æ€§è£œèªã®ç¨®é¡ã‚’åˆ¤å®š
            if attr_token.pos_ in ['NOUN', 'PROPN']:
                attr_type = 'nominal_predicate'
            elif attr_token.pos_ == 'ADJ':
                attr_type = 'adjectival_predicate'
            else:
                attr_type = 'general_attribute'
            
            # è£œèªå¥å…¨ä½“ã‚’æ§‹ç¯‰
            phrase_tokens = list(attr_token.subtree)
            phrase_text = ' '.join([t.text for t in phrase_tokens])
            
            return {
                'phrase': phrase_text,
                'attribute': attr_token.text,
                'copula': head_verb.text if head_verb.lemma_ == 'be' else None,
                'type': attr_type
            }
        except Exception as e:
            print(f"å±æ€§è£œèªæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return {'phrase': None}
    
    def _extract_relcl_phrase(self, doc, relcl_dep) -> Dict[str, Any]:
        """é–¢ä¿‚ç¯€ã®å®Œå…¨çµ±åˆå‡¦ç†"""
        try:
            relcl_verb = relcl_dep['token']
            head_noun = relcl_verb.head
            
            # é–¢ä¿‚ä»£åè©ã‚’æ¢ã™
            rel_pronoun = None
            for child in relcl_verb.children:
                if child.dep_ in ['nsubj', 'nsubjpass'] and child.pos_ == 'PRON':
                    rel_pronoun = child.text
                    break
            
            # é–¢ä¿‚ç¯€ã¨å…ˆè¡Œè©ã‚’å«ã‚€å®Œå…¨ãªåè©å¥ã‚’å–å¾—
            complete_noun_phrase = self._get_complete_noun_phrase(head_noun)
            
            # é–¢ä¿‚ç¯€éƒ¨åˆ†ã®ãƒ†ã‚­ã‚¹ãƒˆã‚‚ä¿æŒï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            relcl_tokens = list(relcl_verb.subtree)
            relcl_text = ' '.join([t.text for t in relcl_tokens])
            
            return {
                'phrase': complete_noun_phrase,
                'head_noun': head_noun.text,
                'relative_clause': relcl_text,
                'relative_pronoun': rel_pronoun,
                'type': 'relative_clause'
            }
        except Exception as e:
            print(f"é–¢ä¿‚ç¯€æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return {'phrase': None}
    
    def _extract_expl_phrase(self, doc, expl_dep) -> Dict[str, Any]:
        """è™šè¾thereæ§‹æ–‡ã®ç‰¹æ®Šå‡¦ç†"""
        try:
            expl_token = expl_dep['token']  # "there"
            verb_token = expl_token.head
            
            # çœŸã®ä¸»èªã‚’æ¢ã™
            real_subject = None
            for child in verb_token.children:
                if child.dep_ in ['nsubj', 'nsubjpass'] and child.text.lower() != 'there':
                    real_subject = child
                    break
            
            if real_subject:
                # thereæ§‹æ–‡ã‚’é€šå¸¸ã®æ§‹æ–‡ã«å¤‰æ›
                subject_phrase = ' '.join([t.text for t in real_subject.subtree])
                return {
                    'phrase': f"{subject_phrase} {verb_token.text}",
                    'restructured_subject': subject_phrase,
                    'existential_verb': verb_token.text,
                    'type': 'existential_restructured',
                    'original': f"There {verb_token.text} {subject_phrase}"
                }
        except Exception as e:
            print(f"è™šè¾æ§‹æ–‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return {'phrase': None}
    
    def _extract_acl_phrase(self, doc, acl_dep) -> Dict[str, Any]:
        """å½¢å®¹è©ç¯€ã®é«˜åº¦å‡¦ç†"""
        try:
            acl_verb = acl_dep['token']
            head_noun = acl_verb.head
            
            # ACLå½¢å¼ã‚’åˆ†é¡
            if any(child.pos_ == 'PART' for child in acl_verb.children):
                acl_type = 'infinitive_clause'
            elif acl_verb.tag_.startswith('VBG'):
                acl_type = 'participial_clause'
            else:
                acl_type = 'general_adjectival'
            
            # å½¢å®¹è©ç¯€å…¨ä½“ã‚’æ§‹ç¯‰
            acl_tokens = list(acl_verb.subtree)
            acl_text = ' '.join([t.text for t in acl_tokens])
            
            return {
                'phrase': f"{head_noun.text} {acl_text}",
                'head_noun': head_noun.text,
                'adjectival_clause': acl_text,
                'type': acl_type
            }
        except Exception as e:
            print(f"å½¢å®¹è©ç¯€æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return {'phrase': None}
    
    def _extract_appos_phrase(self, doc, appos_dep) -> Dict[str, Any]:
        """åŒæ ¼èªå¥ã®çµ±åˆå‡¦ç†"""
        try:
            appos_token = appos_dep['token']
            head_noun = appos_token.head
            
            # åŒæ ¼èªå¥å…¨ä½“ã‚’æ§‹ç¯‰
            appos_tokens = list(appos_token.subtree)
            appos_text = ' '.join([t.text for t in appos_tokens])
            
            return {
                'phrase': f"{head_noun.text}, {appos_text}",
                'head_noun': head_noun.text,
                'apposition': appos_text,
                'type': 'apposition_expansion'
            }
        except Exception as e:
            print(f"åŒæ ¼èªå¥æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return {'phrase': None}
    
    def _extract_mark_phrase(self, doc, mark_dep) -> Dict[str, Any]:
        """å¾“å±æ¥ç¶šè©ãƒãƒ¼ã‚«ãƒ¼ã®å‡¦ç†"""
        try:
            mark_token = mark_dep['token']
            clause_head = mark_token.head
            
            # å¾“å±ç¯€å…¨ä½“ã‚’æ§‹ç¯‰
            clause_tokens = list(clause_head.subtree)
            clause_text = ' '.join([t.text for t in clause_tokens])
            
            # ãƒãƒ¼ã‚«ãƒ¼ã®æ„å‘³åˆ†é¡
            marker_type = 'causal' if mark_token.text.lower() in ['because', 'since', 'as'] else \
                         'temporal' if mark_token.text.lower() in ['when', 'while', 'after', 'before'] else \
                         'conditional' if mark_token.text.lower() in ['if', 'unless', 'provided'] else \
                         'concessive' if mark_token.text.lower() in ['although', 'though', 'even'] else \
                         'general'
            
            return {
                'phrase': clause_text,
                'marker': mark_token.text,
                'subordinate_clause': clause_text,
                'type': marker_type
            }
        except Exception as e:
            print(f"å¾“å±æ¥ç¶šè©æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return {'phrase': None}
    
    def _classify_prep_semantic_role(self, prep_text: str) -> str:
        """å‰ç½®è©ã®æ„å‘³å½¹å‰²åˆ†é¡"""
        prep_lower = prep_text.lower()
        if prep_lower in ['in', 'on', 'at', 'under', 'over', 'beside']:
            return 'location'
        elif prep_lower in ['during', 'after', 'before', 'since', 'until']:
            return 'time'
        elif prep_lower in ['with', 'by', 'through']:
            return 'manner'
        elif prep_lower in ['for', 'to']:
            return 'purpose'
        elif prep_lower in ['from', 'out']:
            return 'source'
        else:
            return 'general'
    
    # ãƒ•ã‚§ãƒ¼ã‚º3ä¾å­˜é–¢ä¿‚æ¤œå‡ºãƒ¡ã‚½ãƒƒãƒ‰ç¾¤
    
    def _detect_prep_dependencies(self, doc) -> List[Dict[str, Any]]:
        """å‰ç½®è©ä¾å­˜é–¢ä¿‚ã®æ¤œå‡º"""
        prep_deps = []
        for token in doc:
            if token.dep_ == 'prep':
                prep_deps.append({
                    'token': token,
                    'phrase': self._extract_prep_phrase(doc, {'token': token})['phrase'],
                    'dependency': 'prep'
                })
        return prep_deps
    
    def _detect_amod_dependencies(self, doc) -> List[Dict[str, Any]]:
        """å½¢å®¹è©ä¿®é£¾èªä¾å­˜é–¢ä¿‚ã®æ¤œå‡º"""
        amod_deps = []
        for token in doc:
            if token.dep_ == 'amod':
                amod_deps.append({
                    'token': token,
                    'phrase': self._extract_amod_phrase(doc, {'token': token})['phrase'],
                    'dependency': 'amod'
                })
        return amod_deps
    
    def _detect_advmod_dependencies(self, doc) -> List[Dict[str, Any]]:
        """å‰¯è©ä¿®é£¾èªä¾å­˜é–¢ä¿‚ã®æ¤œå‡º"""
        advmod_deps = []
        for token in doc:
            if token.dep_ == 'advmod':
                advmod_deps.append({
                    'token': token,
                    'phrase': self._extract_advmod_phrase(doc, {'token': token})['phrase'],
                    'dependency': 'advmod'
                })
        return advmod_deps
    
    def _detect_det_dependencies(self, doc) -> List[Dict[str, Any]]:
        """é™å®šè©ä¾å­˜é–¢ä¿‚ã®æ¤œå‡º"""
        det_deps = []
        for token in doc:
            if token.dep_ == 'det':
                det_deps.append({
                    'token': token,
                    'phrase': self._extract_det_phrase(doc, {'token': token})['phrase'],
                    'dependency': 'det'
                })
        return det_deps
    
    def _detect_attr_dependencies(self, doc) -> List[Dict[str, Any]]:
        """å±æ€§è£œèªä¾å­˜é–¢ä¿‚ã®æ¤œå‡º"""
        attr_deps = []
        for token in doc:
            if token.dep_ == 'attr':
                attr_deps.append({
                    'token': token,
                    'phrase': self._extract_attr_phrase(doc, {'token': token})['phrase'],
                    'dependency': 'attr'
                })
        return attr_deps
    
    def _detect_relcl_dependencies(self, doc) -> List[Dict[str, Any]]:
        """é–¢ä¿‚ç¯€ä¾å­˜é–¢ä¿‚ã®æ¤œå‡º"""
        relcl_deps = []
        for token in doc:
            if token.dep_ == 'relcl':
                relcl_deps.append({
                    'token': token,
                    'phrase': self._extract_relcl_phrase(doc, {'token': token})['phrase'],
                    'dependency': 'relcl'
                })
        return relcl_deps
    
    def _detect_expl_dependencies(self, doc) -> List[Dict[str, Any]]:
        """è™šè¾ä¾å­˜é–¢ä¿‚ã®æ¤œå‡º"""
        expl_deps = []
        for token in doc:
            if token.dep_ == 'expl':
                expl_deps.append({
                    'token': token,
                    'phrase': self._extract_expl_phrase(doc, {'token': token})['phrase'],
                    'dependency': 'expl'
                })
        return expl_deps
    
    def _detect_acl_dependencies(self, doc) -> List[Dict[str, Any]]:
        """å½¢å®¹è©ç¯€ä¾å­˜é–¢ä¿‚ã®æ¤œå‡º"""
        acl_deps = []
        for token in doc:
            if token.dep_ == 'acl':
                acl_deps.append({
                    'token': token,
                    'phrase': self._extract_acl_phrase(doc, {'token': token})['phrase'],
                    'dependency': 'acl'
                })
        return acl_deps
    
    def _detect_appos_dependencies(self, doc) -> List[Dict[str, Any]]:
        """åŒæ ¼èªå¥ä¾å­˜é–¢ä¿‚ã®æ¤œå‡º"""
        appos_deps = []
        for token in doc:
            if token.dep_ == 'appos':
                appos_deps.append({
                    'token': token,
                    'phrase': self._extract_appos_phrase(doc, {'token': token})['phrase'],
                    'dependency': 'appos'
                })
        return appos_deps
    
    def _detect_mark_dependencies(self, doc) -> List[Dict[str, Any]]:
        """å¾“å±æ¥ç¶šè©ãƒãƒ¼ã‚«ãƒ¼ä¾å­˜é–¢ä¿‚ã®æ¤œå‡º"""
        mark_deps = []
        for token in doc:
            if token.dep_ == 'mark':
                mark_deps.append({
                    'token': token,
                    'phrase': self._extract_mark_phrase(doc, {'token': token})['phrase'],
                    'dependency': 'mark'
                })
        return mark_deps
    
    def _apply_phase3_enhancements(self, doc, slots) -> Dict[str, Any]:
        """ğŸš€ ãƒ•ã‚§ãƒ¼ã‚º3é«˜åº¦æ–‡æ³•æ©Ÿèƒ½ã®å®Œå…¨çµ±åˆ"""
        
        def get_slot_values(slot_items):
            if isinstance(slot_items, list):
                return [item for item in slot_items if item and item != '...']
            elif isinstance(slot_items, dict):
                return [v for v in slot_items.values() if v and v != '...']
            elif slot_items and slot_items != '...':
                return [slot_items]
            return []
        
        # æ—¢å­˜ã‚¹ãƒ­ãƒƒãƒˆå€¤ã®åé›†ï¼ˆé‡è¤‡é˜²æ­¢ï¼‰
        all_existing_values = []
        for slot_name, slot_items in slots.items():
            all_existing_values.extend(get_slot_values(slot_items))
        
        # 1. å‰ç½®è©å¥å‡¦ç† (prep)
        preps = self._detect_prep_dependencies(doc)
        for prep in preps:
            phrase = prep['phrase']
            if phrase and phrase not in all_existing_values:
                # æ„å‘³å½¹å‰²ã«åŸºã¥ãã‚¹ãƒ­ãƒƒãƒˆå‰²ã‚Šå½“ã¦
                semantic_role = self._classify_prep_semantic_role(prep['token'].text)
                if semantic_role in ['location', 'time']:
                    slots['M3'].append(phrase)
                else:
                    slots['M2'].append(phrase)
        
        # 2. å½¢å®¹è©ä¿®é£¾èªå‡¦ç† (amod) - åè©å¥ã«çµ±åˆ
        amods = self._detect_amod_dependencies(doc)
        for amod in amods:
            phrase = amod['phrase']
            if phrase and phrase not in all_existing_values:
                # ä¸»èªãƒ»ç›®çš„èªã®æ‹¡å¼µã¨ã—ã¦çµ±åˆ
                head_noun = amod['token'].head
                if head_noun.dep_ in ['nsubj', 'nsubjpass']:
                    if not slots['S'] or slots['S'] == ['...']:
                        slots['S'] = [phrase]
                    else:
                        slots['S'][0] = phrase  # æ‹¡å¼µ
                elif head_noun.dep_ in ['dobj', 'pobj']:
                    if not slots['O1'] or slots['O1'] == ['...']:
                        slots['O1'] = [phrase]
                    else:
                        slots['O1'][0] = phrase  # æ‹¡å¼µ
        
        # 3. å‰¯è©ä¿®é£¾èªå‡¦ç† (advmod)
        advmods = self._detect_advmod_dependencies(doc)
        for advmod in advmods:
            phrase = advmod['phrase']
            if phrase and phrase not in all_existing_values:
                head_token = advmod['token'].head
                if head_token.pos_ == 'VERB':
                    slots['M2'].append(phrase)
                elif head_token.pos_ in ['ADJ', 'ADV']:
                    # å½¢å®¹è©ãƒ»å‰¯è©ã®å¼·åŒ–ã¨ã—ã¦å‡¦ç†ï¼ˆåŸ‹ã‚è¾¼ã¿ï¼‰
                    pass  # é€šå¸¸ã¯å…ƒã®èªå¥ã«çµ±åˆæ¸ˆã¿
        
        # 4. é™å®šè©å‡¦ç† (det) - é€šå¸¸ã¯åŸ‹ã‚è¾¼ã¿å‡¦ç†ã®ã¿
        dets = self._detect_det_dependencies(doc)
        # ç‰¹æ®Šãªé™å®šè©ã®ã¿ç‹¬ç«‹å‡¦ç†ï¼ˆé‡è©ãªã©ï¼‰
        
        # 5. å±æ€§è£œèªå‡¦ç† (attr)
        attrs = self._detect_attr_dependencies(doc)
        for attr in attrs:
            phrase = attr['phrase']
            if phrase and phrase not in all_existing_values:
                slots['C1'].append(phrase)
        
        # 6. é–¢ä¿‚ç¯€å‡¦ç† (relcl)
        relcls = self._detect_relcl_dependencies(doc)
        for relcl in relcls:
            phrase = relcl['phrase']
            if phrase and phrase not in all_existing_values:
                # ä¸»èªãƒ»ç›®çš„èªã®æ‹¡å¼µã¨ã—ã¦å‡¦ç†
                head_noun = relcl['token'].head
                if head_noun.dep_ in ['nsubj', 'nsubjpass']:
                    if not slots['S'] or slots['S'] == ['...']:
                        slots['S'] = [phrase]
                    else:
                        slots['S'][0] = phrase
                elif head_noun.dep_ in ['dobj', 'pobj']:
                    if not slots['O1'] or slots['O1'] == ['...']:
                        slots['O1'] = [phrase]
                    else:
                        slots['O1'][0] = phrase
        
        # 7. è™šè¾thereæ§‹æ–‡å‡¦ç† (expl)
        expls = self._detect_expl_dependencies(doc)
        for expl in expls:
            phrase = expl['phrase']
            if phrase and phrase not in all_existing_values:
                # æ§‹é€ ã‚’å†ç·¨æˆ
                slots['S'] = [phrase]
        
        # 8. å½¢å®¹è©ç¯€å‡¦ç† (acl)
        acls = self._detect_acl_dependencies(doc)
        for acl in acls:
            phrase = acl['phrase']
            if phrase and phrase not in all_existing_values:
                # åè©å¥ã®æ‹¡å¼µã¨ã—ã¦å‡¦ç†
                head_noun = acl['token'].head
                if head_noun.dep_ in ['nsubj', 'nsubjpass']:
                    if not slots['S'] or slots['S'] == ['...']:
                        slots['S'] = [phrase]
                elif head_noun.dep_ in ['dobj', 'pobj']:
                    if not slots['O1'] or slots['O1'] == ['...']:
                        slots['O1'] = [phrase]
        
        # 9. åŒæ ¼èªå¥å‡¦ç† (appos)
        apposs = self._detect_appos_dependencies(doc)
        for appos in apposs:
            phrase = appos['phrase']
            if phrase and phrase not in all_existing_values:
                # æ‹¡å¼µçš„ãªæƒ…å ±ã¨ã—ã¦å‡¦ç†
                slots['M1'].append(phrase)
        
        # 10. å¾“å±æ¥ç¶šè©å‡¦ç† (mark)
        marks = self._detect_mark_dependencies(doc)
        for mark in marks:
            phrase = mark['phrase']
            if phrase and phrase not in all_existing_values:
                # å¾“å±ç¯€ã¨ã—ã¦å‡¦ç†
                slots['M2'].append(phrase)
        
        # ğŸ¯ ãƒ•ã‚§ãƒ¼ã‚º3çµ±è¨ˆæ›´æ–°
        self.phase3_stats = {
            'prep_phrases': [prep['phrase'] for prep in preps if prep['phrase']],
            'amod_phrases': [amod['phrase'] for amod in amods if amod['phrase']],
            'advmod_phrases': [advmod['phrase'] for advmod in advmods if advmod['phrase']],
            'det_phrases': [det['phrase'] for det in dets if det['phrase']],
            'attr_phrases': [attr['phrase'] for attr in attrs if attr['phrase']],
            'relcl_phrases': [relcl['phrase'] for relcl in relcls if relcl['phrase']],
            'expl_phrases': [expl['phrase'] for expl in expls if expl['phrase']],
            'acl_phrases': [acl['phrase'] for acl in acls if acl['phrase']],
            'appos_phrases': [appos['phrase'] for appos in apposs if appos['phrase']],
            'mark_phrases': [mark['phrase'] for mark in marks if mark['phrase']]
        }
        
        return slots
        
    def _process_relative_clause_subslots(self, verb, sub_slots, doc):
        """é–¢ä¿‚è©ç¯€ã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå‡¦ç† - Rephraseãƒ«ãƒ¼ãƒ«æº–æ‹ """
        print(f"ğŸ” é–¢ä¿‚è©ç¯€ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå‡¦ç†: {verb.text}")
        
        # é–¢ä¿‚è©ç¯€å†…ã®è¦ç´ ã‚’é †åºé€šã‚Šã«å‡¦ç†
        # Excelã®ä¾‹: the manager who had recently taken charge of the project
        # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆé †: the manager who (1) -> had (2) -> recently (3) -> taken (4) -> charge of the project (5)
        
        clause_elements = []
        
        # é–¢ä¿‚è©ç¯€å†…ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åé›†ã—ã¦é †åºä»˜ã‘
        for token in doc[verb.left_edge.i:verb.right_edge.i + 1]:
            if token.i >= verb.left_edge.i and token.i <= verb.right_edge.i:
                # é–¢ä¿‚ä»£åè© + å…ˆè¡Œè©ã®å‡¦ç†
                if token.text.lower() in ['who', 'which', 'that', 'whom'] and token.dep_ in ['nsubj', 'dobj', 'pobj']:
                    # å…ˆè¡Œè©ã‚’å«ã‚ãŸå½¢ã§å‡¦ç†ï¼ˆä¾‹ï¼šthe manager whoï¼‰
                    antecedent = token.head
                    while antecedent.head != antecedent and antecedent.head.dep_ not in ['ROOT']:
                        if any(child.dep_ == 'relcl' and child == verb for child in antecedent.children):
                            break
                        antecedent = antecedent.head
                    
                    # å…ˆè¡Œè©ã®å®Œå…¨ãªåè©å¥ã‚’æ§‹ç¯‰
                    antecedent_phrase = self._get_noun_phrase_before_relative(antecedent, token)
                    clause_elements.append({
                        'position': token.i - verb.left_edge.i,
                        'value': f"{antecedent_phrase} {token.text}",
                        'type': 'antecedent_relativizer',
                        'slot_type': 'sub-s' if token.dep_ == 'nsubj' else 'sub-o1'
                    })
                    
                # åŠ©å‹•è©
                elif token.dep_ in ['aux', 'auxpass']:
                    clause_elements.append({
                        'position': token.i - verb.left_edge.i,
                        'value': token.text,
                        'type': 'auxiliary',
                        'slot_type': 'sub-aux'
                    })
                    
                # å‰¯è©
                elif token.dep_ == 'advmod':
                    clause_elements.append({
                        'position': token.i - verb.left_edge.i,
                        'value': token.text,
                        'type': 'adverb',
                        'slot_type': 'sub-m2'
                    })
                    
                # å‹•è©ï¼ˆãƒ¡ã‚¤ãƒ³ã®é–¢ä¿‚è©ç¯€å‹•è©ï¼‰
                elif token == verb:
                    clause_elements.append({
                        'position': token.i - verb.left_edge.i,
                        'value': token.text,
                        'type': 'verb',
                        'slot_type': 'sub-v'
                    })
                    
                # å‹•è©ã®ç›®çš„èªã‚„è£œèªã®å¥
                elif token.dep_ in ['dobj', 'pobj'] and token.pos_ in ['NOUN', 'PRON']:
                    obj_phrase = self._get_complete_noun_phrase(token)
                    clause_elements.append({
                        'position': token.i - verb.left_edge.i,
                        'value': obj_phrase,
                        'type': 'object_phrase',
                        'slot_type': 'sub-o1'
                    })
        
        # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®
        clause_elements.sort(key=lambda x: x['position'])
        
        for element in clause_elements:
            slot_type = element['slot_type']
            sub_slots[slot_type].append({
                'value': element['value'],
                'type': element['type'],
                'rule_id': f'relative-clause-{element["type"]}'
            })
            print(f"  âœ… {slot_type}: {element['value']}")
    
    def _get_noun_phrase_before_relative(self, antecedent, relativizer):
        """é–¢ä¿‚ä»£åè©ã®å…ˆè¡Œè©éƒ¨åˆ†ã‚’å–å¾—"""
        # å† è©ã‚„ä¿®é£¾èªã‚’å«ã‚€åè©å¥ã‚’æ§‹ç¯‰
        phrase_tokens = []
        
        # å·¦å´ã®ä¿®é£¾èªï¼ˆthe, my, ãªã©ã®å† è©ãƒ»æ‰€æœ‰æ ¼ï¼‰
        for child in antecedent.children:
            if child.i < antecedent.i and child.dep_ in ['det', 'amod', 'compound', 'poss']:
                phrase_tokens.append(child)
        
        # ä¸­å¿ƒã®åè©
        phrase_tokens.append(antecedent)
        
        # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
        phrase_tokens.sort(key=lambda x: x.i)
        
        return ' '.join([t.text for t in phrase_tokens])

    def _process_adverbial_clause_subslots(self, verb, sub_slots, doc):
        """å‰¯è©ç¯€ã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå‡¦ç† - Rephrase100%å–ã‚Šã“ã¼ã—ãªã—ãƒ«ãƒ¼ãƒ«å¯¾å¿œ"""
        print(f"ğŸ” å‰¯è©ç¯€ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå‡¦ç†: {verb.text}")
        
        # æ¥ç¶šè©ã®æ¤œå‡ºã¨å‡¦ç†ï¼ˆWhen, If, Because, etc.ï¼‰
        conjunction = None
        for child in verb.children:
            if child.dep_ == 'mark':
                conjunction = child.text
                break
        
        # æ¥ç¶šè©ãŒãªã„å ´åˆã¯æ–‡é ­ã‹ã‚‰æ¢ã™
        if not conjunction:
            # å‰¯è©ç¯€ã®é–‹å§‹ã‚’æ¢ã™
            for token in doc:
                if (token.text.lower() in ['when', 'if', 'because', 'although', 'while', 'since', 'before', 'after', 'unless', 'until'] and
                    token.i < verb.i):
                    conjunction = token.text
                    break
        
        # æ¥ç¶šè©ã‚’sub-m3ã«é…ç½®ï¼ˆæ™‚é–“ãƒ»æ¡ä»¶ãƒ»ç†ç”±ãªã©ã®ä¿®é£¾ï¼‰
        if conjunction:
            sub_slots['sub-m3'].append({
                'value': conjunction,
                'type': 'conjunction',
                'rule_id': 'adverbial-clause-conjunction'
            })
            print(f"  âœ… sub-m3: {conjunction} (æ¥ç¶šè©)")
        
        # sub-s (å‰¯è©ç¯€å†…ã®ä¸»èª)
        for child in verb.children:
            if child.dep_ == 'nsubj':
                sub_slots['sub-s'].append({
                    'value': self._get_complete_noun_phrase(child),
                    'type': 'noun_phrase',
                    'rule_id': 'adverbial-clause-subject'
                })
                print(f"  âœ… sub-s: {self._get_complete_noun_phrase(child)}")
        
        # sub-v (å‰¯è©ç¯€å†…ã®å‹•è©)
        sub_slots['sub-v'].append({
            'value': verb.text,
            'type': 'verb',
            'rule_id': 'adverbial-clause-verb'
        })
        print(f"  âœ… sub-v: {verb.text}")
        
        # sub-o1 (å‰¯è©ç¯€å†…ã®ç›®çš„èª)
        for child in verb.children:
            if child.dep_ == 'dobj':
                sub_slots['sub-o1'].append({
                    'value': self._get_complete_noun_phrase(child),
                    'type': 'noun_phrase',
                    'rule_id': 'adverbial-clause-object'
                })
                print(f"  âœ… sub-o1: {self._get_complete_noun_phrase(child)}")
        
        # sub-m2 (å‰¯è©ç¯€å†…ã®æ–¹æ³•ãƒ»æ‰‹æ®µ)
        for child in verb.children:
            if child.dep_ == 'advmod' and child.pos_ == 'ADV':
                sub_slots['sub-m2'].append({
                    'value': child.text,
                    'type': 'adverb',
                    'rule_id': 'adverbial-clause-manner'
                })
                print(f"  âœ… sub-m2: {child.text}")
        
        # sub-m3 (å‰¯è©ç¯€å†…ã®å ´æ‰€ãƒ»æ™‚é–“)
        for child in verb.children:
            if child.dep_ == 'prep':
                prep_phrase = self._get_prepositional_phrase(child)
                if self._is_temporal_or_locative(child):
                    sub_slots['sub-m3'].append({
                        'value': prep_phrase,
                        'type': 'prepositional_phrase',
                        'rule_id': 'adverbial-clause-location-time'
                    })
                    print(f"  âœ… sub-m3: {prep_phrase}")
        
        # sub-aux (å‰¯è©ç¯€å†…ã®åŠ©å‹•è©)
        for child in verb.children:
            if child.dep_ == 'aux' or child.dep_ == 'auxpass':
                sub_slots['sub-aux'].append({
                    'value': child.text,
                    'type': 'auxiliary',
                    'rule_id': 'adverbial-clause-auxiliary'
                })
                print(f"  âœ… sub-aux: {child.text}")
        
        # sub-c1 (å‰¯è©ç¯€å†…ã®è£œèª)
        for child in verb.children:
            if child.dep_ == 'attr' or child.dep_ == 'acomp':
                sub_slots['sub-c1'].append({
                    'value': self._get_complete_noun_phrase(child) if child.pos_ in ['NOUN', 'PROPN'] else child.text,
                    'type': 'complement',
                    'rule_id': 'adverbial-clause-complement'
                })
                print(f"  âœ… sub-c1: {child.text}")
    
    def _get_prepositional_phrase(self, prep_token):
        """å‰ç½®è©å¥ã®å®Œå…¨ãªå–å¾—"""
        phrase_parts = [prep_token.text]
        
        for child in prep_token.children:
            if child.dep_ == 'pobj':
                phrase_parts.append(self._get_complete_noun_phrase(child))
        
        return ' '.join(phrase_parts)
    
    def _is_temporal_or_locative(self, prep_token):
        """å‰ç½®è©ãŒæ™‚é–“ãƒ»å ´æ‰€ã‚’è¡¨ã™ã‹ã®åˆ¤å®š"""
        temporal_locative_preps = [
            'at', 'in', 'on', 'by', 'during', 'after', 'before', 'since', 'until',
            'over', 'under', 'above', 'below', 'near', 'next', 'behind', 'beside',
            'through', 'across', 'around', 'within', 'outside', 'inside'
        ]
        return prep_token.text.lower() in temporal_locative_preps
