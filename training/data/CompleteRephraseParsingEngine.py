#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Rephrase Parsing Engine v3.0 - å®Œå…¨ç‰ˆ
Rephraseãƒ«ãƒ¼ãƒ«è¾æ›¸100%æ´»ç”¨ + Sub-slotå®Œå…¨å¯¾å¿œ + 5æ–‡å‹ãƒ•ãƒ«ã‚»ãƒƒãƒˆå¯¾å¿œ
"""

import spacy
import json
import os
import re
from typing import Dict, List, Any, Optional, Tuple, Union

# spaCyåˆæœŸåŒ–
try:
    nlp = spacy.load("en_core_web_sm")
    SPACY_AVAILABLE = True
    print("âœ… spaCyèªå½™èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
except (OSError, ImportError) as e:
    nlp = None
    SPACY_AVAILABLE = False
    print(f"âš ï¸ spaCyåˆæœŸåŒ–å¤±æ•—: {e}")

class CompleteRephraseParsingEngine:
    """å®Œå…¨ç‰ˆRephraseå“è©åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³ v3.0"""
    
    def __init__(self):
        self.engine_name = "Complete Rephrase Parsing Engine v3.0"
        self.rules_data = self.load_rules()
        self.nlp = nlp if SPACY_AVAILABLE else None
        
        # Rephraseã‚¹ãƒ­ãƒƒãƒˆå®Œå…¨å®šç¾©
        self.main_slots = ['S', 'Aux', 'V', 'O1', 'O2', 'C1', 'C2', 'M1', 'M2', 'M3']
        self.sub_slots = ['sub-m1', 'sub-s', 'sub-aux', 'sub-v', 'sub-o1', 'sub-o2', 'sub-m2', 'sub-m3', 'sub-c1']
        
        # ãƒ«ãƒ¼ãƒ«å„ªå…ˆåº¦ãƒãƒƒãƒ”ãƒ³ã‚°
        self.rule_priority_map = {}
        self._build_rule_priority_map()
        
    def load_rules(self):
        """Rephraseãƒ«ãƒ¼ãƒ«è¾æ›¸ã®å®Œå…¨èª­ã¿è¾¼ã¿"""
        rules_file = os.path.join(os.path.dirname(__file__), 'rephrase_rules_v1.0.json')
        try:
            with open(rules_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"âœ… Rephraseãƒ«ãƒ¼ãƒ«è¾æ›¸èª­ã¿è¾¼ã¿å®Œäº†: {len(data.get('rules', []))} ãƒ«ãƒ¼ãƒ«")
                return data
        except FileNotFoundError:
            print(f"âŒ ãƒ«ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {rules_file}")
            return {}
    
    def _build_rule_priority_map(self):
        """ãƒ«ãƒ¼ãƒ«å„ªå…ˆåº¦ãƒãƒƒãƒ—ã®æ§‹ç¯‰"""
        if 'rules' not in self.rules_data:
            return
            
        for rule in self.rules_data['rules']:
            rule_id = rule.get('id', '')
            priority = rule.get('priority', 50)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå„ªå…ˆåº¦
            self.rule_priority_map[rule_id] = priority
    
    def analyze_sentence(self, sentence: str) -> Dict[str, Any]:
        """
        å®Œå…¨ç‰ˆæ–‡è¦ç´ åˆ†è§£
        
        Returns:
            {
                'main_slots': {...},      # ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ­ãƒƒãƒˆ
                'sub_structures': [...],  # ã‚µãƒ–æ§‹é€ ï¼ˆé–¢ä¿‚è©ç¯€ç­‰ï¼‰
                'sentence_type': '...',   # æ–‡å‹
                'metadata': {...}         # ãƒ¡ã‚¿æƒ…å ±
            }
        """
        if not self.nlp:
            return {"error": "spaCy not available"}
            
        try:
            # Step 1: spaCyå®Œå…¨è§£æ
            doc = self.nlp(sentence)
            spacy_analysis = self._comprehensive_spacy_analysis(doc)
            
            # Step 2: æ–‡æ§‹é€ ã®éšå±¤åˆ†æ
            sentence_hierarchy = self._analyze_sentence_hierarchy(doc, spacy_analysis)
            
            # Step 3: Rephraseãƒ«ãƒ¼ãƒ«21å€‹ã®å®Œå…¨é©ç”¨
            rephrase_slots = self._apply_complete_rephrase_rules(doc, sentence_hierarchy)
            
            # Step 4: Sub-slotæ§‹é€ ã®ç”Ÿæˆ
            sub_structures = self._generate_subslot_structures(doc, sentence_hierarchy)
            
            # Step 5: æ–‡å‹åˆ¤å®šï¼ˆç¬¬1ã€œ5æ–‡å‹ï¼‰
            sentence_pattern = self._determine_sentence_pattern(rephrase_slots, sub_structures)
            
            return {
                'main_slots': rephrase_slots,
                'sub_structures': sub_structures,
                'sentence_type': sentence_pattern,
                'metadata': {
                    'engine': self.engine_name,
                    'rules_applied': len([r for r in rephrase_slots.values() if r]),
                    'complexity_score': self._calculate_complexity(sentence_hierarchy)
                }
            }
            
        except Exception as e:
            print(f"å®Œå…¨ç‰ˆãƒ‘ãƒ¼ã‚·ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _comprehensive_spacy_analysis(self, doc) -> Dict[str, Any]:
        """spaCyã«ã‚ˆã‚‹åŒ…æ‹¬çš„è¨€èªè§£æ"""
        
        analysis = {
            'tokens': [],
            'dependencies': [],
            'clauses': {
                'main': None,
                'subordinate': [],
                'relative': [],
                'infinitive': [],
                'participial': []
            },
            'phrases': {
                'noun_phrases': [],
                'verb_phrases': [],
                'prep_phrases': [],
                'adj_phrases': []
            },
            'entities': [],
            'sentence_structure': None
        }
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«è§£æ
        for token in doc:
            token_info = {
                'text': token.text,
                'lemma': token.lemma_,
                'pos': token.pos_,
                'tag': token.tag_,
                'dep': token.dep_,
                'head': token.head.text if token.head != token else 'ROOT',
                'children': [child.text for child in token.children],
                'is_punct': token.is_punct,
                'is_stop': token.is_stop
            }
            analysis['tokens'].append(token_info)
            
            # ä¾å­˜é–¢ä¿‚ã®è¨˜éŒ²
            if token.dep_ != 'ROOT':
                analysis['dependencies'].append({
                    'child': token.text,
                    'relation': token.dep_,
                    'head': token.head.text
                })
        
        # ä¸»ç¯€ãƒ»å¾“å±ç¯€ã®ç‰¹å®š
        root_verb = None
        for token in doc:
            if token.dep_ == "ROOT" and token.pos_ == "VERB":
                root_verb = token
                break
                
        analysis['clauses']['main'] = root_verb
        
        # å¾“å±ç¯€ã®ç‰¹å®š
        for token in doc:
            if token.dep_ == "advcl":  # å‰¯è©ç¯€
                analysis['clauses']['subordinate'].append({
                    'verb': token,
                    'type': 'adverbial',
                    'marker': self._find_clause_marker(token)
                })
            elif token.dep_ == "relcl":  # é–¢ä¿‚è©ç¯€
                analysis['clauses']['relative'].append({
                    'verb': token,
                    'modified_noun': token.head,
                    'relativizer': self._find_relativizer(token)
                })
            elif token.dep_ == "xcomp":  # è£œæ–‡ç¯€
                analysis['clauses']['infinitive'].append({
                    'verb': token,
                    'type': 'infinitive' if 'to' in [c.text for c in token.head.children] else 'bare_infinitive'
                })
        
        # ãƒ•ãƒ¬ãƒ¼ã‚ºãƒ¬ãƒ™ãƒ«è§£æ
        analysis['phrases']['noun_phrases'] = self._extract_noun_phrases(doc)
        analysis['phrases']['verb_phrases'] = self._extract_verb_phrases(doc)
        analysis['phrases']['prep_phrases'] = self._extract_prep_phrases(doc)
        
        # å›ºæœ‰è¡¨ç¾
        analysis['entities'] = [(ent.text, ent.label_) for ent in doc.ents]
        
        return analysis
    
    def _analyze_sentence_hierarchy(self, doc, spacy_analysis) -> Dict[str, Any]:
        """æ–‡æ§‹é€ ã®éšå±¤åˆ†æ"""
        
        hierarchy = {
            'main_clause': {
                'subject': None,
                'verb': spacy_analysis['clauses']['main'],
                'objects': [],
                'complements': [],
                'modifiers': []
            },
            'subordinate_structures': [],
            'sentence_complexity': 'simple'  # simple, compound, complex, compound-complex
        }
        
        # ä¸»ç¯€ã®æ§‹æˆè¦ç´ ç‰¹å®š
        main_verb = spacy_analysis['clauses']['main']
        if main_verb:
            # ä¸»èªç‰¹å®š
            for token in doc:
                if token.dep_ == "nsubj" and token.head == main_verb:
                    hierarchy['main_clause']['subject'] = {
                        'core': token,
                        'full_phrase': self._get_complete_noun_phrase(token),
                        'type': 'simple' if not self._has_modifying_clause(token) else 'complex'
                    }
                    break
            
            # ç›®çš„èªç‰¹å®š
            for token in doc:
                if token.head == main_verb:
                    if token.dep_ == "dobj":
                        hierarchy['main_clause']['objects'].append({
                            'core': token,
                            'full_phrase': self._get_complete_noun_phrase(token),
                            'type': 'direct',
                            'position': 1
                        })
                    elif token.dep_ in ["iobj", "dative"]:
                        hierarchy['main_clause']['objects'].append({
                            'core': token,
                            'full_phrase': self._get_complete_noun_phrase(token),
                            'type': 'indirect',
                            'position': 2
                        })
        
        # å¾“å±æ§‹é€ ã®åˆ†æ
        for rel_clause in spacy_analysis['clauses']['relative']:
            substructure = self._analyze_subclause_structure(rel_clause['verb'], 'relative')
            hierarchy['subordinate_structures'].append(substructure)
            
        for adv_clause in spacy_analysis['clauses']['subordinate']:
            substructure = self._analyze_subclause_structure(adv_clause['verb'], 'adverbial')
            hierarchy['subordinate_structures'].append(substructure)
        
        # æ–‡ã®è¤‡é›‘åº¦åˆ¤å®š
        if hierarchy['subordinate_structures']:
            if len(hierarchy['subordinate_structures']) == 1:
                hierarchy['sentence_complexity'] = 'complex'
            else:
                hierarchy['sentence_complexity'] = 'compound-complex'
        
        return hierarchy
    
    def _apply_complete_rephrase_rules(self, doc, hierarchy) -> Dict[str, List[Dict[str, Any]]]:
        """Rephraseãƒ«ãƒ¼ãƒ«21å€‹ã®å®Œå…¨é©ç”¨"""
        
        slots = {slot: [] for slot in self.main_slots}
        
        if 'rules' not in self.rules_data:
            print("âš ï¸ ãƒ«ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return slots
        
        # ãƒ«ãƒ¼ãƒ«ã‚’å„ªå…ˆåº¦é †ã§ã‚½ãƒ¼ãƒˆ
        rules = sorted(self.rules_data['rules'], 
                      key=lambda r: r.get('priority', 50), 
                      reverse=True)
        
        applied_rules = []
        
        for rule in rules:
            rule_id = rule.get('id', '')
            
            try:
                # ãƒ«ãƒ¼ãƒ«ã®é©ç”¨
                if self._should_apply_rule(rule, doc, hierarchy):
                    result = self._apply_single_rule(rule, doc, hierarchy, slots)
                    if result:
                        applied_rules.append(rule_id)
                        print(f"âœ… ãƒ«ãƒ¼ãƒ«é©ç”¨: {rule_id}")
                    
            except Exception as e:
                print(f"âš ï¸ ãƒ«ãƒ¼ãƒ«é©ç”¨ã‚¨ãƒ©ãƒ¼ {rule_id}: {e}")
        
        print(f"ğŸ“Š é©ç”¨ã•ã‚ŒãŸãƒ«ãƒ¼ãƒ«æ•°: {len(applied_rules)}/21")
        
        # æ±ç”¨çš„ãªå‹•è©æ¤œå‡ºï¼ˆãƒ«ãƒ¼ãƒ«ã§æ•ç²ã•ã‚Œãªã‹ã£ãŸå ´åˆï¼‰
        if not slots['V']:
            generic_verb = self._extract_generic_verb(doc, hierarchy)
            if generic_verb:
                slots['V'].append({
                    'value': generic_verb,
                    'rule_id': 'generic-verb',
                    'confidence': 0.7
                })
                print(f"âœ… æ±ç”¨å‹•è©æ¤œå‡º: {generic_verb}")
        
        # æ±ç”¨çš„ãªç›®çš„èªæ¤œå‡ºï¼ˆãƒ«ãƒ¼ãƒ«ã§æ•ç²ã•ã‚Œãªã‹ã£ãŸå ´åˆï¼‰
        if not slots['O1']:
            generic_object = self._extract_generic_object(doc, hierarchy)
            if generic_object:
                slots['O1'].append({
                    'value': generic_object,
                    'rule_id': 'generic-object',
                    'confidence': 0.7
                })
                print(f"âœ… æ±ç”¨ç›®çš„èªæ¤œå‡º: {generic_object}")
        
        # SVOOæ§‹é€ ã®æ¤œå‡ºï¼ˆé–“æ¥ç›®çš„èªã¨ç›´æ¥ç›®çš„èªã®åˆ†é›¢ï¼‰
        self._extract_ditransitive_objects(doc, slots)
        
        # æ±ç”¨çš„ãªæ™‚é–“è¡¨ç¾æ¤œå‡ºï¼ˆãƒ«ãƒ¼ãƒ«ã§æ•ç²ã•ã‚Œãªã‹ã£ãŸå ´åˆï¼‰
        if not slots['M3']:
            generic_time = self._extract_generic_time_expression(doc)
            if generic_time:
                slots['M3'].append({
                    'value': generic_time,
                    'rule_id': 'generic-time',
                    'confidence': 0.8
                })
                print(f"âœ… æ±ç”¨æ™‚é–“è¡¨ç¾æ¤œå‡º: {generic_time}")
        
        # æ±ç”¨çš„ãªåŠ©å‹•è©æ¤œå‡ºï¼ˆç¸®ç´„å½¢å¯¾å¿œï¼‰
        if not slots['Aux']:
            generic_aux = self._extract_generic_auxiliary(doc)
            if generic_aux:
                slots['Aux'].append({
                    'value': generic_aux,
                    'rule_id': 'generic-aux',
                    'confidence': 0.8
                })
                print(f"âœ… æ±ç”¨åŠ©å‹•è©æ¤œå‡º: {generic_aux}")
        
        return slots
    
    def _should_apply_rule(self, rule: Dict[str, Any], doc, hierarchy) -> bool:
        """ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨ã™ã¹ãã‹ã©ã†ã‹ã®åˆ¤å®š"""
        
        rule_id = rule.get('id', 'unknown')
        trigger = rule.get('trigger', {})
        
        print(f"ğŸ” ãƒ«ãƒ¼ãƒ«åˆ¤å®š: {rule_id}")
        
        # tokenãƒˆãƒªã‚¬ãƒ¼ã®ç¢ºèª
        if 'token' in trigger:
            target_token = trigger['token']
            doc_tokens = [token.text for token in doc]
            token_match = target_token in doc_tokens
            print(f"  tokenãƒˆãƒªã‚¬ãƒ¼: '{target_token}' â†’ æ–‡æ›¸å†…: {doc_tokens} â†’ ãƒãƒƒãƒ: {token_match}")
            if not token_match:
                return False
        
        # lemmaãƒˆãƒªã‚¬ãƒ¼ã®ç¢ºèª
        if 'lemma' in trigger:
            lemmas = trigger['lemma'] if isinstance(trigger['lemma'], list) else [trigger['lemma']]
            doc_lemmas = [token.lemma_ for token in doc]
            lemma_match = any(token.lemma_ in lemmas for token in doc)
            print(f"  lemmaãƒˆãƒªã‚¬ãƒ¼: {lemmas} â†’ æ–‡æ›¸å†…: {doc_lemmas} â†’ ãƒãƒƒãƒ: {lemma_match}")
            if not lemma_match:
                return False
        
        # posãƒˆãƒªã‚¬ãƒ¼ã®ç¢ºèª
        if 'pos' in trigger:
            pos_tags = trigger['pos'] if isinstance(trigger['pos'], list) else [trigger['pos']]
            doc_pos = [token.pos_ for token in doc]
            pos_match = any(token.pos_ in pos_tags for token in doc)
            print(f"  POSãƒˆãƒªã‚¬ãƒ¼: {pos_tags} â†’ æ–‡æ›¸å†…: {doc_pos} â†’ ãƒãƒƒãƒ: {pos_match}")
            if not pos_match:
                return False
        
        # patternãƒˆãƒªã‚¬ãƒ¼ã®ç¢ºèª
        if 'pattern' in trigger:
            pattern = trigger['pattern']
            pattern_match = bool(re.search(pattern, doc.text))
            print(f"  ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒˆãƒªã‚¬ãƒ¼: {pattern} â†’ ãƒãƒƒãƒ: {pattern_match}")
            if not pattern_match:
                return False
        
        print(f"  â†’ ãƒ«ãƒ¼ãƒ«é©ç”¨å¯¾è±¡: {rule_id}")
        return True
    
    def _apply_single_rule(self, rule: Dict[str, Any], doc, hierarchy, slots: Dict[str, List]) -> bool:
        """å˜ä¸€ãƒ«ãƒ¼ãƒ«ã®é©ç”¨"""
        
        rule_id = rule.get('id', '')
        assignment = rule.get('assign', {})
        
        if isinstance(assignment, list):
            # è¤‡æ•°å‰²ã‚Šå½“ã¦ã®å ´åˆ
            for assign_item in assignment:
                self._execute_assignment(assign_item, doc, hierarchy, slots, rule_id)
            return True
        else:
            # å˜ä¸€å‰²ã‚Šå½“ã¦ã®å ´åˆ
            return self._execute_assignment(assignment, doc, hierarchy, slots, rule_id)
    
    def _execute_assignment(self, assignment: Dict[str, Any], doc, hierarchy, slots: Dict[str, List], rule_id: str) -> bool:
        """å‰²ã‚Šå½“ã¦ã®å®Ÿè¡Œ"""
        
        slot = assignment.get('slot', '')
        assign_type = assignment.get('type', 'word')
        
        if slot not in slots:
            return False
        
        # å€¤ã®æ±ºå®š
        value = self._determine_assignment_value(assignment, doc, hierarchy, rule_id)
        
        if value:
            slots[slot].append({
                'value': value,
                'type': assign_type,
                'rule_id': rule_id
            })
            return True
            
        return False
    
    def _determine_assignment_value(self, assignment: Dict[str, Any], doc, hierarchy, rule_id: str) -> Optional[str]:
        """å‰²ã‚Šå½“ã¦å€¤ã®æ±ºå®š"""
        
        # ç‰¹å®šã®ãƒ«ãƒ¼ãƒ«ã«åŸºã¥ãå€¤æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯
        if rule_id.startswith('aux-'):
            return self._extract_auxiliary_value(doc, hierarchy)
        elif rule_id.startswith('V-'):
            return self._extract_verb_value(doc, hierarchy, rule_id)
        elif rule_id.startswith('time-'):
            return self._extract_temporal_value(doc, hierarchy)
        elif rule_id.startswith('subject-'):
            return self._extract_subject_value(doc, hierarchy)
        else:
            # æ±ç”¨çš„ãªå€¤æŠ½å‡º
            return self._extract_generic_value(assignment, doc, hierarchy)
    
    def _generate_subslot_structures(self, doc, hierarchy) -> List[Dict[str, Any]]:
        """Sub-slotæ§‹é€ ã®ç”Ÿæˆï¼ˆé–¢ä¿‚è©ç¯€ãƒ»å¾“å±ç¯€ç”¨ï¼‰"""
        
        sub_structures = []
        
        for sub_structure in hierarchy['subordinate_structures']:
            sub_slots = {slot: [] for slot in self.sub_slots}
            
            # Sub-slotã®å‰²ã‚Šå½“ã¦
            clause_type = sub_structure['type']
            clause_verb = sub_structure['verb']
            
            if clause_type == 'relative':
                # é–¢ä¿‚è©ç¯€ã®sub-slotå‡¦ç†
                self._process_relative_clause_subslots(clause_verb, sub_slots, doc)
            elif clause_type == 'adverbial':
                # å‰¯è©ç¯€ã®sub-slotå‡¦ç†
                self._process_adverbial_clause_subslots(clause_verb, sub_slots, doc)
            
            sub_structures.append({
                'type': clause_type,
                'verb': clause_verb.text,
                'sub_slots': sub_slots,
                'parent_element': sub_structure.get('modified_element', '')
            })
        
        return sub_structures
    
    def _determine_sentence_pattern(self, main_slots: Dict[str, List], sub_structures: List) -> str:
        """5æ–‡å‹ã®æ­£ç¢ºãªåˆ¤å®š"""
        
        has_s = bool(main_slots.get('S'))
        has_v = bool(main_slots.get('V'))
        has_o1 = bool(main_slots.get('O1'))
        has_o2 = bool(main_slots.get('O2'))
        has_c1 = bool(main_slots.get('C1'))
        has_aux = bool(main_slots.get('Aux'))
        
        # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
        print(f"ğŸ” æ–‡å‹åˆ¤å®š: S={has_s}, V={has_v}, O1={has_o1}, O2={has_o2}, C1={has_c1}, Aux={has_aux}")
        
        if has_s and has_v:
            if has_o1 and has_o2:
                return "ç¬¬4æ–‡å‹ (SVOO)"
            elif has_o1 and has_c1:
                return "ç¬¬5æ–‡å‹ (SVOC)"
            elif has_o1:
                return "ç¬¬3æ–‡å‹ (SVO)"
            elif has_c1:
                return "ç¬¬2æ–‡å‹ (SVC)"
            else:
                return "ç¬¬1æ–‡å‹ (SV)"
        elif has_v:
            return "å‘½ä»¤æ–‡ã¾ãŸã¯ç‰¹æ®Šæ§‹é€ "
        
        return "ä¸å®Œå…¨ãªæ–‡æ§‹é€ "
    
    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤
    def _find_clause_marker(self, verb_token) -> Optional[str]:
        """ç¯€ã®ãƒãƒ¼ã‚«ãƒ¼ã‚’æ¢ã™"""
        for child in verb_token.children:
            if child.dep_ == "mark":
                return child.text
        return None
    
    def _find_relativizer(self, verb_token) -> Optional[str]:
        """é–¢ä¿‚ä»£åè©ã‚’æ¢ã™"""
        for token in verb_token.doc:
            if token.dep_ == "nsubj" and token.head == verb_token and token.pos_ == "PRON":
                if token.text.lower() in ['who', 'which', 'that', 'whom', 'whose']:
                    return token.text
        return None
    
    def _get_complete_noun_phrase(self, token) -> str:
        """å®Œå…¨ãªåè©å¥ã‚’å–å¾—"""
        phrase_tokens = [token]
        
        # å·¦å´ã®ä¿®é£¾èª
        for child in token.children:
            if child.i < token.i and child.dep_ in ['det', 'amod', 'compound', 'nummod']:
                phrase_tokens.append(child)
        
        # å³å´ã®ä¿®é£¾èªï¼ˆå‰ç½®è©å¥ã¯é™¤ãï¼‰
        for child in token.children:
            if child.i > token.i and child.dep_ in ['amod', 'compound']:
                phrase_tokens.append(child)
        
        # é–¢ä¿‚è©ç¯€ãŒã‚ã‚‹å ´åˆã¯å«ã‚ã‚‹
        for child in token.children:
            if child.dep_ == 'relcl':
                rel_phrase = self._get_relative_clause_phrase(child)
                return f"{' '.join(sorted([t.text for t in phrase_tokens], key=lambda x: token.doc[[t.text for t in token.doc].index(x)].i))} {rel_phrase}"
        
        return ' '.join(sorted([t.text for t in phrase_tokens], key=lambda x: token.doc[[t.text for t in token.doc].index(x)].i))
    
    def _get_relative_clause_phrase(self, rel_verb) -> str:
        """é–¢ä¿‚è©ç¯€ã®å®Œå…¨ãªãƒ•ãƒ¬ãƒ¼ã‚ºã‚’å–å¾—"""
        # é–¢ä¿‚ä»£åè©ã®ç‰¹å®š
        relativizer = self._find_relativizer(rel_verb)
        if not relativizer:
            relativizer = "that"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            
        # é–¢ä¿‚è©ç¯€å†…ã®è¦ç´ ã‚’åé›†
        clause_tokens = [rel_verb]
        for descendant in rel_verb.subtree:
            if descendant != rel_verb:
                clause_tokens.append(descendant)
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½ç½®é †ã§ã‚½ãƒ¼ãƒˆ
        clause_tokens.sort(key=lambda t: t.i)
        clause_text = ' '.join([t.text for t in clause_tokens])
        
        return f"{relativizer} {clause_text}".strip()
    
    def _has_modifying_clause(self, token) -> bool:
        """ä¿®é£¾ç¯€ã‚’æŒã¤ã‹ã©ã†ã‹ã®ç¢ºèª"""
        for child in token.children:
            if child.dep_ in ['relcl', 'acl']:
                return True
        return False
    
    def _analyze_subclause_structure(self, verb_token, clause_type: str) -> Dict[str, Any]:
        """å¾“å±ç¯€æ§‹é€ ã®åˆ†æ"""
        return {
            'type': clause_type,
            'verb': verb_token,
            'modified_element': verb_token.head.text if clause_type == 'relative' else None,
            'complexity': 'simple'  # ç°¡ç•¥åŒ–
        }
    
    def _calculate_complexity(self, hierarchy) -> int:
        """æ–‡ã®è¤‡é›‘åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        base_score = 1
        if hierarchy['subordinate_structures']:
            base_score += len(hierarchy['subordinate_structures']) * 2
        
        return base_score
    
    # === Step 1: åŸºæœ¬æŠ½å‡ºãƒ¡ã‚½ãƒƒãƒ‰ã®å®Ÿè£… ===
    
    def _extract_subject_value(self, doc, hierarchy) -> Optional[str]:
        """ä¸»èªã®æ­£ç¢ºãªæŠ½å‡º - Rephraseãƒ«ãƒ¼ãƒ«å¯¾å¿œ"""
        
        # ä¸»ç¯€ã®ä¸»èªã‚’å„ªå…ˆ
        main_subject = hierarchy.get('main_clause', {}).get('subject')
        
        if main_subject and main_subject['type'] == 'complex':
            # è¤‡é›‘ãªä¸»èªï¼ˆé–¢ä¿‚è©ç¯€ä»˜ãï¼‰ã®å‡¦ç†
            return main_subject['full_phrase']
        elif main_subject:
            # å˜ç´”ãªä¸»èª
            return main_subject['full_phrase']
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: spaCyã‹ã‚‰ç›´æ¥æŠ½å‡º
        for token in doc:
            if token.dep_ == "nsubj" and token.head.dep_ == "ROOT":
                return self._get_complete_noun_phrase(token)
        
        return None
    
    def _extract_verb_value(self, doc, hierarchy, rule_id: str) -> Optional[str]:
        """å‹•è©ã®æ­£ç¢ºãªæŠ½å‡º - ãƒ«ãƒ¼ãƒ«åˆ¥å‡¦ç†"""
        
        print(f"ğŸ” å‹•è©æŠ½å‡ºé–‹å§‹ - ãƒ«ãƒ¼ãƒ«: {rule_id}")
        print(f"  éšå±¤ãƒ‡ãƒ¼ã‚¿: {hierarchy.keys()}")
        
        main_verb = hierarchy.get('main_clause', {}).get('verb')
        print(f"  éšå±¤ã‹ã‚‰å–å¾—ã—ãŸå‹•è©: {main_verb}")
        
        if main_verb:
            # åŸºæœ¬å‹•è©ã®æŠ½å‡º
            verb_text = main_verb.text
            print(f"  â†’ å‹•è©ãƒ†ã‚­ã‚¹ãƒˆ: '{verb_text}'")
            
            # ç‰¹å®šã®ãƒ«ãƒ¼ãƒ«ã«åŸºã¥ãèª¿æ•´
            if 'progressive' in rule_id:
                # é€²è¡Œå½¢ã®å‡¦ç†
                for child in main_verb.children:
                    if child.dep_ == "aux" and child.lemma_ == "be":
                        return f"{child.text} {verb_text}"
            
            elif 'perfect' in rule_id:
                # å®Œäº†å½¢ã®å‡¦ç†
                for child in main_verb.children:
                    if child.dep_ == "aux" and child.lemma_ == "have":
                        return f"{child.text} {verb_text}"
            
            return verb_text
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ROOTå‹•è©ã‚’æ¢ã™
        print(f"  ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ROOTå‹•è©ã‚’æ¤œç´¢")
        for token in doc:
            if token.dep_ == "ROOT" and token.pos_ == "VERB":
                print(f"  â†’ ROOTå‹•è©ç™ºè¦‹: '{token.text}' (pos: {token.pos_}, dep: {token.dep_})")
                return token.text
        
        print(f"  â†’ å‹•è©è¦‹ã¤ã‹ã‚‰ãš")
        return None
    
    def _extract_generic_verb(self, doc, hierarchy) -> Optional[str]:
        """æ±ç”¨çš„ãªå‹•è©æ¤œå‡ºï¼ˆãƒ«ãƒ¼ãƒ«ã§æ•ç²ã•ã‚Œãªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        
        # ROOTå‹•è©ã‚’æœ€å„ªå…ˆ
        for token in doc:
            if token.dep_ == "ROOT" and token.pos_ == "VERB":
                return token.text
        
        # ä»–ã®å‹•è©ã‚’æ¤œç´¢
        for token in doc:
            if token.pos_ == "VERB":
                return token.text
                
        return None
    
    def _extract_generic_object(self, doc, hierarchy) -> Optional[str]:
        """æ±ç”¨çš„ãªç›®çš„èªæ¤œå‡ºï¼ˆãƒ«ãƒ¼ãƒ«ã§æ•ç²ã•ã‚Œãªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        
        # ç›´æ¥ç›®çš„èªï¼ˆdobjï¼‰ã‚’æœ€å„ªå…ˆ
        for token in doc:
            if token.dep_ == "dobj":
                return self._get_complete_noun_phrase(token)
        
        # é–“æ¥ç›®çš„èªï¼ˆiobjï¼‰
        for token in doc:
            if token.dep_ == "iobj":
                return self._get_complete_noun_phrase(token)
                
        # è£œèªï¼ˆattr, pcompï¼‰
        for token in doc:
            if token.dep_ in ["attr", "pcomp"]:
                return self._get_complete_noun_phrase(token)
                
        return None
    
    def _extract_generic_time_expression(self, doc) -> Optional[str]:
        """æ±ç”¨çš„ãªæ™‚é–“è¡¨ç¾æ¤œå‡º"""
        
        # æ˜ç¢ºãªæ™‚é–“ãƒ‘ã‚¿ãƒ¼ãƒ³
        time_patterns = [
            r'\b(today|tomorrow|yesterday)\b',
            r'\b(last|this|next)\s+(week|month|year|morning|afternoon|evening|night)\b',
            r'\b(every|each)\s+(day|week|month|morning|afternoon|evening)\b',
            r'\b(a\s+few|several|many)\s+(days?|weeks?|months?|years?)\s+ago\b',
            r'\b(that|this)\s+(morning|afternoon|evening|night)\b',
            r'\b\d+\s*(am|pm)\b',
            r'\b(at|on|in)\s+(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\b'
        ]
        
        text = doc.text
        for pattern in time_patterns:
            import re
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(0)
        
        # spaCyã®æ™‚é–“ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
        for ent in doc.ents:
            if ent.label_ in ["TIME", "DATE"]:
                return ent.text
        
        # å‰¯è©å¥ï¼ˆæ™‚é–“è¡¨ç¾ã®å¯èƒ½æ€§ï¼‰
        time_advs = []
        for token in doc:
            if token.pos_ == "ADV" and token.dep_ in ["advmod", "npadvmod"]:
                # æ™‚é–“ã‚’ç¤ºå”†ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
                if any(keyword in token.text.lower() for keyword in ['day', 'morning', 'afternoon', 'evening', 'night', 'today', 'ago']):
                    return self._get_complete_adverb_phrase(token)
        
        return None
    
    def _extract_generic_auxiliary(self, doc) -> Optional[str]:
        """æ±ç”¨çš„ãªåŠ©å‹•è©æ¤œå‡ºï¼ˆç¸®ç´„å½¢å¯¾å¿œï¼‰"""
        
        # spaCyãŒè§£æã—ãŸç¸®ç´„å½¢ã‚’ç¢ºèª
        for token in doc:
            if token.text.lower() == "ca" and token.pos_ == "AUX":
                # "ca"ã¯"can"ã®ç¸®ç´„å½¢ã®ä¸€éƒ¨
                for next_token in doc:
                    if next_token.i == token.i + 1 and next_token.text in ["n't", "not"]:
                        return "cannot"
                        
        # å®Œå…¨ãªç¸®ç´„å½¢ãƒãƒƒãƒ”ãƒ³ã‚°
        contractions = {
            "can't": "cannot",
            "won't": "will not", 
            "shouldn't": "should not",
            "wouldn't": "would not",
            "couldn't": "could not",
            "mustn't": "must not",
            "needn't": "need not"
        }
        
        # å…ƒã®æ–‡ã‹ã‚‰ç¸®ç´„å½¢ã®æ¤œå‡º
        text_lower = doc.text.lower()
        for contraction, expansion in contractions.items():
            if contraction in text_lower:
                return expansion
        
        # AUXå“è©ã®æ¤œå‡º
        for token in doc:
            if token.pos_ == "AUX" and token.dep_ == "aux":
                # å¦å®šã®å ´åˆ
                for child in token.children:
                    if child.dep_ == "neg" or child.text in ["n't", "not"]:
                        return f"{token.lemma_} not"
                return token.text
                
        return None
    
    def _get_complete_adverb_phrase(self, token) -> str:
        """å®Œå…¨ãªå‰¯è©å¥ã®å–å¾—"""
        phrase_tokens = []
        
        # ä¾å­˜é–¢ä¿‚ã‚’æŒã¤å­è¦ç´ ã‚’å«ã‚ã‚‹
        def collect_phrase(tok):
            phrase_tokens.append(tok)
            for child in tok.children:
                if child.dep_ in ["det", "amod", "compound", "nummod"]:
                    collect_phrase(child)
        
        collect_phrase(token)
        
        # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
        phrase_tokens.sort(key=lambda t: t.i)
        return " ".join([t.text for t in phrase_tokens])
    
    def _extract_ditransitive_objects(self, doc, slots: Dict[str, List]):
        """SVOOæ§‹é€ ã®é–“æ¥ç›®çš„èªã¨ç›´æ¥ç›®çš„èªã®é©åˆ‡ãªåˆ†é›¢"""
        
        # ã™ã§ã«O1ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿å‡¦ç†
        if not slots.get('O1'):
            return
            
        # é–“æ¥ç›®çš„èªï¼ˆiobjï¼‰ã®æ¤œå‡º
        indirect_obj = None
        direct_obj = None
        
        for token in doc:
            if token.dep_ == "iobj":
                indirect_obj = self._get_complete_noun_phrase(token)
            elif token.dep_ == "dobj":
                direct_obj = self._get_complete_noun_phrase(token)
        
        # SVOOæ§‹é€ ã®å ´åˆã€é©åˆ‡ã«ã‚¹ãƒ­ãƒƒãƒˆã‚’å‰²ã‚Šå½“ã¦
        if indirect_obj and direct_obj:
            # O1ã‚’é–“æ¥ç›®çš„èªã«ã€O2ã‚’ç›´æ¥ç›®çš„èªã«
            slots['O1'] = [{
                'value': indirect_obj,
                'rule_id': 'ditransitive-iobj',
                'confidence': 0.9
            }]
            slots['O2'] = [{
                'value': direct_obj,
                'rule_id': 'ditransitive-dobj', 
                'confidence': 0.9
            }]
            print(f"âœ… SVOOæ§‹é€ æ¤œå‡º: O1={indirect_obj}, O2={direct_obj}")
            
        # å‰ç½®è©å¥ã«ã‚ˆã‚‹é–“æ¥ç›®çš„èªã®æ¤œå‡º
        elif not indirect_obj:
            for token in doc:
                if token.dep_ == "prep" and token.text.lower() == "to":
                    prep_obj = None
                    for child in token.children:
                        if child.dep_ == "pobj":
                            prep_obj = self._get_complete_noun_phrase(child)
                            break
                    
                    if prep_obj and direct_obj:
                        # å‰ç½®è©å¥ã‚’é–“æ¥ç›®çš„èªã¨ã—ã¦å‡¦ç†
                        slots['O2'] = [{
                            'value': f"to {prep_obj}",
                            'rule_id': 'prepositional-iobj',
                            'confidence': 0.8
                        }]
                        print(f"âœ… å‰ç½®è©å¥é–“æ¥ç›®çš„èªæ¤œå‡º: O2=to {prep_obj}")
    
    def _extract_auxiliary_value(self, doc, hierarchy) -> Optional[str]:
        """åŠ©å‹•è©ã®æ­£ç¢ºãªæŠ½å‡º - ç¸®ç´„å½¢å¯¾å¿œ"""
        
        main_verb = hierarchy.get('main_clause', {}).get('verb')
        if not main_verb:
            return None
        
        auxiliaries = []
        
        # åŠ©å‹•è©ã®åé›†
        for child in main_verb.children:
            if child.dep_ == "aux":
                aux_text = child.text
                
                # ç¸®ç´„å½¢ã®ä¿®æ­£
                if aux_text == "ca" and child.i < len(doc) - 1:
                    next_token = doc[child.i + 1]
                    if next_token.text == "n't":
                        aux_text = "cannot"  # can't -> cannot
                elif aux_text == "wo" and child.i < len(doc) - 1:
                    next_token = doc[child.i + 1]
                    if next_token.text == "n't":
                        aux_text = "will not"  # won't -> will not
                
                auxiliaries.append({
                    'text': aux_text,
                    'position': child.i
                })
        
        # ä½ç½®é †ã§ã‚½ãƒ¼ãƒˆ
        auxiliaries.sort(key=lambda x: x['position'])
        
        if auxiliaries:
            return ' '.join([aux['text'] for aux in auxiliaries])
        
        return None
    
    def _extract_temporal_value(self, doc, hierarchy) -> Optional[str]:
        """æ™‚é–“è¡¨ç¾ã®æŠ½å‡º - Rephraseãƒ«ãƒ¼ãƒ«å¯¾å¿œ"""
        
        temporal_expressions = []
        
        # npadvmodæ™‚é–“è¡¨ç¾
        for token in doc:
            if token.dep_ == "npadvmod" and self._is_temporal_word(token.text):
                temporal_expressions.append({
                    'text': self._get_complete_noun_phrase(token),
                    'position': token.i,
                    'type': 'npadvmod'
                })
        
        # å›ºæœ‰è¡¨ç¾ï¼ˆæ™‚é–“ï¼‰
        for ent in doc.ents:
            if ent.label_ in ['TIME', 'DATE']:
                temporal_expressions.append({
                    'text': ent.text,
                    'position': ent.start,
                    'type': 'named_entity'
                })
        
        # "ago"æ§‹é€ ã®ç‰¹åˆ¥å‡¦ç†
        for i, token in enumerate(doc):
            if token.text.lower() == "ago" and i >= 2:
                # "a few days ago" ã®ã‚ˆã†ãªæ§‹é€ 
                phrase_tokens = []
                j = i - 1
                while j >= 0 and doc[j].dep_ in ['det', 'amod', 'nummod', 'noun']:
                    phrase_tokens.insert(0, doc[j])
                    j -= 1
                    if len(phrase_tokens) >= 4:  # å®‰å…¨åˆ¶é™
                        break
                
                if phrase_tokens:
                    phrase_tokens.append(token)  # "ago"ã‚’è¿½åŠ 
                    full_phrase = ' '.join([t.text for t in phrase_tokens])
                    temporal_expressions.append({
                        'text': full_phrase,
                        'position': phrase_tokens[0].i,
                        'type': 'ago_structure'
                    })
        
        # æœ€ã‚‚é©åˆ‡ãªæ™‚é–“è¡¨ç¾ã‚’é¸æŠï¼ˆæ–‡é ­ã«è¿‘ã„ã‚‚ã®ã‚’å„ªå…ˆï¼‰
        if temporal_expressions:
            temporal_expressions.sort(key=lambda x: x['position'])
            return temporal_expressions[0]['text']
        
        return None
    
    def _is_temporal_word(self, word: str) -> bool:
        """æ™‚é–“ã‚’è¡¨ã™å˜èªã‹ã©ã†ã‹åˆ¤å®š - æ‹¡å¼µç‰ˆ"""
        temporal_words = {
            # æ™‚é–“å¸¯
            'morning', 'afternoon', 'evening', 'night', 'midnight', 'noon',
            # æ—¥
            'today', 'yesterday', 'tomorrow',
            'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday',
            # æœˆ
            'january', 'february', 'march', 'april', 'may', 'june',
            'july', 'august', 'september', 'october', 'november', 'december',
            # æœŸé–“
            'week', 'month', 'year', 'day', 'hour', 'minute', 'second',
            'weekend', 'weekday',
            # é »åº¦
            'always', 'never', 'often', 'sometimes', 'usually', 'rarely',
            'daily', 'weekly', 'monthly', 'yearly',
            # ãã®ä»–
            'now', 'then', 'soon', 'late', 'early', 'recently', 'lately'
        }
        
        return word.lower() in temporal_words
    
    # å®Ÿè£…äºˆå®šã®ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤
    def _extract_noun_phrases(self, doc): return []
    def _extract_verb_phrases(self, doc): return []
    def _extract_prep_phrases(self, doc): return []
    def _extract_generic_value(self, assignment: Dict[str, Any], doc, hierarchy) -> Optional[str]:
        """æ±ç”¨çš„ãªå€¤æŠ½å‡º - ãƒ«ãƒ¼ãƒ«è¾æ›¸å¯¾å¿œ"""
        
        slot = assignment.get('slot', '')
        
        # ã‚¹ãƒ­ãƒƒãƒˆåˆ¥ã®æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯
        if slot == 'S':
            return self._extract_subject_value(doc, hierarchy)
        elif slot == 'V':
            return self._extract_verb_value(doc, hierarchy, 'generic')
        elif slot == 'Aux':
            return self._extract_auxiliary_value(doc, hierarchy)
        elif slot == 'O1':
            return self._extract_direct_object_value(doc, hierarchy)
        elif slot == 'O2':
            return self._extract_indirect_object_value(doc, hierarchy)
        elif slot == 'M3':
            return self._extract_temporal_value(doc, hierarchy)
        elif slot in ['M1', 'M2']:
            return self._extract_modifier_value(doc, hierarchy, slot)
        
        return None
    
    def _extract_direct_object_value(self, doc, hierarchy) -> Optional[str]:
        """ç›´æ¥ç›®çš„èªã®æŠ½å‡º"""
        
        main_verb = hierarchy.get('main_clause', {}).get('verb')
        if not main_verb:
            return None
        
        # ç›´æ¥ç›®çš„èªã‚’æ¢ã™
        for token in doc:
            if token.dep_ == "dobj" and token.head == main_verb:
                return self._get_complete_noun_phrase(token)
        
        return None
    
    def _extract_indirect_object_value(self, doc, hierarchy) -> Optional[str]:
        """é–“æ¥ç›®çš„èªã®æŠ½å‡º"""
        
        main_verb = hierarchy.get('main_clause', {}).get('verb')
        if not main_verb:
            return None
        
        # é–“æ¥ç›®çš„èªã‚’æ¢ã™
        for token in doc:
            if token.dep_ in ["iobj", "dative"] and token.head == main_verb:
                return self._get_complete_noun_phrase(token)
        
        return None
    
    def _extract_modifier_value(self, doc, hierarchy, slot: str) -> Optional[str]:
        """ä¿®é£¾èªã®æŠ½å‡º - M1/M2/M3åˆ†é¡"""
        
        if slot == 'M1':
            # å ´æ‰€ãƒ»çŠ¶æ³ä¿®é£¾èª
            return self._extract_locative_modifier(doc)
        elif slot == 'M2':
            # æ–¹æ³•ãƒ»æ‰‹æ®µä¿®é£¾èª
            return self._extract_manner_modifier(doc)
        elif slot == 'M3':
            # æ™‚é–“ãƒ»é »åº¦ä¿®é£¾èª
            return self._extract_temporal_value(doc, hierarchy)
        
        return None
    
    def _extract_locative_modifier(self, doc) -> Optional[str]:
        """å ´æ‰€ä¿®é£¾èªã®æŠ½å‡º"""
        
        for token in doc:
            if token.pos_ == "ADP" and token.text.lower() in ['at', 'in', 'on', 'near', 'by']:
                # å‰ç½®è©å¥ã®å ´åˆ
                for child in token.children:
                    if child.dep_ == "pobj":
                        # æ™‚é–“è¡¨ç¾ã§ãªã„ã“ã¨ã‚’ç¢ºèª
                        if not self._is_temporal_word(child.text):
                            phrase = f"{token.text} {self._get_complete_noun_phrase(child)}"
                            return phrase
        
        return None
    
    def _extract_manner_modifier(self, doc) -> Optional[str]:
        """æ–¹æ³•ãƒ»æ‰‹æ®µä¿®é£¾èªã®æŠ½å‡º"""
        
        for token in doc:
            if token.pos_ == "ADP" and token.text.lower() in ['with', 'by', 'through']:
                for child in token.children:
                    if child.dep_ == "pobj":
                        phrase = f"{token.text} {self._get_complete_noun_phrase(child)}"
                        return phrase
        
        return None
    def _process_relative_clause_subslots(self, verb, sub_slots, doc): pass
    def _process_adverbial_clause_subslots(self, verb, sub_slots, doc): pass
