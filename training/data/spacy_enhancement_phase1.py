#!/usr/bin/env python3
"""
spaCyå®Œå…¨å¯¾å¿œRephraseã‚¨ãƒ³ã‚¸ãƒ³æ‹¡å¼µ - ãƒ•ã‚§ãƒ¼ã‚º1
é«˜é »åº¦ãƒ»é«˜ä¾¡å€¤ä¾å­˜é–¢ä¿‚ã®å®Œå…¨çµ±åˆ

å¯¾è±¡ä¾å­˜é–¢ä¿‚:
1. compound (è¤‡åˆèª)
2. conj + cc (ä¸¦åˆ—æ§‹é€ )
3. neg (å¦å®š)
4. nummod (æ•°è©ä¿®é£¾)
"""

import json
from typing import Dict, List, Any

class SpacyRephraseExtension:
    def __init__(self):
        self.new_rules = []
        self.dependency_mapping = {
            # ãƒ•ã‚§ãƒ¼ã‚º1: é«˜é »åº¦ãƒ»é«˜ä¾¡å€¤ä¾å­˜é–¢ä¿‚
            'compound': {
                'slot': 'M1',  # ä¿®é£¾èªã‚¹ãƒ­ãƒƒãƒˆ
                'priority': 25,
                'description': 'è¤‡åˆèªãƒ»è¤‡åˆåè©ã®çµ±åˆå‡¦ç†'
            },
            'conj': {
                'slot': 'M2',  # ä¸¦åˆ—è¦ç´ ã‚¹ãƒ­ãƒƒãƒˆ
                'priority': 24,
                'description': 'ä¸¦åˆ—æ§‹é€ ï¼ˆand, or, butç­‰ï¼‰ã®çµ±åˆå‡¦ç†'
            },
            'cc': {
                'slot': 'M2',  # ç­‰ä½æ¥ç¶šè©
                'priority': 23,
                'description': 'ç­‰ä½æ¥ç¶šè©ï¼ˆand, or, butç­‰ï¼‰ã®å‡¦ç†'
            },
            'neg': {
                'slot': 'M1',  # å¦å®šä¿®é£¾
                'priority': 26,
                'description': 'å¦å®šè¡¨ç¾ï¼ˆnot, never, noç­‰ï¼‰ã®å‡¦ç†'
            },
            'nummod': {
                'slot': 'M1',  # æ•°è©ä¿®é£¾
                'priority': 22,
                'description': 'æ•°è©ä¿®é£¾ï¼ˆthree books, first timeç­‰ï¼‰ã®å‡¦ç†'
            }
        }
        
    def generate_compound_rule(self) -> Dict[str, Any]:
        """è¤‡åˆèªå‡¦ç†ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ"""
        return {
            "id": "compound-noun-phrase-M1",
            "description": "è¤‡åˆèªãƒ»è¤‡åˆåè©ã®å®Œå…¨çµ±åˆï¼ˆNew York, ice creamç­‰ï¼‰",
            "priority": 25,
            "triggers": {
                "dependency_conditions": ["compound"]
            },
            "conditions": {
                "dependency_pattern": {
                    "target_dep": "compound",
                    "head_pos": ["NOUN", "PROPN"],
                    "child_pos": ["NOUN", "PROPN", "ADJ"]
                }
            },
            "assignments": [
                {
                    "slot": "M1",
                    "extraction_method": "compound_phrase",
                    "value_source": "full_compound_phrase"
                }
            ]
        }
    
    def generate_conjunction_rules(self) -> List[Dict[str, Any]]:
        """ä¸¦åˆ—æ§‹é€ å‡¦ç†ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ"""
        rules = []
        
        # ä¸¦åˆ—è¦ç´ ãƒ«ãƒ¼ãƒ«
        conj_rule = {
            "id": "conjunction-parallel-M2",
            "description": "ä¸¦åˆ—æ§‹é€ ã®å®Œå…¨çµ±åˆï¼ˆcats and dogs, red or blueç­‰ï¼‰",
            "priority": 24,
            "triggers": {
                "dependency_conditions": ["conj"]
            },
            "conditions": {
                "dependency_pattern": {
                    "target_dep": "conj",
                    "parallel_elements": True
                }
            },
            "assignments": [
                {
                    "slot": "M2",
                    "extraction_method": "conjunction_phrase",
                    "value_source": "full_parallel_structure"
                }
            ]
        }
        
        # ç­‰ä½æ¥ç¶šè©ãƒ«ãƒ¼ãƒ«
        cc_rule = {
            "id": "coordinating-conjunction-M2",
            "description": "ç­‰ä½æ¥ç¶šè©ã®çµ±åˆï¼ˆand, or, but, soç­‰ï¼‰",
            "priority": 23,
            "triggers": {
                "dependency_conditions": ["cc"]
            },
            "conditions": {
                "dependency_pattern": {
                    "target_dep": "cc",
                    "conjunction_words": ["and", "or", "but", "so", "yet", "nor"]
                }
            },
            "assignments": [
                {
                    "slot": "M2",
                    "extraction_method": "coordinating_conjunction",
                    "value_source": "conjunction_with_context"
                }
            ]
        }
        
        rules.extend([conj_rule, cc_rule])
        return rules
    
    def generate_negation_rule(self) -> Dict[str, Any]:
        """å¦å®šè¡¨ç¾å‡¦ç†ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ"""
        return {
            "id": "negation-modifier-M1",
            "description": "å¦å®šè¡¨ç¾ã®å®Œå…¨çµ±åˆï¼ˆnot, never, no, noneç­‰ï¼‰",
            "priority": 26,
            "triggers": {
                "dependency_conditions": ["neg"]
            },
            "conditions": {
                "dependency_pattern": {
                    "target_dep": "neg",
                    "negation_words": ["not", "n't", "never", "no", "none", "nothing", "nobody", "nowhere"],
                    "scope_detection": True
                }
            },
            "assignments": [
                {
                    "slot": "M1",
                    "extraction_method": "negation_scope",
                    "value_source": "negation_with_scope"
                }
            ]
        }
    
    def generate_nummod_rule(self) -> Dict[str, Any]:
        """æ•°è©ä¿®é£¾å‡¦ç†ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ"""
        return {
            "id": "numeric-modifier-M1",
            "description": "æ•°è©ä¿®é£¾ã®å®Œå…¨çµ±åˆï¼ˆthree books, first time, 100 dollarsç­‰ï¼‰",
            "priority": 22,
            "triggers": {
                "dependency_conditions": ["nummod"]
            },
            "conditions": {
                "dependency_pattern": {
                    "target_dep": "nummod",
                    "numeric_types": ["cardinal", "ordinal"],
                    "head_pos": ["NOUN", "PROPN"]
                }
            },
            "assignments": [
                {
                    "slot": "M1",
                    "extraction_method": "numeric_phrase",
                    "value_source": "number_with_noun"
                }
            ]
        }
    
    def generate_all_phase1_rules(self) -> List[Dict[str, Any]]:
        """ãƒ•ã‚§ãƒ¼ã‚º1å…¨ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ"""
        rules = []
        
        # 1. è¤‡åˆèªãƒ«ãƒ¼ãƒ«
        rules.append(self.generate_compound_rule())
        
        # 2. ä¸¦åˆ—æ§‹é€ ãƒ«ãƒ¼ãƒ«
        rules.extend(self.generate_conjunction_rules())
        
        # 3. å¦å®šè¡¨ç¾ãƒ«ãƒ¼ãƒ«
        rules.append(self.generate_negation_rule())
        
        # 4. æ•°è©ä¿®é£¾ãƒ«ãƒ¼ãƒ«
        rules.append(self.generate_nummod_rule())
        
        return rules
    
    def create_enhanced_rules_file(self, output_file: str = "enhanced_rephrase_rules_phase1.json"):
        """æ‹¡å¼µãƒ«ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ"""
        
        # æ—¢å­˜ãƒ«ãƒ¼ãƒ«èª­ã¿è¾¼ã¿
        try:
            with open("rephrase_rules_v1.0.json", 'r', encoding='utf-8') as f:
                existing_rules = json.load(f)
                print(f"âœ… æ—¢å­˜ãƒ«ãƒ¼ãƒ«èª­ã¿è¾¼ã¿: {len(existing_rules)}å€‹")
        except FileNotFoundError:
            existing_rules = {}
            print("âš ï¸ æ—¢å­˜ãƒ«ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
        
        # ãƒ•ã‚§ãƒ¼ã‚º1æ–°ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ
        phase1_rules = self.generate_all_phase1_rules()
        
        # ãƒ«ãƒ¼ãƒ«çµ±åˆ
        enhanced_rules = existing_rules.copy()
        for rule in phase1_rules:
            enhanced_rules[rule["id"]] = rule
            
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(enhanced_rules, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸš€ æ‹¡å¼µãƒ«ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå®Œäº†: {output_file}")
        print(f"  æ—¢å­˜ãƒ«ãƒ¼ãƒ«: {len(existing_rules)}å€‹")
        print(f"  æ–°è¦ãƒ«ãƒ¼ãƒ«: {len(phase1_rules)}å€‹")
        print(f"  åˆè¨ˆãƒ«ãƒ¼ãƒ«: {len(enhanced_rules)}å€‹")
        
        return enhanced_rules
    
    def generate_test_sentences(self) -> List[str]:
        """ãƒ•ã‚§ãƒ¼ã‚º1æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆç”¨ä¾‹æ–‡ç”Ÿæˆ"""
        return [
            # compound ãƒ†ã‚¹ãƒˆ
            "I live in New York City.",
            "She loves ice cream and chocolate cake.",
            "The software engineer works hard.",
            
            # conj + cc ãƒ†ã‚¹ãƒˆ  
            "Cats and dogs are popular pets.",
            "We can go by car or by train.",
            "He is smart but lazy.",
            
            # neg ãƒ†ã‚¹ãƒˆ
            "I do not like coffee.",
            "She never goes to parties.",
            "There is no time left.",
            
            # nummod ãƒ†ã‚¹ãƒˆ
            "I have three books on the table.",
            "This is my first time here.",
            "The car costs 20000 dollars."
        ]

def main():
    print("ğŸš€ spaCyå®Œå…¨å¯¾å¿œRephraseã‚¨ãƒ³ã‚¸ãƒ³æ‹¡å¼µ - ãƒ•ã‚§ãƒ¼ã‚º1é–‹å§‹")
    
    # æ‹¡å¼µã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    extension = SpacyRephraseExtension()
    
    # æ‹¡å¼µãƒ«ãƒ¼ãƒ«ç”Ÿæˆ
    enhanced_rules = extension.create_enhanced_rules_file()
    
    # ãƒ†ã‚¹ãƒˆç”¨ä¾‹æ–‡ç”Ÿæˆ
    test_sentences = extension.generate_test_sentences()
    
    print(f"\nğŸ“ ãƒ•ã‚§ãƒ¼ã‚º1ãƒ†ã‚¹ãƒˆç”¨ä¾‹æ–‡ ({len(test_sentences)}å€‹):")
    for i, sentence in enumerate(test_sentences, 1):
        print(f"  {i:2d}. {sentence}")
    
    print(f"\nğŸ¯ ãƒ•ã‚§ãƒ¼ã‚º1å¯¾è±¡ä¾å­˜é–¢ä¿‚:")
    for dep, info in extension.dependency_mapping.items():
        print(f"  - {dep}: {info['description']}")
    
    print(f"\nâœ… ãƒ•ã‚§ãƒ¼ã‚º1æº–å‚™å®Œäº†ï¼æ¬¡ã¯å®Ÿè£…ã¨ãƒ†ã‚¹ãƒˆã§ã™ã€‚")

if __name__ == "__main__":
    main()
