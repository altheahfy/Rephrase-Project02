#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CompleteRephraseParsingEngine v4.1 - æ±ºå®šçš„ãªä¿®æ­£ç‰ˆ
å®Œå…¨ãªå€¤æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯å®Ÿè£…ã«ã‚ˆã‚Šã€ãƒ«ãƒ¼ãƒ«è¾æ›¸ã‚’çœŸã«100%æ´»ç”¨
"""

import json
import re
import spacy
from typing import Dict, List, Any, Optional, Tuple
import os

class PerfectRephraseParsingEngine:
    """ãƒ«ãƒ¼ãƒ«è¾æ›¸å®Œå…¨æ´»ç”¨ãƒ»æ±ºå®šç‰ˆãƒ‘ãƒ¼ã‚·ãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, rules_file: str = 'rephrase_rules_v1.0.json'):
        """ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–"""
        print("ğŸŒŸ PerfectRephraseParsingEngine v4.1 åˆæœŸåŒ–ä¸­...")
        
        # spaCyãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        try:
            self.nlp = spacy.load("en_core_web_sm")
            print("âœ… spaCyèªå½™èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
        except OSError:
            raise Exception("spaCyãƒ¢ãƒ‡ãƒ« 'en_core_web_sm' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # ãƒ«ãƒ¼ãƒ«è¾æ›¸èª­ã¿è¾¼ã¿
        self.rules_file = rules_file
        self._load_rules()
        
        self.engine_name = "Perfect Rephrase Parsing Engine v4.1"
        print(f"âœ… {self.engine_name} åˆæœŸåŒ–å®Œäº†")
    
    def _load_rules(self):
        """ãƒ«ãƒ¼ãƒ«è¾æ›¸ã®å®Œå…¨èª­ã¿è¾¼ã¿"""
        try:
            with open(self.rules_file, 'r', encoding='utf-8') as f:
                self.rules_data = json.load(f)
            
            # åŸºæœ¬ãƒ«ãƒ¼ãƒ«ã¨ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«ã‚’åˆ†é›¢
            self.basic_rules = self.rules_data.get('rules', [])
            self.pattern_rules = self.rules_data.get('patterns', [])
            self.post_processing = self.rules_data.get('post_processing', [])
            
            total_rules = len(self.basic_rules) + len(self.pattern_rules) + len(self.post_processing)
            
            print(f"âœ… ãƒ«ãƒ¼ãƒ«è¾æ›¸èª­ã¿è¾¼ã¿å®Œäº†:")
            print(f"   åŸºæœ¬ãƒ«ãƒ¼ãƒ«: {len(self.basic_rules)}å€‹")
            print(f"   ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«: {len(self.pattern_rules)}å€‹") 
            print(f"   å¾Œå‡¦ç†ãƒ«ãƒ¼ãƒ«: {len(self.post_processing)}å€‹")
            print(f"   ç·è¨ˆ: {total_rules}ãƒ«ãƒ¼ãƒ«")
            
        except FileNotFoundError:
            raise Exception(f"ãƒ«ãƒ¼ãƒ«è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ« '{self.rules_file}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        except json.JSONDecodeError:
            raise Exception(f"ãƒ«ãƒ¼ãƒ«è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ« '{self.rules_file}' ã®JSONå½¢å¼ãŒä¸æ­£ã§ã™")
    
    def analyze_sentence(self, sentence: str) -> Dict[str, Any]:
        """æ–‡ã®å®Œå…¨åˆ†æ"""
        try:
            print(f"\nğŸŒŸ Perfect Engine: {sentence}")
            print("=" * 60)
            
            # Step 1: spaCyå‰å‡¦ç†
            doc = self.nlp(sentence)
            
            # Step 2: åŸºæœ¬ãƒ«ãƒ¼ãƒ«é©ç”¨ï¼ˆå®Ÿè£…æ¸ˆã¿å€¤æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯ä½¿ç”¨ï¼‰
            basic_slots = self._apply_enhanced_basic_rules(doc)
            
            # Step 3: ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«é©ç”¨ï¼ˆå®Œå…¨å®Ÿè£…ï¼‰
            pattern_results = self._apply_enhanced_pattern_rules(doc, basic_slots)
            
            # Step 4: çµ±åˆã¨æœ€é©åŒ–
            final_slots = self._integrate_enhanced_results(basic_slots, pattern_results)
            
            # Step 5: å¾Œå‡¦ç†
            final_slots = self._apply_enhanced_post_processing(final_slots, doc)
            
            # Step 6: æ–‡å‹åˆ¤å®š
            sentence_pattern = self._determine_sentence_pattern(final_slots)
            
            # Step 7: çµ±è¨ˆæƒ…å ±
            applied_rules_count = sum(1 for slot_list in final_slots.values() for item in slot_list if item)
            total_available_rules = len(self.basic_rules) + len(self.pattern_rules)
            rule_utilization = (applied_rules_count / total_available_rules) * 100 if total_available_rules > 0 else 0
            
            return {
                'main_slots': final_slots,
                'sentence_type': sentence_pattern,
                'metadata': {
                    'engine': self.engine_name,
                    'rules_applied': applied_rules_count,
                    'total_rules_available': total_available_rules,
                    'rule_utilization_rate': f"{rule_utilization:.1f}%",
                    'processing_success': rule_utilization >= 40.0  # 40%ä»¥ä¸Šã§æˆåŠŸ
                }
            }
            
        except Exception as e:
            print(f"âš ï¸ Perfect Engine ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _apply_enhanced_basic_rules(self, doc) -> Dict[str, List]:
        """åŸºæœ¬ãƒ«ãƒ¼ãƒ«ã®å®Œå…¨å®Ÿè£…ç‰ˆé©ç”¨"""
        
        print("ğŸ”„ Enhanced åŸºæœ¬ãƒ«ãƒ¼ãƒ«é©ç”¨é–‹å§‹")
        
        # ã‚¹ãƒ­ãƒƒãƒˆåˆæœŸåŒ–
        slots = {slot: [] for slot in ['S', 'Aux', 'V', 'O1', 'O2', 'C1', 'C2', 'M1', 'M2', 'M3']}
        
        applied_rules = []
        
        # å„ãƒ«ãƒ¼ãƒ«ã‚’é †æ¬¡é©ç”¨
        for rule in self.basic_rules:
            rule_id = rule.get('id', 'unknown')
            
            try:
                # å®Œå…¨ãªãƒˆãƒªã‚¬ãƒ¼åˆ¤å®š
                if self._complete_should_apply_rule(rule, doc):
                    print(f"  â†’ ãƒ«ãƒ¼ãƒ«é©ç”¨: {rule_id}")
                    
                    # å®Œå…¨ãªå€¤æ±ºå®šãƒ»å‰²ã‚Šå½“ã¦å®Ÿè¡Œ
                    success = self._execute_complete_rule(rule, doc, slots)
                    if success:
                        applied_rules.append(rule_id)
                        print(f"âœ… ãƒ«ãƒ¼ãƒ«å®Œäº†: {rule_id}")
                
            except Exception as e:
                print(f"âš ï¸ ãƒ«ãƒ¼ãƒ«é©ç”¨ã‚¨ãƒ©ãƒ¼ {rule_id}: {e}")
        
        print(f"ğŸ“Š åŸºæœ¬ãƒ«ãƒ¼ãƒ«é©ç”¨æ•°: {len(applied_rules)}/{len(self.basic_rules)}")
        
        return slots
    
    def _complete_should_apply_rule(self, rule: Dict[str, Any], doc) -> bool:
        """å®Œå…¨ãªãƒˆãƒªã‚¬ãƒ¼åˆ¤å®š"""
        
        trigger = rule.get('trigger', {})
        rule_id = rule.get('id', 'unknown')
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ç¢ºèª
        if 'token' in trigger:
            target_token = trigger['token']
            doc_tokens = [token.text for token in doc]
            if target_token not in doc_tokens:
                return False
        
        # lemmaç¢ºèª
        if 'lemma' in trigger:
            lemmas = trigger['lemma'] if isinstance(trigger['lemma'], list) else [trigger['lemma']]
            doc_lemmas = [token.lemma_ for token in doc]
            if not any(lemma in doc_lemmas for lemma in lemmas):
                return False
        
        # POSç¢ºèª
        if 'pos' in trigger:
            pos_tags = trigger['pos'] if isinstance(trigger['pos'], list) else [trigger['pos']]
            doc_pos = [token.pos_ for token in doc]
            if not any(pos in doc_pos for pos in pos_tags):
                return False
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ç¢ºèª
        if 'pattern' in trigger:
            pattern = trigger['pattern']
            if not re.search(pattern, doc.text, re.IGNORECASE):
                return False
        
        # ä½ç½®åˆ¶ç´„ç¢ºèª
        if 'position' in trigger:
            position = trigger['position']
            if position == 'before_first_main_verb':
                main_verb = self._find_main_verb(doc)
                if main_verb:
                    # è©³ç´°ãªä½ç½®ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡ç•¥ç‰ˆï¼‰
                    pass
        
        # æ„å‘³åˆ¶ç´„ç¢ºèª
        if 'sense' in trigger:
            sense = trigger['sense']
            if sense == 'exist_locative':
                # thereæ§‹æ–‡ã‚„å­˜åœ¨ã‚’è¡¨ã™beå‹•è©ã®ç¢ºèª
                if not self._check_existence_pattern(doc):
                    return False
        
        return True
    
    def _execute_complete_rule(self, rule: Dict[str, Any], doc, slots: Dict[str, List]) -> bool:
        """å®Œå…¨ãªãƒ«ãƒ¼ãƒ«å®Ÿè¡Œ"""
        
        assignment = rule.get('assign', {})
        rule_id = rule.get('id', 'unknown')
        
        if isinstance(assignment, list):
            # è¤‡æ•°å‰²ã‚Šå½“ã¦
            success = True
            for assign_item in assignment:
                if not self._execute_complete_single_assignment(assign_item, doc, slots, rule_id):
                    success = False
            return success
        else:
            # å˜ä¸€å‰²ã‚Šå½“ã¦
            return self._execute_complete_single_assignment(assignment, doc, slots, rule_id)
    
    def _execute_complete_single_assignment(self, assignment: Dict[str, Any], doc, slots: Dict[str, List], rule_id: str) -> bool:
        """å®Œå…¨ãªå˜ä¸€å‰²ã‚Šå½“ã¦å®Ÿè¡Œ"""
        
        slot = assignment.get('slot', '')
        assign_type = assignment.get('type', 'word')
        
        if slot not in slots:
            return False
        
        # å®Œå…¨ãªå€¤æ±ºå®š
        value = self._determine_complete_value(assignment, doc, rule_id)
        
        if value:
            slot_entry = {
                'value': value,
                'type': assign_type,
                'rule_id': rule_id,
                'confidence': assignment.get('confidence', 0.9)
            }
            
            slots[slot].append(slot_entry)
            print(f"  ğŸ“ ã‚¹ãƒ­ãƒƒãƒˆå‰²ã‚Šå½“ã¦: {slot} = '{value}' (ãƒ«ãƒ¼ãƒ«: {rule_id})")
            return True
        
        return False
    
    def _determine_complete_value(self, assignment: Dict[str, Any], doc, rule_id: str) -> Optional[str]:
        """å®Œå…¨ãªå€¤æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯"""
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ«ãƒ¼ãƒ«
        if 'text_rule' in assignment:
            text_rule = assignment['text_rule']
            if text_rule == 'first_pronoun_noun':
                for token in doc:
                    if token.pos_ in ['PRON', 'NOUN', 'PROPN']:
                        return token.text
            elif text_rule == 'first_verb':
                for token in doc:
                    if token.pos_ == 'VERB':
                        return token.text
            elif text_rule == 'first_aux':
                for token in doc:
                    if token.pos_ == 'AUX' or token.lemma_ in ['will', 'can', 'may', 'must', 'should', 'could', 'would']:
                        return token.text
        
        # è¦ç´ ãƒ«ãƒ¼ãƒ«
        if 'element_rule' in assignment:
            element_rule = assignment['element_rule']
            if element_rule == 'subject':
                return self._extract_subject(doc)
            elif element_rule == 'main_verb':
                return self._extract_main_verb(doc)
            elif element_rule == 'direct_object':
                return self._extract_direct_object(doc)
            elif element_rule == 'auxiliary':
                return self._extract_auxiliary(doc)
        
        # ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚°ãƒ«ãƒ¼ãƒ—
        if 'capture' in assignment:
            capture_num = assignment['capture']
            return self._extract_by_capture(doc, capture_num, rule_id)
        
        # ãƒ«ãƒ¼ãƒ« ID ãƒ™ãƒ¼ã‚¹æŠ½å‡º
        return self._rule_id_based_extraction(rule_id, doc)
    
    def _extract_subject(self, doc) -> Optional[str]:
        """ä¸»èªæŠ½å‡º"""
        for token in doc:
            if token.dep_ == 'nsubj' or token.dep_ == 'nsubjpass':
                # ä¿®é£¾èªã‚‚å«ã‚€å®Œå…¨ãªä¸»èª
                subject_tokens = []
                
                # é™å®šè©ãƒ»å½¢å®¹è©ã‚’å«ã‚ã‚‹
                for child in token.children:
                    if child.dep_ in ['det', 'amod', 'compound']:
                        subject_tokens.append((child.i, child.text))
                
                subject_tokens.append((token.i, token.text))
                subject_tokens.sort(key=lambda x: x[0])
                
                return ' '.join([t[1] for t in subject_tokens])
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®ä»£åè©ãƒ»åè©
        for token in doc:
            if token.pos_ in ['PRON', 'NOUN', 'PROPN'] and token.i < 3:  # æ–‡é ­è¿‘ã
                return token.text
        
        return None
    
    def _extract_main_verb(self, doc) -> Optional[str]:
        """ä¸»å‹•è©æŠ½å‡º"""
        # ROOTå‹•è©ã‚’æ¢ã™
        for token in doc:
            if token.dep_ == 'ROOT' and token.pos_ == 'VERB':
                return token.text
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®å‹•è©
        for token in doc:
            if token.pos_ == 'VERB':
                return token.text
        
        return None
    
    def _extract_direct_object(self, doc) -> Optional[str]:
        """ç›´æ¥ç›®çš„èªæŠ½å‡º"""
        for token in doc:
            if token.dep_ == 'dobj':
                # ä¿®é£¾èªã‚’å«ã‚€å®Œå…¨ãªç›®çš„èª
                obj_tokens = []
                
                for child in token.children:
                    if child.dep_ in ['det', 'amod']:
                        obj_tokens.append((child.i, child.text))
                
                obj_tokens.append((token.i, token.text))
                obj_tokens.sort(key=lambda x: x[0])
                
                return ' '.join([t[1] for t in obj_tokens])
        
        return None
    
    def _extract_auxiliary(self, doc) -> Optional[str]:
        """åŠ©å‹•è©æŠ½å‡º"""
        for token in doc:
            if token.pos_ == 'AUX' or token.lemma_ in ['will', 'can', 'may', 'must', 'should', 'could', 'would']:
                return token.text
        return None
    
    def _extract_by_capture(self, doc, capture_num: int, rule_id: str) -> Optional[str]:
        """ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚°ãƒ«ãƒ¼ãƒ—ã«ã‚ˆã‚‹æŠ½å‡º"""
        # ãƒ«ãƒ¼ãƒ«ç‰¹åŒ–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        if rule_id == 'time-M3':
            # æ™‚é–“è¡¨ç¾ã®ã‚­ãƒ£ãƒ—ãƒãƒ£
            time_patterns = [
                r'\b(last night|yesterday|tomorrow|today)\b',
                r'\b(last|next|this) (week|month|year)\b',
                r'\b(an? \w+ ago)\b'
            ]
            
            for pattern in time_patterns:
                match = re.search(pattern, doc.text, re.IGNORECASE)
                if match:
                    return match.group(1) if capture_num == 1 else match.group()
        
        return None
    
    def _rule_id_based_extraction(self, rule_id: str, doc) -> Optional[str]:
        """ãƒ«ãƒ¼ãƒ«IDåˆ¥ç‰¹åŒ–æŠ½å‡º"""
        
        if rule_id == 'aux-will':
            for token in doc:
                if token.lemma_ == 'will':
                    return token.text
        
        elif rule_id == 'aux-have':
            for token in doc:
                if token.lemma_ == 'have' and any(child.tag_.startswith('VB') and child.tag_ != 'VBG' for child in token.children):
                    return token.text
        
        elif rule_id.startswith('V-'):
            # å‹•è©ãƒ«ãƒ¼ãƒ«
            lemma_target = rule_id.split('-')[1] if len(rule_id.split('-')) > 1 else None
            if lemma_target:
                for token in doc:
                    if token.lemma_ == lemma_target:
                        return token.text
        
        elif rule_id == 'subject-pronoun-np-front':
            return self._extract_subject(doc)
        
        elif rule_id == 'be-progressive':
            # é€²è¡Œå½¢ã®beå‹•è©
            for token in doc:
                if token.lemma_ == 'be' and any(child.tag_ == 'VBG' for child in token.children):
                    return token.text
        
        elif rule_id.startswith('time-'):
            # æ™‚é–“è¡¨ç¾
            time_patterns = [
                r'\b(yesterday|tomorrow|today|tonight|last night)\b',
                r'\b(last|next|this) (week|month|year|Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)\b'
            ]
            
            for pattern in time_patterns:
                match = re.search(pattern, doc.text, re.IGNORECASE)
                if match:
                    return match.group()
        
        elif rule_id.startswith('place-'):
            # å ´æ‰€è¡¨ç¾
            place_pattern = r'\b(on|in|under|by|at)\s+([^.,!?]+)'
            match = re.search(place_pattern, doc.text, re.IGNORECASE)
            if match:
                return match.group(2).strip()
        
        return None
    
    def _apply_enhanced_pattern_rules(self, doc, basic_slots) -> Dict[str, Any]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«ã®å®Œå…¨å®Ÿè£…"""
        
        print("ğŸ”„ Enhanced ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«é©ç”¨é–‹å§‹")
        
        pattern_results = {}
        applied_patterns = []
        
        for pattern in self.pattern_rules:
            pattern_id = pattern.get('id', 'unknown')
            
            try:
                print(f"ğŸ” ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¤å®š: {pattern_id}")
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³é©ç”¨æ¡ä»¶åˆ¤å®š
                if self._should_apply_enhanced_pattern(pattern, doc, basic_slots):
                    print(f"  â†’ ãƒ‘ã‚¿ãƒ¼ãƒ³é©ç”¨: {pattern_id}")
                    
                    # ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè¡Œ
                    result = self._execute_enhanced_pattern(pattern, doc, basic_slots)
                    if result:
                        pattern_results[pattern_id] = result
                        applied_patterns.append(pattern_id)
                        print(f"âœ… ãƒ‘ã‚¿ãƒ¼ãƒ³å®Œäº†: {pattern_id}")
                
            except Exception as e:
                print(f"âš ï¸ ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¨ãƒ©ãƒ¼ {pattern_id}: {e}")
        
        print(f"ğŸ“Š ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«é©ç”¨æ•°: {len(applied_patterns)}/{len(self.pattern_rules)}")
        
        return pattern_results
    
    def _should_apply_enhanced_pattern(self, pattern: Dict[str, Any], doc, basic_slots) -> bool:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³é©ç”¨åˆ¤å®š"""
        
        # å‹•è©ç¢ºèª
        if 'verbs' in pattern:
            verbs_config = pattern['verbs']
            required_lemmas = verbs_config.get('lemmas', [])
            
            doc_lemmas = [token.lemma_ for token in doc]
            if not any(lemma in doc_lemmas for lemma in required_lemmas):
                print(f"  âŒ å¿…è¦å‹•è©ä¸åœ¨: {required_lemmas}")
                return False
            print(f"  âœ… å¿…è¦å‹•è©ç¢ºèª: {required_lemmas}")
        
        # æ¡ä»¶ç¢ºèªï¼ˆç·©å’Œç‰ˆ - ã‚¹ãƒ­ãƒƒãƒˆã®å­˜åœ¨ã‚’å‰æã¨ã—ãªã„ï¼‰
        if 'required_slots' in pattern:
            required = pattern['required_slots']
            # ã™ã¹ã¦ã®ã‚¹ãƒ­ãƒƒãƒˆãŒåŸ‹ã¾ã£ã¦ã„ã‚‹ã“ã¨ã‚’å¿…é ˆã¨ã—ãªã„
            # ä¸€éƒ¨ã®ã‚¹ãƒ­ãƒƒãƒˆãŒã‚ã‚Œã°é©ç”¨ã‚’è©¦ã¿ã‚‹
            print(f"  âœ… æ¡ä»¶ç·©å’Œ: å¿…è¦ã‚¹ãƒ­ãƒƒãƒˆ {required}")
        
        return True
    
    def _execute_enhanced_pattern(self, pattern: Dict[str, Any], doc, basic_slots) -> Optional[Dict[str, Any]]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè¡Œ"""
        
        pattern_id = pattern.get('id', '')
        
        if pattern_id == 'ditransitive_SVO1O2':
            return self._handle_enhanced_ditransitive(pattern, doc, basic_slots)
        elif pattern_id == 'causative_make_SVO1C2':
            return self._handle_enhanced_causative(pattern, doc, basic_slots)
        elif pattern_id == 'copular_become_SC1':
            return self._handle_enhanced_copular(pattern, doc, basic_slots)
        elif pattern_id == 'cognition_verb_that_clause':
            return self._handle_enhanced_cognition(pattern, doc, basic_slots)
        
        return None
    
    def _handle_enhanced_ditransitive(self, pattern, doc, basic_slots):
        """ç¬¬4æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³å‡¦ç†"""
        print("  ğŸ¯ Enhanced ç¬¬4æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè¡Œ")
        
        ditransitive_verbs = ['give', 'show', 'tell', 'send', 'teach', 'buy', 'make']
        
        for token in doc:
            if token.lemma_ in ditransitive_verbs:
                # ç›®çš„èªã‚’2ã¤æ¢ã™
                objects = []
                for child in token.children:
                    if child.dep_ in ['dobj', 'dative', 'iobj']:
                        objects.append(child.text)
                
                if len(objects) >= 2:
                    return {
                        'pattern_type': 'ç¬¬4æ–‡å‹ (SVOO)',
                        'verb': token.text,
                        'indirect_object': objects[0],
                        'direct_object': objects[1],
                        'confidence': 0.95
                    }
                elif len(objects) >= 1:
                    # 1ã¤ã§ã‚‚ç›®çš„èªãŒã‚ã‚Œã°éƒ¨åˆ†çš„æˆåŠŸ
                    return {
                        'pattern_type': 'ç¬¬4æ–‡å‹å€™è£œ (SVO)',
                        'verb': token.text,
                        'object': objects[0],
                        'confidence': 0.75
                    }
        
        return None
    
    def _handle_enhanced_causative(self, pattern, doc, basic_slots):
        """ä½¿å½¹ãƒ‘ã‚¿ãƒ¼ãƒ³å‡¦ç†"""
        print("  ğŸ¯ Enhanced ä½¿å½¹ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè¡Œ")
        
        for token in doc:
            if token.lemma_ == 'make':
                # make + O + bare infinitive
                objects = []
                complements = []
                
                for child in token.children:
                    if child.dep_ == 'dobj':
                        objects.append(child.text)
                    elif child.dep_ in ['xcomp', 'ccomp'] and child.pos_ == 'VERB':
                        complements.append(child.text)
                
                if objects and complements:
                    return {
                        'pattern_type': 'ä½¿å½¹ (make SVO1C2)',
                        'causative_verb': token.text,
                        'causee': objects[0],
                        'action': complements[0],
                        'confidence': 0.9
                    }
                elif objects:
                    # éƒ¨åˆ†çš„ãƒãƒƒãƒ
                    return {
                        'pattern_type': 'ä½¿å½¹å€™è£œ (make SVO1)',
                        'causative_verb': token.text,
                        'causee': objects[0],
                        'confidence': 0.7
                    }
        
        return None
    
    def _handle_enhanced_copular(self, pattern, doc, basic_slots):
        """é€£çµå‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³å‡¦ç†"""
        print("  ğŸ¯ Enhanced é€£çµå‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè¡Œ")
        
        copular_verbs = ['become', 'seem', 'appear', 'remain', 'stay', 'get', 'turn']
        
        for token in doc:
            if token.lemma_ in copular_verbs:
                complements = []
                
                for child in token.children:
                    if child.dep_ in ['attr', 'acomp', 'acomp']:
                        # å½¢å®¹è©ãƒ»åè©è£œèªã‚’å–å¾—
                        complements.append(child.text)
                
                if complements:
                    return {
                        'pattern_type': 'é€£çµå‹•è© (SVC)',
                        'copular_verb': token.text,
                        'complement': complements[0],
                        'confidence': 0.88
                    }
        
        return None
    
    def _handle_enhanced_cognition(self, pattern, doc, basic_slots):
        """èªè­˜å‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³å‡¦ç†"""
        print("  ğŸ¯ Enhanced èªè­˜å‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè¡Œ")
        
        cognition_verbs = ['know', 'think', 'believe', 'realize', 'understand', 'figure']
        
        for token in doc:
            if token.lemma_ in cognition_verbs:
                # thatç¯€ã¾ãŸã¯è£œèªç¯€ã‚’æ¢ã™
                clauses = []
                
                for child in token.children:
                    if child.dep_ in ['ccomp', 'xcomp']:
                        clauses.append(child.text)
                
                # that ã®å­˜åœ¨ç¢ºèª
                has_that = 'that' in doc.text.lower()
                
                if clauses or has_that:
                    return {
                        'pattern_type': 'èªè­˜å‹•è©+ç¯€',
                        'cognition_verb': token.text,
                        'has_that_clause': has_that,
                        'clause_content': clauses[0] if clauses else 'implicit',
                        'confidence': 0.87
                    }
        
        return None
    
    # è£œåŠ©ãƒ¡ã‚½ãƒƒãƒ‰
    def _find_main_verb(self, doc):
        """ä¸»å‹•è©æ¤œå‡º"""
        for token in doc:
            if token.dep_ == 'ROOT' and token.pos_ == 'VERB':
                return token
        return None
    
    def _check_existence_pattern(self, doc) -> bool:
        """å­˜åœ¨ãƒ‘ã‚¿ãƒ¼ãƒ³ç¢ºèª"""
        # there is/are æ§‹æ–‡
        if doc.text.lower().startswith('there'):
            return True
        
        # be + å ´æ‰€è¡¨ç¾
        for token in doc:
            if token.lemma_ == 'be':
                for child in token.children:
                    if child.dep_ == 'prep' or child.pos_ == 'ADP':
                        return True
        
        return False
    
    def _integrate_enhanced_results(self, basic_slots, pattern_results):
        """çµæœçµ±åˆ"""
        print("ğŸ”„ Enhanced çµæœçµ±åˆå®Ÿè¡Œ")
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³çµæœã‚’åŸºæœ¬ã‚¹ãƒ­ãƒƒãƒˆã«åæ˜ 
        for pattern_id, pattern_result in pattern_results.items():
            if pattern_result and 'pattern_type' in pattern_result:
                print(f"ğŸ“Š ãƒ‘ã‚¿ãƒ¼ãƒ³çµ±åˆ: {pattern_id} â†’ {pattern_result['pattern_type']}")
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³å›ºæœ‰ã®æƒ…å ±ã‚’è¿½åŠ ã‚¹ãƒ­ãƒƒãƒˆã¨ã—ã¦è¨˜éŒ²
                if 'verb' in pattern_result:
                    verb_entry = {
                        'value': pattern_result['verb'],
                        'rule_id': f'pattern-{pattern_id}',
                        'confidence': pattern_result.get('confidence', 0.8),
                        'pattern_info': pattern_result
                    }
                    basic_slots['V'].append(verb_entry)
        
        return basic_slots
    
    def _apply_enhanced_post_processing(self, slots, doc):
        """å¾Œå‡¦ç†"""
        print("ğŸ”„ Enhanced å¾Œå‡¦ç†å®Ÿè¡Œ")
        
        # é‡è¤‡é™¤å»ï¼ˆä¿¡é ¼åº¦é †ï¼‰
        for slot, items in slots.items():
            if len(items) > 1:
                slots[slot] = sorted(items, key=lambda x: x.get('confidence', 0), reverse=True)[:2]  # ä¸Šä½2ã¤ã¾ã§ä¿æŒ
        
        return slots
    
    def _determine_sentence_pattern(self, slots):
        """æ–‡å‹åˆ¤å®š"""
        s_present = bool(slots['S'])
        v_present = bool(slots['V'])
        o1_present = bool(slots['O1'])
        o2_present = bool(slots['O2'])
        c1_present = bool(slots['C1'])
        
        if s_present and v_present and o1_present and o2_present:
            return "ç¬¬4æ–‡å‹ (SVOO)"
        elif s_present and v_present and o1_present and c1_present:
            return "ç¬¬5æ–‡å‹ (SVOC)"
        elif s_present and v_present and c1_present:
            return "ç¬¬2æ–‡å‹ (SVC)"
        elif s_present and v_present and o1_present:
            return "ç¬¬3æ–‡å‹ (SVO)"
        elif s_present and v_present:
            return "ç¬¬1æ–‡å‹ (SV)"
        else:
            return "ä¸å®Œå…¨æ–‡å‹"

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
if __name__ == "__main__":
    engine = PerfectRephraseParsingEngine()
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_sentences = [
        "She will give him a book.",
        "They made him work hard.",  
        "He became a doctor.",
        "I know that he is right."
    ]
    
    for sentence in test_sentences:
        result = engine.analyze_sentence(sentence)
        print(f"\nçµæœ: {result}")
        print("-" * 40)
