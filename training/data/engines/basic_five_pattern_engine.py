#!/usr/bin/env python3
"""
Basic Five Pattern Engine - Lightweight Integrated Version
Pure Stanza Engine V3.1ã‹ã‚‰æŠ½å‡ºã—ãŸçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«ã‚ˆã‚‹è»½é‡åŸºæœ¬5æ–‡å‹ã‚¨ãƒ³ã‚¸ãƒ³

Features:
1. Pure Stanza Engine V3.1ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ç¶™æ‰¿
2. Grammar Master Controllerçµ±åˆä»•æ§˜æº–æ‹ 
3. ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ’é™¤ï¼ˆçŸ¥è­˜ãƒ™ãƒ¼ã‚¹é§†å‹•ï¼‰
4. åŸºæœ¬5æ–‡å‹ + åŠ©å‹•è©æ§‹æ–‡ã«ç‰¹åŒ–
"""

import stanza
from typing import Dict, List, Optional, Any
import time

class BasicFivePatternEngine:
    """åŸºæœ¬5æ–‡å‹ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆçµ±åˆå‹è»½é‡ç‰ˆï¼‰"""
    
    def __init__(self):
        """çµ±åˆä»•æ§˜å¯¾å¿œã®è»½é‡5æ–‡å‹ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–"""
        print("ğŸš€ Basic Five Pattern Engine åˆæœŸåŒ–ä¸­...")
        
        # Stanza NLP ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        self.nlp = stanza.Pipeline('en', verbose=False)
        
        # Pure Stanza Engine V3.1ã‹ã‚‰æŠ½å‡ºã—ãŸçŸ¥è­˜ãƒ™ãƒ¼ã‚¹
        self.sentence_patterns = self._load_sentence_patterns()
        self.modifier_mappings = self._load_modifier_mappings()
        
        self.name = "BasicFivePatternEngine"
        self.version = "1.0"
        
        print("âœ… åŸºæœ¬5æ–‡å‹ã‚¨ãƒ³ã‚¸ãƒ³æº–å‚™å®Œäº†")
    
    def _load_sentence_patterns(self) -> Dict[str, Any]:
        """Pure Stanza Engine V3.1ã‹ã‚‰æŠ½å‡ºï¼šåŸºæœ¬æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³"""
        return {
            # åŸºæœ¬5æ–‡å‹ï¼ˆå„ªå…ˆåº¦ã‚’èª¿æ•´ï¼‰
            "SVOO": {
                "required_relations": ["nsubj", "iobj", "obj", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "iobj": "O1", "obj": "O2", "root": "V"},
                "priority": 1  # æœ€é«˜å„ªå…ˆåº¦ï¼ˆæœ€ã‚‚å…·ä½“çš„ï¼‰
            },
            "SVOC": {
                "required_relations": ["nsubj", "obj", "xcomp", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "obj": "O1", "xcomp": "C2", "root": "V"},
                "priority": 2
            },
            "SVO": {
                "required_relations": ["nsubj", "obj", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "obj": "O1", "root": "V"},
                "priority": 3
            },
            "SVC": {
                "required_relations": ["nsubj", "cop", "root"],
                "root_pos": ["ADJ", "NOUN"],
                "mapping": {"nsubj": "S", "cop": "V", "root": "C1"},
                "priority": 4
            },
            "SV": {
                "required_relations": ["nsubj", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "root": "V"},
                "priority": 5  # æœ€ã‚‚æ±ç”¨çš„ãªã®ã§ä½å„ªå…ˆåº¦
            },
            
            # åŠ©å‹•è©æ§‹æ–‡ï¼ˆPure Stanza Engine V3.1ã‹ã‚‰ç¶™æ‰¿ï¼‰
            "S_AUX_VO": {
                "required_relations": ["nsubj", "aux", "obj", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "aux": "Aux", "obj": "O1", "root": "V"},
                "priority": 6
            },
            "S_AUX_VC": {
                "required_relations": ["nsubj", "aux", "cop", "root"],
                "root_pos": ["ADJ", "NOUN"],
                "mapping": {"nsubj": "S", "aux": "Aux", "cop": "V", "root": "C1"},
                "priority": 7
            },
            "S_AUX_V": {
                "required_relations": ["nsubj", "aux", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj": "S", "aux": "Aux", "root": "V"},
                "priority": 8
            },
            
            # å—å‹•æ…‹ãƒ‘ã‚¿ãƒ¼ãƒ³
            "PASSIVE": {
                "required_relations": ["nsubj:pass", "aux:pass", "root"],
                "root_pos": ["VERB"],
                "mapping": {"nsubj:pass": "S", "aux:pass": "Aux", "root": "V"},
                "priority": 9
            }
        }
    
    def _load_modifier_mappings(self) -> Dict[str, str]:
        """Pure Stanza Engine V3.1ã‹ã‚‰æŠ½å‡ºï¼šä¿®é£¾èªãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆåŸºæœ¬5æ–‡å‹ã«ç‰¹åŒ–ï¼‰"""
        return {
            # åŸºæœ¬ä¿®é£¾èªï¼ˆæ–‡å‹æ§‹é€ ã«é–¢ã‚ã‚‹ã‚‚ã®ã®ã¿ï¼‰
            "advmod": "M2",     # å‰¯è©ä¿®é£¾èªï¼ˆquickly, hard, etc.ï¼‰
            "nmod": "M1",       # åè©ä¿®é£¾èª
            "obl": "M3",        # æ–œæ ¼èªï¼ˆå‰ç½®è©å¥ãªã©ï¼‰
            "tmod": "M3",       # æ™‚é–“ä¿®é£¾
            "neg": "M3",        # å¦å®š
            
            # é™¤å¤–ã™ã‚‹ä¿®é£¾èªï¼ˆä»–ã‚¨ãƒ³ã‚¸ãƒ³ã®å½¹å‰²ï¼‰
            # "det": å† è©ãƒ»é™å®šè©ã¯åŸºæœ¬5æ–‡å‹ã§ã¯æ‰±ã‚ãªã„
            # "case": å‰ç½®è©ã¯å‰ç½®è©å¥ã‚¨ãƒ³ã‚¸ãƒ³ã®å½¹å‰²
            # "aux": åŠ©å‹•è©ã¯ãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¨ãƒ³ã‚¸ãƒ³ã®å½¹å‰²
            # "agent": å‹•ä½œä¸»ã¯å—å‹•æ…‹ã‚¨ãƒ³ã‚¸ãƒ³ã®å½¹å‰²
        }
    
    def process_sentence(self, sentence: str) -> Optional[Dict]:
        """çµ±åˆä»•æ§˜æº–æ‹ ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†ãƒ¡ã‚½ãƒƒãƒ‰"""
        if not sentence or len(sentence.strip()) < 2:
            return None
        
        start_time = time.time()
        
        try:
            # Stanzaè§£æ
            doc = self.nlp(sentence)
            sent = doc.sentences[0]
            
            # åŸºæœ¬5æ–‡å‹ã®æ¤œå‡ºã¨å‡¦ç†
            result = self._analyze_basic_patterns(sent)
            
            if result:
                processing_time = time.time() - start_time
                return {
                    "engine": self.name,
                    "version": self.version,
                    "pattern": result["pattern"],
                    "slots": result["slots"],
                    "confidence": result["confidence"],
                    "processing_time": processing_time,
                    "processed": True
                }
                
        except Exception as e:
            print(f"âš ï¸ Basic Pattern Engine Error: {e}")
            return None
        
        return None
    
    def _analyze_basic_patterns(self, sent) -> Optional[Dict]:
        """çŸ¥è­˜ãƒ™ãƒ¼ã‚¹é§†å‹•ã®åŸºæœ¬æ–‡å‹è§£æ"""
        
        # ROOTèªæ¤œå‡º
        root_word = self._find_root_word(sent)
        if not root_word:
            return None
        
        # ä¾å­˜é–¢ä¿‚ãƒãƒƒãƒ—æ§‹ç¯‰
        dep_relations = {}
        for word in sent.words:
            if word.deprel not in dep_relations:
                dep_relations[word.deprel] = []
            dep_relations[word.deprel].append(word)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼ˆå„ªå…ˆåº¦é †ï¼‰
        for pattern_name, pattern_info in sorted(
            self.sentence_patterns.items(), 
            key=lambda x: x[1]["priority"]
        ):
            if self._matches_pattern(pattern_info, dep_relations, root_word):
                slots = self._build_slots(pattern_info, dep_relations, sent)
                if slots:
                    return {
                        "pattern": pattern_name,
                        "slots": slots,
                        "confidence": self._calculate_confidence(pattern_name, slots)
                    }
        
        return None
    
    def _matches_pattern(self, pattern_info: Dict, dep_relations: Dict, root_word) -> bool:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°åˆ¤å®š"""
        # å¿…è¦ãªä¾å­˜é–¢ä¿‚ã®å­˜åœ¨ç¢ºèª
        required_relations = pattern_info["required_relations"]
        for rel in required_relations:
            if rel not in dep_relations:
                return False
        
        # ROOTèªã®å“è©ãƒã‚§ãƒƒã‚¯
        root_pos_allowed = pattern_info["root_pos"]
        if root_word.pos not in root_pos_allowed:
            return False
        
        return True
    
    def _build_slots(self, pattern_info: Dict, dep_relations: Dict, sent) -> Dict[str, str]:
        """ã‚¹ãƒ­ãƒƒãƒˆæ§‹ç¯‰ï¼ˆçŸ¥è­˜ãƒ™ãƒ¼ã‚¹é§†å‹•ï¼‰"""
        slots = {}
        mapping = pattern_info["mapping"]
        
        # ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ­ãƒƒãƒˆæ§‹ç¯‰
        for dep_rel, slot in mapping.items():
            if dep_rel in dep_relations:
                words = dep_relations[dep_rel]
                if words:
                    # è¤‡æ•°ã®èªãŒã‚ã‚‹å ´åˆã¯æœ€åˆã®èªã‚’ä½¿ç”¨
                    target_word = words[0]
                    # èªå¥å¢ƒç•Œã®æ‹¡å¼µ
                    expanded_text = self._expand_phrase_boundary(target_word, sent)
                    slots[slot] = expanded_text
            elif dep_rel == "root":
                # ROOTèªã®å‡¦ç†
                root_word = self._find_root_word(sent)
                if root_word and slot in ["V"]:
                    slots[slot] = root_word.text
        
        # ä¿®é£¾èªã®å‡¦ç†
        modifier_slots = {"M1": [], "M2": [], "M3": []}
        for word in sent.words:
            if word.deprel in self.modifier_mappings:
                slot = self.modifier_mappings[word.deprel]
                if slot.startswith("M"):  # M1, M2, M3ã‚¹ãƒ­ãƒƒãƒˆ
                    expanded_text = self._expand_phrase_boundary(word, sent)
                    if expanded_text not in modifier_slots[slot]:
                        modifier_slots[slot].append(expanded_text)
        
        # ä¿®é£¾èªã‚¹ãƒ­ãƒƒãƒˆã‚’çµ±åˆ
        for slot, values in modifier_slots.items():
            if values:
                slots[slot] = ", ".join(values)
        
        return slots
    
    def _find_root_word(self, sent):
        """ROOTèªæ¤œå‡º"""
        for word in sent.words:
            if word.deprel == 'root':
                return word
        return None
    
    def _expand_phrase_boundary(self, word, sent) -> str:
        """åŸºæœ¬5æ–‡å‹ã«é©ã—ãŸèªå¥å¢ƒç•Œæ‹¡å¼µ"""
        # èªå¥å¢ƒç•Œæ‹¡å¼µã®ä¾å­˜é–¢ä¿‚ï¼ˆåŸºæœ¬æ–‡å‹æ§‹é€ ã«å¿…è¦ãªã‚‚ã®ã®ã¿ï¼‰
        expand_deps = ['compound', 'amod', 'nummod']  # è¤‡åˆèªã€å½¢å®¹è©ã€æ•°é‡è©ã®ã¿
        
        words_to_include = [word]
        
        # å­è¦ç´ ã®æ¢ç´¢
        for other_word in sent.words:
            if (other_word.head == word.id and 
                other_word.deprel in expand_deps):
                words_to_include.append(other_word)
        
        # å† è©ãƒ»é™å®šè©ã¯åŸºæœ¬æ–‡å‹ã§ã¯å«ã‚ã‚‹ï¼ˆã‚¹ãƒ­ãƒƒãƒˆã«ã¯è¨­å®šã—ãªã„ï¼‰
        for other_word in sent.words:
            if (other_word.head == word.id and 
                other_word.deprel == 'det'):
                words_to_include.append(other_word)
        
        # ä½ç½®é †ã§ã‚½ãƒ¼ãƒˆ
        words_to_include.sort(key=lambda w: w.id)
        
        return " ".join([w.text for w in words_to_include])
    
    def _calculate_confidence(self, pattern_name: str, slots: Dict) -> float:
        """ä¿¡é ¼åº¦è¨ˆç®—"""
        base_confidence = 0.85
        
        # ã‚¹ãƒ­ãƒƒãƒˆæ•°ã«ã‚ˆã‚‹ãƒœãƒ¼ãƒŠã‚¹
        slot_bonus = len(slots) * 0.02
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³å›ºæœ‰ã®ãƒœãƒ¼ãƒŠã‚¹
        pattern_bonus = {
            "SVO": 0.05,    # æœ€ã‚‚ä¸€èˆ¬çš„
            "SV": 0.03,     # ã‚·ãƒ³ãƒ—ãƒ«
            "SVC": 0.04,    # æ˜ç¢ºãªæ§‹é€ 
            "SVOO": 0.08,   # è¤‡é›‘ã ãŒæ˜ç¢º
            "SVOC": 0.06    # è¤‡é›‘æ§‹é€ 
        }.get(pattern_name, 0.0)
        
        confidence = min(0.98, base_confidence + slot_bonus + pattern_bonus)
        return confidence

def test_basic_five_pattern_engine():
    """ãƒ†ã‚¹ãƒˆé–¢æ•°"""
    engine = BasicFivePatternEngine()
    
    test_sentences = [
        "I love programming.",
        "She is a teacher.",
        "He gave her a book.",
        "We consider him smart.",
        "The dog runs quickly.",
        "I can speak English.",
        "They are working hard.",
        "The book was written by John."
    ]
    
    print("\nğŸ§ª Testing Basic Five Pattern Engine")
    print("=" * 60)
    
    for i, sentence in enumerate(test_sentences, 1):
        print(f"\nâœ… Test {i}: {sentence}")
        result = engine.process_sentence(sentence)
        if result:
            print(f"    Pattern: {result['pattern']}")
            print(f"    Slots: {result['slots']}")
            print(f"    Confidence: {result['confidence']:.3f}")
        else:
            print("    âŒ No pattern detected")

if __name__ == "__main__":
    test_basic_five_pattern_engine()
