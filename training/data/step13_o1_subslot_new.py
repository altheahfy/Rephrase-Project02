#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Rule Dictionary v2.0 - O1(ç›´æ¥ç›®çš„èª)ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
spaCyä¾å­˜æ§‹é€ è§£æã«ã‚ˆã‚‹å‹•çš„ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º

ç›´æ¥ç›®çš„èªã®è¤‡é›‘æ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³:
1. é–¢ä¿‚ä»£åè©ä»˜ãç›®çš„èª: "The book that you recommended" â†’ sub-o1: 'The book that', sub-v: 'recommended'
2. åŒæ ¼thatç¯€: "The fact that he left" â†’ sub-s: 'The fact that he', sub-v: 'left'  
3. ä¸å®šè©ç›®çš„èª: "To go home" â†’ sub-v: 'To go', sub-o1: 'home'
4. å‹•åè©ç›®çš„èª: "Reading books" â†’ sub-v: 'Reading', sub-o1: 'books'
5. è¤‡åˆç›®çš„èª: "apples and oranges" â†’ wordæ‰±ã„ (Væ§‹é€ ãªã—)
"""

import spacy
from typing import Dict, List, Tuple, Any

class O1SubslotGenerator:
    """O1(ç›´æ¥ç›®çš„èª)ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.nlp = spacy.load("en_core_web_sm")
    
    def generate_o1_subslots(self, slot_phrase: str, phrase_type: str) -> Dict[str, Dict[str, Any]]:
        """
        O1(ç›´æ¥ç›®çš„èª)ã‚¹ãƒ­ãƒƒãƒˆã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ
        
        Args:
            slot_phrase: ç›®çš„èªãƒ•ãƒ¬ãƒ¼ã‚º
            phrase_type: ãƒ•ãƒ¬ãƒ¼ã‚ºã‚¿ã‚¤ãƒ— (word/phrase/clause)
            
        Returns:
            Dict: ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆè¾æ›¸
        """
        doc = self.nlp(slot_phrase)
        
        if phrase_type == "word":
            # å˜èªã®å ´åˆã¯ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ä¸è¦
            return {}
        elif phrase_type == "phrase":
            return self._extract_o1_phrase_subslots(doc)
        elif phrase_type == "clause":
            return self._extract_o1_clause_subslots(doc)
        else:
            return {}
    
    def _extract_o1_phrase_subslots(self, doc):
        """O1 Phraseã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # é–¢ä¿‚ä»£åè©ã®æ¤œå‡ºï¼ˆã‚ˆã‚ŠæŸ”è»Ÿãªæ¡ä»¶ï¼‰
        rel_pronouns = ["who", "whom", "whose", "which", "that"]
        rel_pronoun_token = None
        
        for token in doc:
            if token.text.lower() in rel_pronouns:
                # é–¢ä¿‚ä»£åè©ã®æ¡ä»¶ã‚’ç·©å’Œï¼šnsubjä»¥å¤–ã‚‚æ¤œå‡º
                if token.dep_ in ["nsubj", "dobj", "pobj", "nsubjpass"] or token.pos_ == "PRON":
                    rel_pronoun_token = token
                    break
        
        if rel_pronoun_token:
            subslots.update(self._extract_relative_clause_subslots(doc, rel_pronoun_token))
        
        # ä¸å®šè©ä¸»èªã®å‡¦ç†: "To learn English is important"
        elif doc[0].text.lower() == "to" and doc[0].pos_ == "PART":
            subslots.update(self._extract_infinitive_subject_subslots(doc))
        
        # å‹•åè©ä¸»èªã®å‡¦ç†: "Reading books is fun"
        else:
            gerund_tokens = [token for token in doc if token.pos_ == "VERB" and token.tag_ == "VBG"]
            if gerund_tokens:
                subslots.update(self._extract_gerund_subject_subslots(doc, gerund_tokens[0]))
        
        # è¤‡åˆä¸»èªã®å‡¦ç†: "John and Mary are here"
        and_tokens = [token for token in doc if token.text.lower() == "and" and token.dep_ == "cc"]
        if and_tokens:
            subslots.update(self._extract_compound_subject_subslots(doc))
        
        # advmodï¼ˆå‰¯è©ä¿®é£¾èªï¼‰ã‚’sub-m2ã¨ã—ã¦æœ€å¾Œã«è¿½åŠ 
        advmod_tokens = [token for token in doc if token.dep_ == "advmod"]
        if advmod_tokens:
            advmod_text = ' '.join([token.text for token in advmod_tokens])
            subslots['sub-m2'] = {
                'text': advmod_text,
                'tokens': [token.text for token in advmod_tokens],
                'token_indices': [token.i for token in advmod_tokens]
            }
            print(f"âœ… sub-m2ã¨ã—ã¦å‡¦ç†: '{advmod_text}'")
        
        # TODO: å®Œå…¨ãª10å€‹ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºã‚’å®Ÿè£…äºˆå®š
        # complete_subslots = self._detect_all_subslots(doc)
        # subslots.update(complete_subslots)
        
        return subslots
    
    def _extract_o1_clause_subslots(self, doc):
        """O1 Clauseã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # advmodï¼ˆå‰¯è©ä¿®é£¾èªï¼‰ã‚’sub-m2ã¨ã—ã¦å‡¦ç†
        advmod_tokens = [token for token in doc if token.dep_ == "advmod"]
        if advmod_tokens:
            advmod_text = ' '.join([token.text for token in advmod_tokens])
            subslots['sub-m2'] = {
                'text': advmod_text,
                'tokens': [token.text for token in advmod_tokens],
                'token_indices': [token.i for token in advmod_tokens]
            }
            print(f"âœ… sub-m2ã¨ã—ã¦å‡¦ç†: '{advmod_text}'")
        
        # ã¾ãšåŒæ ¼thatç¯€ã‚’å„ªå…ˆãƒã‚§ãƒƒã‚¯
        that_token = None
        for token in doc:
            if token.text.lower() == "that":
                # thatç¯€ã®æ¤œå‡ºæ¡ä»¶ã‚’åºƒã’ã‚‹
                if token.dep_ in ["acl", "ccomp", "mark", "dobj"] or (token.pos_ == "SCONJ"):
                    that_token = token
                    break
        
        if that_token:
            # åŒæ ¼thatç¯€ã‹ã©ã†ã‹ã‚’åˆ¤å®šï¼ˆåè©ã®å¾Œã«thatãŒã‚ã‚‹å ´åˆï¼‰
            has_noun_before = False
            for token in doc:
                if token.i < that_token.i and token.pos_ in ["NOUN", "PROPN"]:
                    has_noun_before = True
                    break
            
            if has_noun_before:
                return self._extract_appositive_that_clause_subslots(doc, that_token)
        
        # ç–‘å•è©ç¯€ã®æ¤œå‡ºï¼ˆwhat, where, when, how ãªã©ï¼‰
        wh_words = ["what", "where", "when", "how", "why", "which"]
        wh_word_token = None
        
        for token in doc:
            if token.text.lower() in wh_words:
                # ç–‘å•è©ã®æ¡ä»¶ï¼šdobj, advmod, nsubj ãªã©ã®é–¢ä¿‚
                if token.dep_ in ["dobj", "advmod", "nsubj", "pobj"] or token.pos_ in ["PRON", "SCONJ"]:
                    wh_word_token = token
                    break
        
        if wh_word_token:
            return self._extract_wh_clause_subslots(doc, wh_word_token)
        
        # é–¢ä¿‚ä»£åè©ã®æ¤œå‡ºï¼ˆclauseå†…ï¼‰
        rel_pronouns = ["who", "whom", "whose", "which", "that"]
        rel_pronoun_token = None
        
        for token in doc:
            if token.text.lower() in rel_pronouns:
                rel_pronoun_token = token
                break
        
        if rel_pronoun_token:
            return self._extract_relative_clause_s_subslots(doc, rel_pronoun_token)
        
        # ãã®ä»–ã®é–¢ä¿‚ç¯€å‡¦ç†
        complex_subslots = self._extract_complex_s_clause(doc)
        subslots.update(complex_subslots)
        
        # TODO: å®Œå…¨ãª10å€‹ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºã‚’å®Ÿè£…äºˆå®š
        # complete_subslots = self._detect_all_subslots(doc)
        # subslots.update(complete_subslots)
        
        return subslots
    
    def _extract_wh_clause_subslots(self, doc, wh_word_token):
        """ç–‘å•è©ç¯€ã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡ºï¼ˆwhat you said ãªã©ï¼‰"""
        subslots = {}
        
        # ç–‘å•è©ç¯€ã®æ§‹é€ è§£æ: what(dobj) you(nsubj) said(ROOT)
        verb_token = None
        subject_tokens = []
        
        for token in doc:
            if token.pos_ == "VERB" and token.dep_ == "ROOT":
                verb_token = token
            elif token.dep_ in ["nsubj", "nsubjpass"]:
                subject_tokens.append(token)
        
        if verb_token and subject_tokens:
            # sub-o1: ç–‘å•è©
            subslots['sub-o1'] = {
                'text': wh_word_token.text,
                'tokens': [wh_word_token.text],
                'token_indices': [wh_word_token.i]
            }
            
            # sub-s: ä¸»èª
            subject_text = ' '.join([t.text for t in subject_tokens])
            subslots['sub-s'] = {
                'text': subject_text,
                'tokens': [t.text for t in subject_tokens],
                'token_indices': [t.i for t in subject_tokens]
            }
            
            # sub-v: å‹•è©
            subslots['sub-v'] = {
                'text': verb_token.text,
                'tokens': [verb_token.text],
                'token_indices': [verb_token.i]
            }
        
        return subslots
    
    def _extract_relative_clause_s_subslots(self, doc, rel_pronoun_token):
        """é–¢ä¿‚ä»£åè©ã‚’å«ã‚€S Clauseã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # é–¢ä¿‚ä»£åè©ã®å‰ã®åè©å¥ã‚’ç‰¹å®š
        noun_phrase_tokens = []
        for token in doc:
            if token.i < rel_pronoun_token.i:
                noun_phrase_tokens.append(token)
        
        # é–¢ä¿‚ä»£åè©ã®å½¹å‰²ã‚’åˆ¤å®š
        rel_verb = None
        for token in doc:
            if token.i > rel_pronoun_token.i and token.pos_ == "VERB":
                rel_verb = token
                break
        
        if rel_verb:
            # é–¢ä¿‚ä»£åè©ãŒç›®çš„èªã®å ´åˆ (whom)
            if rel_pronoun_token.text.lower() == "whom":
                # sub-o1: åè©å¥ + whom
                if noun_phrase_tokens:
                    noun_phrase_text = ' '.join([t.text for t in noun_phrase_tokens])
                    subslots['sub-o1'] = {
                        'text': f"{noun_phrase_text} {rel_pronoun_token.text}",
                        'tokens': [t.text for t in noun_phrase_tokens] + [rel_pronoun_token.text],
                        'token_indices': [t.i for t in noun_phrase_tokens] + [rel_pronoun_token.i]
                    }
            else:
                # é–¢ä¿‚ä»£åè©ãŒä¸»èªã®å ´åˆ (who)
                if noun_phrase_tokens:
                    noun_phrase_text = ' '.join([t.text for t in noun_phrase_tokens])
                    subslots['sub-s'] = {
                        'text': f"{noun_phrase_text} {rel_pronoun_token.text}",
                        'tokens': [t.text for t in noun_phrase_tokens] + [rel_pronoun_token.text],
                        'token_indices': [t.i for t in noun_phrase_tokens] + [rel_pronoun_token.i]
                    }
            
            # sub-v: é–¢ä¿‚ç¯€å†…å‹•è©
            subslots['sub-v'] = {
                'text': rel_verb.text,
                'tokens': [rel_verb.text],
                'token_indices': [rel_verb.i]
            }
            
            # sub-s: é–¢ä¿‚ç¯€å†…ä¸»èª (whomã®å ´åˆ)
            subjects = [child for child in rel_verb.children if child.dep_ == "nsubj"]
            if subjects and rel_pronoun_token.text.lower() == "whom":
                subslots['sub-s'] = {
                    'text': subjects[0].text,
                    'tokens': [subjects[0].text],
                    'token_indices': [subjects[0].i]
                }
        
        return subslots
    
    def _extract_relative_clause_subslots(self, doc, rel_pronoun_token):
        """é–¢ä¿‚ä»£åè©ä»˜ãä¸»èªã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # é–¢ä¿‚ä»£åè©ã®å‰ã«ã‚ã‚‹åè©å¥ã‚’ç‰¹å®š
        noun_phrase_tokens = []
        for token in doc:
            if token.i < rel_pronoun_token.i:
                noun_phrase_tokens.append(token)
        
        if noun_phrase_tokens:
            # sub-s: åè©å¥ + é–¢ä¿‚ä»£åè©
            noun_phrase_text = ' '.join([t.text for t in noun_phrase_tokens])
            subslots['sub-s'] = {
                'text': f"{noun_phrase_text} {rel_pronoun_token.text}",
                'tokens': [t.text for t in noun_phrase_tokens] + [rel_pronoun_token.text],
                'token_indices': [t.i for t in noun_phrase_tokens] + [rel_pronoun_token.i]
            }
        
        # é–¢ä¿‚ç¯€å†…ã®å‹•è©ã‚’ç‰¹å®š
        rel_clause_verb = None
        for token in doc:
            if token.i > rel_pronoun_token.i and token.pos_ == "VERB":
                rel_clause_verb = token
                break
        
        if rel_clause_verb:
            # sub-v: é–¢ä¿‚ç¯€å†…å‹•è©
            subslots['sub-v'] = {
                'text': rel_clause_verb.text,
                'tokens': [rel_clause_verb.text],
                'token_indices': [rel_clause_verb.i]
            }
            
            # é–¢ä¿‚ç¯€å†…ã®ä¸»èªã‚’å‡¦ç†
            subjects = [child for child in rel_clause_verb.children if child.dep_ == "nsubj"]
            if subjects:
                # sub-s: é–¢ä¿‚ç¯€å†…ä¸»èª (ä¾‹: "The man whom I met" ã® "I")
                if 'sub-s' not in subslots:  # æ—¢ã«sub-sãŒã‚ã‚‹å ´åˆã¯ä¸Šæ›¸ãã—ãªã„
                    subslots['sub-s2'] = {  # è¿½åŠ ã®ä¸»èªã¨ã—ã¦å‡¦ç†
                        'text': subjects[0].text,
                        'tokens': [subjects[0].text],
                        'token_indices': [subjects[0].i]
                    }
            
            # sub-o1: é–¢ä¿‚ç¯€å†…ç›®çš„èª
            objects = [child for child in rel_clause_verb.children if child.dep_ == "dobj"]
            if objects:
                subslots['sub-o1'] = {
                    'text': objects[0].text,
                    'tokens': [objects[0].text],
                    'token_indices': [objects[0].i]
                }
        
        return subslots
    
    def _extract_infinitive_subject_subslots(self, doc):
        """ä¸å®šè©ä¸»èªã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # "To learn English" ã®å‡¦ç†
        to_token = doc[0]  # "to"
        main_verb = None
        
        for token in doc[1:]:
            if token.pos_ == "VERB":
                main_verb = token
                break
        
        if main_verb:
            # sub-v: "to + å‹•è©" (Rephraseãƒ«ãƒ¼ãƒ«: ä¸å®šè©çµ±åˆ)
            subslots['sub-v'] = {
                'text': f"{to_token.text} {main_verb.text}",
                'tokens': [to_token.text, main_verb.text],
                'token_indices': [to_token.i, main_verb.i]
            }
            
            # sub-o1: ä¸å®šè©ã®ç›®çš„èª
            objects = [child for child in main_verb.children if child.dep_ == "dobj"]
            if objects:
                subslots['sub-o1'] = {
                    'text': objects[0].text,
                    'tokens': [objects[0].text],
                    'token_indices': [objects[0].i]
                }
        
        return subslots
    
    def _extract_gerund_subject_subslots(self, doc, gerund_token):
        """å‹•åè©ä¸»èªã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # sub-v: å‹•åè© (èª­ã‚€å‹•ä½œãªã®ã§å‹•è©ã¨ã—ã¦å‡¦ç†)
        subslots['sub-v'] = {
            'text': gerund_token.text,
            'tokens': [gerund_token.text],
            'token_indices': [gerund_token.i]
        }
        
        # sub-o1: å‹•åè©ã®ç›®çš„èª
        objects = [child for child in gerund_token.children if child.dep_ == "dobj"]
        if objects:
            subslots['sub-o1'] = {
                'text': objects[0].text,
                'tokens': [objects[0].text],
                'token_indices': [objects[0].i]
            }
        
        return subslots
    
    def _extract_compound_subject_subslots(self, doc):
        """è¤‡åˆä¸»èªã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # "John and Mary" ã¯Væ§‹é€ ãŒç„¡ã„ã®ã§ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ä¸è¦
        # wordã‚¿ã‚¤ãƒ—ã¨ã—ã¦å‡¦ç†ã™ã¹ã
        return subslots
    
    def _extract_appositive_that_clause_subslots(self, doc, that_token):
        """åŒæ ¼thatç¯€ã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # thatç¯€ã®å‰ã®åè©å¥ã‚’ç‰¹å®šï¼ˆå† è©ã‚‚å«ã‚ã‚‹ï¼‰
        noun_phrase_tokens = []
        main_noun = None
        for token in doc:
            if token.i < that_token.i:
                if token.pos_ in ["NOUN", "PROPN"]:
                    main_noun = token
                elif token.pos_ == "DET" and not noun_phrase_tokens:
                    # å† è©ã‹ã‚‰åè©å¥ã®é–‹å§‹
                    noun_phrase_tokens.append(token)
        
        if main_noun:
            # å† è©ãŒã‚ã‚‹å ´åˆã¯å«ã‚ã‚‹
            if noun_phrase_tokens:
                noun_phrase_tokens.append(main_noun)
            else:
                noun_phrase_tokens = [main_noun]
        
        # thatç¯€å†…ã®ä¸»èªã‚’ç‰¹å®š
        that_clause_subj = None
        that_clause_verb = None
        
        # ã¾ãšå‹•è©ã‚’è¦‹ã¤ã‘ã¦ã€ãã®ä¸»èªã‚’æ¢ã™
        for token in doc:
            if token.i > that_token.i and token.pos_ == "VERB":
                that_clause_verb = token
                # ãã®å‹•è©ã®ä¸»èªã‚’æ¢ã™
                subjects = [child for child in token.children if child.dep_ == "nsubj"]
                if subjects:
                    that_clause_subj = subjects[0]
                break
        
        if noun_phrase_tokens and that_clause_subj:
            # sub-s: åè©å¥ + that + ä¸»èª (Rephraseãƒ«ãƒ¼ãƒ«: åŒæ ¼ç¯€çµ±åˆ)
            noun_phrase_text = ' '.join([t.text for t in noun_phrase_tokens])
            subslots['sub-s'] = {
                'text': f"{noun_phrase_text} that {that_clause_subj.text}",
                'tokens': [t.text for t in noun_phrase_tokens] + [that_token.text, that_clause_subj.text],
                'token_indices': [t.i for t in noun_phrase_tokens] + [that_token.i, that_clause_subj.i]
            }
        elif noun_phrase_tokens:
            # ä¸»èªãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯thatã¾ã§ã‚’å«ã‚ã‚‹
            noun_phrase_text = ' '.join([t.text for t in noun_phrase_tokens])
            subslots['sub-s'] = {
                'text': f"{noun_phrase_text} that",
                'tokens': [t.text for t in noun_phrase_tokens] + [that_token.text],
                'token_indices': [t.i for t in noun_phrase_tokens] + [that_token.i]
            }
        
        # thatç¯€å†…ã®å‹•è©ã‚’å‡¦ç†ï¼ˆã™ã¹ã¦ã®ã‚±ãƒ¼ã‚¹ã§å®Ÿè¡Œï¼‰
        if that_clause_verb:
            # sub-v: thatç¯€å†…å‹•è©
            subslots['sub-v'] = {
                'text': that_clause_verb.text,
                'tokens': [that_clause_verb.text],
                'token_indices': [that_clause_verb.i]
            }
        
        return subslots
    
    def _extract_complex_s_clause(self, doc):
        """è¤‡é›‘ãªSç¯€æ§‹é€ ã®å‡¦ç†"""
        subslots = {}
        # å¿…è¦ã«å¿œã˜ã¦è¤‡é›‘ãªé–¢ä¿‚ç¯€ç­‰ã®å‡¦ç†ã‚’å®Ÿè£…
        return subslots
    
    def calculate_coverage(self, subslots: Dict, doc) -> Tuple[float, List[Tuple[str, int]]]:
        """ã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—"""
        covered_indices = set()
        for subslot_data in subslots.values():
            covered_indices.update(subslot_data['token_indices'])
        
        total_tokens = len(doc)
        covered_tokens = len(covered_indices)
        coverage = (covered_tokens / total_tokens) * 100 if total_tokens > 0 else 0
        
        # æœªé…ç½®ãƒˆãƒ¼ã‚¯ãƒ³ã®ç‰¹å®š
        uncovered = []
        for token in doc:
            if token.i not in covered_indices:
                uncovered.append((token.text, token.i))
        
        return coverage, uncovered


def test_o1_subslots():
    generator = O1SubslotGenerator()
    
    test_cases = [
        ("apple", "word"),
        ("car", "word"), 
        ("apples and oranges", "word"),  # Væ§‹é€ ãªã—
        ("the book that you recommended", "clause"),  # SVæ§‹é€ ãŒã‚ã‚‹ã®ã§clause
        ("the man whom I met", "clause"),  # SVæ§‹é€ ãŒã‚ã‚‹ã®ã§clause
        ("to go home", "phrase"),  # Væ§‹é€ 
        ("reading books", "phrase"),  # Væ§‹é€ 
        ("the fact that he left", "clause"),
        ("what you said", "clause"),
        # 10å€‹ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºãƒ†ã‚¹ãƒˆç”¨ã®è¤‡é›‘ãªã‚±ãƒ¼ã‚¹
        ("the big red car that must have been made very carefully", "clause"),  # è¤‡æ•°è¦ç´ 
        ("making her crazy for him", "phrase"),  # C2ãƒ†ã‚¹ãƒˆ (crazyãŒè£œèª)
        ("a very important decision", "phrase"),  # M1ãƒ†ã‚¹ãƒˆ (very, important)
    ]
    
    print("=== O1ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ ===\n")
    
    for slot_phrase, phrase_type in test_cases:
        print("=" * 50)
        print(f"O1 SlotPhrase: '{slot_phrase}'")
        print(f"PhraseType: {phrase_type}")
        print("=" * 50)
        
        subslots = generator.generate_o1_subslots(slot_phrase, phrase_type)
        
        if not subslots:
            print("åˆ¤å®š: wordã‚¿ã‚¤ãƒ—ï¼šã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ä¸è¦")
        else:
            print(f"ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ: {len(subslots)}å€‹")
            for subslot_type, data in subslots.items():
                print(f"  {subslot_type}: '{data['text']}'")
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—
            doc = generator.nlp(slot_phrase)
            coverage, uncovered = generator.calculate_coverage(subslots, doc)
            print(f"\nã‚«ãƒãƒ¬ãƒƒã‚¸: {coverage:.1f}% ({len(doc) - len(uncovered)}/{len(doc)})")
            
            if coverage == 100.0:
                print("âœ… å®Œå…¨ã‚«ãƒãƒ¬ãƒƒã‚¸")
            else:
                print(f"âš ï¸ æœªé…ç½®: {uncovered}")
        
        print()
    
    def _detect_all_subslots(self, doc):
        """å®Œå…¨ãª10å€‹ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºã‚¨ãƒ³ã‚¸ãƒ³"""
        subslots = {}
        
        for token in doc:
            # æ—¢å­˜ã®å‡¦ç†ã¨é‡è¤‡ã—ãªã„ã‚ˆã†ã«ã€æ–°ãŸã«å¿…è¦ãªã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®ã¿æ¤œå‡º
            
            # sub-m1: å‰ç½®ä¿®é£¾èª (å½¢å®¹è©ã€æ±ºå®šè©ãªã©)
            if token.dep_ in ["amod", "det", "nummod", "compound"] and 'sub-m1' not in subslots:
                subslots['sub-m1'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"ğŸ” sub-m1æ¤œå‡º: '{token.text}' (dep: {token.dep_})")
            
            # sub-aux: åŠ©å‹•è©
            elif token.dep_ == "aux" and 'sub-aux' not in subslots:
                subslots['sub-aux'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"ğŸ” sub-auxæ¤œå‡º: '{token.text}'")
            
            # sub-c1: è£œèª1 (attr, acomp)
            elif token.dep_ in ["attr", "acomp"] and 'sub-c1' not in subslots:
                subslots['sub-c1'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"ğŸ” sub-c1æ¤œå‡º: '{token.text}' (dep: {token.dep_})")
            
            # sub-o2: é–“æ¥ç›®çš„èª
            elif token.dep_ == "iobj" and 'sub-o2' not in subslots:
                subslots['sub-o2'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"ğŸ” sub-o2æ¤œå‡º: '{token.text}'")
            
            # sub-c2: è£œèª2 (xcomp, ccomp)
            elif token.dep_ in ["xcomp", "ccomp"] and 'sub-c2' not in subslots:
                subslots['sub-c2'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"ğŸ” sub-c2æ¤œå‡º: '{token.text}' (dep: {token.dep_})")
            
            # sub-m3: å¾Œç½®ä¿®é£¾èª (prep, acl, relcl)
            elif token.dep_ in ["prep", "acl", "relcl"] and 'sub-m3' not in subslots:
                # å‰ç½®è©å¥å…¨ä½“ã‚’å–å¾—
                prep_phrase_tokens = [token]
                if token.dep_ == "prep":
                    # å‰ç½®è©å¥ã®ç›®çš„èªã‚‚å«ã‚ã‚‹
                    for child in token.children:
                        if child.dep_ == "pobj":
                            prep_phrase_tokens.append(child)
                
                prep_phrase_text = ' '.join([t.text for t in prep_phrase_tokens])
                subslots['sub-m3'] = {
                    'text': prep_phrase_text,
                    'tokens': [t.text for t in prep_phrase_tokens],
                    'token_indices': [t.i for t in prep_phrase_tokens]
                }
                print(f"ğŸ” sub-m3æ¤œå‡º: '{prep_phrase_text}' (dep: {token.dep_})")
        
        return subslots


if __name__ == "__main__":
    test_o1_subslots()
