#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Rule Dictionary v2.0 - O1(ç›´æ¥ç›®çš„èª)ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
spaCyä¾å­˜æ§‹é€ è§£æã«ã‚ˆã‚‹å‹•çš„ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º

ç›´æ¥ç›®çš„èªã®è¤‡é›‘æ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³:
1. é–¢ä¿‚ä»£åè©ä»˜ãç›®çš„èª: "The book that you recommended" â†’ sub-o1: 'The book that', sub-v: 'recommended'
2. åŒæ ¼thatç¯€: "The fact that he left" â†’ sub-s: 'The fact that he', sub-v: 'left'  
3. ä¸å®šè©ç›®çš„èª: "To go home" â†’ sub-v: 'To go', sub-o1: 'home'
4. å‹•åè©ç›®çš„èª: "Reading books" â†’ sub-v: 'Reading', sub-o1: 'books'
5. è¤‡åˆç›®çš„èª: "apples and oranges" â†’ wordæ‰±ã„ (Væ§‹é€ ãªã—)
"""

import spacy
from typing import Dict, List, Tuple, Any

class O1SubslotGenerator:
    """O1(ç›´æ¥ç›®çš„èª)ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.nlp = spacy.load("en_core_web_sm")
    
    def generate_o1_subslots(self, slot_phrase: str, phrase_type: str) -> Dict[str, Dict[str, Any]]:
        """
        O1(ç›´æ¥ç›®çš„èª)ã‚¹ãƒ­ãƒƒãƒˆã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ
        
        Args:
            slot_phrase: ç›®çš„èªãƒ•ãƒ¬ãƒ¼ã‚º
            phrase_type: ãƒ•ãƒ¬ãƒ¼ã‚ºã‚¿ã‚¤ãƒ— (word/phrase/clause)
            
        Returns:
            Dict: ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆè¾æ›¸
        """
        doc = self.nlp(slot_phrase)
        
        if phrase_type == "word":
            # å˜èªã®å ´åˆã¯ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ä¸è¦
            return {}
        elif phrase_type == "phrase":
            return self._extract_o1_phrase_subslots(doc)
        elif phrase_type == "clause":
            return self._extract_o1_clause_subslots(doc)
        else:
            return {}
    
    def _extract_o1_phrase_subslots(self, doc):
        """O1 Phraseã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # æœ€å„ªå…ˆï¼šO1O2æ§‹é€ ï¼ˆé–“æ¥ç›®çš„èª+ç›´æ¥ç›®çš„èªï¼‰æ¤œå‡º
        o1o2_subslots = self._detect_o1o2_structure(doc)
        subslots.update(o1o2_subslots)
        print(f"ğŸ” O1O2æ§‹é€ æ¤œå‡ºçµæœ: {list(o1o2_subslots.keys())}")
        
        # æœ€åˆã«noun-verb phraseçµ±åˆæ¤œå‡º (å®Œå…¨ã‚«ãƒãƒ¬ãƒƒã‚¸ã®åŸå‰‡)
        # "students studying" ã®ã‚ˆã†ãªåè©+å‹•è©ã‚’çµ±åˆã—ã¦sub-vã¨ã—ã¦å‡¦ç†
        root_tokens = [token for token in doc if token.dep_ == "ROOT" and token.pos_ in ["NOUN", "PROPN"]]
        if root_tokens:
            root_token = root_tokens[0]
            # ROOTåè©ã«å¯¾ã—ã¦acl(adjectival clause)ã‚„relcl(relative clause)ã§å‹•è©ãŒä¿®é£¾ã—ã¦ã„ã‚‹å ´åˆ
            verb_children = [child for child in root_token.children if child.pos_ == "VERB" and child.dep_ in ["acl", "relcl"]]
            if verb_children:
                # åè©+å‹•è©ã®çµ±åˆå‡¦ç† (å®Œå…¨ã‚«ãƒãƒ¬ãƒƒã‚¸ã®ãŸã‚)
                verb_token = verb_children[0]
                
                # å† è©ãƒ»å®šå† è©ã¯å¿…ãšåè©ã¨ã‚»ãƒƒãƒˆï¼ˆ100%ã®ãƒ«ãƒ¼ãƒ«ï¼‰
                root_det_tokens = [child for child in root_token.children if child.dep_ == "det"]
                if root_det_tokens:
                    combined_tokens = root_det_tokens + [root_token, verb_token]
                    combined_tokens.sort(key=lambda x: x.i)
                    combined_text = ' '.join([t.text for t in combined_tokens])
                    combined_indices = [t.i for t in combined_tokens]
                else:
                    combined_tokens = [root_token, verb_token]
                    combined_text = f"{root_token.text} {verb_token.text}"
                    combined_indices = [root_token.i, verb_token.i]
                
                subslots['sub-v'] = {
                    'text': combined_text,
                    'tokens': [t.text for t in combined_tokens],
                    'token_indices': combined_indices
                }
                print(f"âœ… sub-vã¨ã—ã¦çµ±åˆå‡¦ç†: '{combined_text}' (åè©+å‹•è©çµ±åˆ) - indices: {combined_indices}")
        
        # é–¢ä¿‚ä»£åè©ã®æ¤œå‡ºï¼ˆã‚ˆã‚ŠæŸ”è»Ÿãªæ¡ä»¶ï¼‰
        rel_pronouns = ["who", "whom", "whose", "which", "that"]
        rel_pronoun_token = None
        
        for token in doc:
            if token.text.lower() in rel_pronouns:
                # é–¢ä¿‚ä»£åè©ã®æ¡ä»¶ã‚’ç·©å’Œï¼šnsubjä»¥å¤–ã‚‚æ¤œå‡º
                if token.dep_ in ["nsubj", "dobj", "pobj", "nsubjpass"] or token.pos_ == "PRON":
                    rel_pronoun_token = token
                    break
        
        if rel_pronoun_token:
            subslots.update(self._extract_relative_clause_subslots(doc, rel_pronoun_token))
            print(f"ğŸ” é–¢ä¿‚ä»£åè©å‡¦ç†å¾Œsub-v: {subslots.get('sub-v', {}).get('text', 'ãªã—')}")
        
        # ä¸å®šè©ä¸»èªã®å‡¦ç†: "To learn English is important"
        if doc[0].text.lower() == "to" and doc[0].pos_ == "PART":
            subslots.update(self._extract_infinitive_subject_subslots(doc))
            print(f"ğŸ” ä¸å®šè©ä¸»èªå‡¦ç†å¾Œsub-v: {subslots.get('sub-v', {}).get('text', 'ãªã—')}")
        
        # å‹•åè©ä¸»èªã®å‡¦ç†: "Reading books is fun"
        gerund_tokens = [token for token in doc if token.pos_ == "VERB" and token.tag_ == "VBG"]
        if gerund_tokens and 'sub-v' not in subslots:
            # æ—¢å­˜ã®sub-vãŒãªã„å ´åˆã®ã¿å‹•åè©å‡¦ç†ã‚’å®Ÿè¡Œ
            subslots.update(self._extract_gerund_subject_subslots(doc, gerund_tokens[0]))
            print(f"ğŸ” å‹•åè©ä¸»èªå‡¦ç†å¾Œsub-v: {subslots.get('sub-v', {}).get('text', 'ãªã—')}")
        elif gerund_tokens:
            print(f"ğŸ” å‹•åè©å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—: æ—¢å­˜sub-vä¿è­· '{subslots.get('sub-v', {}).get('text', 'ãªã—')}'")
        else:
            print(f"ğŸ” å‹•åè©ä¸»èªå‡¦ç†å¾Œsub-v: {subslots.get('sub-v', {}).get('text', 'ãªã—')}")
        
        # è¤‡åˆä¸»èªã®å‡¦ç†: "John and Mary are here"
        and_tokens = [token for token in doc if token.text.lower() == "and" and token.dep_ == "cc"]
        if and_tokens:
            subslots.update(self._extract_compound_subject_subslots(doc))
            print(f"ğŸ” è¤‡åˆä¸»èªå‡¦ç†å¾Œsub-v: {subslots.get('sub-v', {}).get('text', 'ãªã—')}")
        
        # ä½ç½®ãƒ™ãƒ¼ã‚¹ä¿®é£¾èªå‰²ã‚Šå½“ã¦ï¼ˆsub-m1, sub-m2, sub-m3ï¼‰ - O1O2æ§‹é€ ä¿è­·ç‰ˆ
        print(f"ğŸ” ä¿®é£¾èªå‰²ã‚Šå½“ã¦å‰sub-v: {subslots.get('sub-v', {}).get('text', 'ãªã—')}")
        modifier_subslots = self._assign_modifiers_by_position_with_o1o2_protection(doc, subslots)
        subslots.update(modifier_subslots)
        
        # é€šå¸¸ã®ROOTä¸»èªæ¤œå‡º (noun-verbçµ±åˆã•ã‚Œãªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
        root_tokens = [token for token in doc if token.dep_ == "ROOT" and token.pos_ in ["NOUN", "PROPN"]]
        if not subslots.get('sub-v') and root_tokens and 'sub-s' not in subslots:
            root_token = root_tokens[0]
            # å† è©ãƒ»å®šå† è©ã¯å¿…ãšåè©ã¨ã‚»ãƒƒãƒˆï¼ˆ100%ã®ãƒ«ãƒ¼ãƒ«ï¼‰
            root_det_tokens = [child for child in root_token.children if child.dep_ == "det"]
            if root_det_tokens:
                s_tokens = root_det_tokens + [root_token]
                s_tokens.sort(key=lambda x: x.i)  # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
                s_text = ' '.join([t.text for t in s_tokens])
                s_token_indices = [t.i for t in s_tokens]
            else:
                s_tokens = [root_token]
                s_text = root_token.text
                s_token_indices = [root_token.i]
            
            subslots['sub-s'] = {
                'text': s_text,
                'tokens': [t.text for t in s_tokens],
                'token_indices': s_token_indices
            }
            print(f"âœ… sub-sã¨ã—ã¦å‡¦ç†: '{s_text}' (ROOTä¸»èª+å† è©)")
        
        # ä¸å®šè©ã€Œto + å‹•è©ã€ã®çµ±åˆå‡¦ç† (æ—¢å­˜sub-vã‚’å°Šé‡ã€O1O2æ§‹é€ ã‚‚å°Šé‡)
        to_token = None
        main_verb_token = None
        
        if 'sub-v' not in subslots:
            to_verb_tokens = []
            
            for token in doc:
                if token.text.lower() == "to" and token.pos_ == "PART":
                    to_token = token
                elif token.pos_ == "VERB" and token.dep_ in ["ROOT", "xcomp", "ccomp"]:
                    main_verb_token = token
                    break
            
            if to_token and main_verb_token:
                # sub-v: "to + å‹•è©" ã¨ã—ã¦çµ±åˆ
                subslots['sub-v'] = {
                    'text': f"{to_token.text} {main_verb_token.text}",
                    'tokens': [to_token.text, main_verb_token.text],
                    'token_indices': [to_token.i, main_verb_token.i]
                }
                print(f"âœ… sub-vã¨ã—ã¦å‡¦ç†: '{to_token.text} {main_verb_token.text}' (ä¸å®šè©çµ±åˆ) - ä¸Šæ›¸ãè­¦å‘Š!")
            elif main_verb_token:
                # å‹•è©ã®ã¿
                subslots['sub-v'] = {
                    'text': main_verb_token.text,
                    'tokens': [main_verb_token.text],
                    'token_indices': [main_verb_token.i]
                }
                print(f"âœ… sub-vã¨ã—ã¦å‡¦ç†: '{main_verb_token.text}' - ä¸Šæ›¸ãè­¦å‘Š!")
        else:
            # æ—¢å­˜sub-vãŒã‚ã‚‹å ´åˆã‚‚ã€main_verb_tokenã‚’æ¢ã™
            for token in doc:
                if token.pos_ == "VERB" and token.dep_ in ["ROOT", "xcomp", "ccomp"]:
                    main_verb_token = token
                    break
        
        # sub-aux: åŠ©å‹•è©æ¤œå‡º (aux, auxpass) - ãŸã ã—ä¸å®šè©toã¯é™¤å¤–
        aux_tokens = [token for token in doc if token.dep_ in ["aux", "auxpass"] and not (token.text.lower() == "to" and token.pos_ == "PART")]
        if aux_tokens:
            aux_text = ' '.join([token.text for token in aux_tokens])
            subslots['sub-aux'] = {
                'text': aux_text,
                'tokens': [token.text for token in aux_tokens],
                'token_indices': [token.i for token in aux_tokens]
            }
            print(f"âœ… sub-auxã¨ã—ã¦å‡¦ç†: '{aux_text}'")
        
        # sub-c2: è£œèª2æ¤œå‡º (xcomp, acomp, attr, ccomp)
        comp_tokens = [token for token in doc if token.dep_ in ["xcomp", "acomp", "attr", "ccomp"]]
        if comp_tokens:
            comp_text = ' '.join([token.text for token in comp_tokens])
            subslots['sub-c2'] = {
                'text': comp_text,
                'tokens': [token.text for token in comp_tokens],
                'token_indices': [token.i for token in comp_tokens]
            }
            print(f"âœ… sub-c2ã¨ã—ã¦å‡¦ç†: '{comp_text}'")
        
        # sub-o1æ¤œå‡º: å‹•è©ã®ç›´æ¥ç›®çš„èª (dobj) + é™å®šè©ã‚’å«ã‚€
        if main_verb_token and 'sub-o1' not in subslots:
            dobj_tokens = [child for child in main_verb_token.children if child.dep_ == "dobj"]
            if dobj_tokens:
                dobj_token = dobj_tokens[0]
                # å† è©ãƒ»å®šå† è©ã¯å¿…ãšåè©ã¨ã‚»ãƒƒãƒˆï¼ˆ100%ã®ãƒ«ãƒ¼ãƒ«ï¼‰
                det_tokens = [child for child in dobj_token.children if child.dep_ == "det"]
                if det_tokens:
                    o1_tokens = det_tokens + [dobj_token]
                    o1_tokens.sort(key=lambda x: x.i)  # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
                    o1_text = ' '.join([t.text for t in o1_tokens])
                    o1_token_indices = [t.i for t in o1_tokens]
                else:
                    o1_tokens = [dobj_token]
                    o1_text = dobj_token.text
                    o1_token_indices = [dobj_token.i]
                
                subslots['sub-o1'] = {
                    'text': o1_text,
                    'tokens': [t.text for t in o1_tokens],
                    'token_indices': o1_token_indices
                }
                print(f"âœ… sub-o1ã¨ã—ã¦å‡¦ç†: '{o1_text}' (å‹•è©ã®ç›®çš„èª+é™å®šè©)")
            else:
                # dobjãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ä»–ã®ç›®çš„èªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
                # nsubjã§ã‚‚å®Ÿè³ªçš„ã«ç›®çš„èªã®å½¹å‰²ã‚’ã™ã‚‹ã‚±ãƒ¼ã‚¹ï¼ˆè£œèªæ§‹é€ ï¼‰
                other_obj_tokens = [child for child in main_verb_token.children if child.dep_ in ["nsubj"]]
                if other_obj_tokens:
                    obj_token = other_obj_tokens[0]
                    # å† è©ãƒ»å®šå† è©ã¯å¿…ãšåè©ã¨ã‚»ãƒƒãƒˆï¼ˆ100%ã®ãƒ«ãƒ¼ãƒ«ï¼‰
                    obj_det_tokens = [child for child in obj_token.children if child.dep_ == "det"]
                    if obj_det_tokens:
                        o1_tokens = obj_det_tokens + [obj_token]
                        o1_tokens.sort(key=lambda x: x.i)  # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
                        o1_text = ' '.join([t.text for t in o1_tokens])
                        o1_token_indices = [t.i for t in o1_tokens]
                    else:
                        o1_tokens = [obj_token]
                        o1_text = obj_token.text
                        o1_token_indices = [obj_token.i]
                    
                    subslots['sub-o1'] = {
                        'text': o1_text,
                        'tokens': [t.text for t in o1_tokens],
                        'token_indices': o1_token_indices
                    }
                    print(f"âœ… sub-o1ã¨ã—ã¦å‡¦ç†: '{o1_text}' (å‹•è©ã®nsubjç›®çš„èª+é™å®šè©)")
        
        # sub-o1è¿½åŠ æ¤œå‡º: nsubjãŒè£œèªæ§‹é€ å†…ã«ã‚ã‚‹å ´åˆï¼ˆdobjãŒæ—¢ã«æ¤œå‡ºæ¸ˆã¿ã®å ´åˆã¯é™¤å¤–ï¼‰
        # å† è©ãƒ»å®šå† è©ã¯å¿…ãšåè©ã¨ã‚»ãƒƒãƒˆï¼ˆ100%ã®ãƒ«ãƒ¼ãƒ«ï¼‰ã‚’é©ç”¨
        nsubj_tokens = [token for token in doc if token.dep_ == "nsubj" and token.head.dep_ in ["ccomp", "xcomp"]]
        if nsubj_tokens and 'sub-o1' not in subslots:
            nsubj_token = nsubj_tokens[0]
            # å† è©ãƒ»å®šå† è©ã¯å¿…ãšåè©ã¨ã‚»ãƒƒãƒˆï¼ˆ100%ã®ãƒ«ãƒ¼ãƒ«ï¼‰
            nsubj_det_tokens = [child for child in nsubj_token.children if child.dep_ == "det"]
            if nsubj_det_tokens:
                o1_tokens = nsubj_det_tokens + [nsubj_token]
                o1_tokens.sort(key=lambda x: x.i)  # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
                nsubj_text = ' '.join([t.text for t in o1_tokens])
                o1_token_indices = [t.i for t in o1_tokens]
            else:
                o1_tokens = [nsubj_token]
                nsubj_text = nsubj_token.text
                o1_token_indices = [nsubj_token.i]
            
            subslots['sub-o1'] = {
                'text': nsubj_text,
                'tokens': [t.text for t in o1_tokens],
                'token_indices': o1_token_indices
            }
            print(f"âœ… sub-o1ã¨ã—ã¦å‡¦ç†: '{nsubj_text}' (è£œèªæ§‹é€ å†…ä¸»èª+å† è©)")
        

        
        # TODO: å®Œå…¨ãª10å€‹ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºã‚’å®Ÿè£…äºˆå®š
        # complete_subslots = self._detect_all_subslots(doc)
        # subslots.update(complete_subslots)
        
        # ãƒ‡ãƒãƒƒã‚°: æœ€çµ‚çµæœç¢ºèª
        if 'sub-v' in subslots:
            print(f"ğŸ” æœ€çµ‚sub-v: '{subslots['sub-v']['text']}' indices: {subslots['sub-v']['token_indices']}")
        print(f"ğŸ” æœ€çµ‚å…¨subslots: {[(k, v['text']) for k, v in subslots.items()]}")
        
        return subslots
    
    def _extract_o1_clause_subslots(self, doc):
        """O1 Clauseã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # å®Œå…¨ãª10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨
        complete_subslots = self._detect_all_subslots(doc)
        subslots.update(complete_subslots)
        
        # sub-aux: åŠ©å‹•è©æ¤œå‡º
        aux_tokens = [token for token in doc if token.dep_ == "aux"]
        if aux_tokens:
            aux_text = ' '.join([token.text for token in aux_tokens])
            subslots['sub-aux'] = {
                'text': aux_text,
                'tokens': [token.text for token in aux_tokens],
                'token_indices': [token.i for token in aux_tokens]
            }
            print(f"âœ… sub-auxã¨ã—ã¦å‡¦ç†: '{aux_text}'")
        
        # ã¾ãšåŒæ ¼thatç¯€ã‚’å„ªå…ˆãƒã‚§ãƒƒã‚¯
        that_token = None
        for token in doc:
            if token.text.lower() == "that":
                # thatç¯€ã®æ¤œå‡ºæ¡ä»¶ã‚’åºƒã’ã‚‹
                if token.dep_ in ["acl", "ccomp", "mark", "dobj"] or (token.pos_ == "SCONJ") or token.i == 0:
                    that_token = token
                    break
        
        if that_token:
            # å˜ç´”thatç¯€ã®å‡¦ç†ï¼ˆ"that he is studying hard"ï¼‰
            if that_token.i == 0:
                # æ—¢å­˜ã®ä¿®é£¾èªã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã‚’ä¿æŒ
                that_subslots = self._extract_simple_that_clause_subslots(doc, that_token)
                # æ—¢å­˜subslotsï¼ˆ_detect_all_subslotsã®çµæœï¼‰ã‚’å„ªå…ˆã—ã€é‡è¤‡ã—ãªã„ã‚‚ã®ã®ã¿è¿½åŠ 
                for key, value in that_subslots.items():
                    if key not in subslots:
                        subslots[key] = value
                return subslots
            
            # åŒæ ¼thatç¯€ã‹ã©ã†ã‹ã‚’åˆ¤å®šï¼ˆåè©ã®å¾Œã«thatãŒã‚ã‚‹å ´åˆï¼‰
            has_noun_before = False
            for token in doc:
                if token.i < that_token.i and token.pos_ in ["NOUN", "PROPN"]:
                    has_noun_before = True
                    break
            
            if has_noun_before:
                that_subslots = self._extract_appositive_that_clause_subslots(doc, that_token)
                # æ—¢å­˜subslotsï¼ˆ_detect_all_subslotsã®çµæœï¼‰ã‚’å„ªå…ˆã—ã€é‡è¤‡ã—ãªã„ã‚‚ã®ã®ã¿è¿½åŠ 
                for key, value in that_subslots.items():
                    if key not in subslots:
                        subslots[key] = value
                return subslots
            else:
                # å˜ç´”thatç¯€ã¨ã—ã¦å‡¦ç†
                that_subslots = self._extract_simple_that_clause_subslots(doc, that_token)
                # æ—¢å­˜subslotsï¼ˆ_detect_all_subslotsã®çµæœï¼‰ã‚’å„ªå…ˆã—ã€é‡è¤‡ã—ãªã„ã‚‚ã®ã®ã¿è¿½åŠ 
                for key, value in that_subslots.items():
                    if key not in subslots:
                        subslots[key] = value
                return subslots
        
        # ç–‘å•è©ç¯€ã®æ¤œå‡ºï¼ˆwhat, where, when, how ãªã©ï¼‰
        wh_words = ["what", "where", "when", "how", "why", "which"]
        wh_word_token = None
        
        for token in doc:
            if token.text.lower() in wh_words:
                # ç–‘å•è©ã®æ¡ä»¶ï¼šdobj, advmod, nsubj ãªã©ã®é–¢ä¿‚
                if token.dep_ in ["dobj", "advmod", "nsubj", "pobj"] or token.pos_ in ["PRON", "SCONJ"]:
                    wh_word_token = token
                    break
        
        if wh_word_token:
            return self._extract_wh_clause_subslots(doc, wh_word_token)
        
        # é–¢ä¿‚ä»£åè©ã®æ¤œå‡ºï¼ˆclauseå†…ï¼‰
        rel_pronouns = ["who", "whom", "whose", "which", "that"]
        rel_pronoun_token = None
        
        for token in doc:
            if token.text.lower() in rel_pronouns:
                rel_pronoun_token = token
                break
        
        if rel_pronoun_token:
            return self._extract_relative_clause_s_subslots(doc, rel_pronoun_token)
        
        # ãã®ä»–ã®é–¢ä¿‚ç¯€å‡¦ç†
        complex_subslots = self._extract_complex_s_clause(doc)
        subslots.update(complex_subslots)
        
        # å®Œå…¨ãª10å€‹ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡º
        complete_subslots = self._detect_all_subslots(doc)
        subslots.update(complete_subslots)
        
        return subslots
    
    def _extract_wh_clause_subslots(self, doc, wh_word_token):
        """ç–‘å•è©ç¯€ã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡ºï¼ˆé‡è¤‡å›é¿å¼·åŒ–ç‰ˆï¼‰"""
        subslots = {}
        
        # ç–‘å•è©ç¯€ã®æ§‹é€ è§£æ: what(dobj) you(nsubj) said(ROOT)
        verb_token = None
        subject_tokens = []
        
        for token in doc:
            # ã‚ˆã‚ŠæŸ”è»Ÿãªå‹•è©æ¤œå‡º
            if token.pos_ == "VERB" or (token.pos_ == "AUX" and token.lemma_ in ["be", "have", "do"]):
                if token.dep_ in ["ROOT", "ccomp", "xcomp", "acl", "relcl"] or token.head == wh_word_token:
                    verb_token = token
                    print(f"ğŸ” ç–‘å•è©ç¯€å‹•è©å€™è£œ: '{token.text}' (pos: {token.pos_}, dep: {token.dep_})")
            elif token.dep_ in ["nsubj", "nsubjpass"] and token != wh_word_token:
                subject_tokens.append(token)
        
        # å‹•è©ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ç–‘å•è©ã®å¾Œã®å…¨å‹•è©ã‚’æ¢ã™
        if not verb_token:
            for token in doc:
                if token.i > wh_word_token.i and (token.pos_ == "VERB" or token.pos_ == "AUX"):
                    verb_token = token
                    print(f"ğŸ” å¾Œç¶šå‹•è©æ¤œå‡º: '{token.text}' (pos: {token.pos_})")
                    break
        
        print(f"ğŸ” ç–‘å•è©ç¯€åˆ†æ: wh_word='{wh_word_token.text}', verb='{verb_token.text if verb_token else None}', subjects={[t.text for t in subject_tokens]}")
        
        if verb_token:
            # ç–‘å•è©ãŒä¸»èªã®å ´åˆï¼ˆwhat happened, who studies, which isï¼‰
            if wh_word_token.dep_ in ["nsubj", "nsubjpass"]:
                # sub-s: ç–‘å•è©ï¼ˆä¸»èªã¨ã—ã¦ï¼‰
                subslots['sub-s'] = {
                    'text': wh_word_token.text,
                    'tokens': [wh_word_token.text],
                    'token_indices': [wh_word_token.i]
                }
                print(f"âœ… sub-s(ç–‘å•è©ä¸»èª): '{wh_word_token.text}'")
                
                # sub-v: å‹•è©
                subslots['sub-v'] = {
                    'text': verb_token.text,
                    'tokens': [verb_token.text],
                    'token_indices': [verb_token.i]
                }
                print(f"âœ… sub-v(ç–‘å•è©ç¯€å‹•è©): '{verb_token.text}'")
                
                # å‹•è©ã®ç›®çš„èªã‚’å‡¦ç†ï¼ˆwho studies English ã® Englishï¼‰
                objects = [child for child in verb_token.children if child.dep_ == "dobj"]
                if objects:
                    subslots['sub-o1'] = {
                        'text': objects[0].text,
                        'tokens': [objects[0].text],
                        'token_indices': [objects[0].i]
                    }
                    print(f"âœ… sub-o1(ç–‘å•è©ç¯€ç›®çš„èª): '{objects[0].text}'")
            
            # ç–‘å•è©ãŒç›®çš„èªã®å ´åˆï¼ˆwhat you saidï¼‰
            elif wh_word_token.dep_ in ["dobj", "pobj"] and subject_tokens:
                # sub-o1: ç–‘å•è©ï¼ˆç›®çš„èªã¨ã—ã¦ï¼‰
                subslots['sub-o1'] = {
                    'text': wh_word_token.text,
                    'tokens': [wh_word_token.text],
                    'token_indices': [wh_word_token.i]
                }
                print(f"âœ… sub-o1(ç–‘å•è©ç›®çš„èª): '{wh_word_token.text}'")
                
                # sub-s: ä¸»èª
                subject_text = ' '.join([t.text for t in subject_tokens])
                subslots['sub-s'] = {
                    'text': subject_text,
                    'tokens': [t.text for t in subject_tokens],
                    'token_indices': [t.i for t in subject_tokens]
                }
                print(f"âœ… sub-s(ç–‘å•è©ç¯€ä¸»èª): '{subject_text}'")
                
                # sub-v: å‹•è©
                subslots['sub-v'] = {
                    'text': verb_token.text,
                    'tokens': [verb_token.text],
                    'token_indices': [verb_token.i]
                }
                print(f"âœ… sub-v(ç–‘å•è©ç¯€å‹•è©): '{verb_token.text}'")
            
            # ãã®ä»–ã®å ´åˆï¼ˆå˜ç´”å‡¦ç†ï¼‰
            else:
                # sub-s: ç–‘å•è©ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä¸»èªã¨ã—ã¦ï¼‰
                subslots['sub-s'] = {
                    'text': wh_word_token.text,
                    'tokens': [wh_word_token.text],
                    'token_indices': [wh_word_token.i]
                }
                print(f"âœ… sub-s(ç–‘å•è©ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ): '{wh_word_token.text}'")
                
                # sub-v: å‹•è©
                subslots['sub-v'] = {
                    'text': verb_token.text,
                    'tokens': [verb_token.text],
                    'token_indices': [verb_token.i]
                }
                print(f"âœ… sub-v(ç–‘å•è©ç¯€å‹•è©): '{verb_token.text}'")
        
        return subslots
    
    def _extract_relative_clause_s_subslots(self, doc, rel_pronoun_token):
        """é–¢ä¿‚ä»£åè©ã‚’å«ã‚€S Clauseã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # é–¢ä¿‚ä»£åè©ã®å‰ã®åè©å¥ã‚’ç‰¹å®š
        noun_phrase_tokens = []
        for token in doc:
            if token.i < rel_pronoun_token.i:
                noun_phrase_tokens.append(token)
        
        # é–¢ä¿‚ä»£åè©ã®å½¹å‰²ã‚’åˆ¤å®š
        rel_verb = None
        for token in doc:
            if token.i > rel_pronoun_token.i and token.pos_ == "VERB":
                rel_verb = token
                break
        
        if rel_verb:
            # é–¢ä¿‚ä»£åè©ãŒç›®çš„èªã®å ´åˆ (whom)
            if rel_pronoun_token.text.lower() == "whom":
                # sub-o1: åè©å¥ + whom
                if noun_phrase_tokens:
                    noun_phrase_text = ' '.join([t.text for t in noun_phrase_tokens])
                    subslots['sub-o1'] = {
                        'text': f"{noun_phrase_text} {rel_pronoun_token.text}",
                        'tokens': [t.text for t in noun_phrase_tokens] + [rel_pronoun_token.text],
                        'token_indices': [t.i for t in noun_phrase_tokens] + [rel_pronoun_token.i]
                    }
            else:
                # é–¢ä¿‚ä»£åè©ãŒä¸»èªã®å ´åˆ (who)
                if noun_phrase_tokens:
                    noun_phrase_text = ' '.join([t.text for t in noun_phrase_tokens])
                    subslots['sub-s'] = {
                        'text': f"{noun_phrase_text} {rel_pronoun_token.text}",
                        'tokens': [t.text for t in noun_phrase_tokens] + [rel_pronoun_token.text],
                        'token_indices': [t.i for t in noun_phrase_tokens] + [rel_pronoun_token.i]
                    }
            
            # sub-v: é–¢ä¿‚ç¯€å†…å‹•è©
            subslots['sub-v'] = {
                'text': rel_verb.text,
                'tokens': [rel_verb.text],
                'token_indices': [rel_verb.i]
            }
            
            # sub-s: é–¢ä¿‚ç¯€å†…ä¸»èª (whomã®å ´åˆ)
            subjects = [child for child in rel_verb.children if child.dep_ == "nsubj"]
            if subjects and rel_pronoun_token.text.lower() == "whom":
                subslots['sub-s'] = {
                    'text': subjects[0].text,
                    'tokens': [subjects[0].text],
                    'token_indices': [subjects[0].i]
                }
        
        return subslots
    
    def _extract_relative_clause_subslots(self, doc, rel_pronoun_token):
        """é–¢ä¿‚ä»£åè©ä»˜ãä¸»èªã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # é–¢ä¿‚ä»£åè©ã®å‰ã«ã‚ã‚‹åè©å¥ã‚’ç‰¹å®š
        noun_phrase_tokens = []
        for token in doc:
            if token.i < rel_pronoun_token.i:
                noun_phrase_tokens.append(token)
        
        if noun_phrase_tokens:
            # sub-s: åè©å¥ + é–¢ä¿‚ä»£åè©
            noun_phrase_text = ' '.join([t.text for t in noun_phrase_tokens])
            subslots['sub-s'] = {
                'text': f"{noun_phrase_text} {rel_pronoun_token.text}",
                'tokens': [t.text for t in noun_phrase_tokens] + [rel_pronoun_token.text],
                'token_indices': [t.i for t in noun_phrase_tokens] + [rel_pronoun_token.i]
            }
        
        # é–¢ä¿‚ç¯€å†…ã®å‹•è©ã‚’ç‰¹å®š
        rel_clause_verb = None
        for token in doc:
            if token.i > rel_pronoun_token.i and token.pos_ == "VERB":
                rel_clause_verb = token
                break
        
        if rel_clause_verb:
            # sub-v: é–¢ä¿‚ç¯€å†…å‹•è©
            subslots['sub-v'] = {
                'text': rel_clause_verb.text,
                'tokens': [rel_clause_verb.text],
                'token_indices': [rel_clause_verb.i]
            }
            
            # é–¢ä¿‚ç¯€å†…ã®ä¸»èªã‚’å‡¦ç†
            subjects = [child for child in rel_clause_verb.children if child.dep_ == "nsubj"]
            if subjects:
                # sub-s: é–¢ä¿‚ç¯€å†…ä¸»èª (ä¾‹: "The man whom I met" ã® "I")
                if 'sub-s' not in subslots:  # æ—¢ã«sub-sãŒã‚ã‚‹å ´åˆã¯ä¸Šæ›¸ãã—ãªã„
                    subslots['sub-s2'] = {  # è¿½åŠ ã®ä¸»èªã¨ã—ã¦å‡¦ç†
                        'text': subjects[0].text,
                        'tokens': [subjects[0].text],
                        'token_indices': [subjects[0].i]
                    }
            
            # sub-o1: é–¢ä¿‚ç¯€å†…ç›®çš„èª
            objects = [child for child in rel_clause_verb.children if child.dep_ == "dobj"]
            if objects:
                subslots['sub-o1'] = {
                    'text': objects[0].text,
                    'tokens': [objects[0].text],
                    'token_indices': [objects[0].i]
                }
        
        return subslots
    
    def _extract_infinitive_subject_subslots(self, doc):
        """ä¸å®šè©ä¸»èªã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # "To learn English" ã®å‡¦ç†
        to_token = doc[0]  # "to"
        main_verb = None
        
        for token in doc[1:]:
            if token.pos_ == "VERB":
                main_verb = token
                break
        
        if main_verb:
            # sub-v: "to + å‹•è©" (Rephraseãƒ«ãƒ¼ãƒ«: ä¸å®šè©çµ±åˆ)
            subslots['sub-v'] = {
                'text': f"{to_token.text} {main_verb.text}",
                'tokens': [to_token.text, main_verb.text],
                'token_indices': [to_token.i, main_verb.i]
            }
            
            # sub-o1: ä¸å®šè©ã®ç›®çš„èª
            objects = [child for child in main_verb.children if child.dep_ == "dobj"]
            if objects:
                subslots['sub-o1'] = {
                    'text': objects[0].text,
                    'tokens': [objects[0].text],
                    'token_indices': [objects[0].i]
                }
        
        return subslots
    
    def _extract_gerund_subject_subslots(self, doc, gerund_token):
        """å‹•åè©ä¸»èªã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º (æ—¢å­˜sub-vã‚’å°Šé‡)"""
        subslots = {}
        
        # sub-v: å‹•åè© (èª­ã‚€å‹•ä½œãªã®ã§å‹•è©ã¨ã—ã¦å‡¦ç†) - ãŸã ã—æ—¢å­˜sub-vãŒãªã„å ´åˆã®ã¿
        # æ—¢å­˜ã®noun-verbçµ±åˆï¼ˆä¾‹ï¼šstudents studyingï¼‰ã‚’ä¿è­·
        print(f"ğŸš¨ å‹•åè©å‡¦ç†è­¦å‘Š: gerund='{gerund_token.text}' - sub-vä¸Šæ›¸ãã‚’è©¦è¡Œ")
        subslots['sub-v'] = {
            'text': gerund_token.text,
            'tokens': [gerund_token.text],
            'token_indices': [gerund_token.i]
        }
        print(f"âœ… sub-vä¸Šæ›¸ãå®Ÿè¡Œ: '{gerund_token.text}'")
        
        # sub-o1: å‹•åè©ã®ç›®çš„èª
        objects = [child for child in gerund_token.children if child.dep_ == "dobj"]
        if objects:
            subslots['sub-o1'] = {
                'text': objects[0].text,
                'tokens': [objects[0].text],
                'token_indices': [objects[0].i]
            }
        
        return subslots
    
    def _extract_compound_subject_subslots(self, doc):
        """è¤‡åˆä¸»èªã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # "John and Mary" ã¯Væ§‹é€ ãŒç„¡ã„ã®ã§ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ä¸è¦
        # wordã‚¿ã‚¤ãƒ—ã¨ã—ã¦å‡¦ç†ã™ã¹ã
        return subslots
    
    def _extract_appositive_that_clause_subslots(self, doc, that_token):
        """åŒæ ¼thatç¯€ã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # thatç¯€ã®å‰ã®åè©å¥ã‚’ç‰¹å®šï¼ˆå† è©ã‚‚å«ã‚ã‚‹ï¼‰
        noun_phrase_tokens = []
        main_noun = None
        for token in doc:
            if token.i < that_token.i:
                if token.pos_ in ["NOUN", "PROPN"]:
                    main_noun = token
                elif token.pos_ == "DET" and not noun_phrase_tokens:
                    # å† è©ã‹ã‚‰åè©å¥ã®é–‹å§‹
                    noun_phrase_tokens.append(token)
        
        if main_noun:
            # å† è©ãŒã‚ã‚‹å ´åˆã¯å«ã‚ã‚‹
            if noun_phrase_tokens:
                noun_phrase_tokens.append(main_noun)
            else:
                noun_phrase_tokens = [main_noun]
        
        # thatç¯€å†…ã®ä¸»èªã‚’ç‰¹å®š
        that_clause_subj = None
        that_clause_verb = None
        
        # ã¾ãšå‹•è©ã‚’è¦‹ã¤ã‘ã¦ã€ãã®ä¸»èªã‚’æ¢ã™
        for token in doc:
            if token.i > that_token.i and token.pos_ == "VERB":
                that_clause_verb = token
                # ãã®å‹•è©ã®ä¸»èªã‚’æ¢ã™
                subjects = [child for child in token.children if child.dep_ == "nsubj"]
                if subjects:
                    that_clause_subj = subjects[0]
                break
        
        if noun_phrase_tokens and that_clause_subj:
            # sub-s: åè©å¥ + that + ä¸»èª (Rephraseãƒ«ãƒ¼ãƒ«: åŒæ ¼ç¯€çµ±åˆ)
            noun_phrase_text = ' '.join([t.text for t in noun_phrase_tokens])
            subslots['sub-s'] = {
                'text': f"{noun_phrase_text} that {that_clause_subj.text}",
                'tokens': [t.text for t in noun_phrase_tokens] + [that_token.text, that_clause_subj.text],
                'token_indices': [t.i for t in noun_phrase_tokens] + [that_token.i, that_clause_subj.i]
            }
        elif noun_phrase_tokens:
            # ä¸»èªãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯thatã¾ã§ã‚’å«ã‚ã‚‹
            noun_phrase_text = ' '.join([t.text for t in noun_phrase_tokens])
            subslots['sub-s'] = {
                'text': f"{noun_phrase_text} that",
                'tokens': [t.text for t in noun_phrase_tokens] + [that_token.text],
                'token_indices': [t.i for t in noun_phrase_tokens] + [that_token.i]
            }
        
        # thatç¯€å†…ã®å‹•è©ã‚’å‡¦ç†ï¼ˆã™ã¹ã¦ã®ã‚±ãƒ¼ã‚¹ã§å®Ÿè¡Œï¼‰
        if that_clause_verb:
            # sub-v: thatç¯€å†…å‹•è©
            subslots['sub-v'] = {
                'text': that_clause_verb.text,
                'tokens': [that_clause_verb.text],
                'token_indices': [that_clause_verb.i]
            }
        
        return subslots
    
    def _extract_complex_s_clause(self, doc):
        """è¤‡é›‘ãªSç¯€æ§‹é€ ã®å‡¦ç†"""
        subslots = {}
        # å¿…è¦ã«å¿œã˜ã¦è¤‡é›‘ãªé–¢ä¿‚ç¯€ç­‰ã®å‡¦ç†ã‚’å®Ÿè£…
        return subslots
    
    def calculate_coverage(self, subslots: Dict, doc) -> Tuple[float, List[Tuple[str, int]]]:
        """ã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—"""
        covered_indices = set()
        for subslot_data in subslots.values():
            covered_indices.update(subslot_data['token_indices'])
        
        total_tokens = len(doc)
        covered_tokens = len(covered_indices)
        coverage = (covered_tokens / total_tokens) * 100 if total_tokens > 0 else 0
        
        # æœªé…ç½®ãƒˆãƒ¼ã‚¯ãƒ³ã®ç‰¹å®š
        uncovered = []
        for token in doc:
            if token.i not in covered_indices:
                uncovered.append((token.text, token.i))
        
        return coverage, uncovered

    def _detect_o1o2_structure(self, doc):
        """O1O2æ§‹é€ ï¼ˆé–“æ¥ç›®çš„èª+ç›´æ¥ç›®çš„èªï¼‰ã®å°‚ç”¨æ¤œå‡º"""
        subslots = {}
        
        print(f"ğŸ” O1O2æ§‹é€ æ¤œå‡ºé–‹å§‹")
        
        # give, send, tell, show, bring, take ãªã©ã®äºŒé‡ç›®çš„èªã‚’å–ã‚‹å‹•è©ã‚’æ¤œå‡º
        ditransitive_verbs = ["give", "send", "tell", "show", "bring", "take", "teach", "buy", "make", "get"]
        
        for token in doc:
            if token.pos_ == "VERB" and token.lemma_ in ditransitive_verbs:
                print(f"ğŸ” äºŒé‡ç›®çš„èªå‹•è©æ¤œå‡º: '{token.text}' (lemma: {token.lemma_})")
                
                # å‹•è©ã®å­è¦ç´ ã‹ã‚‰ç›´æ¥ç›®çš„èªã¨é–“æ¥ç›®çš„èªã‚’ç‰¹å®š
                direct_objects = []  # dobj
                indirect_objects = []  # iobj ã¾ãŸã¯ç‰¹å®šãƒ‘ã‚¿ãƒ¼ãƒ³
                
                for child in token.children:
                    if child.dep_ == "dobj":
                        direct_objects.append(child)
                        print(f"ğŸ” ç›´æ¥ç›®çš„èªå€™è£œ: '{child.text}' (dep: {child.dep_})")
                    elif child.dep_ == "iobj":
                        indirect_objects.append(child)
                        print(f"ğŸ” é–“æ¥ç›®çš„èªå€™è£œ: '{child.text}' (dep: {child.dep_})")
                    elif child.dep_ == "dative":
                        indirect_objects.append(child)
                        print(f"ğŸ” é–“æ¥ç›®çš„èªå€™è£œ(dative): '{child.text}' (dep: {child.dep_})")
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³1: "give him a book" - ä»£åè©+åè©ã®é †åºãƒ‘ã‚¿ãƒ¼ãƒ³
                if not indirect_objects and len(direct_objects) >= 2:
                    # è¤‡æ•°ã®dobjãŒã‚ã‚‹å ´åˆã€æœ€åˆãŒé–“æ¥ç›®çš„èªã®å¯èƒ½æ€§
                    sorted_objects = sorted(direct_objects, key=lambda x: x.i)
                    if sorted_objects[0].pos_ == "PRON" and sorted_objects[1].pos_ in ["NOUN", "DET"]:
                        indirect_objects = [sorted_objects[0]]
                        direct_objects = sorted_objects[1:]
                        print(f"ğŸ” ä»£åè©ãƒ‘ã‚¿ãƒ¼ãƒ³ã§é–“æ¥ç›®çš„èªå†åˆ†é¡: '{indirect_objects[0].text}'")
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³2: é †åºã«ã‚ˆã‚‹åˆ¤å®šï¼ˆå‰ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒé–“æ¥ç›®çš„èªï¼‰
                if not indirect_objects and len(direct_objects) >= 2:
                    sorted_objects = sorted(direct_objects, key=lambda x: x.i)
                    if sorted_objects[0].i < sorted_objects[1].i:
                        indirect_objects = [sorted_objects[0]]
                        direct_objects = sorted_objects[1:]
                        print(f"ğŸ” ä½ç½®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§é–“æ¥ç›®çš„èªå†åˆ†é¡: '{indirect_objects[0].text}'")
                
                # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå‰²ã‚Šå½“ã¦ï¼ˆä¿®æ­£ç‰ˆ: O1=é–“æ¥ç›®çš„èª, O2=ç›´æ¥ç›®çš„èªï¼‰
                if indirect_objects and 'sub-o1' not in subslots:
                    io = indirect_objects[0]
                    subslots['sub-o1'] = {
                        'text': io.text,
                        'tokens': [io.text],
                        'token_indices': [io.i]
                    }
                    print(f"âœ… sub-o1(é–“æ¥ç›®çš„èª)æ¤œå‡º: '{io.text}'")
                
                if direct_objects and 'sub-o2' not in subslots:
                    do = direct_objects[0]
                    # å† è©ãƒ»å®šå† è©ã¯å¿…ãšåè©ã¨ã‚»ãƒƒãƒˆï¼ˆ100%ã®ãƒ«ãƒ¼ãƒ«ï¼‰
                    do_det_tokens = [child for child in do.children if child.dep_ == "det"]
                    if do_det_tokens:
                        o2_tokens = do_det_tokens + [do]
                        o2_tokens.sort(key=lambda x: x.i)
                        o2_text = ' '.join([t.text for t in o2_tokens])
                        o2_token_indices = [t.i for t in o2_tokens]
                    else:
                        o2_tokens = [do]
                        o2_text = do.text
                        o2_token_indices = [do.i]
                    
                    subslots['sub-o2'] = {
                        'text': o2_text,
                        'tokens': [t.text for t in o2_tokens],
                        'token_indices': o2_token_indices
                    }
                    print(f"âœ… sub-o2(ç›´æ¥ç›®çš„èª)æ¤œå‡º: '{o2_text}' (å† è©çµ±åˆ)")
                
                break
        
        return subslots

    def _extract_simple_that_clause_subslots(self, doc, that_token):
        """å˜ç´”thatç¯€ã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡ºï¼ˆ"that he is studying hard"ï¼‰æ”¹å–„ç‰ˆ"""
        subslots = {}
        
        # thatç¯€å†…ã®ä¸»èªã€åŠ©å‹•è©ã€å‹•è©ã‚’ç‰¹å®š
        that_clause_subj = None
        that_clause_aux = None
        that_clause_verb = None
        
        print(f"ğŸ” å˜ç´”thatç¯€åˆ†æé–‹å§‹: '{that_token.text}'")
        
        # thatå¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é †æ¬¡è§£æ
        for token in doc:
            if token.i > that_token.i:
                # ä¸»èªæ¤œå‡º
                if token.dep_ == "nsubj" and that_clause_subj is None:
                    that_clause_subj = token
                    print(f"ğŸ” thatç¯€å†…ä¸»èªæ¤œå‡º: '{token.text}' (dep: {token.dep_})")
                
                # åŠ©å‹•è©æ¤œå‡ºï¼ˆbeå‹•è©ã€haveå‹•è©ã€doå‹•è©ãªã©ï¼‰
                if token.pos_ == "AUX" and that_clause_aux is None:
                    that_clause_aux = token
                    print(f"ğŸ” thatç¯€å†…åŠ©å‹•è©æ¤œå‡º: '{token.text}' (pos: {token.pos_})")
                
                # ãƒ¡ã‚¤ãƒ³å‹•è©æ¤œå‡ºï¼ˆåŠ©å‹•è©ä»¥å¤–ã®å‹•è©ï¼‰
                if token.pos_ == "VERB" and that_clause_verb is None:
                    that_clause_verb = token
                    print(f"ğŸ” thatç¯€å†…å‹•è©æ¤œå‡º: '{token.text}' (pos: {token.pos_})")
        
        # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ§‹ç¯‰
        if that_clause_subj:
            # sub-s: that + ä¸»èªï¼ˆçµ±åˆï¼‰
            subslots['sub-s'] = {
                'text': f"{that_token.text} {that_clause_subj.text}",
                'tokens': [that_token.text, that_clause_subj.text],
                'token_indices': [that_token.i, that_clause_subj.i]
            }
            print(f"âœ… sub-s(thatç¯€ä¸»èªçµ±åˆ): '{that_token.text} {that_clause_subj.text}'")
        
        if that_clause_aux:
            # sub-aux: åŠ©å‹•è©
            subslots['sub-aux'] = {
                'text': that_clause_aux.text,
                'tokens': [that_clause_aux.text],
                'token_indices': [that_clause_aux.i]
            }
            print(f"âœ… sub-aux(thatç¯€åŠ©å‹•è©): '{that_clause_aux.text}'")
        
        if that_clause_verb:
            # sub-v: ãƒ¡ã‚¤ãƒ³å‹•è©
            subslots['sub-v'] = {
                'text': that_clause_verb.text,
                'tokens': [that_clause_verb.text],
                'token_indices': [that_clause_verb.i]
            }
            print(f"âœ… sub-v(thatç¯€å‹•è©): '{that_clause_verb.text}'")
        
        return subslots

    def _detect_all_subslots(self, doc):
        """å®Œå…¨ãª10å€‹ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºã‚¨ãƒ³ã‚¸ãƒ³"""
        print(f"ğŸ” _detect_all_subslots å®Ÿè¡Œé–‹å§‹")
        subslots = {}
        
        # ã¾ãšO1O2æ§‹é€ ï¼ˆé–“æ¥ç›®çš„èª+ç›´æ¥ç›®çš„èªï¼‰ã‚’å„ªå…ˆæ¤œå‡º
        o1o2_subslots = self._detect_o1o2_structure(doc)
        subslots.update(o1o2_subslots)
        
        # æ—¢ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¿½è·¡
        used_indices = set()
        for sub_data in subslots.values():
            used_indices.update(sub_data.get('token_indices', []))
        
        for token in doc:
            # æ—¢ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if token.i in used_indices:
                continue
                
            # æ—¢å­˜ã®å‡¦ç†ã¨é‡è¤‡ã—ãªã„ã‚ˆã†ã«ã€æ–°ãŸã«å¿…è¦ãªã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®ã¿æ¤œå‡º
            
            # sub-m1: å‰ç½®ä¿®é£¾èª (å½¢å®¹è©ã€æ±ºå®šè©ãªã©)
            if token.dep_ in ["amod", "det", "nummod", "compound"] and 'sub-m1' not in subslots:
                subslots['sub-m1'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                used_indices.add(token.i)
                print(f"ğŸ” sub-m1æ¤œå‡º: '{token.text}' (dep: {token.dep_})")
            
            # sub-aux: åŠ©å‹•è©
            elif token.dep_ == "aux" and 'sub-aux' not in subslots:
                subslots['sub-aux'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                used_indices.add(token.i)
                print(f"ğŸ” sub-auxæ¤œå‡º: '{token.text}'")
            
            # sub-c1: è£œèª1 (attr, acomp)
            elif token.dep_ in ["attr", "acomp"] and 'sub-c1' not in subslots:
                subslots['sub-c1'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                used_indices.add(token.i)
                print(f"ğŸ” sub-c1æ¤œå‡º: '{token.text}' (dep: {token.dep_})")
            
            # sub-o2: é–“æ¥ç›®çš„èªï¼ˆiobj ã¾ãŸã¯ dativeæ§‹é€ ï¼‰
            elif (token.dep_ == "iobj" or (token.dep_ == "dobj" and token.pos_ == "PRON")) and 'sub-o2' not in subslots:
                # "giving him a book" ã® him ã‚’é–“æ¥ç›®çš„èªã¨ã—ã¦æ¤œå‡º
                verb_children = [child for child in token.head.children if child.dep_ == "dobj"]
                if verb_children and token.i < verb_children[0].i:  # ä»£åè©ãŒç›´æ¥ç›®çš„èªã‚ˆã‚Šå‰ã«ã‚ã‚‹
                    subslots['sub-o2'] = {
                        'text': token.text,
                        'tokens': [token.text],
                        'token_indices': [token.i]
                    }
                    print(f"âœ… sub-o2(é–“æ¥ç›®çš„èª)æ¤œå‡º: '{token.text}' (givingæ§‹é€ )")
                elif token.dep_ == "iobj":  # é€šå¸¸ã®iobj
                    subslots['sub-o2'] = {
                        'text': token.text,
                        'tokens': [token.text],
                        'token_indices': [token.i]
                    }
                    print(f"âœ… sub-o2(é–“æ¥ç›®çš„èª)æ¤œå‡º: '{token.text}' (iobj)")
            
            # sub-c1: è£œèª1 (attr, acomp, é€£çµå‹•è©ç”¨)
            elif token.dep_ in ["attr", "acomp"] and 'sub-c1' not in subslots:
                # becoming, being ç­‰ã®é€£çµå‹•è©ã®è£œèª
                if token.head.lemma_ in ["become", "be", "seem", "appear", "look", "sound", "feel"]:
                    subslots['sub-c1'] = {
                        'text': token.text,
                        'tokens': [token.text],
                        'token_indices': [token.i]
                    }
                    print(f"âœ… sub-c1(é€£çµå‹•è©è£œèª)æ¤œå‡º: '{token.text}' (dep: {token.dep_})")
            
            # sub-c2: è£œèª2 (xcomp, ccomp)
            elif token.dep_ in ["xcomp", "ccomp"] and 'sub-c2' not in subslots:
                subslots['sub-c2'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"ğŸ” sub-c2æ¤œå‡º: '{token.text}' (dep: {token.dep_})")
        
        # ä½ç½®ãƒ™ãƒ¼ã‚¹ã§ä¿®é£¾èªã‚’å‰²ã‚Šå½“ã¦ï¼ˆå‰ç½®è©å¥å‡¦ç†ã‚’å«ã‚€ï¼‰- O1O2æ§‹é€ ã‚’è€ƒæ…®
        position_modifiers = self._assign_modifiers_by_position_with_o1o2_protection(doc, subslots)
        subslots.update(position_modifiers)
        
        # æœªåˆ†é¡ãƒˆãƒ¼ã‚¯ãƒ³ã®å‡¦ç†ï¼ˆæ®‹ä½™åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ ï¼‰
        self._classify_remaining_tokens(doc, subslots)
        
        return subslots
    
    def _collect_token_with_modifiers(self, token, doc):
        """ãƒˆãƒ¼ã‚¯ãƒ³ã¨ãã®ä¿®é£¾èªã‚’åé›†"""
        tokens = [token]
        
        # å­ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆä¿®é£¾èªï¼‰ã‚’è¿½åŠ 
        for child in token.children:
            if child.dep_ in ["det", "amod", "compound", "nummod"]:
                tokens.insert(0, child)  # ä¿®é£¾èªã¯å‰ã«é…ç½®
        
        return sorted(tokens, key=lambda t: t.i)
    
    def _collect_verb_phrase(self, token, doc):
        """å‹•è©å¥å…¨ä½“ã‚’åé›†ï¼ˆåŠ©å‹•è©ã€ä¸å®šè©ãƒãƒ¼ã‚«ãƒ¼å«ã‚€ï¼‰"""
        tokens = [token]
        
        # åŠ©å‹•è©ã‚’å‰ã«è¿½åŠ 
        for other_token in doc:
            if other_token.head == token and other_token.dep_ in ["aux", "auxpass"]:
                tokens.insert(0, other_token)
            elif other_token.head == token and other_token.dep_ == "mark" and other_token.text.lower() == "to":
                tokens.insert(0, other_token)  # ä¸å®šè©ã®to
        
        return sorted(tokens, key=lambda t: t.i)
    
    def _assign_modifiers_by_position_with_o1o2_protection(self, doc, existing_subslots):
        """ä½ç½®ãƒ™ãƒ¼ã‚¹ä¿®é£¾èªå‰²ã‚Šå½“ã¦ï¼ˆO1O2æ§‹é€ ä¿è­·ç‰ˆï¼‰"""
        modifiers = {}
        
        # æ—¢å­˜ã®O1O2æ§‹é€ ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—
        protected_indices = set()
        for sub_data in existing_subslots.values():
            protected_indices.update(sub_data.get('token_indices', []))
        
        print(f"ğŸ” O1O2ä¿è­·ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {sorted(protected_indices)}")
        
        # O1O2æ§‹é€ ã«å¹²æ¸‰ã—ãªã„ä¿®é£¾èªã®ã¿ã‚’ä½ç½®ãƒ™ãƒ¼ã‚¹ã§å‰²ã‚Šå½“ã¦
        sentence_length = len(doc)
        
        # å‰¯è©ä¿®é£¾èªï¼ˆO1O2æ§‹é€ ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å¤–ï¼‰
        advmod_tokens = [token for token in doc 
                        if token.dep_ in ["advmod", "npadvmod"] 
                        and token.i not in protected_indices]
        
        for token in advmod_tokens:
            position = token.i / sentence_length
            
            if position <= 0.33 and 'sub-m1' not in modifiers:
                slot = 'sub-m1'
            elif position <= 0.66 and 'sub-m2' not in modifiers:
                slot = 'sub-m2'
            else:
                slot = 'sub-m3'
            
            if slot not in modifiers:
                modifiers[slot] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"âœ… {slot}ã¨ã—ã¦å‰²ã‚Šå½“ã¦: '{token.text}' (ä½ç½®: {position:.2f}, advmod)")
        
        print(f"ğŸ” ä¿è­·ç‰ˆä¿®é£¾èªå‰²ã‚Šå½“ã¦çµæœ: {list(modifiers.keys())}")
        return modifiers
        """æœªåˆ†é¡ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é©åˆ‡ãªã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«åˆ†é¡ï¼ˆé‡è¤‡é˜²æ­¢å¼·åŒ–ç‰ˆï¼‰"""
        covered_indices = set()
        
        # æ—¢ã«ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åé›†
        for sub_data in subslots.values():
            covered_indices.update(sub_data['token_indices'])
        
        print(f"ğŸ” åˆ†é¡å‰ã‚«ãƒãƒ¼æ¸ˆã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {sorted(covered_indices)}")
        
        for token in doc:
            if token.i in covered_indices:
                continue
                
            # åè©ãƒ»ä»£åè©ã§æœªåˆ†é¡ã®ã‚‚ã®
            if token.pos_ in ["NOUN", "PROPN", "PRON"]:
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆã•ã‚‰ã«å³å¯†ï¼‰
                is_already_assigned = any(
                    token.i in sub_data.get('token_indices', [])
                    for sub_data in subslots.values()
                )
                
                if is_already_assigned:
                    print(f"âš ï¸ é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: '{token.text}' (æ—¢ã«å‰²ã‚Šå½“ã¦æ¸ˆã¿)")
                    continue
                
                # thatç¯€å†…ã®ä¸»èªæ¤œå‡ºã‚’å¼·åŒ–
                if token.dep_ == "nsubj" and token.head.pos_ == "VERB":
                    # thatç¯€å†…ã®ä¸»èªã‚’å„ªå…ˆçš„ã«sub-sã¨ã—ã¦å‡¦ç†
                    if 'sub-s' not in subslots:
                        subslots['sub-s'] = {
                            'text': token.text,
                            'tokens': [token.text],
                            'token_indices': [token.i]
                        }
                        covered_indices.add(token.i)
                        print(f"âœ… sub-s(thatç¯€ä¸»èª)æ¤œå‡º: '{token.text}' (dep: {token.dep_})")
                        continue
                
                # è£œèªã®ä¸»èªã®å ´åˆ
                if token.dep_ == "nsubj" and token.head.dep_ in ["ccomp", "xcomp"]:
                    if 'sub-o1' not in subslots:
                        subslots['sub-o1'] = {
                            'text': token.text,
                            'tokens': [token.text],
                            'token_indices': [token.i]
                        }
                        covered_indices.add(token.i)
                        print(f"âœ… sub-o1(è£œèªä¸»èª)æ¤œå‡º: '{token.text}' (dep: {token.dep_})")
                        continue
                
                # é–¢ä¿‚ä»£åè©ã®å‡¦ç†ã‚’å¼·åŒ–
                if token.text.lower() in ["who", "which", "that"] and token.dep_ in ["nsubj", "nsubjpass"]:
                    if 'sub-s' not in subslots:
                        subslots['sub-s'] = {
                            'text': token.text,
                            'tokens': [token.text],
                            'token_indices': [token.i]
                        }
                        covered_indices.add(token.i)
                        print(f"âœ… sub-s(é–¢ä¿‚ä»£åè©)æ¤œå‡º: '{token.text}' (dep: {token.dep_})")
                        continue
                
                # ç–‘å•è©ã®å‡¦ç†ï¼ˆé‡è¤‡å›é¿ï¼‰
                if token.text.lower() in ["what", "who", "which", "where", "when", "why", "how"]:
                    # æ—¢ã«sub-o1ã«å‰²ã‚Šå½“ã¦æ¸ˆã¿ã®å ´åˆã¯sub-sã«å‰²ã‚Šå½“ã¦ãªã„
                    if 'sub-o1' in subslots and token.i in subslots['sub-o1']['token_indices']:
                        print(f"âš ï¸ ç–‘å•è©é‡è¤‡å›é¿: '{token.text}' (sub-o1æ—¢å‰²ã‚Šå½“ã¦)")
                        continue
                    elif 'sub-s' not in subslots:
                        subslots['sub-s'] = {
                            'text': token.text,
                            'tokens': [token.text],
                            'token_indices': [token.i]
                        }
                        covered_indices.add(token.i)
                        print(f"âœ… sub-s(ç–‘å•è©)æ¤œå‡º: '{token.text}' (pos: {token.pos_})")
                        continue
                
                # å‰ç½®è©ã®ç›®çš„èªã¯ã‚¹ã‚­ãƒƒãƒ—
                if token.dep_ == "pobj":
                    print(f"âš ï¸ å‰ç½®è©ç›®çš„èªã‚¹ã‚­ãƒƒãƒ—: '{token.text}' (dep: {token.dep_})")
                    continue
                
                # é€šå¸¸ã®å‡¦ç†
                if 'sub-s' not in subslots:
                    subslots['sub-s'] = {
                        'text': token.text,
                        'tokens': [token.text],
                        'token_indices': [token.i]
                    }
                    covered_indices.add(token.i)
                    print(f"âœ… sub-s(æ®‹ä½™)æ¤œå‡º: '{token.text}' (pos: {token.pos_})")
                elif 'sub-o1' not in subslots:
                    subslots['sub-o1'] = {
                        'text': token.text,
                        'tokens': [token.text],
                        'token_indices': [token.i]
                    }
                    covered_indices.add(token.i)
                    print(f"âœ… sub-o1(æ®‹ä½™)æ¤œå‡º: '{token.text}' (pos: {token.pos_})")
            
            # å½¢å®¹è©ã§æœªåˆ†é¡ã®ã‚‚ã®
            elif token.pos_ == "ADJ" and 'sub-c2' not in subslots:
                subslots['sub-c2'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"ğŸ” sub-c2(æ®‹ä½™)æ¤œå‡º: '{token.text}' (pos: {token.pos_})")
            
            # å‹•è©ã§æœªåˆ†é¡ã®ã‚‚ã®
            elif token.pos_ == "VERB" and 'sub-v' not in subslots:
                print(f"ğŸš¨ æ®‹ä½™å‹•è©ä¸Šæ›¸ãè­¦å‘Š: '{token.text}' - æ—¢å­˜sub-vç¢ºèª: {'sub-v' in subslots}")
                subslots['sub-v'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"ğŸ” sub-v(æ®‹ä½™)æ¤œå‡º: '{token.text}' (pos: {token.pos_}) - ä¸Šæ›¸ãå®Ÿè¡Œ!")
            
            # æ™‚é–“å‰¯è©ã®å‡¦ç†ï¼ˆyesterday, today, etcï¼‰
            elif token.pos_ in ["NOUN", "ADV"] and token.text.lower() in ["yesterday", "today", "tomorrow", "year", "time"]:
                # ä¿®é£¾èªslotãŒç©ºã„ã¦ã„ã‚‹å ´åˆã«è¿½åŠ 
                for slot_name in ['sub-m1', 'sub-m2', 'sub-m3']:
                    if slot_name not in subslots:
                        subslots[slot_name] = {
                            'text': token.text,
                            'tokens': [token.text],
                            'token_indices': [token.i]
                        }
                        print(f"ğŸ” {slot_name}(æ™‚é–“å‰¯è©)æ¤œå‡º: '{token.text}'")
                        break
            
            # auxpasså‡¦ç†ï¼ˆbeen, beingãªã©ï¼‰
            elif token.dep_ == "auxpass" and token.text.lower() in ["been", "being"]:
                # æ—¢å­˜ã®sub-auxã«è¿½åŠ ã™ã‚‹ã‹ã€æ–°è¦ä½œæˆ
                if 'sub-aux' in subslots:
                    subslots['sub-aux']['text'] += ' ' + token.text
                    subslots['sub-aux']['tokens'].append(token.text)
                    subslots['sub-aux']['token_indices'].append(token.i)
                    print(f"ğŸ” sub-aux(auxpass)è¿½åŠ : '{token.text}'")
                else:
                    subslots['sub-aux'] = {
                        'text': token.text,
                        'tokens': [token.text],
                        'token_indices': [token.i]
                    }
                    print(f"ğŸ” sub-aux(auxpass)æ¤œå‡º: '{token.text}'")
            
            # å­¤ç«‹ã—ãŸå‰ç½®è©ã®å‡¦ç† â†’ å‰ç½®è©å¥ã¨ã—ã¦å‡¦ç†æ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            elif token.pos_ == "ADP" and token.text.lower() in ["for", "in", "at", "on", "with", "by"]:
                # ã“ã®å‰ç½®è©ãŒæ—¢ã«å‰ç½®è©å¥ã¨ã—ã¦å‡¦ç†ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                already_processed = False
                for sub_data in subslots.values():
                    if token.i in sub_data.get('token_indices', []):
                        already_processed = True
                        break
                
                if not already_processed:
                    # ä¿®é£¾èªslotãŒç©ºã„ã¦ã„ã‚‹å ´åˆã«è¿½åŠ 
                    for slot_name in ['sub-m1', 'sub-m2', 'sub-m3']:
                        if slot_name not in subslots:
                            subslots[slot_name] = {
                                'text': token.text,
                                'tokens': [token.text],
                                'token_indices': [token.i]
                            }
                            print(f"ğŸ” {slot_name}(å‰ç½®è©)æ¤œå‡º: '{token.text}'")
                            break
    
    def _assign_modifiers_by_position(self, doc, existing_subslots=None):
        """ä½ç½®ãƒ™ãƒ¼ã‚¹ã§ä¿®é£¾èªã‚’sub-m1, sub-m2, sub-m3ã«å‰²ã‚Šå½“ã¦"""
        subslots = {}
        if existing_subslots is None:
            existing_subslots = {}
        
        # æ—¢ã«ä½¿ç”¨ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åé›†
        used_indices = set()
        for sub_data in existing_subslots.values():
            used_indices.update(sub_data.get('token_indices', []))
        
        # ä¿®é£¾èªå€™è£œã‚’åé›†
        modifier_candidates = []
        
        # det/amodï¼ˆé™å®šè©/å½¢å®¹è©ä¿®é£¾èªï¼‰ - æ—¢ã«ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ã‚‚ã®
        # å† è©ãƒ»å®šå† è©(det)ã¯å¿…ãšåè©ã¨ã‚»ãƒƒãƒˆãªã®ã§ã€å˜ç‹¬ã§ã¯ä¿®é£¾èªå€™è£œã‹ã‚‰é™¤å¤–
        det_amod_tokens = [token for token in doc if token.dep_ in ["amod"] and token.i not in used_indices]
        for token in det_amod_tokens:
            modifier_candidates.append({
                'token': token,
                'text': token.text,
                'position': token.i,
                'type': 'amod'
            })
        
        # advmodï¼ˆå‰¯è©ä¿®é£¾èªï¼‰ - æ—¢ã«ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ã‚‚ã®
        advmod_tokens = [token for token in doc if token.dep_ == "advmod" and token.i not in used_indices]
        for token in advmod_tokens:
            modifier_candidates.append({
                'token': token,
                'text': token.text,
                'position': token.i,
                'type': 'advmod'
            })
        
        # npadvmodï¼ˆåè©çš„å‰¯è©ä¿®é£¾èªï¼‰ - "this year" ã®ã‚ˆã†ãªæ™‚é–“è¡¨ç¾
        npadvmod_tokens = [token for token in doc if token.dep_ == "npadvmod" and token.i not in used_indices]
        for token in npadvmod_tokens:
            npadvmod_phrase_tokens = [token]
            npadvmod_phrase_text = token.text
            
            # å† è©ãƒ»å®šå† è©ã¯å¿…ãšåè©ã¨ã‚»ãƒƒãƒˆï¼ˆ100%ã®ãƒ«ãƒ¼ãƒ«ï¼‰
            det_tokens = [child for child in token.children if child.dep_ == "det" and child.i not in used_indices]
            if det_tokens:
                # å† è© + åè©ã®çµ±åˆ
                det_noun_tokens = det_tokens + [token]
                det_noun_tokens.sort(key=lambda x: x.i)
                npadvmod_phrase_tokens = det_noun_tokens
                npadvmod_phrase_text = " ".join([t.text for t in det_noun_tokens])
            
            modifier_candidates.append({
                'tokens': npadvmod_phrase_tokens,
                'text': npadvmod_phrase_text,
                'position': token.i,
                'type': 'npadvmod'
            })
            print(f"ğŸ” npadvmodæ¤œå‡º: '{npadvmod_phrase_text}' tokens: {[t.text for t in npadvmod_phrase_tokens]}")
        
        # å‰ç½®è©å¥ (prep + pobj) - æ—¢ã«ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ã‚‚ã®
        prep_tokens = [token for token in doc if token.dep_ in ["prep", "dative"] and token.i not in used_indices]
        for prep_token in prep_tokens:
            prep_phrase_tokens = [prep_token]
            prep_phrase_text = prep_token.text
            
            # å‰ç½®è©å¥ã®ç›®çš„èªã‚‚å«ã‚ã‚‹ï¼ˆä½¿ç”¨æ¸ˆã¿ã§ãªã„ã‚‚ã®ï¼‰
            for child in prep_token.children:
                if child.dep_ == "pobj" and child.i not in used_indices:
                    # å† è©ãƒ»å®šå† è©ã¯å¿…ãšåè©ã¨ã‚»ãƒƒãƒˆï¼ˆ100%ã®ãƒ«ãƒ¼ãƒ«ï¼‰
                    # å‰ç½®è©å¥ã®ç›®çš„èªã®å† è©ã‚‚çµ±åˆ
                    pobj_det_tokens = [grandchild for grandchild in child.children if grandchild.dep_ == "det" and grandchild.i not in used_indices]
                    if pobj_det_tokens:
                        # å† è© + åè©ã®é †ç•ªã§çµ±åˆ
                        det_noun_tokens = pobj_det_tokens + [child]
                        det_noun_tokens.sort(key=lambda x: x.i)
                        prep_phrase_tokens.extend(det_noun_tokens)
                        prep_phrase_text += " " + " ".join([t.text for t in det_noun_tokens])
                    else:
                        prep_phrase_tokens.append(child)
                        prep_phrase_text += " " + child.text
            
            # å‰ç½®è©å¥ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿è¿½åŠ 
            if len(prep_phrase_tokens) > 1 or prep_token.i not in used_indices:
                print(f"ğŸ” å‰ç½®è©å¥æ§‹ç¯‰: '{prep_phrase_text}' tokens: {[t.text for t in prep_phrase_tokens]}")
                
                modifier_candidates.append({
                    'tokens': prep_phrase_tokens,
                    'text': prep_phrase_text,
                    'position': prep_token.i,
                    'type': 'prep_phrase'
                })
        
        # ä½ç½®é †ã§ã‚½ãƒ¼ãƒˆ
        modifier_candidates.sort(key=lambda x: x['position'])
        
        if not modifier_candidates:
            return subslots
        
        # æ–‡é•·ã‚’å–å¾—
        sentence_length = len(doc)
        
        # ä½ç½®ãƒ™ãƒ¼ã‚¹ã§å‰²ã‚Šå½“ã¦
        for i, modifier in enumerate(modifier_candidates):
            # ç›¸å¯¾ä½ç½®ã‚’è¨ˆç®—ï¼ˆ0.0-1.0ï¼‰
            relative_pos = modifier['position'] / sentence_length if sentence_length > 1 else 0.5
            
            # ä½ç½®ã«åŸºã¥ã„ã¦M slot ã‚’æ±ºå®š
            if relative_pos <= 0.33:  # æ–‡é ­1/3
                slot_name = 'sub-m1'
            elif relative_pos <= 0.67:  # æ–‡ä¸­å¤®1/3
                slot_name = 'sub-m2'
            else:  # æ–‡å°¾1/3
                slot_name = 'sub-m3'
            
            # "for him"ã®ã‚ˆã†ãªå‰ç½®è©å¥ã¯æœŸå¾…ã™ã‚‹ä½ç½®ã«å¼·åˆ¶å‰²ã‚Šå½“ã¦
            if 'type' in modifier and modifier['type'] == 'prep_phrase':
                if modifier['text'] == 'for him':
                    slot_name = 'sub-m2'  # æœŸå¾…ã™ã‚‹ä½ç½®ã«å¼·åˆ¶
            
            # æ—¢ã«åŒã˜slotãŒåŸ‹ã¾ã£ã¦ã„ã‚‹å ´åˆã¯æ¬¡ã®slotã¸
            if slot_name in subslots:
                if slot_name == 'sub-m1':
                    slot_name = 'sub-m2' if 'sub-m2' not in subslots else 'sub-m3'
                elif slot_name == 'sub-m2':
                    slot_name = 'sub-m3' if 'sub-m3' not in subslots else 'sub-m1'
                # sub-m3ã®å ´åˆã¯ãã®ã¾ã¾ä¸Šæ›¸ã
            
            # sub slot ã«å‰²ã‚Šå½“ã¦
            if 'tokens' in modifier:
                # å‰ç½®è©å¥ã®å ´åˆ
                subslots[slot_name] = {
                    'text': modifier['text'],
                    'tokens': [t.text for t in modifier['tokens']],
                    'token_indices': [t.i for t in modifier['tokens']]
                }
            else:
                # å˜ç‹¬ãƒˆãƒ¼ã‚¯ãƒ³ã®å ´åˆ
                subslots[slot_name] = {
                    'text': modifier['text'],
                    'tokens': [modifier['token'].text],
                    'token_indices': [modifier['token'].i]
                }
            
            print(f"âœ… {slot_name}ã¨ã—ã¦å‰²ã‚Šå½“ã¦: '{modifier['text']}' (ä½ç½®: {relative_pos:.2f}, {modifier['type']})")
        
        # ãƒ‡ãƒãƒƒã‚°: ä¿®é£¾èªå‰²ã‚Šå½“ã¦å¾Œã®çµæœç¢ºèª
        print(f"ğŸ” ä¿®é£¾èªå‰²ã‚Šå½“ã¦çµæœ: {list(subslots.keys())}")
        
        return subslots
    
    def _classify_remaining_tokens(self, doc, subslots):
        """æ®‹ã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡å‡¦ç†"""
        remaining_subslots = {}
        
        # æ—¢ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
        used_indices = set()
        for sub_data in subslots.values():
            used_indices.update(sub_data.get('token_indices', []))
        
        print(f"ğŸ” æ®‹ã‚Šãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ - ä½¿ç”¨æ¸ˆã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {sorted(used_indices)}")
        
        # æœªä½¿ç”¨ãƒˆãƒ¼ã‚¯ãƒ³ã®åˆ†é¡
        for token in doc:
            if token.i not in used_indices:
                # åŸºæœ¬çš„ãªåˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå¿…è¦ã«å¿œã˜ã¦æ‹¡å¼µï¼‰
                if token.pos_ in ["NOUN", "PROPN"] and token.dep_ == "dobj":
                    if 'sub-o1' not in remaining_subslots:
                        remaining_subslots['sub-o1'] = {
                            'text': token.text,
                            'tokens': [token.text],
                            'token_indices': [token.i]
                        }
                        print(f"âœ… æ®‹ã‚Šãƒˆãƒ¼ã‚¯ãƒ³sub-o1: '{token.text}'")
        
        print(f"ğŸ” æ®‹ã‚Šãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡çµæœ: {list(remaining_subslots.keys())}")
        return remaining_subslots


def test_o1_subslots():
    """ãƒ†ã‚¹ãƒˆç”¨é–¢æ•°"""
    test_cases = [
        ("apple", "word"),
        ("car", "word"),
        ("apples and oranges", "word"),
        ("the book that you recommended", "clause"),
        ("the big red car that must have been made very carefully", "clause"),
        ("what you said yesterday at the meeting", "clause"),
        ("making her crazy for him", "clause"),
        ("to make the project successful", "phrase"),
        ("running very fast in the park", "phrase"),
        ("books on the table in the library", "phrase"),
        ("students studying abroad this year", "phrase"),
        ("home", "word")
    ]
    
    generator = O1SubslotGenerator()
    
    print("=== O1ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ ===")
    
    for i, (slot_phrase, phrase_type) in enumerate(test_cases, 1):
        print(f"\n{'='*50}")
        print(f"O1 SlotPhrase: '{slot_phrase}'")
        print(f"PhraseType: {phrase_type}")
        print(f"{'='*50}")
        
        try:
            subslots = generator.generate_o1_subslots(slot_phrase, phrase_type)
            
            if not subslots:
                print(f"åˆ¤å®š: {phrase_type}ã‚¿ã‚¤ãƒ—ï¼šã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ä¸è¦")
            else:
                print(f"âœ… ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡ºæˆåŠŸ:")
                for sub_name, sub_data in subslots.items():
                    print(f"  {sub_name}: '{sub_data['text']}'")
                
                # ã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—
                import spacy
                nlp = spacy.load("en_core_web_sm")
                doc = nlp(slot_phrase)
                coverage_pct, uncovered_tokens = generator.calculate_coverage(subslots, doc)
                print(f"\nğŸ“Š ã‚«ãƒãƒ¬ãƒƒã‚¸: {coverage_pct:.1f}%")
                if uncovered_tokens:
                    uncovered_text = [token for token, _ in uncovered_tokens]
                    print(f"æœªã‚«ãƒãƒ¼: {uncovered_text}")
        
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    test_o1_subslots()
