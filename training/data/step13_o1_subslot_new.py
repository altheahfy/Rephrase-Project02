#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Rule Dictionary v2.0 - O1(ç›´æ¥ç›®çš„èª)ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
spaCyä¾å­˜æ§‹é€ è§£æã«ã‚ˆã‚‹å‹•çš„ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º

ç›´æ¥ç›®çš„èªã®è¤‡é›‘æ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³:
1. é–¢ä¿‚ä»£åè©ä»˜ãç›®çš„èª: "The book that you recommended" â†’ sub-o1: 'The book that', sub-v: 'recommended'
2. åŒæ ¼thatç¯€: "The fact that he left" â†’ sub-s: 'The fact that he', sub-v: 'left'  
3. ä¸å®šè©ç›®çš„èª: "To go home" â†’ sub-v: 'To go', sub-o1: 'home'
4. å‹•åè©ç›®çš„èª: "Reading books" â†’ sub-v: 'Reading', sub-o1: 'books'
5. è¤‡åˆç›®çš„èª: "apples and oranges" â†’ wordæ‰±ã„ (Væ§‹é€ ãªã—)
"""

import spacy
from typing import Dict, List, Tuple, Any

class O1SubslotGenerator:
    """O1(ç›´æ¥ç›®çš„èª)ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.nlp = spacy.load("en_core_web_sm")
    
    def generate_o1_subslots(self, slot_phrase: str, phrase_type: str) -> Dict[str, Dict[str, Any]]:
        """
        O1(ç›´æ¥ç›®çš„èª)ã‚¹ãƒ­ãƒƒãƒˆã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ
        
        Args:
            slot_phrase: ç›®çš„èªãƒ•ãƒ¬ãƒ¼ã‚º
            phrase_type: ãƒ•ãƒ¬ãƒ¼ã‚ºã‚¿ã‚¤ãƒ— (word/phrase/clause)
            
        Returns:
            Dict: ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆè¾æ›¸
        """
        doc = self.nlp(slot_phrase)
        
        if phrase_type == "word":
            # å˜èªã®å ´åˆã¯ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ä¸è¦
            return {}
        elif phrase_type == "phrase":
            return self._extract_o1_phrase_subslots(doc)
        elif phrase_type == "clause":
            return self._extract_o1_clause_subslots(doc)
        else:
            return {}
    
    def _extract_o1_phrase_subslots(self, doc):
        """O1 Phraseã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # é–¢ä¿‚ä»£åè©ã®æ¤œå‡ºï¼ˆã‚ˆã‚ŠæŸ”è»Ÿãªæ¡ä»¶ï¼‰
        rel_pronouns = ["who", "whom", "whose", "which", "that"]
        rel_pronoun_token = None
        
        for token in doc:
            if token.text.lower() in rel_pronouns:
                # é–¢ä¿‚ä»£åè©ã®æ¡ä»¶ã‚’ç·©å’Œï¼šnsubjä»¥å¤–ã‚‚æ¤œå‡º
                if token.dep_ in ["nsubj", "dobj", "pobj", "nsubjpass"] or token.pos_ == "PRON":
                    rel_pronoun_token = token
                    break
        
        if rel_pronoun_token:
            subslots.update(self._extract_relative_clause_subslots(doc, rel_pronoun_token))
        
        # ä¸å®šè©ä¸»èªã®å‡¦ç†: "To learn English is important"
        elif doc[0].text.lower() == "to" and doc[0].pos_ == "PART":
            subslots.update(self._extract_infinitive_subject_subslots(doc))
        
        # å‹•åè©ä¸»èªã®å‡¦ç†: "Reading books is fun"
        else:
            gerund_tokens = [token for token in doc if token.pos_ == "VERB" and token.tag_ == "VBG"]
            if gerund_tokens:
                subslots.update(self._extract_gerund_subject_subslots(doc, gerund_tokens[0]))
        
        # è¤‡åˆä¸»èªã®å‡¦ç†: "John and Mary are here"
        and_tokens = [token for token in doc if token.text.lower() == "and" and token.dep_ == "cc"]
        if and_tokens:
            subslots.update(self._extract_compound_subject_subslots(doc))
        
        # ä½ç½®ãƒ™ãƒ¼ã‚¹ä¿®é£¾èªå‰²ã‚Šå½“ã¦ï¼ˆsub-m1, sub-m2, sub-m3ï¼‰ - æ—¢ã«ä½¿ç”¨ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤ã
        modifier_subslots = self._assign_modifiers_by_position(doc, subslots)
        subslots.update(modifier_subslots)
        
        # ä¸å®šè©ã€Œto + å‹•è©ã€ã®çµ±åˆå‡¦ç†
        to_verb_tokens = []
        to_token = None
        main_verb_token = None
        
        for token in doc:
            if token.text.lower() == "to" and token.pos_ == "PART":
                to_token = token
            elif token.pos_ == "VERB" and token.dep_ in ["ROOT", "xcomp", "ccomp"]:
                main_verb_token = token
                break
        
        if to_token and main_verb_token:
            # sub-v: "to + å‹•è©" ã¨ã—ã¦çµ±åˆ
            subslots['sub-v'] = {
                'text': f"{to_token.text} {main_verb_token.text}",
                'tokens': [to_token.text, main_verb_token.text],
                'token_indices': [to_token.i, main_verb_token.i]
            }
            print(f"âœ… sub-vã¨ã—ã¦å‡¦ç†: '{to_token.text} {main_verb_token.text}' (ä¸å®šè©çµ±åˆ)")
        elif main_verb_token:
            # å‹•è©ã®ã¿
            subslots['sub-v'] = {
                'text': main_verb_token.text,
                'tokens': [main_verb_token.text],
                'token_indices': [main_verb_token.i]
            }
            print(f"âœ… sub-vã¨ã—ã¦å‡¦ç†: '{main_verb_token.text}'")
        
        # sub-aux: åŠ©å‹•è©æ¤œå‡º (aux, auxpass) - ãŸã ã—ä¸å®šè©toã¯é™¤å¤–
        aux_tokens = [token for token in doc if token.dep_ in ["aux", "auxpass"] and not (token.text.lower() == "to" and token.pos_ == "PART")]
        if aux_tokens:
            aux_text = ' '.join([token.text for token in aux_tokens])
            subslots['sub-aux'] = {
                'text': aux_text,
                'tokens': [token.text for token in aux_tokens],
                'token_indices': [token.i for token in aux_tokens]
            }
            print(f"âœ… sub-auxã¨ã—ã¦å‡¦ç†: '{aux_text}'")
        
        # sub-c2: è£œèª2æ¤œå‡º (xcomp, acomp, attr, ccomp)
        comp_tokens = [token for token in doc if token.dep_ in ["xcomp", "acomp", "attr", "ccomp"]]
        if comp_tokens:
            comp_text = ' '.join([token.text for token in comp_tokens])
            subslots['sub-c2'] = {
                'text': comp_text,
                'tokens': [token.text for token in comp_tokens],
                'token_indices': [token.i for token in comp_tokens]
            }
            print(f"âœ… sub-c2ã¨ã—ã¦å‡¦ç†: '{comp_text}'")
        
        # sub-o1æ¤œå‡º: å‹•è©ã®ç›´æ¥ç›®çš„èª (dobj) + é™å®šè©ã‚’å«ã‚€
        if main_verb_token and 'sub-o1' not in subslots:
            dobj_tokens = [child for child in main_verb_token.children if child.dep_ == "dobj"]
            if dobj_tokens:
                dobj_token = dobj_tokens[0]
                # å† è©ãƒ»å®šå† è©ã¯å¿…ãšåè©ã¨ã‚»ãƒƒãƒˆï¼ˆ100%ã®ãƒ«ãƒ¼ãƒ«ï¼‰
                det_tokens = [child for child in dobj_token.children if child.dep_ == "det"]
                if det_tokens:
                    o1_tokens = det_tokens + [dobj_token]
                    o1_tokens.sort(key=lambda x: x.i)  # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
                    o1_text = ' '.join([t.text for t in o1_tokens])
                    o1_token_indices = [t.i for t in o1_tokens]
                else:
                    o1_tokens = [dobj_token]
                    o1_text = dobj_token.text
                    o1_token_indices = [dobj_token.i]
                
                subslots['sub-o1'] = {
                    'text': o1_text,
                    'tokens': [t.text for t in o1_tokens],
                    'token_indices': o1_token_indices
                }
                print(f"âœ… sub-o1ã¨ã—ã¦å‡¦ç†: '{o1_text}' (å‹•è©ã®ç›®çš„èª+é™å®šè©)")
            else:
                # dobjãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ä»–ã®ç›®çš„èªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
                # nsubjã§ã‚‚å®Ÿè³ªçš„ã«ç›®çš„èªã®å½¹å‰²ã‚’ã™ã‚‹ã‚±ãƒ¼ã‚¹ï¼ˆè£œèªæ§‹é€ ï¼‰
                other_obj_tokens = [child for child in main_verb_token.children if child.dep_ in ["nsubj"]]
                if other_obj_tokens:
                    obj_token = other_obj_tokens[0]
                    # å† è©ãƒ»å®šå† è©ã¯å¿…ãšåè©ã¨ã‚»ãƒƒãƒˆï¼ˆ100%ã®ãƒ«ãƒ¼ãƒ«ï¼‰
                    obj_det_tokens = [child for child in obj_token.children if child.dep_ == "det"]
                    if obj_det_tokens:
                        o1_tokens = obj_det_tokens + [obj_token]
                        o1_tokens.sort(key=lambda x: x.i)  # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
                        o1_text = ' '.join([t.text for t in o1_tokens])
                        o1_token_indices = [t.i for t in o1_tokens]
                    else:
                        o1_tokens = [obj_token]
                        o1_text = obj_token.text
                        o1_token_indices = [obj_token.i]
                    
                    subslots['sub-o1'] = {
                        'text': o1_text,
                        'tokens': [t.text for t in o1_tokens],
                        'token_indices': o1_token_indices
                    }
                    print(f"âœ… sub-o1ã¨ã—ã¦å‡¦ç†: '{o1_text}' (å‹•è©ã®nsubjç›®çš„èª+é™å®šè©)")
        
        # sub-o1è¿½åŠ æ¤œå‡º: nsubjãŒè£œèªæ§‹é€ å†…ã«ã‚ã‚‹å ´åˆï¼ˆdobjãŒæ—¢ã«æ¤œå‡ºæ¸ˆã¿ã®å ´åˆã¯é™¤å¤–ï¼‰
        # å† è©ãƒ»å®šå† è©ã¯å¿…ãšåè©ã¨ã‚»ãƒƒãƒˆï¼ˆ100%ã®ãƒ«ãƒ¼ãƒ«ï¼‰ã‚’é©ç”¨
        nsubj_tokens = [token for token in doc if token.dep_ == "nsubj" and token.head.dep_ in ["ccomp", "xcomp"]]
        if nsubj_tokens and 'sub-o1' not in subslots:
            nsubj_token = nsubj_tokens[0]
            # å† è©ãƒ»å®šå† è©ã¯å¿…ãšåè©ã¨ã‚»ãƒƒãƒˆï¼ˆ100%ã®ãƒ«ãƒ¼ãƒ«ï¼‰
            nsubj_det_tokens = [child for child in nsubj_token.children if child.dep_ == "det"]
            if nsubj_det_tokens:
                o1_tokens = nsubj_det_tokens + [nsubj_token]
                o1_tokens.sort(key=lambda x: x.i)  # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
                nsubj_text = ' '.join([t.text for t in o1_tokens])
                o1_token_indices = [t.i for t in o1_tokens]
            else:
                o1_tokens = [nsubj_token]
                nsubj_text = nsubj_token.text
                o1_token_indices = [nsubj_token.i]
            
            subslots['sub-o1'] = {
                'text': nsubj_text,
                'tokens': [t.text for t in o1_tokens],
                'token_indices': o1_token_indices
            }
            print(f"âœ… sub-o1ã¨ã—ã¦å‡¦ç†: '{nsubj_text}' (è£œèªæ§‹é€ å†…ä¸»èª+å† è©)")
        

        
        # TODO: å®Œå…¨ãª10å€‹ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºã‚’å®Ÿè£…äºˆå®š
        # complete_subslots = self._detect_all_subslots(doc)
        # subslots.update(complete_subslots)
        
        return subslots
    
    def _extract_o1_clause_subslots(self, doc):
        """O1 Clauseã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # å®Œå…¨ãª10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨
        complete_subslots = self._detect_all_subslots(doc)
        subslots.update(complete_subslots)
        
        # sub-aux: åŠ©å‹•è©æ¤œå‡º
        aux_tokens = [token for token in doc if token.dep_ == "aux"]
        if aux_tokens:
            aux_text = ' '.join([token.text for token in aux_tokens])
            subslots['sub-aux'] = {
                'text': aux_text,
                'tokens': [token.text for token in aux_tokens],
                'token_indices': [token.i for token in aux_tokens]
            }
            print(f"âœ… sub-auxã¨ã—ã¦å‡¦ç†: '{aux_text}'")
        
        # ã¾ãšåŒæ ¼thatç¯€ã‚’å„ªå…ˆãƒã‚§ãƒƒã‚¯
        that_token = None
        for token in doc:
            if token.text.lower() == "that":
                # thatç¯€ã®æ¤œå‡ºæ¡ä»¶ã‚’åºƒã’ã‚‹
                if token.dep_ in ["acl", "ccomp", "mark", "dobj"] or (token.pos_ == "SCONJ"):
                    that_token = token
                    break
        
        if that_token:
            # åŒæ ¼thatç¯€ã‹ã©ã†ã‹ã‚’åˆ¤å®šï¼ˆåè©ã®å¾Œã«thatãŒã‚ã‚‹å ´åˆï¼‰
            has_noun_before = False
            for token in doc:
                if token.i < that_token.i and token.pos_ in ["NOUN", "PROPN"]:
                    has_noun_before = True
                    break
            
            if has_noun_before:
                return self._extract_appositive_that_clause_subslots(doc, that_token)
        
        # ç–‘å•è©ç¯€ã®æ¤œå‡ºï¼ˆwhat, where, when, how ãªã©ï¼‰
        wh_words = ["what", "where", "when", "how", "why", "which"]
        wh_word_token = None
        
        for token in doc:
            if token.text.lower() in wh_words:
                # ç–‘å•è©ã®æ¡ä»¶ï¼šdobj, advmod, nsubj ãªã©ã®é–¢ä¿‚
                if token.dep_ in ["dobj", "advmod", "nsubj", "pobj"] or token.pos_ in ["PRON", "SCONJ"]:
                    wh_word_token = token
                    break
        
        if wh_word_token:
            return self._extract_wh_clause_subslots(doc, wh_word_token)
        
        # é–¢ä¿‚ä»£åè©ã®æ¤œå‡ºï¼ˆclauseå†…ï¼‰
        rel_pronouns = ["who", "whom", "whose", "which", "that"]
        rel_pronoun_token = None
        
        for token in doc:
            if token.text.lower() in rel_pronouns:
                rel_pronoun_token = token
                break
        
        if rel_pronoun_token:
            return self._extract_relative_clause_s_subslots(doc, rel_pronoun_token)
        
        # ãã®ä»–ã®é–¢ä¿‚ç¯€å‡¦ç†
        complex_subslots = self._extract_complex_s_clause(doc)
        subslots.update(complex_subslots)
        
        # å®Œå…¨ãª10å€‹ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡º
        complete_subslots = self._detect_all_subslots(doc)
        subslots.update(complete_subslots)
        
        return subslots
    
    def _extract_wh_clause_subslots(self, doc, wh_word_token):
        """ç–‘å•è©ç¯€ã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡ºï¼ˆwhat you said ãªã©ï¼‰"""
        subslots = {}
        
        # ç–‘å•è©ç¯€ã®æ§‹é€ è§£æ: what(dobj) you(nsubj) said(ROOT)
        verb_token = None
        subject_tokens = []
        
        for token in doc:
            if token.pos_ == "VERB" and token.dep_ == "ROOT":
                verb_token = token
            elif token.dep_ in ["nsubj", "nsubjpass"]:
                subject_tokens.append(token)
        
        if verb_token and subject_tokens:
            # sub-o1: ç–‘å•è©
            subslots['sub-o1'] = {
                'text': wh_word_token.text,
                'tokens': [wh_word_token.text],
                'token_indices': [wh_word_token.i]
            }
            
            # sub-s: ä¸»èª
            subject_text = ' '.join([t.text for t in subject_tokens])
            subslots['sub-s'] = {
                'text': subject_text,
                'tokens': [t.text for t in subject_tokens],
                'token_indices': [t.i for t in subject_tokens]
            }
            
            # sub-v: å‹•è©
            subslots['sub-v'] = {
                'text': verb_token.text,
                'tokens': [verb_token.text],
                'token_indices': [verb_token.i]
            }
        
        return subslots
    
    def _extract_relative_clause_s_subslots(self, doc, rel_pronoun_token):
        """é–¢ä¿‚ä»£åè©ã‚’å«ã‚€S Clauseã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # é–¢ä¿‚ä»£åè©ã®å‰ã®åè©å¥ã‚’ç‰¹å®š
        noun_phrase_tokens = []
        for token in doc:
            if token.i < rel_pronoun_token.i:
                noun_phrase_tokens.append(token)
        
        # é–¢ä¿‚ä»£åè©ã®å½¹å‰²ã‚’åˆ¤å®š
        rel_verb = None
        for token in doc:
            if token.i > rel_pronoun_token.i and token.pos_ == "VERB":
                rel_verb = token
                break
        
        if rel_verb:
            # é–¢ä¿‚ä»£åè©ãŒç›®çš„èªã®å ´åˆ (whom)
            if rel_pronoun_token.text.lower() == "whom":
                # sub-o1: åè©å¥ + whom
                if noun_phrase_tokens:
                    noun_phrase_text = ' '.join([t.text for t in noun_phrase_tokens])
                    subslots['sub-o1'] = {
                        'text': f"{noun_phrase_text} {rel_pronoun_token.text}",
                        'tokens': [t.text for t in noun_phrase_tokens] + [rel_pronoun_token.text],
                        'token_indices': [t.i for t in noun_phrase_tokens] + [rel_pronoun_token.i]
                    }
            else:
                # é–¢ä¿‚ä»£åè©ãŒä¸»èªã®å ´åˆ (who)
                if noun_phrase_tokens:
                    noun_phrase_text = ' '.join([t.text for t in noun_phrase_tokens])
                    subslots['sub-s'] = {
                        'text': f"{noun_phrase_text} {rel_pronoun_token.text}",
                        'tokens': [t.text for t in noun_phrase_tokens] + [rel_pronoun_token.text],
                        'token_indices': [t.i for t in noun_phrase_tokens] + [rel_pronoun_token.i]
                    }
            
            # sub-v: é–¢ä¿‚ç¯€å†…å‹•è©
            subslots['sub-v'] = {
                'text': rel_verb.text,
                'tokens': [rel_verb.text],
                'token_indices': [rel_verb.i]
            }
            
            # sub-s: é–¢ä¿‚ç¯€å†…ä¸»èª (whomã®å ´åˆ)
            subjects = [child for child in rel_verb.children if child.dep_ == "nsubj"]
            if subjects and rel_pronoun_token.text.lower() == "whom":
                subslots['sub-s'] = {
                    'text': subjects[0].text,
                    'tokens': [subjects[0].text],
                    'token_indices': [subjects[0].i]
                }
        
        return subslots
    
    def _extract_relative_clause_subslots(self, doc, rel_pronoun_token):
        """é–¢ä¿‚ä»£åè©ä»˜ãä¸»èªã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # é–¢ä¿‚ä»£åè©ã®å‰ã«ã‚ã‚‹åè©å¥ã‚’ç‰¹å®š
        noun_phrase_tokens = []
        for token in doc:
            if token.i < rel_pronoun_token.i:
                noun_phrase_tokens.append(token)
        
        if noun_phrase_tokens:
            # sub-s: åè©å¥ + é–¢ä¿‚ä»£åè©
            noun_phrase_text = ' '.join([t.text for t in noun_phrase_tokens])
            subslots['sub-s'] = {
                'text': f"{noun_phrase_text} {rel_pronoun_token.text}",
                'tokens': [t.text for t in noun_phrase_tokens] + [rel_pronoun_token.text],
                'token_indices': [t.i for t in noun_phrase_tokens] + [rel_pronoun_token.i]
            }
        
        # é–¢ä¿‚ç¯€å†…ã®å‹•è©ã‚’ç‰¹å®š
        rel_clause_verb = None
        for token in doc:
            if token.i > rel_pronoun_token.i and token.pos_ == "VERB":
                rel_clause_verb = token
                break
        
        if rel_clause_verb:
            # sub-v: é–¢ä¿‚ç¯€å†…å‹•è©
            subslots['sub-v'] = {
                'text': rel_clause_verb.text,
                'tokens': [rel_clause_verb.text],
                'token_indices': [rel_clause_verb.i]
            }
            
            # é–¢ä¿‚ç¯€å†…ã®ä¸»èªã‚’å‡¦ç†
            subjects = [child for child in rel_clause_verb.children if child.dep_ == "nsubj"]
            if subjects:
                # sub-s: é–¢ä¿‚ç¯€å†…ä¸»èª (ä¾‹: "The man whom I met" ã® "I")
                if 'sub-s' not in subslots:  # æ—¢ã«sub-sãŒã‚ã‚‹å ´åˆã¯ä¸Šæ›¸ãã—ãªã„
                    subslots['sub-s2'] = {  # è¿½åŠ ã®ä¸»èªã¨ã—ã¦å‡¦ç†
                        'text': subjects[0].text,
                        'tokens': [subjects[0].text],
                        'token_indices': [subjects[0].i]
                    }
            
            # sub-o1: é–¢ä¿‚ç¯€å†…ç›®çš„èª
            objects = [child for child in rel_clause_verb.children if child.dep_ == "dobj"]
            if objects:
                subslots['sub-o1'] = {
                    'text': objects[0].text,
                    'tokens': [objects[0].text],
                    'token_indices': [objects[0].i]
                }
        
        return subslots
    
    def _extract_infinitive_subject_subslots(self, doc):
        """ä¸å®šè©ä¸»èªã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # "To learn English" ã®å‡¦ç†
        to_token = doc[0]  # "to"
        main_verb = None
        
        for token in doc[1:]:
            if token.pos_ == "VERB":
                main_verb = token
                break
        
        if main_verb:
            # sub-v: "to + å‹•è©" (Rephraseãƒ«ãƒ¼ãƒ«: ä¸å®šè©çµ±åˆ)
            subslots['sub-v'] = {
                'text': f"{to_token.text} {main_verb.text}",
                'tokens': [to_token.text, main_verb.text],
                'token_indices': [to_token.i, main_verb.i]
            }
            
            # sub-o1: ä¸å®šè©ã®ç›®çš„èª
            objects = [child for child in main_verb.children if child.dep_ == "dobj"]
            if objects:
                subslots['sub-o1'] = {
                    'text': objects[0].text,
                    'tokens': [objects[0].text],
                    'token_indices': [objects[0].i]
                }
        
        return subslots
    
    def _extract_gerund_subject_subslots(self, doc, gerund_token):
        """å‹•åè©ä¸»èªã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # sub-v: å‹•åè© (èª­ã‚€å‹•ä½œãªã®ã§å‹•è©ã¨ã—ã¦å‡¦ç†)
        subslots['sub-v'] = {
            'text': gerund_token.text,
            'tokens': [gerund_token.text],
            'token_indices': [gerund_token.i]
        }
        
        # sub-o1: å‹•åè©ã®ç›®çš„èª
        objects = [child for child in gerund_token.children if child.dep_ == "dobj"]
        if objects:
            subslots['sub-o1'] = {
                'text': objects[0].text,
                'tokens': [objects[0].text],
                'token_indices': [objects[0].i]
            }
        
        return subslots
    
    def _extract_compound_subject_subslots(self, doc):
        """è¤‡åˆä¸»èªã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # "John and Mary" ã¯Væ§‹é€ ãŒç„¡ã„ã®ã§ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ä¸è¦
        # wordã‚¿ã‚¤ãƒ—ã¨ã—ã¦å‡¦ç†ã™ã¹ã
        return subslots
    
    def _extract_appositive_that_clause_subslots(self, doc, that_token):
        """åŒæ ¼thatç¯€ã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # thatç¯€ã®å‰ã®åè©å¥ã‚’ç‰¹å®šï¼ˆå† è©ã‚‚å«ã‚ã‚‹ï¼‰
        noun_phrase_tokens = []
        main_noun = None
        for token in doc:
            if token.i < that_token.i:
                if token.pos_ in ["NOUN", "PROPN"]:
                    main_noun = token
                elif token.pos_ == "DET" and not noun_phrase_tokens:
                    # å† è©ã‹ã‚‰åè©å¥ã®é–‹å§‹
                    noun_phrase_tokens.append(token)
        
        if main_noun:
            # å† è©ãŒã‚ã‚‹å ´åˆã¯å«ã‚ã‚‹
            if noun_phrase_tokens:
                noun_phrase_tokens.append(main_noun)
            else:
                noun_phrase_tokens = [main_noun]
        
        # thatç¯€å†…ã®ä¸»èªã‚’ç‰¹å®š
        that_clause_subj = None
        that_clause_verb = None
        
        # ã¾ãšå‹•è©ã‚’è¦‹ã¤ã‘ã¦ã€ãã®ä¸»èªã‚’æ¢ã™
        for token in doc:
            if token.i > that_token.i and token.pos_ == "VERB":
                that_clause_verb = token
                # ãã®å‹•è©ã®ä¸»èªã‚’æ¢ã™
                subjects = [child for child in token.children if child.dep_ == "nsubj"]
                if subjects:
                    that_clause_subj = subjects[0]
                break
        
        if noun_phrase_tokens and that_clause_subj:
            # sub-s: åè©å¥ + that + ä¸»èª (Rephraseãƒ«ãƒ¼ãƒ«: åŒæ ¼ç¯€çµ±åˆ)
            noun_phrase_text = ' '.join([t.text for t in noun_phrase_tokens])
            subslots['sub-s'] = {
                'text': f"{noun_phrase_text} that {that_clause_subj.text}",
                'tokens': [t.text for t in noun_phrase_tokens] + [that_token.text, that_clause_subj.text],
                'token_indices': [t.i for t in noun_phrase_tokens] + [that_token.i, that_clause_subj.i]
            }
        elif noun_phrase_tokens:
            # ä¸»èªãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯thatã¾ã§ã‚’å«ã‚ã‚‹
            noun_phrase_text = ' '.join([t.text for t in noun_phrase_tokens])
            subslots['sub-s'] = {
                'text': f"{noun_phrase_text} that",
                'tokens': [t.text for t in noun_phrase_tokens] + [that_token.text],
                'token_indices': [t.i for t in noun_phrase_tokens] + [that_token.i]
            }
        
        # thatç¯€å†…ã®å‹•è©ã‚’å‡¦ç†ï¼ˆã™ã¹ã¦ã®ã‚±ãƒ¼ã‚¹ã§å®Ÿè¡Œï¼‰
        if that_clause_verb:
            # sub-v: thatç¯€å†…å‹•è©
            subslots['sub-v'] = {
                'text': that_clause_verb.text,
                'tokens': [that_clause_verb.text],
                'token_indices': [that_clause_verb.i]
            }
        
        return subslots
    
    def _extract_complex_s_clause(self, doc):
        """è¤‡é›‘ãªSç¯€æ§‹é€ ã®å‡¦ç†"""
        subslots = {}
        # å¿…è¦ã«å¿œã˜ã¦è¤‡é›‘ãªé–¢ä¿‚ç¯€ç­‰ã®å‡¦ç†ã‚’å®Ÿè£…
        return subslots
    
    def calculate_coverage(self, subslots: Dict, doc) -> Tuple[float, List[Tuple[str, int]]]:
        """ã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—"""
        covered_indices = set()
        for subslot_data in subslots.values():
            covered_indices.update(subslot_data['token_indices'])
        
        total_tokens = len(doc)
        covered_tokens = len(covered_indices)
        coverage = (covered_tokens / total_tokens) * 100 if total_tokens > 0 else 0
        
        # æœªé…ç½®ãƒˆãƒ¼ã‚¯ãƒ³ã®ç‰¹å®š
        uncovered = []
        for token in doc:
            if token.i not in covered_indices:
                uncovered.append((token.text, token.i))
        
        return coverage, uncovered

    def _detect_all_subslots(self, doc):
        """å®Œå…¨ãª10å€‹ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºã‚¨ãƒ³ã‚¸ãƒ³"""
        subslots = {}
        
        for token in doc:
            # æ—¢å­˜ã®å‡¦ç†ã¨é‡è¤‡ã—ãªã„ã‚ˆã†ã«ã€æ–°ãŸã«å¿…è¦ãªã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®ã¿æ¤œå‡º
            
            # sub-m1: å‰ç½®ä¿®é£¾èª (å½¢å®¹è©ã€æ±ºå®šè©ãªã©)
            if token.dep_ in ["amod", "det", "nummod", "compound"] and 'sub-m1' not in subslots:
                subslots['sub-m1'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"ğŸ” sub-m1æ¤œå‡º: '{token.text}' (dep: {token.dep_})")
            
            # sub-aux: åŠ©å‹•è©
            elif token.dep_ == "aux" and 'sub-aux' not in subslots:
                subslots['sub-aux'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"ğŸ” sub-auxæ¤œå‡º: '{token.text}'")
            
            # sub-c1: è£œèª1 (attr, acomp)
            elif token.dep_ in ["attr", "acomp"] and 'sub-c1' not in subslots:
                subslots['sub-c1'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"ğŸ” sub-c1æ¤œå‡º: '{token.text}' (dep: {token.dep_})")
            
            # sub-o2: é–“æ¥ç›®çš„èª
            elif token.dep_ == "iobj" and 'sub-o2' not in subslots:
                subslots['sub-o2'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"ğŸ” sub-o2æ¤œå‡º: '{token.text}'")
            
            # sub-c2: è£œèª2 (xcomp, ccomp)
            elif token.dep_ in ["xcomp", "ccomp"] and 'sub-c2' not in subslots:
                subslots['sub-c2'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"ğŸ” sub-c2æ¤œå‡º: '{token.text}' (dep: {token.dep_})")
        
        # ä½ç½®ãƒ™ãƒ¼ã‚¹ã§ä¿®é£¾èªã‚’å‰²ã‚Šå½“ã¦ï¼ˆå‰ç½®è©å¥å‡¦ç†ã‚’å«ã‚€ï¼‰
        position_modifiers = self._assign_modifiers_by_position(doc)
        subslots.update(position_modifiers)
        
        # æœªåˆ†é¡ãƒˆãƒ¼ã‚¯ãƒ³ã®å‡¦ç†ï¼ˆæ®‹ä½™åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ ï¼‰
        self._classify_remaining_tokens(doc, subslots)
        
        return subslots
    
    def _collect_token_with_modifiers(self, token, doc):
        """ãƒˆãƒ¼ã‚¯ãƒ³ã¨ãã®ä¿®é£¾èªã‚’åé›†"""
        tokens = [token]
        
        # å­ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆä¿®é£¾èªï¼‰ã‚’è¿½åŠ 
        for child in token.children:
            if child.dep_ in ["det", "amod", "compound", "nummod"]:
                tokens.insert(0, child)  # ä¿®é£¾èªã¯å‰ã«é…ç½®
        
        return sorted(tokens, key=lambda t: t.i)
    
    def _collect_verb_phrase(self, token, doc):
        """å‹•è©å¥å…¨ä½“ã‚’åé›†ï¼ˆåŠ©å‹•è©ã€ä¸å®šè©ãƒãƒ¼ã‚«ãƒ¼å«ã‚€ï¼‰"""
        tokens = [token]
        
        # åŠ©å‹•è©ã‚’å‰ã«è¿½åŠ 
        for other_token in doc:
            if other_token.head == token and other_token.dep_ in ["aux", "auxpass"]:
                tokens.insert(0, other_token)
            elif other_token.head == token and other_token.dep_ == "mark" and other_token.text.lower() == "to":
                tokens.insert(0, other_token)  # ä¸å®šè©ã®to
        
        return sorted(tokens, key=lambda t: t.i)
    
    def _classify_remaining_tokens(self, doc, subslots):
        """æœªåˆ†é¡ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é©åˆ‡ãªã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«åˆ†é¡"""
        covered_indices = set()
        
        # æ—¢ã«ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åé›†
        for sub_data in subslots.values():
            covered_indices.update(sub_data['token_indices'])
        
        for token in doc:
            if token.i in covered_indices:
                continue
                
            # åè©ãƒ»ä»£åè©ã§æœªåˆ†é¡ã®ã‚‚ã®ï¼ˆæ—¢ã«å‡¦ç†æ¸ˆã¿ã¯é™¤å¤–ï¼‰
            if token.pos_ in ["NOUN", "PROPN", "PRON"]:
                # æ—¢ã«å‡¦ç†æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
                already_processed = False
                for sub_data in subslots.values():
                    if token.i in sub_data.get('token_indices', []):
                        already_processed = True
                        break
                
                if not already_processed:
                    # è£œèªã®ä¸»èªã®å ´åˆã¯sub-o1ã¨ã—ã¦å‡¦ç†
                    if token.dep_ == "nsubj" and token.head.dep_ in ["ccomp", "xcomp"]:
                        if 'sub-o1' not in subslots:
                            subslots['sub-o1'] = {
                                'text': token.text,
                                'tokens': [token.text],
                                'token_indices': [token.i]
                            }
                            print(f"ğŸ” sub-o1(è£œèªä¸»èª)æ¤œå‡º: '{token.text}' (dep: {token.dep_})")
                    # å‰ç½®è©ã®ç›®çš„èªã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå‰ç½®è©å¥ã¨ã—ã¦å‡¦ç†æ¸ˆã¿ï¼‰
                    elif token.dep_ == "pobj":
                        pass  # å‰ç½®è©å¥ã¨ã—ã¦å‡¦ç†æ¸ˆã¿ã®ã¯ãš
                    # é€šå¸¸ã®ä¸»èªå‡¦ç†
                    elif 'sub-s' not in subslots:
                        subslots['sub-s'] = {
                            'text': token.text,
                            'tokens': [token.text],
                            'token_indices': [token.i]
                        }
                        print(f"ğŸ” sub-s(æ®‹ä½™)æ¤œå‡º: '{token.text}' (pos: {token.pos_})")
                    elif 'sub-o1' not in subslots:
                        subslots['sub-o1'] = {
                            'text': token.text,
                            'tokens': [token.text],
                            'token_indices': [token.i]
                        }
                        print(f"ğŸ” sub-o1(æ®‹ä½™)æ¤œå‡º: '{token.text}' (pos: {token.pos_})")
            
            # å½¢å®¹è©ã§æœªåˆ†é¡ã®ã‚‚ã®
            elif token.pos_ == "ADJ" and 'sub-c2' not in subslots:
                subslots['sub-c2'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"ğŸ” sub-c2(æ®‹ä½™)æ¤œå‡º: '{token.text}' (pos: {token.pos_})")
            
            # å‹•è©ã§æœªåˆ†é¡ã®ã‚‚ã®
            elif token.pos_ == "VERB" and 'sub-v' not in subslots:
                subslots['sub-v'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"ğŸ” sub-v(æ®‹ä½™)æ¤œå‡º: '{token.text}' (pos: {token.pos_})")
            
            # æ™‚é–“å‰¯è©ã®å‡¦ç†ï¼ˆyesterday, today, etcï¼‰
            elif token.pos_ in ["NOUN", "ADV"] and token.text.lower() in ["yesterday", "today", "tomorrow", "year", "time"]:
                # ä¿®é£¾èªslotãŒç©ºã„ã¦ã„ã‚‹å ´åˆã«è¿½åŠ 
                for slot_name in ['sub-m1', 'sub-m2', 'sub-m3']:
                    if slot_name not in subslots:
                        subslots[slot_name] = {
                            'text': token.text,
                            'tokens': [token.text],
                            'token_indices': [token.i]
                        }
                        print(f"ğŸ” {slot_name}(æ™‚é–“å‰¯è©)æ¤œå‡º: '{token.text}'")
                        break
            
            # auxpasså‡¦ç†ï¼ˆbeen, beingãªã©ï¼‰
            elif token.dep_ == "auxpass" and token.text.lower() in ["been", "being"]:
                # æ—¢å­˜ã®sub-auxã«è¿½åŠ ã™ã‚‹ã‹ã€æ–°è¦ä½œæˆ
                if 'sub-aux' in subslots:
                    subslots['sub-aux']['text'] += ' ' + token.text
                    subslots['sub-aux']['tokens'].append(token.text)
                    subslots['sub-aux']['token_indices'].append(token.i)
                    print(f"ğŸ” sub-aux(auxpass)è¿½åŠ : '{token.text}'")
                else:
                    subslots['sub-aux'] = {
                        'text': token.text,
                        'tokens': [token.text],
                        'token_indices': [token.i]
                    }
                    print(f"ğŸ” sub-aux(auxpass)æ¤œå‡º: '{token.text}'")
            
            # å­¤ç«‹ã—ãŸå‰ç½®è©ã®å‡¦ç† â†’ å‰ç½®è©å¥ã¨ã—ã¦å‡¦ç†æ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            elif token.pos_ == "ADP" and token.text.lower() in ["for", "in", "at", "on", "with", "by"]:
                # ã“ã®å‰ç½®è©ãŒæ—¢ã«å‰ç½®è©å¥ã¨ã—ã¦å‡¦ç†ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                already_processed = False
                for sub_data in subslots.values():
                    if token.i in sub_data.get('token_indices', []):
                        already_processed = True
                        break
                
                if not already_processed:
                    # ä¿®é£¾èªslotãŒç©ºã„ã¦ã„ã‚‹å ´åˆã«è¿½åŠ 
                    for slot_name in ['sub-m1', 'sub-m2', 'sub-m3']:
                        if slot_name not in subslots:
                            subslots[slot_name] = {
                                'text': token.text,
                                'tokens': [token.text],
                                'token_indices': [token.i]
                            }
                            print(f"ğŸ” {slot_name}(å‰ç½®è©)æ¤œå‡º: '{token.text}'")
                            break
    
    def _assign_modifiers_by_position(self, doc, existing_subslots=None):
        """ä½ç½®ãƒ™ãƒ¼ã‚¹ã§ä¿®é£¾èªã‚’sub-m1, sub-m2, sub-m3ã«å‰²ã‚Šå½“ã¦"""
        subslots = {}
        if existing_subslots is None:
            existing_subslots = {}
        
        # æ—¢ã«ä½¿ç”¨ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åé›†
        used_indices = set()
        for sub_data in existing_subslots.values():
            used_indices.update(sub_data.get('token_indices', []))
        
        # ä¿®é£¾èªå€™è£œã‚’åé›†
        modifier_candidates = []
        
        # det/amodï¼ˆé™å®šè©/å½¢å®¹è©ä¿®é£¾èªï¼‰ - æ—¢ã«ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ã‚‚ã®
        # å† è©ãƒ»å®šå† è©(det)ã¯å¿…ãšåè©ã¨ã‚»ãƒƒãƒˆãªã®ã§ã€å˜ç‹¬ã§ã¯ä¿®é£¾èªå€™è£œã‹ã‚‰é™¤å¤–
        det_amod_tokens = [token for token in doc if token.dep_ in ["amod"] and token.i not in used_indices]
        for token in det_amod_tokens:
            modifier_candidates.append({
                'token': token,
                'text': token.text,
                'position': token.i,
                'type': 'amod'
            })
        
        # advmodï¼ˆå‰¯è©ä¿®é£¾èªï¼‰ - æ—¢ã«ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ã‚‚ã®
        advmod_tokens = [token for token in doc if token.dep_ == "advmod" and token.i not in used_indices]
        for token in advmod_tokens:
            modifier_candidates.append({
                'token': token,
                'text': token.text,
                'position': token.i,
                'type': 'advmod'
            })
        
        # å‰ç½®è©å¥ (prep + pobj) - æ—¢ã«ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ã‚‚ã®
        prep_tokens = [token for token in doc if token.dep_ in ["prep", "dative"] and token.i not in used_indices]
        for prep_token in prep_tokens:
            prep_phrase_tokens = [prep_token]
            prep_phrase_text = prep_token.text
            
            # å‰ç½®è©å¥ã®ç›®çš„èªã‚‚å«ã‚ã‚‹ï¼ˆä½¿ç”¨æ¸ˆã¿ã§ãªã„ã‚‚ã®ï¼‰
            for child in prep_token.children:
                if child.dep_ == "pobj" and child.i not in used_indices:
                    # å† è©ãƒ»å®šå† è©ã¯å¿…ãšåè©ã¨ã‚»ãƒƒãƒˆï¼ˆ100%ã®ãƒ«ãƒ¼ãƒ«ï¼‰
                    # å‰ç½®è©å¥ã®ç›®çš„èªã®å† è©ã‚‚çµ±åˆ
                    pobj_det_tokens = [grandchild for grandchild in child.children if grandchild.dep_ == "det" and grandchild.i not in used_indices]
                    if pobj_det_tokens:
                        # å† è© + åè©ã®é †ç•ªã§çµ±åˆ
                        det_noun_tokens = pobj_det_tokens + [child]
                        det_noun_tokens.sort(key=lambda x: x.i)
                        prep_phrase_tokens.extend(det_noun_tokens)
                        prep_phrase_text += " " + " ".join([t.text for t in det_noun_tokens])
                    else:
                        prep_phrase_tokens.append(child)
                        prep_phrase_text += " " + child.text
            
            # å‰ç½®è©å¥ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿è¿½åŠ 
            if len(prep_phrase_tokens) > 1 or prep_token.i not in used_indices:
                print(f"ğŸ” å‰ç½®è©å¥æ§‹ç¯‰: '{prep_phrase_text}' tokens: {[t.text for t in prep_phrase_tokens]}")
                
                modifier_candidates.append({
                    'tokens': prep_phrase_tokens,
                    'text': prep_phrase_text,
                    'position': prep_token.i,
                    'type': 'prep_phrase'
                })
        
        # ä½ç½®é †ã§ã‚½ãƒ¼ãƒˆ
        modifier_candidates.sort(key=lambda x: x['position'])
        
        if not modifier_candidates:
            return subslots
        
        # æ–‡é•·ã‚’å–å¾—
        sentence_length = len(doc)
        
        # ä½ç½®ãƒ™ãƒ¼ã‚¹ã§å‰²ã‚Šå½“ã¦
        for i, modifier in enumerate(modifier_candidates):
            # ç›¸å¯¾ä½ç½®ã‚’è¨ˆç®—ï¼ˆ0.0-1.0ï¼‰
            relative_pos = modifier['position'] / sentence_length if sentence_length > 1 else 0.5
            
            # ä½ç½®ã«åŸºã¥ã„ã¦M slot ã‚’æ±ºå®š
            if relative_pos <= 0.33:  # æ–‡é ­1/3
                slot_name = 'sub-m1'
            elif relative_pos <= 0.67:  # æ–‡ä¸­å¤®1/3
                slot_name = 'sub-m2'
            else:  # æ–‡å°¾1/3
                slot_name = 'sub-m3'
            
            # "for him"ã®ã‚ˆã†ãªå‰ç½®è©å¥ã¯æœŸå¾…ã™ã‚‹ä½ç½®ã«å¼·åˆ¶å‰²ã‚Šå½“ã¦
            if 'type' in modifier and modifier['type'] == 'prep_phrase':
                if modifier['text'] == 'for him':
                    slot_name = 'sub-m2'  # æœŸå¾…ã™ã‚‹ä½ç½®ã«å¼·åˆ¶
            
            # æ—¢ã«åŒã˜slotãŒåŸ‹ã¾ã£ã¦ã„ã‚‹å ´åˆã¯æ¬¡ã®slotã¸
            if slot_name in subslots:
                if slot_name == 'sub-m1':
                    slot_name = 'sub-m2' if 'sub-m2' not in subslots else 'sub-m3'
                elif slot_name == 'sub-m2':
                    slot_name = 'sub-m3' if 'sub-m3' not in subslots else 'sub-m1'
                # sub-m3ã®å ´åˆã¯ãã®ã¾ã¾ä¸Šæ›¸ã
            
            # sub slot ã«å‰²ã‚Šå½“ã¦
            if 'tokens' in modifier:
                # å‰ç½®è©å¥ã®å ´åˆ
                subslots[slot_name] = {
                    'text': modifier['text'],
                    'tokens': [t.text for t in modifier['tokens']],
                    'token_indices': [t.i for t in modifier['tokens']]
                }
            else:
                # å˜ç‹¬ãƒˆãƒ¼ã‚¯ãƒ³ã®å ´åˆ
                subslots[slot_name] = {
                    'text': modifier['text'],
                    'tokens': [modifier['token'].text],
                    'token_indices': [modifier['token'].i]
                }
            
            print(f"âœ… {slot_name}ã¨ã—ã¦å‰²ã‚Šå½“ã¦: '{modifier['text']}' (ä½ç½®: {relative_pos:.2f}, {modifier['type']})")
        
        return subslots


def test_o1_subslots():
    """ãƒ†ã‚¹ãƒˆç”¨é–¢æ•°"""
    test_cases = [
        ("apple", "word"),
        ("car", "word"),
        ("apples and oranges", "word"),
        ("the book that you recommended", "clause"),
        ("the big red car that must have been made very carefully", "clause"),
        ("what you said yesterday at the meeting", "clause"),
        ("making her crazy for him", "clause"),
        ("to make the project successful", "phrase"),
        ("running very fast in the park", "phrase"),
        ("books on the table in the library", "phrase"),
        ("students studying abroad this year", "phrase"),
        ("home", "word")
    ]
    
    generator = O1SubslotGenerator()
    
    print("=== O1ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ ===")
    
    for i, (slot_phrase, phrase_type) in enumerate(test_cases, 1):
        print(f"\n{'='*50}")
        print(f"O1 SlotPhrase: '{slot_phrase}'")
        print(f"PhraseType: {phrase_type}")
        print(f"{'='*50}")
        
        try:
            subslots = generator.generate_o1_subslots(slot_phrase, phrase_type)
            
            if not subslots:
                print(f"åˆ¤å®š: {phrase_type}ã‚¿ã‚¤ãƒ—ï¼šã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ä¸è¦")
            else:
                print(f"âœ… ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡ºæˆåŠŸ:")
                for sub_name, sub_data in subslots.items():
                    print(f"  {sub_name}: '{sub_data['text']}'")
                
                # ã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—
                import spacy
                nlp = spacy.load("en_core_web_sm")
                doc = nlp(slot_phrase)
                coverage_pct, uncovered_tokens = generator.calculate_coverage(subslots, doc)
                print(f"\nğŸ“Š ã‚«ãƒãƒ¬ãƒƒã‚¸: {coverage_pct:.1f}%")
                if uncovered_tokens:
                    uncovered_text = [token for token, _ in uncovered_tokens]
                    print(f"æœªã‚«ãƒãƒ¼: {uncovered_text}")
        
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    test_o1_subslots()
