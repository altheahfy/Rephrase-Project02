#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Rule Dictionary v2.0 - O1(ç›´æ¥ç›®çš„èª)ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
spaCyä¾å­˜æ§‹é€ è§£æã«ã‚ˆã‚‹å‹•çš„ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º

ç›´æ¥ç›®çš„èªã®è¤‡é›‘æ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³:
1. é–¢ä¿‚ä»£åè©ä»˜ãç›®çš„èª: "The book that you recommended" â†’ sub-o1: 'The book that', sub-v: 'recommended'
2. åŒæ ¼thatç¯€: "The fact that he left" â†’ sub-s: 'The fact that he', sub-v: 'left'  
3. ä¸å®šè©ç›®çš„èª: "To go home" â†’ sub-v: 'To go', sub-o1: 'home'
4. å‹•åè©ç›®çš„èª: "Reading books" â†’ sub-v: 'Reading', sub-o1: 'books'
5. è¤‡åˆç›®çš„èª: "apples and oranges" â†’ wordæ‰±ã„ (Væ§‹é€ ãªã—)
"""

import spacy
from typing import Dict, List, Tuple, Any

class O1SubslotGenerator:
    """O1(ç›´æ¥ç›®çš„èª)ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.nlp = spacy.load("en_core_web_sm")
    
    def generate_o1_subslots(self, slot_phrase: str, phrase_type: str) -> Dict[str, Dict[str, Any]]:
        """
        O1(ç›´æ¥ç›®çš„èª)ã‚¹ãƒ­ãƒƒãƒˆã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ
        
        Args:
            slot_phrase: ç›®çš„èªãƒ•ãƒ¬ãƒ¼ã‚º
            phrase_type: ãƒ•ãƒ¬ãƒ¼ã‚ºã‚¿ã‚¤ãƒ— (word/phrase/clause)
            
        Returns:
            Dict: ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆè¾æ›¸
        """
        doc = self.nlp(slot_phrase)
        
        if phrase_type == "word":
            # å˜èªã®å ´åˆã¯ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ä¸è¦
            return {}
        elif phrase_type == "phrase":
            return self._extract_o1_phrase_subslots(doc)
        elif phrase_type == "clause":
            return self._extract_o1_clause_subslots(doc)
        else:
            return {}
    
    def _extract_o1_phrase_subslots(self, doc):
        """O1 Phraseã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # é–¢ä¿‚ä»£åè©ã®æ¤œå‡ºï¼ˆã‚ˆã‚ŠæŸ”è»Ÿãªæ¡ä»¶ï¼‰
        rel_pronouns = ["who", "whom", "whose", "which", "that"]
        rel_pronoun_token = None
        
        for token in doc:
            if token.text.lower() in rel_pronouns:
                # é–¢ä¿‚ä»£åè©ã®æ¡ä»¶ã‚’ç·©å’Œï¼šnsubjä»¥å¤–ã‚‚æ¤œå‡º
                if token.dep_ in ["nsubj", "dobj", "pobj", "nsubjpass"] or token.pos_ == "PRON":
                    rel_pronoun_token = token
                    break
        
        if rel_pronoun_token:
            subslots.update(self._extract_relative_clause_subslots(doc, rel_pronoun_token))
        
        # ä¸å®šè©ä¸»èªã®å‡¦ç†: "To learn English is important"
        elif doc[0].text.lower() == "to" and doc[0].pos_ == "PART":
            subslots.update(self._extract_infinitive_subject_subslots(doc))
        
        # å‹•åè©ä¸»èªã®å‡¦ç†: "Reading books is fun"
        else:
            gerund_tokens = [token for token in doc if token.pos_ == "VERB" and token.tag_ == "VBG"]
            if gerund_tokens:
                subslots.update(self._extract_gerund_subject_subslots(doc, gerund_tokens[0]))
        
        # è¤‡åˆä¸»èªã®å‡¦ç†: "John and Mary are here"
        and_tokens = [token for token in doc if token.text.lower() == "and" and token.dep_ == "cc"]
        if and_tokens:
            subslots.update(self._extract_compound_subject_subslots(doc))
        
        # ä½ç½®ãƒ™ãƒ¼ã‚¹ä¿®é£¾èªå‰²ã‚Šå½“ã¦ï¼ˆsub-m1, sub-m2, sub-m3ï¼‰
        modifier_subslots = self._assign_modifiers_by_position(doc)
        subslots.update(modifier_subslots)
        
        # sub-aux: åŠ©å‹•è©æ¤œå‡º (aux, auxpass)
        aux_tokens = [token for token in doc if token.dep_ in ["aux", "auxpass"]]
        if aux_tokens:
            aux_text = ' '.join([token.text for token in aux_tokens])
            subslots['sub-aux'] = {
                'text': aux_text,
                'tokens': [token.text for token in aux_tokens],
                'token_indices': [token.i for token in aux_tokens]
            }
            print(f"âœ… sub-auxã¨ã—ã¦å‡¦ç†: '{aux_text}'")
        
        # sub-c2: è£œèª2æ¤œå‡º (xcomp, acomp, attr, ccomp)
        comp_tokens = [token for token in doc if token.dep_ in ["xcomp", "acomp", "attr", "ccomp"]]
        if comp_tokens:
            comp_text = ' '.join([token.text for token in comp_tokens])
            subslots['sub-c2'] = {
                'text': comp_text,
                'tokens': [token.text for token in comp_tokens],
                'token_indices': [token.i for token in comp_tokens]
            }
            print(f"âœ… sub-c2ã¨ã—ã¦å‡¦ç†: '{comp_text}'")
        
        # sub-o1è¿½åŠ æ¤œå‡º: nsubjãŒè£œèªæ§‹é€ å†…ã«ã‚ã‚‹å ´åˆ
        nsubj_tokens = [token for token in doc if token.dep_ == "nsubj" and token.head.dep_ in ["ccomp", "xcomp"]]
        if nsubj_tokens and 'sub-o1' not in subslots:
            nsubj_text = ' '.join([token.text for token in nsubj_tokens])
            subslots['sub-o1'] = {
                'text': nsubj_text,
                'tokens': [token.text for token in nsubj_tokens],
                'token_indices': [token.i for token in nsubj_tokens]
            }
            print(f"âœ… sub-o1ã¨ã—ã¦å‡¦ç†: '{nsubj_text}' (è£œèªæ§‹é€ å†…ä¸»èª)")
        

        
        # TODO: å®Œå…¨ãª10å€‹ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºã‚’å®Ÿè£…äºˆå®š
        # complete_subslots = self._detect_all_subslots(doc)
        # subslots.update(complete_subslots)
        
        return subslots
    
    def _extract_o1_clause_subslots(self, doc):
        """O1 Clauseã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # ä½ç½®ãƒ™ãƒ¼ã‚¹ä¿®é£¾èªå‰²ã‚Šå½“ã¦
        modifier_subslots = self._assign_modifiers_by_position(doc)
        subslots.update(modifier_subslots)
        
        # sub-aux: åŠ©å‹•è©æ¤œå‡º
        aux_tokens = [token for token in doc if token.dep_ == "aux"]
        if aux_tokens:
            aux_text = ' '.join([token.text for token in aux_tokens])
            subslots['sub-aux'] = {
                'text': aux_text,
                'tokens': [token.text for token in aux_tokens],
                'token_indices': [token.i for token in aux_tokens]
            }
            print(f"âœ… sub-auxã¨ã—ã¦å‡¦ç†: '{aux_text}'")
        
        # ã¾ãšåŒæ ¼thatç¯€ã‚’å„ªå…ˆãƒã‚§ãƒƒã‚¯
        that_token = None
        for token in doc:
            if token.text.lower() == "that":
                # thatç¯€ã®æ¤œå‡ºæ¡ä»¶ã‚’åºƒã’ã‚‹
                if token.dep_ in ["acl", "ccomp", "mark", "dobj"] or (token.pos_ == "SCONJ"):
                    that_token = token
                    break
        
        if that_token:
            # åŒæ ¼thatç¯€ã‹ã©ã†ã‹ã‚’åˆ¤å®šï¼ˆåè©ã®å¾Œã«thatãŒã‚ã‚‹å ´åˆï¼‰
            has_noun_before = False
            for token in doc:
                if token.i < that_token.i and token.pos_ in ["NOUN", "PROPN"]:
                    has_noun_before = True
                    break
            
            if has_noun_before:
                return self._extract_appositive_that_clause_subslots(doc, that_token)
        
        # ç–‘å•è©ç¯€ã®æ¤œå‡ºï¼ˆwhat, where, when, how ãªã©ï¼‰
        wh_words = ["what", "where", "when", "how", "why", "which"]
        wh_word_token = None
        
        for token in doc:
            if token.text.lower() in wh_words:
                # ç–‘å•è©ã®æ¡ä»¶ï¼šdobj, advmod, nsubj ãªã©ã®é–¢ä¿‚
                if token.dep_ in ["dobj", "advmod", "nsubj", "pobj"] or token.pos_ in ["PRON", "SCONJ"]:
                    wh_word_token = token
                    break
        
        if wh_word_token:
            return self._extract_wh_clause_subslots(doc, wh_word_token)
        
        # é–¢ä¿‚ä»£åè©ã®æ¤œå‡ºï¼ˆclauseå†…ï¼‰
        rel_pronouns = ["who", "whom", "whose", "which", "that"]
        rel_pronoun_token = None
        
        for token in doc:
            if token.text.lower() in rel_pronouns:
                rel_pronoun_token = token
                break
        
        if rel_pronoun_token:
            return self._extract_relative_clause_s_subslots(doc, rel_pronoun_token)
        
        # ãã®ä»–ã®é–¢ä¿‚ç¯€å‡¦ç†
        complex_subslots = self._extract_complex_s_clause(doc)
        subslots.update(complex_subslots)
        
        # TODO: å®Œå…¨ãª10å€‹ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºã‚’å®Ÿè£…äºˆå®š
        # complete_subslots = self._detect_all_subslots(doc)
        # subslots.update(complete_subslots)
        
        return subslots
    
    def _extract_wh_clause_subslots(self, doc, wh_word_token):
        """ç–‘å•è©ç¯€ã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡ºï¼ˆwhat you said ãªã©ï¼‰"""
        subslots = {}
        
        # ç–‘å•è©ç¯€ã®æ§‹é€ è§£æ: what(dobj) you(nsubj) said(ROOT)
        verb_token = None
        subject_tokens = []
        
        for token in doc:
            if token.pos_ == "VERB" and token.dep_ == "ROOT":
                verb_token = token
            elif token.dep_ in ["nsubj", "nsubjpass"]:
                subject_tokens.append(token)
        
        if verb_token and subject_tokens:
            # sub-o1: ç–‘å•è©
            subslots['sub-o1'] = {
                'text': wh_word_token.text,
                'tokens': [wh_word_token.text],
                'token_indices': [wh_word_token.i]
            }
            
            # sub-s: ä¸»èª
            subject_text = ' '.join([t.text for t in subject_tokens])
            subslots['sub-s'] = {
                'text': subject_text,
                'tokens': [t.text for t in subject_tokens],
                'token_indices': [t.i for t in subject_tokens]
            }
            
            # sub-v: å‹•è©
            subslots['sub-v'] = {
                'text': verb_token.text,
                'tokens': [verb_token.text],
                'token_indices': [verb_token.i]
            }
        
        return subslots
    
    def _extract_relative_clause_s_subslots(self, doc, rel_pronoun_token):
        """é–¢ä¿‚ä»£åè©ã‚’å«ã‚€S Clauseã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # é–¢ä¿‚ä»£åè©ã®å‰ã®åè©å¥ã‚’ç‰¹å®š
        noun_phrase_tokens = []
        for token in doc:
            if token.i < rel_pronoun_token.i:
                noun_phrase_tokens.append(token)
        
        # é–¢ä¿‚ä»£åè©ã®å½¹å‰²ã‚’åˆ¤å®š
        rel_verb = None
        for token in doc:
            if token.i > rel_pronoun_token.i and token.pos_ == "VERB":
                rel_verb = token
                break
        
        if rel_verb:
            # é–¢ä¿‚ä»£åè©ãŒç›®çš„èªã®å ´åˆ (whom)
            if rel_pronoun_token.text.lower() == "whom":
                # sub-o1: åè©å¥ + whom
                if noun_phrase_tokens:
                    noun_phrase_text = ' '.join([t.text for t in noun_phrase_tokens])
                    subslots['sub-o1'] = {
                        'text': f"{noun_phrase_text} {rel_pronoun_token.text}",
                        'tokens': [t.text for t in noun_phrase_tokens] + [rel_pronoun_token.text],
                        'token_indices': [t.i for t in noun_phrase_tokens] + [rel_pronoun_token.i]
                    }
            else:
                # é–¢ä¿‚ä»£åè©ãŒä¸»èªã®å ´åˆ (who)
                if noun_phrase_tokens:
                    noun_phrase_text = ' '.join([t.text for t in noun_phrase_tokens])
                    subslots['sub-s'] = {
                        'text': f"{noun_phrase_text} {rel_pronoun_token.text}",
                        'tokens': [t.text for t in noun_phrase_tokens] + [rel_pronoun_token.text],
                        'token_indices': [t.i for t in noun_phrase_tokens] + [rel_pronoun_token.i]
                    }
            
            # sub-v: é–¢ä¿‚ç¯€å†…å‹•è©
            subslots['sub-v'] = {
                'text': rel_verb.text,
                'tokens': [rel_verb.text],
                'token_indices': [rel_verb.i]
            }
            
            # sub-s: é–¢ä¿‚ç¯€å†…ä¸»èª (whomã®å ´åˆ)
            subjects = [child for child in rel_verb.children if child.dep_ == "nsubj"]
            if subjects and rel_pronoun_token.text.lower() == "whom":
                subslots['sub-s'] = {
                    'text': subjects[0].text,
                    'tokens': [subjects[0].text],
                    'token_indices': [subjects[0].i]
                }
        
        return subslots
    
    def _extract_relative_clause_subslots(self, doc, rel_pronoun_token):
        """é–¢ä¿‚ä»£åè©ä»˜ãä¸»èªã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # é–¢ä¿‚ä»£åè©ã®å‰ã«ã‚ã‚‹åè©å¥ã‚’ç‰¹å®š
        noun_phrase_tokens = []
        for token in doc:
            if token.i < rel_pronoun_token.i:
                noun_phrase_tokens.append(token)
        
        if noun_phrase_tokens:
            # sub-s: åè©å¥ + é–¢ä¿‚ä»£åè©
            noun_phrase_text = ' '.join([t.text for t in noun_phrase_tokens])
            subslots['sub-s'] = {
                'text': f"{noun_phrase_text} {rel_pronoun_token.text}",
                'tokens': [t.text for t in noun_phrase_tokens] + [rel_pronoun_token.text],
                'token_indices': [t.i for t in noun_phrase_tokens] + [rel_pronoun_token.i]
            }
        
        # é–¢ä¿‚ç¯€å†…ã®å‹•è©ã‚’ç‰¹å®š
        rel_clause_verb = None
        for token in doc:
            if token.i > rel_pronoun_token.i and token.pos_ == "VERB":
                rel_clause_verb = token
                break
        
        if rel_clause_verb:
            # sub-v: é–¢ä¿‚ç¯€å†…å‹•è©
            subslots['sub-v'] = {
                'text': rel_clause_verb.text,
                'tokens': [rel_clause_verb.text],
                'token_indices': [rel_clause_verb.i]
            }
            
            # é–¢ä¿‚ç¯€å†…ã®ä¸»èªã‚’å‡¦ç†
            subjects = [child for child in rel_clause_verb.children if child.dep_ == "nsubj"]
            if subjects:
                # sub-s: é–¢ä¿‚ç¯€å†…ä¸»èª (ä¾‹: "The man whom I met" ã® "I")
                if 'sub-s' not in subslots:  # æ—¢ã«sub-sãŒã‚ã‚‹å ´åˆã¯ä¸Šæ›¸ãã—ãªã„
                    subslots['sub-s2'] = {  # è¿½åŠ ã®ä¸»èªã¨ã—ã¦å‡¦ç†
                        'text': subjects[0].text,
                        'tokens': [subjects[0].text],
                        'token_indices': [subjects[0].i]
                    }
            
            # sub-o1: é–¢ä¿‚ç¯€å†…ç›®çš„èª
            objects = [child for child in rel_clause_verb.children if child.dep_ == "dobj"]
            if objects:
                subslots['sub-o1'] = {
                    'text': objects[0].text,
                    'tokens': [objects[0].text],
                    'token_indices': [objects[0].i]
                }
        
        return subslots
    
    def _extract_infinitive_subject_subslots(self, doc):
        """ä¸å®šè©ä¸»èªã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # "To learn English" ã®å‡¦ç†
        to_token = doc[0]  # "to"
        main_verb = None
        
        for token in doc[1:]:
            if token.pos_ == "VERB":
                main_verb = token
                break
        
        if main_verb:
            # sub-v: "to + å‹•è©" (Rephraseãƒ«ãƒ¼ãƒ«: ä¸å®šè©çµ±åˆ)
            subslots['sub-v'] = {
                'text': f"{to_token.text} {main_verb.text}",
                'tokens': [to_token.text, main_verb.text],
                'token_indices': [to_token.i, main_verb.i]
            }
            
            # sub-o1: ä¸å®šè©ã®ç›®çš„èª
            objects = [child for child in main_verb.children if child.dep_ == "dobj"]
            if objects:
                subslots['sub-o1'] = {
                    'text': objects[0].text,
                    'tokens': [objects[0].text],
                    'token_indices': [objects[0].i]
                }
        
        return subslots
    
    def _extract_gerund_subject_subslots(self, doc, gerund_token):
        """å‹•åè©ä¸»èªã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # sub-v: å‹•åè© (èª­ã‚€å‹•ä½œãªã®ã§å‹•è©ã¨ã—ã¦å‡¦ç†)
        subslots['sub-v'] = {
            'text': gerund_token.text,
            'tokens': [gerund_token.text],
            'token_indices': [gerund_token.i]
        }
        
        # sub-o1: å‹•åè©ã®ç›®çš„èª
        objects = [child for child in gerund_token.children if child.dep_ == "dobj"]
        if objects:
            subslots['sub-o1'] = {
                'text': objects[0].text,
                'tokens': [objects[0].text],
                'token_indices': [objects[0].i]
            }
        
        return subslots
    
    def _extract_compound_subject_subslots(self, doc):
        """è¤‡åˆä¸»èªã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # "John and Mary" ã¯Væ§‹é€ ãŒç„¡ã„ã®ã§ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ä¸è¦
        # wordã‚¿ã‚¤ãƒ—ã¨ã—ã¦å‡¦ç†ã™ã¹ã
        return subslots
    
    def _extract_appositive_that_clause_subslots(self, doc, that_token):
        """åŒæ ¼thatç¯€ã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡º"""
        subslots = {}
        
        # thatç¯€ã®å‰ã®åè©å¥ã‚’ç‰¹å®šï¼ˆå† è©ã‚‚å«ã‚ã‚‹ï¼‰
        noun_phrase_tokens = []
        main_noun = None
        for token in doc:
            if token.i < that_token.i:
                if token.pos_ in ["NOUN", "PROPN"]:
                    main_noun = token
                elif token.pos_ == "DET" and not noun_phrase_tokens:
                    # å† è©ã‹ã‚‰åè©å¥ã®é–‹å§‹
                    noun_phrase_tokens.append(token)
        
        if main_noun:
            # å† è©ãŒã‚ã‚‹å ´åˆã¯å«ã‚ã‚‹
            if noun_phrase_tokens:
                noun_phrase_tokens.append(main_noun)
            else:
                noun_phrase_tokens = [main_noun]
        
        # thatç¯€å†…ã®ä¸»èªã‚’ç‰¹å®š
        that_clause_subj = None
        that_clause_verb = None
        
        # ã¾ãšå‹•è©ã‚’è¦‹ã¤ã‘ã¦ã€ãã®ä¸»èªã‚’æ¢ã™
        for token in doc:
            if token.i > that_token.i and token.pos_ == "VERB":
                that_clause_verb = token
                # ãã®å‹•è©ã®ä¸»èªã‚’æ¢ã™
                subjects = [child for child in token.children if child.dep_ == "nsubj"]
                if subjects:
                    that_clause_subj = subjects[0]
                break
        
        if noun_phrase_tokens and that_clause_subj:
            # sub-s: åè©å¥ + that + ä¸»èª (Rephraseãƒ«ãƒ¼ãƒ«: åŒæ ¼ç¯€çµ±åˆ)
            noun_phrase_text = ' '.join([t.text for t in noun_phrase_tokens])
            subslots['sub-s'] = {
                'text': f"{noun_phrase_text} that {that_clause_subj.text}",
                'tokens': [t.text for t in noun_phrase_tokens] + [that_token.text, that_clause_subj.text],
                'token_indices': [t.i for t in noun_phrase_tokens] + [that_token.i, that_clause_subj.i]
            }
        elif noun_phrase_tokens:
            # ä¸»èªãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯thatã¾ã§ã‚’å«ã‚ã‚‹
            noun_phrase_text = ' '.join([t.text for t in noun_phrase_tokens])
            subslots['sub-s'] = {
                'text': f"{noun_phrase_text} that",
                'tokens': [t.text for t in noun_phrase_tokens] + [that_token.text],
                'token_indices': [t.i for t in noun_phrase_tokens] + [that_token.i]
            }
        
        # thatç¯€å†…ã®å‹•è©ã‚’å‡¦ç†ï¼ˆã™ã¹ã¦ã®ã‚±ãƒ¼ã‚¹ã§å®Ÿè¡Œï¼‰
        if that_clause_verb:
            # sub-v: thatç¯€å†…å‹•è©
            subslots['sub-v'] = {
                'text': that_clause_verb.text,
                'tokens': [that_clause_verb.text],
                'token_indices': [that_clause_verb.i]
            }
        
        return subslots
    
    def _extract_complex_s_clause(self, doc):
        """è¤‡é›‘ãªSç¯€æ§‹é€ ã®å‡¦ç†"""
        subslots = {}
        # å¿…è¦ã«å¿œã˜ã¦è¤‡é›‘ãªé–¢ä¿‚ç¯€ç­‰ã®å‡¦ç†ã‚’å®Ÿè£…
        return subslots
    
    def calculate_coverage(self, subslots: Dict, doc) -> Tuple[float, List[Tuple[str, int]]]:
        """ã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—"""
        covered_indices = set()
        for subslot_data in subslots.values():
            covered_indices.update(subslot_data['token_indices'])
        
        total_tokens = len(doc)
        covered_tokens = len(covered_indices)
        coverage = (covered_tokens / total_tokens) * 100 if total_tokens > 0 else 0
        
        # æœªé…ç½®ãƒˆãƒ¼ã‚¯ãƒ³ã®ç‰¹å®š
        uncovered = []
        for token in doc:
            if token.i not in covered_indices:
                uncovered.append((token.text, token.i))
        
        return coverage, uncovered

    def _detect_all_subslots(self, doc):
        """å®Œå…¨ãª10å€‹ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºã‚¨ãƒ³ã‚¸ãƒ³"""
        subslots = {}
        
        for token in doc:
            # æ—¢å­˜ã®å‡¦ç†ã¨é‡è¤‡ã—ãªã„ã‚ˆã†ã«ã€æ–°ãŸã«å¿…è¦ãªã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã®ã¿æ¤œå‡º
            
            # sub-m1: å‰ç½®ä¿®é£¾èª (å½¢å®¹è©ã€æ±ºå®šè©ãªã©)
            if token.dep_ in ["amod", "det", "nummod", "compound"] and 'sub-m1' not in subslots:
                subslots['sub-m1'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"ğŸ” sub-m1æ¤œå‡º: '{token.text}' (dep: {token.dep_})")
            
            # sub-aux: åŠ©å‹•è©
            elif token.dep_ == "aux" and 'sub-aux' not in subslots:
                subslots['sub-aux'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"ğŸ” sub-auxæ¤œå‡º: '{token.text}'")
            
            # sub-c1: è£œèª1 (attr, acomp)
            elif token.dep_ in ["attr", "acomp"] and 'sub-c1' not in subslots:
                subslots['sub-c1'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"ğŸ” sub-c1æ¤œå‡º: '{token.text}' (dep: {token.dep_})")
            
            # sub-o2: é–“æ¥ç›®çš„èª
            elif token.dep_ == "iobj" and 'sub-o2' not in subslots:
                subslots['sub-o2'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"ğŸ” sub-o2æ¤œå‡º: '{token.text}'")
            
            # sub-c2: è£œèª2 (xcomp, ccomp)
            elif token.dep_ in ["xcomp", "ccomp"] and 'sub-c2' not in subslots:
                subslots['sub-c2'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"ğŸ” sub-c2æ¤œå‡º: '{token.text}' (dep: {token.dep_})")
            
            # sub-m3: å¾Œç½®ä¿®é£¾èª (prep, acl, relcl)
            elif token.dep_ in ["prep", "acl", "relcl"] and 'sub-m3' not in subslots:
                # å‰ç½®è©å¥å…¨ä½“ã‚’å–å¾—
                prep_phrase_tokens = [token]
                if token.dep_ == "prep":
                    # å‰ç½®è©å¥ã®ç›®çš„èªã‚‚å«ã‚ã‚‹
                    for child in token.children:
                        if child.dep_ == "pobj":
                            prep_phrase_tokens.append(child)
                
                prep_phrase_text = ' '.join([t.text for t in prep_phrase_tokens])
                subslots['sub-m3'] = {
                    'text': prep_phrase_text,
                    'tokens': [t.text for t in prep_phrase_tokens],
                    'token_indices': [t.i for t in prep_phrase_tokens]
                }
                print(f"ğŸ” sub-m3æ¤œå‡º: '{prep_phrase_text}' (dep: {token.dep_})")
        
        return subslots
    
    def _assign_modifiers_by_position(self, doc):
        """ä½ç½®ãƒ™ãƒ¼ã‚¹ã§ä¿®é£¾èªã‚’sub-m1, sub-m2, sub-m3ã«å‰²ã‚Šå½“ã¦"""
        subslots = {}
        
        # ä¿®é£¾èªå€™è£œã‚’åé›†
        modifier_candidates = []
        
        # advmodï¼ˆå‰¯è©ä¿®é£¾èªï¼‰
        advmod_tokens = [token for token in doc if token.dep_ == "advmod"]
        for token in advmod_tokens:
            modifier_candidates.append({
                'token': token,
                'text': token.text,
                'position': token.i,
                'type': 'advmod'
            })
        
        # å‰ç½®è©å¥ (prep + pobj)
        prep_tokens = [token for token in doc if token.dep_ in ["prep", "dative"]]
        for prep_token in prep_tokens:
            prep_phrase_tokens = [prep_token]
            prep_phrase_text = prep_token.text
            
            # å‰ç½®è©å¥ã®ç›®çš„èªã‚‚å«ã‚ã‚‹
            for child in prep_token.children:
                if child.dep_ == "pobj":
                    prep_phrase_tokens.append(child)
                    prep_phrase_text += " " + child.text
            
            modifier_candidates.append({
                'tokens': prep_phrase_tokens,
                'text': prep_phrase_text,
                'position': prep_token.i,
                'type': 'prep_phrase'
            })
        
        # det/amodï¼ˆé™å®šè©/å½¢å®¹è©ä¿®é£¾èªï¼‰
        det_amod_tokens = [token for token in doc if token.dep_ in ["det", "amod"]]
        for token in det_amod_tokens:
            modifier_candidates.append({
                'token': token,
                'text': token.text,
                'position': token.i,
                'type': 'det_amod'
            })
        
        # ä½ç½®é †ã§ã‚½ãƒ¼ãƒˆ
        modifier_candidates.sort(key=lambda x: x['position'])
        
        if not modifier_candidates:
            return subslots
        
        # æ–‡é•·ã‚’å–å¾—
        sentence_length = len(doc)
        
        # ä½ç½®ãƒ™ãƒ¼ã‚¹ã§å‰²ã‚Šå½“ã¦
        for i, modifier in enumerate(modifier_candidates):
            # ç›¸å¯¾ä½ç½®ã‚’è¨ˆç®—ï¼ˆ0.0-1.0ï¼‰
            relative_pos = modifier['position'] / sentence_length if sentence_length > 1 else 0.5
            
            # ä½ç½®ã«åŸºã¥ã„ã¦M slot ã‚’æ±ºå®š
            if relative_pos <= 0.33:  # æ–‡é ­1/3
                slot_name = 'sub-m1'
            elif relative_pos <= 0.67:  # æ–‡ä¸­å¤®1/3
                slot_name = 'sub-m2'
            else:  # æ–‡å°¾1/3
                slot_name = 'sub-m3'
            
            # æ—¢ã«åŒã˜slotãŒåŸ‹ã¾ã£ã¦ã„ã‚‹å ´åˆã¯æ¬¡ã®slotã¸
            if slot_name in subslots:
                if slot_name == 'sub-m1':
                    slot_name = 'sub-m2' if 'sub-m2' not in subslots else 'sub-m3'
                elif slot_name == 'sub-m2':
                    slot_name = 'sub-m3' if 'sub-m3' not in subslots else 'sub-m1'
                # sub-m3ã®å ´åˆã¯ãã®ã¾ã¾ä¸Šæ›¸ã
            
            # sub slot ã«å‰²ã‚Šå½“ã¦
            if 'tokens' in modifier:
                # å‰ç½®è©å¥ã®å ´åˆ
                subslots[slot_name] = {
                    'text': modifier['text'],
                    'tokens': [t.text for t in modifier['tokens']],
                    'token_indices': [t.i for t in modifier['tokens']]
                }
            else:
                # å˜ç‹¬ãƒˆãƒ¼ã‚¯ãƒ³ã®å ´åˆ
                subslots[slot_name] = {
                    'text': modifier['text'],
                    'tokens': [modifier['token'].text],
                    'token_indices': [modifier['token'].i]
                }
            
            print(f"âœ… {slot_name}ã¨ã—ã¦å‰²ã‚Šå½“ã¦: '{modifier['text']}' (ä½ç½®: {relative_pos:.2f}, {modifier['type']})")
        
        return subslots


def test_o1_subslots():
    """ãƒ†ã‚¹ãƒˆç”¨é–¢æ•°"""
    test_cases = [
        ("apple", "word"),
        ("car", "word"),
        ("apples and oranges", "word"),
        ("the book that you recommended", "clause"),
        ("the big red car that must have been made very carefully", "clause"),
        ("what you said yesterday at the meeting", "clause"),
        ("making her crazy for him", "clause"),
        ("to make the project successful", "phrase"),
        ("running very fast in the park", "phrase"),
        ("books on the table in the library", "phrase"),
        ("students studying abroad this year", "phrase"),
        ("home", "word")
    ]
    
    generator = O1SubslotGenerator()
    
    print("=== O1ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ ===")
    
    for i, (slot_phrase, phrase_type) in enumerate(test_cases, 1):
        print(f"\n{'='*50}")
        print(f"O1 SlotPhrase: '{slot_phrase}'")
        print(f"PhraseType: {phrase_type}")
        print(f"{'='*50}")
        
        try:
            subslots = generator.generate_o1_subslots(slot_phrase, phrase_type)
            
            if not subslots:
                print(f"åˆ¤å®š: {phrase_type}ã‚¿ã‚¤ãƒ—ï¼šã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ä¸è¦")
            else:
                print(f"âœ… ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæŠ½å‡ºæˆåŠŸ:")
                for sub_name, sub_data in subslots.items():
                    print(f"  {sub_name}: '{sub_data['text']}'")
                
                # ã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—
                import spacy
                nlp = spacy.load("en_core_web_sm")
                doc = nlp(slot_phrase)
                coverage_pct, uncovered_tokens = generator.calculate_coverage(subslots, doc)
                print(f"\nğŸ“Š ã‚«ãƒãƒ¬ãƒƒã‚¸: {coverage_pct:.1f}%")
                if uncovered_tokens:
                    uncovered_text = [token for token, _ in uncovered_tokens]
                    print(f"æœªã‚«ãƒãƒ¼: {uncovered_text}")
        
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    test_o1_subslots()
