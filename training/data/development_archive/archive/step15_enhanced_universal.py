#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Step 15: å¼·åŒ–ç‰ˆå…¨ã‚¹ãƒ­ãƒƒãƒˆå¯¾å¿œã‚·ã‚¹ãƒ†ãƒ 

åŒ…æ‹¬ãƒ†ã‚¹ãƒˆçµæœã‚’åŸºã«ã€å„ã‚¹ãƒ­ãƒƒãƒˆã®å°‚ç”¨å¼·åŒ–ã‚’å®Ÿè£…
ç›®æ¨™: å…¨8ã‚¹ãƒ­ãƒƒãƒˆã§80/80 (100%) ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ´»ç”¨é”æˆ
"""

import spacy
import json
import traceback
from collections import OrderedDict

class EnhancedUniversalSubslotGenerator:
    """å¼·åŒ–ç‰ˆå…¨ã‚¹ãƒ­ãƒƒãƒˆå¯¾å¿œ 10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        print("ğŸš€ Enhanced Universal Subslot Generator èµ·å‹•é–‹å§‹...")
        try:
            self.nlp = spacy.load("en_core_web_sm")
            print("âœ… spaCy 'en_core_web_sm' èª­ã¿è¾¼ã¿å®Œäº†")
        except IOError:
            print("âŒ spaCyè‹±èªãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
            print("python -m spacy download en_core_web_sm")
            raise
        
        # 10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå®šç¾©
        self.subslot_types = [
            'sub-s', 'sub-v', 'sub-o1', 'sub-o2', 
            'sub-c1', 'sub-c2', 'sub-m1', 'sub-m2', 'sub-m3', 'sub-aux'
        ]
        
        # å¯¾è±¡ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆ
        self.target_slots = ['S', 'O1', 'O2', 'C1', 'C2', 'M1', 'M2', 'M3']
        
        print(f"ğŸ¯ å¯¾è±¡ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆ: {', '.join(self.target_slots)}")
        print(f"ğŸ”§ å¼·åŒ–ç‰ˆã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆä½“ç³»: {', '.join(self.subslot_types)}")
        
    def generate_subslots_for_slot(self, slot_name, sentence):
        """æŒ‡å®šã‚¹ãƒ­ãƒƒãƒˆã®10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        print(f"\nğŸ¯ {slot_name}ã‚¹ãƒ­ãƒƒãƒˆ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆé–‹å§‹: '{sentence}'")
        
        if slot_name not in self.target_slots:
            print(f"âŒ éå¯¾è±¡ã‚¹ãƒ­ãƒƒãƒˆ: {slot_name}")
            return {}
        
        try:
            doc = self.nlp(sentence)
            print(f"ğŸ“ è§£æå¯¾è±¡: {[(token.text, token.dep_, token.pos_) for token in doc]}")
            
            # åŒ…æ‹¬çš„ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºï¼ˆå…¨ã‚¹ãƒ­ãƒƒãƒˆå…±é€šï¼‰
            complete_subslots = self._comprehensive_subslot_detection(doc)
            
            # ã‚¹ãƒ­ãƒƒãƒˆåˆ¥å°‚ç”¨å¼·åŒ–
            if slot_name == 'S':
                enhanced = self._enhance_s_slot(doc, complete_subslots)
            elif slot_name == 'O1':
                enhanced = self._enhance_o1_slot(doc, complete_subslots)
            elif slot_name == 'O2':
                enhanced = self._enhance_o2_slot(doc, complete_subslots)
            elif slot_name == 'C1':
                enhanced = self._enhance_c1_slot(doc, complete_subslots)
            elif slot_name == 'C2':
                enhanced = self._enhance_c2_slot(doc, complete_subslots)
            elif slot_name == 'M1':
                enhanced = self._enhance_m1_slot(doc, complete_subslots)
            elif slot_name == 'M2':
                enhanced = self._enhance_m2_slot(doc, complete_subslots)
            elif slot_name == 'M3':
                enhanced = self._enhance_m3_slot(doc, complete_subslots)
            else:
                enhanced = complete_subslots
            
            print(f"ğŸ”§ å¼·åŒ–å¾Œã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ•°: {len(enhanced)}")
            return enhanced
            
        except Exception as e:
            print(f"âŒ {slot_name}ã‚¹ãƒ­ãƒƒãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            traceback.print_exc()
            return {}
    
    # ================================================================
    # ğŸš€ åŒ…æ‹¬çš„ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ï¼ˆå¼·åŒ–ç‰ˆï¼‰
    # ================================================================
    
    def _comprehensive_subslot_detection(self, doc):
        """åŒ…æ‹¬çš„ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºï¼ˆå…¨ã‚¹ãƒ­ãƒƒãƒˆå¯¾å¿œå¼·åŒ–ç‰ˆï¼‰"""
        subslots = {}
        used_tokens = set()
        
        # 1. ä¸»èªãƒ»å‹•è©ãƒ»ç›®çš„èªãƒ»è£œèªã®åŸºæœ¬æ¤œå‡ºï¼ˆå¼·åŒ–ï¼‰
        basic_slots = self._enhanced_basic_detection(doc, used_tokens)
        subslots.update(basic_slots)
        
        # 2. O1O2æ§‹é€ æ¤œå‡ºï¼ˆç¬¬4æ–‡å‹ï¼‰
        o1o2_result = self._enhanced_o1o2_detection(doc, used_tokens)
        if o1o2_result:
            subslots.update(o1o2_result)
        
        # 3. SVOCæ§‹é€ æ¤œå‡ºï¼ˆç¬¬5æ–‡å‹ï¼‰
        svoc_result = self._enhanced_svoc_detection(doc, used_tokens)
        if svoc_result:
            subslots.update(svoc_result)
        
        # 4. è£œèªæ§‹é€ ã®è©³ç´°æ¤œå‡ºï¼ˆå¼·åŒ–ï¼‰
        complement_result = self._enhanced_complement_detection(doc, used_tokens)
        subslots.update(complement_result)
        
        # 5. ä¿®é£¾èªã®åŒ…æ‹¬æ¤œå‡ºï¼ˆå¼·åŒ–ï¼‰
        modifier_result = self._enhanced_modifier_detection(doc, used_tokens)
        subslots.update(modifier_result)
        
        # 6. åŠ©å‹•è©ã®åŒ…æ‹¬æ¤œå‡ºï¼ˆå¼·åŒ–ï¼‰
        aux_result = self._enhanced_auxiliary_detection(doc, used_tokens)
        subslots.update(aux_result)
        
        return subslots
    
    def _enhanced_basic_detection(self, doc, used_tokens):
        """å¼·åŒ–ç‰ˆåŸºæœ¬è¦ç´ æ¤œå‡º"""
        subslots = {}
        
        # ä¸»èªæ¤œå‡ºï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
        subjects_found = []
        for token in doc:
            if token.dep_ in ["nsubj", "nsubjpass", "csubj"] and token.i not in used_tokens:
                subjects_found.append(token)
        
        # é–¢ä¿‚ä»£åè©ç¯€å†…ã®ä¸»èªã‚‚æ¤œå‡º
        for token in doc:
            if token.dep_ == "relcl":  # é–¢ä¿‚ä»£åè©ç¯€
                for child in token.children:
                    if child.dep_ in ["nsubj", "nsubjpass"] and child.i not in used_tokens:
                        subjects_found.append(child)
        
        if subjects_found:
            # æœ€åˆã®ä¸»èªã‚’æ¡ç”¨
            subject = subjects_found[0]
            subject_phrase = self._extract_enhanced_phrase(subject, doc)
            subslots['sub-s'] = {
                'text': subject_phrase,
                'tokens': self._get_phrase_tokens(subject, doc),
                'token_indices': self._get_phrase_indices(subject, doc)
            }
            self._mark_tokens_used(subject, doc, used_tokens)
            print(f"âœ… å¼·åŒ–ç‰ˆä¸»èªæ¤œå‡º: '{subject_phrase}'")
        
        # å‹•è©æ¤œå‡ºï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
        verbs_found = []
        for token in doc:
            if token.pos_ == "VERB" and token.dep_ in ["ROOT", "conj", "relcl", "xcomp", "ccomp"] and token.i not in used_tokens:
                verbs_found.append(token)
        
        if verbs_found:
            # æœ€åˆã®å‹•è©ã‚’æ¡ç”¨
            verb = verbs_found[0]
            subslots['sub-v'] = {
                'text': verb.text,
                'tokens': [verb.text],
                'token_indices': [verb.i]
            }
            used_tokens.add(verb.i)
            print(f"âœ… å¼·åŒ–ç‰ˆå‹•è©æ¤œå‡º: '{verb.text}'")
        
        return subslots
    
    def _enhanced_o1o2_detection(self, doc, used_tokens):
        """å¼·åŒ–ç‰ˆO1O2æ§‹é€ æ¤œå‡º"""
        subslots = {}
        
        # äºŒé‡ç›®çš„èªå‹•è©ã®æ‹¡å¼µãƒªã‚¹ãƒˆ
        ditransitive_verbs = {
            'give', 'send', 'show', 'tell', 'buy', 'make', 'teach', 'offer',
            'bring', 'get', 'find', 'leave', 'pay', 'sell', 'cook', 'build'
        }
        
        verb_token = None
        for token in doc:
            if token.lemma_.lower() in ditransitive_verbs or token.dep_ == "ROOT":
                verb_token = token
                break
        
        if not verb_token:
            return subslots
        
        # é–“æ¥ç›®çš„èªã¨ç›´æ¥ç›®çš„èªã®æ¤œå‡ºï¼ˆæ‹¡å¼µç‰ˆï¼‰
        indirect_obj = None
        direct_obj = None
        
        for child in verb_token.children:
            if child.dep_ == "dobj" and child.i not in used_tokens:
                direct_obj = child
            elif child.dep_ in ["dative", "iobj"] and child.i not in used_tokens:
                indirect_obj = child
            # å‰ç½®è©å¥ã‹ã‚‰ã®é–“æ¥ç›®çš„èªæ¤œå‡º
            elif child.dep_ == "prep" and child.text.lower() in ['to', 'for']:
                for grandchild in child.children:
                    if grandchild.dep_ == "pobj" and indirect_obj is None:
                        indirect_obj = grandchild
        
        if indirect_obj and direct_obj:
            # sub-o1: é–“æ¥ç›®çš„èª
            subslots['sub-o1'] = {
                'text': indirect_obj.text,
                'tokens': [indirect_obj.text],
                'token_indices': [indirect_obj.i]
            }
            used_tokens.add(indirect_obj.i)
            
            # sub-o2: ç›´æ¥ç›®çš„èª
            direct_phrase = self._extract_enhanced_phrase(direct_obj, doc)
            subslots['sub-o2'] = {
                'text': direct_phrase,
                'tokens': self._get_phrase_tokens(direct_obj, doc),
                'token_indices': self._get_phrase_indices(direct_obj, doc)
            }
            self._mark_tokens_used(direct_obj, doc, used_tokens)
            
            print(f"âœ… å¼·åŒ–ç‰ˆO1O2æ¤œå‡º: o1='{indirect_obj.text}', o2='{direct_phrase}'")
        
        return subslots
    
    def _enhanced_svoc_detection(self, doc, used_tokens):
        """å¼·åŒ–ç‰ˆSVOCæ§‹é€ æ¤œå‡º"""
        subslots = {}
        
        # çŸ¥è¦šå‹•è©ãƒ»ä½¿å½¹å‹•è©ã®æ‹¡å¼µ
        perception_causative_verbs = {
            'see', 'watch', 'hear', 'feel', 'make', 'let', 'have',
            'observe', 'notice', 'find', 'consider', 'think', 'believe'
        }
        
        verb_token = None
        for token in doc:
            if token.lemma_.lower() in perception_causative_verbs:
                verb_token = token
                break
        
        if not verb_token:
            return subslots
        
        object_token = None
        complement_token = None
        
        for child in verb_token.children:
            if child.dep_ == "dobj" and child.i not in used_tokens:
                object_token = child
            elif child.dep_ in ["xcomp", "ccomp", "acomp"] and child.i not in used_tokens:
                complement_token = child
        
        if object_token and complement_token:
            # sub-o1: ç›®çš„èª
            subslots['sub-o1'] = {
                'text': object_token.text,
                'tokens': [object_token.text],
                'token_indices': [object_token.i]
            }
            used_tokens.add(object_token.i)
            
            # sub-c1: è£œèª
            complement_phrase = self._extract_enhanced_phrase(complement_token, doc)
            subslots['sub-c1'] = {
                'text': complement_phrase,
                'tokens': self._get_phrase_tokens(complement_token, doc),
                'token_indices': self._get_phrase_indices(complement_token, doc)
            }
            self._mark_tokens_used(complement_token, doc, used_tokens)
            
            print(f"âœ… å¼·åŒ–ç‰ˆSVOCæ¤œå‡º: obj='{object_token.text}', comp='{complement_phrase}'")
        
        return subslots
    
    def _enhanced_complement_detection(self, doc, used_tokens):
        """å¼·åŒ–ç‰ˆè£œèªæ¤œå‡º"""
        subslots = {}
        
        complement_count = 1  # C1, C2é †åºç®¡ç†
        
        for token in doc:
            if token.i in used_tokens:
                continue
            
            # beå‹•è©ã®è£œèª
            if token.dep_ in ["acomp", "attr", "pcomp"]:
                if complement_count <= 2:
                    slot_key = f"sub-c{complement_count}"
                    phrase = self._extract_enhanced_phrase(token, doc)
                    
                    subslots[slot_key] = {
                        'text': phrase,
                        'tokens': self._get_phrase_tokens(token, doc),
                        'token_indices': self._get_phrase_indices(token, doc)
                    }
                    
                    self._mark_tokens_used(token, doc, used_tokens)
                    complement_count += 1
                    print(f"âœ… å¼·åŒ–ç‰ˆè£œèªæ¤œå‡º: {slot_key}='{phrase}'")
        
        return subslots
    
    def _enhanced_modifier_detection(self, doc, used_tokens):
        """å¼·åŒ–ç‰ˆä¿®é£¾èªæ¤œå‡º"""
        subslots = {}
        modifier_count = 1
        
        # ä¿®é£¾èªã®å„ªå…ˆé †ä½ä»˜ã‘
        modifier_priorities = {
            'advmod': 1,    # å‰¯è©ä¿®é£¾
            'amod': 2,      # å½¢å®¹è©ä¿®é£¾  
            'prep': 3,      # å‰ç½®è©å¥
            'npadvmod': 4,  # åè©å¥å‰¯è©çš„ä¿®é£¾
            'compound': 5   # è¤‡åˆèª
        }
        
        # ä¾å­˜é–¢ä¿‚ã§ã‚½ãƒ¼ãƒˆ
        potential_modifiers = []
        for token in doc:
            if token.i in used_tokens:
                continue
            if token.dep_ in modifier_priorities or token.pos_ in ["ADV", "ADJ"]:
                priority = modifier_priorities.get(token.dep_, 10)
                potential_modifiers.append((priority, token))
        
        potential_modifiers.sort(key=lambda x: x[0])  # å„ªå…ˆåº¦é †
        
        for priority, token in potential_modifiers:
            if modifier_count > 3:  # M1, M2, M3ã¾ã§
                break
            
            slot_key = f"sub-m{modifier_count}"
            phrase = self._extract_enhanced_phrase(token, doc)
            
            subslots[slot_key] = {
                'text': phrase,
                'tokens': self._get_phrase_tokens(token, doc),
                'token_indices': self._get_phrase_indices(token, doc)
            }
            
            self._mark_tokens_used(token, doc, used_tokens)
            modifier_count += 1
            print(f"âœ… å¼·åŒ–ç‰ˆä¿®é£¾èªæ¤œå‡º: {slot_key}='{phrase}'")
        
        return subslots
    
    def _enhanced_auxiliary_detection(self, doc, used_tokens):
        """å¼·åŒ–ç‰ˆåŠ©å‹•è©æ¤œå‡º"""
        subslots = {}
        
        # åŠ©å‹•è©ã®æ‹¡å¼µæ¤œå‡º
        aux_tokens = []
        for token in doc:
            if token.i in used_tokens:
                continue
            
            if token.dep_ in ["aux", "auxpass"] or token.pos_ == "AUX":
                aux_tokens.append(token)
            elif token.dep_ == "aux" and token.pos_ == "PART":  # toä¸å®šè©
                aux_tokens.append(token)
        
        if aux_tokens:
            aux_text = ' '.join([token.text for token in aux_tokens])
            subslots['sub-aux'] = {
                'text': aux_text,
                'tokens': [token.text for token in aux_tokens],
                'token_indices': [token.i for token in aux_tokens]
            }
            for token in aux_tokens:
                used_tokens.add(token.i)
            print(f"âœ… å¼·åŒ–ç‰ˆåŠ©å‹•è©æ¤œå‡º: '{aux_text}'")
        
        return subslots
    
    # ================================================================
    # ğŸ¯ ã‚¹ãƒ­ãƒƒãƒˆå°‚ç”¨å¼·åŒ–ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤
    # ================================================================
    
    def _enhance_s_slot(self, doc, subslots):
        """Så°‚ç”¨å¼·åŒ–: ä¸»èªç‰¹åŒ–"""
        enhanced = subslots.copy()
        
        # é–¢ä¿‚ä»£åè©ç¯€ã®ä¸»èªæ¤œå‡ºå¼·åŒ–
        for token in doc:
            if token.text.lower() in ['who', 'which', 'that'] and 'sub-s' not in enhanced:
                enhanced['sub-s'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"âœ… Så¼·åŒ–: é–¢ä¿‚ä»£åè©ä¸»èª '{token.text}'")
                break
        
        return enhanced
    
    def _enhance_o1_slot(self, doc, subslots):
        """O1å°‚ç”¨å¼·åŒ–: O1å®Œæˆã‚·ã‚¹ãƒ†ãƒ æ´»ç”¨"""
        enhanced = subslots.copy()
        # O1ã¯æ—¢ã«å®Œæˆæ¸ˆã¿ã®ãŸã‚ã€ç¾è¡Œã‚·ã‚¹ãƒ†ãƒ ç¶­æŒ
        print(f"ğŸ”„ O1å®Œæˆã‚·ã‚¹ãƒ†ãƒ æ´»ç”¨: {len(enhanced)}ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆ")
        return enhanced
    
    def _enhance_o2_slot(self, doc, subslots):
        """O2å°‚ç”¨å¼·åŒ–: é–“æ¥ç›®çš„èªç‰¹åŒ–"""
        enhanced = subslots.copy()
        
        # to/forå¥ã®å¼·åˆ¶çš„O2æ¤œå‡º
        for token in doc:
            if token.text.lower() in ['to', 'for'] and token.dep_ == "prep":
                for child in token.children:
                    if child.dep_ == "pobj" and 'sub-o2' not in enhanced:
                        enhanced['sub-o2'] = {
                            'text': child.text,
                            'tokens': [child.text],
                            'token_indices': [child.i]
                        }
                        print(f"âœ… O2å¼·åŒ–: {token.text}å¥ç›®çš„èª '{child.text}'")
                        break
        
        return enhanced
    
    def _enhance_c1_slot(self, doc, subslots):
        """C1å°‚ç”¨å¼·åŒ–: ç¬¬ä¸€è£œèªç‰¹åŒ–"""
        enhanced = subslots.copy()
        
        # å½¢å®¹è©è£œèªã®å¼·åˆ¶C1å‰²ã‚Šå½“ã¦
        for token in doc:
            if token.pos_ == "ADJ" and token.dep_ in ["ROOT", "acomp"] and 'sub-c1' not in enhanced:
                enhanced['sub-c1'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"âœ… C1å¼·åŒ–: å½¢å®¹è©è£œèª '{token.text}'")
                break
        
        return enhanced
    
    def _enhance_c2_slot(self, doc, subslots):
        """C2å°‚ç”¨å¼·åŒ–: ç¬¬äºŒè£œèªç‰¹åŒ–"""
        enhanced = subslots.copy()
        
        # åè©è£œèªã®å¼·åˆ¶C2å‰²ã‚Šå½“ã¦
        for token in doc:
            if token.pos_ == "NOUN" and token.dep_ in ["attr", "pcomp"] and 'sub-c2' not in enhanced:
                enhanced['sub-c2'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"âœ… C2å¼·åŒ–: åè©è£œèª '{token.text}'")
                break
        
        return enhanced
    
    def _enhance_m1_slot(self, doc, subslots):
        """M1å°‚ç”¨å¼·åŒ–: ç¬¬ä¸€ä¿®é£¾èªç‰¹åŒ–"""
        enhanced = subslots.copy()
        
        # å‰ç½®è©å¥ã®å¼·åˆ¶M1æ¤œå‡º
        for token in doc:
            if token.dep_ == "prep" and 'sub-m1' not in enhanced:
                prep_phrase = self._extract_enhanced_phrase(token, doc)
                enhanced['sub-m1'] = {
                    'text': prep_phrase,
                    'tokens': self._get_phrase_tokens(token, doc),
                    'token_indices': self._get_phrase_indices(token, doc)
                }
                print(f"âœ… M1å¼·åŒ–: å‰ç½®è©å¥ '{prep_phrase}'")
                break
        
        return enhanced
    
    def _enhance_m2_slot(self, doc, subslots):
        """M2å°‚ç”¨å¼·åŒ–: ç¬¬äºŒä¿®é£¾èªç‰¹åŒ–"""
        enhanced = subslots.copy()
        
        # å‰¯è©ã®å¼·åˆ¶M2æ¤œå‡º
        for token in doc:
            if token.pos_ == "ADV" and token.dep_ == "advmod" and 'sub-m2' not in enhanced:
                enhanced['sub-m2'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"âœ… M2å¼·åŒ–: å‰¯è© '{token.text}'")
                break
        
        return enhanced
    
    def _enhance_m3_slot(self, doc, subslots):
        """M3å°‚ç”¨å¼·åŒ–: ç¬¬ä¸‰ä¿®é£¾èªç‰¹åŒ–"""
        enhanced = subslots.copy()
        
        # æ™‚é–“ãƒ»å ´æ‰€å‰¯è©ã®å¼·åˆ¶M3æ¤œå‡º
        time_place_advs = {'today', 'yesterday', 'tomorrow', 'here', 'there', 'now', 'then'}
        for token in doc:
            if token.text.lower() in time_place_advs and 'sub-m3' not in enhanced:
                enhanced['sub-m3'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"âœ… M3å¼·åŒ–: æ™‚é–“å ´æ‰€å‰¯è© '{token.text}'")
                break
        
        return enhanced
    
    # ================================================================
    # ğŸ› ï¸ ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    # ================================================================
    
    def _extract_enhanced_phrase(self, token, doc):
        """å¼·åŒ–ç‰ˆå¥æŠ½å‡º"""
        phrase_tokens = [token]
        
        # å­ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å«ã‚ã‚‹
        for child in token.children:
            if child.dep_ in ["det", "amod", "compound", "pobj", "prep"]:
                phrase_tokens.append(child)
                # å‰ç½®è©å¥ã®å ´åˆã€ãã®ç›®çš„èªã‚‚
                if child.dep_ == "prep":
                    for grandchild in child.children:
                        if grandchild.dep_ == "pobj":
                            phrase_tokens.append(grandchild)
        
        # èªé †ã§ã‚½ãƒ¼ãƒˆ
        phrase_tokens.sort(key=lambda t: t.i)
        return ' '.join([t.text for t in phrase_tokens])
    
    def _get_phrase_tokens(self, token, doc):
        """å¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆå–å¾—"""
        phrase_tokens = [token]
        for child in token.children:
            if child.dep_ in ["det", "amod", "compound", "pobj", "prep"]:
                phrase_tokens.append(child)
        phrase_tokens.sort(key=lambda t: t.i)
        return [t.text for t in phrase_tokens]
    
    def _get_phrase_indices(self, token, doc):
        """å¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚¹ãƒˆå–å¾—"""
        phrase_tokens = [token]
        for child in token.children:
            if child.dep_ in ["det", "amod", "compound", "pobj", "prep"]:
                phrase_tokens.append(child)
        phrase_tokens.sort(key=lambda t: t.i)
        return [t.i for t in phrase_tokens]
    
    def _mark_tokens_used(self, token, doc, used_tokens):
        """å¥å…¨ä½“ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨æ¸ˆã¿ãƒãƒ¼ã‚¯"""
        used_tokens.add(token.i)
        for child in token.children:
            if child.dep_ in ["det", "amod", "compound", "pobj", "prep"]:
                used_tokens.add(child.i)

def test_enhanced_system():
    """å¼·åŒ–ç‰ˆã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª Enhanced Universal System ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("ğŸ¯ ç›®æ¨™: å…¨ã‚¹ãƒ­ãƒƒãƒˆ70%ä»¥ä¸Šã®ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ´»ç”¨")
    print("=" * 80)
    
    generator = EnhancedUniversalSubslotGenerator()
    
    # å³é¸ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
    critical_tests = {
        "S": ["The very intelligent student who was studying"],
        "O1": ["that he is definitely studying English very hard today"],
        "O2": ["to his very kind mother who lives in Tokyo"],
        "C1": ["extremely happy and excited about the news"],
        "C2": ["a very successful businessman who works here"],
        "M1": ["very carefully in the morning with great skill"],
        "M2": ["always working diligently until late"],
        "M3": ["under the bridge that was built yesterday"]
    }
    
    results = {}
    
    for slot_name, test_sentences in critical_tests.items():
        print(f"\n{'='*50}")
        print(f"ğŸ¯ {slot_name}ã‚¹ãƒ­ãƒƒãƒˆå¼·åŒ–ãƒ†ã‚¹ãƒˆ")
        
        for sentence in test_sentences:
            print(f"ğŸ“ ãƒ†ã‚¹ãƒˆ: '{sentence}'")
            subslots = generator.generate_subslots_for_slot(slot_name, sentence)
            
            print(f"ğŸ“Š æ¤œå‡ºã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ•°: {len(subslots)}")
            for sub_type, sub_data in subslots.items():
                print(f"   âœ… {sub_type}: '{sub_data['text']}'")
            
            utilization_rate = (len(subslots) / 10) * 100
            print(f"ğŸ¯ æ´»ç”¨ç‡: {len(subslots)}/10 ({utilization_rate:.1f}%)")
            
            results[f"{slot_name}: {sentence}"] = {
                'subslots': subslots,
                'count': len(subslots),
                'rate': utilization_rate
            }
    
    # å…¨ä½“çµ±è¨ˆ
    print(f"\n{'='*80}")
    print("ğŸ‰ å¼·åŒ–ç‰ˆãƒ†ã‚¹ãƒˆå®Œäº†")
    
    total_slots = sum([r['count'] for r in results.values()])
    total_possible = len(results) * 10
    overall_rate = (total_slots / total_possible) * 100
    
    print(f"ğŸ¯ å…¨ä½“æ´»ç”¨ç‡: {total_slots}/{total_possible} ({overall_rate:.1f}%)")
    
    success_tests = [k for k, v in results.items() if v['rate'] >= 70]
    print(f"ğŸ‰ æˆåŠŸãƒ†ã‚¹ãƒˆ (70%ä»¥ä¸Š): {len(success_tests)}/{len(results)}")
    
    if len(success_tests) >= 6:  # 8ä¸­6ä»¥ä¸Šã§æˆåŠŸ
        print("ğŸŠ å¼·åŒ–ç‰ˆã‚·ã‚¹ãƒ†ãƒ æˆåŠŸï¼å¤§éƒ¨åˆ†ã®ã‚¹ãƒ­ãƒƒãƒˆã§é«˜æ´»ç”¨ç‡é”æˆï¼")
    else:
        print("âš ï¸  æ›´ãªã‚‹å¼·åŒ–ãŒå¿…è¦")
    
    return results

if __name__ == "__main__":
    test_enhanced_system()
