#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Step 14: å…¨ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆå¯¾å¿œ 10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 

O1ã‚¹ãƒ­ãƒƒãƒˆã§100%é”æˆã—ãŸ10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ ã‚’ã€
å…¨ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆï¼ˆS, O1, O2, C1, C2, M1, M2, M3ï¼‰ã«é©ç”¨

å¯¾è±¡å¤–ã‚¹ãƒ­ãƒƒãƒˆ: Aux, V (å…ƒã‹ã‚‰å˜ä¸€æ©Ÿèƒ½ã®ãŸã‚é™¤å¤–)
"""

import spacy
import json
import traceback
from collections import OrderedDict

class UniversalSubslotGenerator:
    """å…¨ã‚¹ãƒ­ãƒƒãƒˆå¯¾å¿œ 10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        print("ğŸš€ Universal Subslot Generator èµ·å‹•é–‹å§‹...")
        try:
            self.nlp = spacy.load("en_core_web_sm")
            print("âœ… spaCy 'en_core_web_sm' èª­ã¿è¾¼ã¿å®Œäº†")
        except IOError:
            print("âŒ spaCyè‹±èªãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
            print("python -m spacy download en_core_web_sm")
            raise
        
        # 10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå®šç¾©ï¼ˆO1ã§å®Œæˆã—ãŸã‚‚ã®ï¼‰
        self.subslot_types = [
            'sub-s', 'sub-v', 'sub-o1', 'sub-o2', 
            'sub-c1', 'sub-c2', 'sub-m1', 'sub-m2', 'sub-m3', 'sub-aux'
        ]
        
        # å¯¾è±¡ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆ
        self.target_slots = ['S', 'O1', 'O2', 'C1', 'C2', 'M1', 'M2', 'M3']
        
        print(f"ğŸ¯ å¯¾è±¡ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆ: {', '.join(self.target_slots)}")
        print(f"ğŸ”§ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆä½“ç³»: {', '.join(self.subslot_types)}")
        
    def generate_subslots_for_slot(self, slot_name, sentence):
        """æŒ‡å®šã‚¹ãƒ­ãƒƒãƒˆã®10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ"""
        print(f"\nğŸ¯ {slot_name}ã‚¹ãƒ­ãƒƒãƒˆ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆé–‹å§‹: '{sentence}'")
        
        if slot_name not in self.target_slots:
            print(f"âŒ éå¯¾è±¡ã‚¹ãƒ­ãƒƒãƒˆ: {slot_name}")
            return {}
        
        try:
            doc = self.nlp(sentence)
            print(f"ğŸ“ è§£æå¯¾è±¡: {[(token.text, token.dep_, token.pos_) for token in doc]}")
            
            # ã‚¹ãƒ­ãƒƒãƒˆåˆ¥å°‚ç”¨å‡¦ç†
            if slot_name == 'O1':
                return self._generate_o1_subslots(doc, sentence)
            elif slot_name == 'S':
                return self._generate_s_subslots(doc, sentence)
            elif slot_name == 'O2':
                return self._generate_o2_subslots(doc, sentence)
            elif slot_name == 'C1':
                return self._generate_c1_subslots(doc, sentence)
            elif slot_name == 'C2':
                return self._generate_c2_subslots(doc, sentence)
            elif slot_name == 'M1':
                return self._generate_m1_subslots(doc, sentence)
            elif slot_name == 'M2':
                return self._generate_m2_subslots(doc, sentence)
            elif slot_name == 'M3':
                return self._generate_m3_subslots(doc, sentence)
            
        except Exception as e:
            print(f"âŒ {slot_name}ã‚¹ãƒ­ãƒƒãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            traceback.print_exc()
            return {}
    
    def _generate_o1_subslots(self, doc, sentence):
        """O1ã‚¹ãƒ­ãƒƒãƒˆç”¨ï¼ˆã™ã§ã«å®Œæˆæ¸ˆã¿ã‚·ã‚¹ãƒ†ãƒ ï¼‰"""
        print("ğŸ”„ O1å®Œæˆæ¸ˆã¿ã‚·ã‚¹ãƒ†ãƒ ä½¿ç”¨")
        return self._detect_all_subslots(doc)
    
    def _generate_s_subslots(self, doc, sentence):
        """Sã‚¹ãƒ­ãƒƒãƒˆç”¨ 10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ"""
        print("ğŸ¯ S(Subject)ã‚¹ãƒ­ãƒƒãƒˆå°‚ç”¨å‡¦ç†")
        subslots = {}
        
        # å®Œå…¨10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºé©ç”¨
        complete_subslots = self._detect_all_subslots(doc)
        subslots.update(complete_subslots)
        
        # Så°‚ç”¨å¼·åŒ–: ä¸»èªç‰¹æœ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
        self._enhance_subject_patterns(doc, subslots)
        
        return subslots
    
    def _generate_o2_subslots(self, doc, sentence):
        """O2ã‚¹ãƒ­ãƒƒãƒˆç”¨ 10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ"""
        print("ğŸ¯ O2(Object2)ã‚¹ãƒ­ãƒƒãƒˆå°‚ç”¨å‡¦ç†")
        subslots = {}
        
        # å®Œå…¨10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºé©ç”¨
        complete_subslots = self._detect_all_subslots(doc)
        subslots.update(complete_subslots)
        
        # O2å°‚ç”¨å¼·åŒ–: é–“æ¥ç›®çš„èªç‰¹æœ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
        self._enhance_indirect_object_patterns(doc, subslots)
        
        return subslots
    
    def _generate_c1_subslots(self, doc, sentence):
        """C1ã‚¹ãƒ­ãƒƒãƒˆç”¨ 10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ"""
        print("ğŸ¯ C1(Complement1)ã‚¹ãƒ­ãƒƒãƒˆå°‚ç”¨å‡¦ç†")
        subslots = {}
        
        # å®Œå…¨10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºé©ç”¨
        complete_subslots = self._detect_all_subslots(doc)
        subslots.update(complete_subslots)
        
        # C1å°‚ç”¨å¼·åŒ–: ç¬¬ä¸€è£œèªç‰¹æœ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
        self._enhance_complement_patterns(doc, subslots, complement_type="C1")
        
        return subslots
    
    def _generate_c2_subslots(self, doc, sentence):
        """C2ã‚¹ãƒ­ãƒƒãƒˆç”¨ 10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ"""
        print("ğŸ¯ C2(Complement2)ã‚¹ãƒ­ãƒƒãƒˆå°‚ç”¨å‡¦ç†")
        subslots = {}
        
        # å®Œå…¨10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºé©ç”¨
        complete_subslots = self._detect_all_subslots(doc)
        subslots.update(complete_subslots)
        
        # C2å°‚ç”¨å¼·åŒ–: ç¬¬äºŒè£œèªç‰¹æœ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
        self._enhance_complement_patterns(doc, subslots, complement_type="C2")
        
        return subslots
    
    def _generate_m1_subslots(self, doc, sentence):
        """M1ã‚¹ãƒ­ãƒƒãƒˆç”¨ 10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ"""
        print("ğŸ¯ M1(Modifier1)ã‚¹ãƒ­ãƒƒãƒˆå°‚ç”¨å‡¦ç†")
        subslots = {}
        
        # å®Œå…¨10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºé©ç”¨
        complete_subslots = self._detect_all_subslots(doc)
        subslots.update(complete_subslots)
        
        # M1å°‚ç”¨å¼·åŒ–: ä¿®é£¾èªç‰¹æœ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
        self._enhance_modifier_patterns(doc, subslots, modifier_type="M1")
        
        return subslots
    
    def _generate_m2_subslots(self, doc, sentence):
        """M2ã‚¹ãƒ­ãƒƒãƒˆç”¨ 10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ"""
        print("ğŸ¯ M2(Modifier2)ã‚¹ãƒ­ãƒƒãƒˆå°‚ç”¨å‡¦ç†")
        subslots = {}
        
        # å®Œå…¨10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºé©ç”¨
        complete_subslots = self._detect_all_subslots(doc)
        subslots.update(complete_subslots)
        
        # M2å°‚ç”¨å¼·åŒ–: ä¿®é£¾èªç‰¹æœ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
        self._enhance_modifier_patterns(doc, subslots, modifier_type="M2")
        
        return subslots
    
    def _generate_m3_subslots(self, doc, sentence):
        """M3ã‚¹ãƒ­ãƒƒãƒˆç”¨ 10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ"""
        print("ğŸ¯ M3(Modifier3)ã‚¹ãƒ­ãƒƒãƒˆå°‚ç”¨å‡¦ç†")
        subslots = {}
        
        # å®Œå…¨10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºé©ç”¨
        complete_subslots = self._detect_all_subslots(doc)
        subslots.update(complete_subslots)
        
        # M3å°‚ç”¨å¼·åŒ–: ä¿®é£¾èªç‰¹æœ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
        self._enhance_modifier_patterns(doc, subslots, modifier_type="M3")
        
        return subslots
    
    # ================================================================
    # ğŸš€ å®Œå…¨10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ï¼ˆO1å®Œæˆç‰ˆãƒ™ãƒ¼ã‚¹ï¼‰
    # ================================================================
    
    def _detect_all_subslots(self, doc):
        """å®Œå…¨10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºï¼ˆO1å®Œæˆã‚·ã‚¹ãƒ†ãƒ ã‚ˆã‚Šç§»æ¤ï¼‰"""
        subslots = {}
        used_tokens = set()  # ãƒˆãƒ¼ã‚¯ãƒ³é‡è¤‡é˜²æ­¢
        
        # 1. O1O2æ§‹é€ æ¤œå‡ºï¼ˆç¬¬4æ–‡å‹å¯¾å¿œï¼‰
        o1o2_result = self._detect_o1o2_structure(doc, used_tokens)
        if o1o2_result:
            subslots.update(o1o2_result)
        
        # 2. SVOCæ§‹é€ æ¤œå‡ºï¼ˆç¬¬5æ–‡å‹å¯¾å¿œï¼‰
        svoc_result = self._detect_svoc_structure(doc, used_tokens)
        if svoc_result:
            subslots.update(svoc_result)
        
        # 3. åŸºæœ¬ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡º
        basic_slots = self._detect_basic_slots(doc, used_tokens)
        subslots.update(basic_slots)
        
        # 4. ä¿®é£¾èªæ¤œå‡ºï¼ˆä½ç½®ãƒ™ãƒ¼ã‚¹é…ç½®ï¼‰
        modifier_slots = self._detect_modifier_slots(doc, used_tokens)
        subslots.update(modifier_slots)
        
        # 5. åŠ©å‹•è©æ¤œå‡º
        aux_slots = self._detect_auxiliary_verbs(doc, used_tokens)
        subslots.update(aux_slots)
        
        return subslots
    
    def _detect_o1o2_structure(self, doc, used_tokens):
        """O1O2æ§‹é€ æ¤œå‡ºï¼ˆgive him a book ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰"""
        subslots = {}
        
        # äºŒé‡ç›®çš„èªå‹•è©æ¤œå‡º
        ditransitive_verbs = {'give', 'send', 'show', 'tell', 'buy', 'make', 'teach', 'offer'}
        verb_token = None
        
        for token in doc:
            if token.lemma_.lower() in ditransitive_verbs:
                verb_token = token
                break
        
        if not verb_token:
            return subslots
        
        # é–“æ¥ç›®çš„èªï¼ˆsub-o1ï¼‰ã¨ç›´æ¥ç›®çš„èªï¼ˆsub-o2ï¼‰ã®æ¤œå‡º
        indirect_obj = None  # him
        direct_obj = None    # a book
        
        for child in verb_token.children:
            if child.dep_ == "dobj" and child.i not in used_tokens:
                direct_obj = child
            elif child.dep_ == "dative" and child.i not in used_tokens:
                indirect_obj = child
            elif child.dep_ == "pobj" and child.head.dep_ == "prep" and child.i not in used_tokens:
                if indirect_obj is None:
                    indirect_obj = child
        
        if indirect_obj and direct_obj:
            # sub-o1: é–“æ¥ç›®çš„èª
            subslots['sub-o1'] = {
                'text': indirect_obj.text,
                'tokens': [indirect_obj.text],
                'token_indices': [indirect_obj.i]
            }
            used_tokens.add(indirect_obj.i)
            
            # sub-o2: ç›´æ¥ç›®çš„èª
            direct_obj_phrase = self._extract_phrase(direct_obj)
            subslots['sub-o2'] = {
                'text': direct_obj_phrase,
                'tokens': [token.text for token in doc if token.head == direct_obj or token == direct_obj],
                'token_indices': [token.i for token in doc if token.head == direct_obj or token == direct_obj]
            }
            for token in doc:
                if token.head == direct_obj or token == direct_obj:
                    used_tokens.add(token.i)
            
            print(f"âœ… O1O2æ§‹é€ æ¤œå‡º: him={indirect_obj.text}, book={direct_obj_phrase}")
        
        return subslots
    
    def _detect_svoc_structure(self, doc, used_tokens):
        """SVOCæ§‹é€ æ¤œå‡ºï¼ˆI saw her cry ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰"""
        subslots = {}
        
        # çŸ¥è¦šå‹•è©ãƒ»ä½¿å½¹å‹•è©
        perception_causative_verbs = {'see', 'watch', 'hear', 'feel', 'make', 'let', 'have'}
        verb_token = None
        
        for token in doc:
            if token.lemma_.lower() in perception_causative_verbs:
                verb_token = token
                break
        
        if not verb_token:
            return subslots
        
        # ç›®çš„èªã¨è£œèªã®æ¤œå‡º
        object_token = None
        complement_token = None
        
        for child in verb_token.children:
            if child.dep_ == "dobj" and child.i not in used_tokens:
                object_token = child
            elif child.dep_ in ["xcomp", "ccomp"] and child.i not in used_tokens:
                complement_token = child
        
        if object_token and complement_token:
            # sub-o1: ç›®çš„èª
            subslots['sub-o1'] = {
                'text': object_token.text,
                'tokens': [object_token.text],
                'token_indices': [object_token.i]
            }
            used_tokens.add(object_token.i)
            
            # sub-c1: è£œèª
            complement_phrase = self._extract_phrase(complement_token)
            subslots['sub-c1'] = {
                'text': complement_phrase,
                'tokens': [token.text for token in doc if token.head == complement_token or token == complement_token],
                'token_indices': [token.i for token in doc if token.head == complement_token or token == complement_token]
            }
            for token in doc:
                if token.head == complement_token or token == complement_token:
                    used_tokens.add(token.i)
            
            print(f"âœ… SVOCæ§‹é€ æ¤œå‡º: obj={object_token.text}, comp={complement_phrase}")
        
        return subslots
    
    def _detect_basic_slots(self, doc, used_tokens):
        """åŸºæœ¬ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºï¼ˆS, Vï¼‰"""
        subslots = {}
        
        # ä¸»èªæ¤œå‡º
        for token in doc:
            if token.dep_ == "nsubj" and token.i not in used_tokens:
                subject_phrase = self._extract_phrase(token)
                subslots['sub-s'] = {
                    'text': subject_phrase,
                    'tokens': [t.text for t in doc if t.head == token or t == token],
                    'token_indices': [t.i for t in doc if t.head == token or t == token]
                }
                for t in doc:
                    if t.head == token or t == token:
                        used_tokens.add(t.i)
                break
        
        # å‹•è©æ¤œå‡º
        for token in doc:
            if token.pos_ == "VERB" and token.dep_ in ["ROOT", "conj"] and token.i not in used_tokens:
                subslots['sub-v'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                used_tokens.add(token.i)
                break
        
        return subslots
    
    def _detect_modifier_slots(self, doc, used_tokens):
        """ä¿®é£¾èªã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºï¼ˆä½ç½®ãƒ™ãƒ¼ã‚¹é…ç½®ï¼‰"""
        subslots = {}
        modifier_count = 1
        
        for token in doc:
            if token.i in used_tokens:
                continue
            
            # ä¿®é£¾èªå€™è£œ
            if token.dep_ in ["prep", "advmod", "amod", "npadvmod", "compound"] or token.pos_ in ["ADV", "ADJ"]:
                if modifier_count <= 3:  # M1, M2, M3
                    slot_key = f"sub-m{modifier_count}"
                    phrase = self._extract_phrase(token)
                    
                    subslots[slot_key] = {
                        'text': phrase,
                        'tokens': [t.text for t in doc if t.head == token or t == token],
                        'token_indices': [t.i for t in doc if t.head == token or t == token]
                    }
                    
                    for t in doc:
                        if t.head == token or t == token:
                            used_tokens.add(t.i)
                    
                    modifier_count += 1
        
        return subslots
    
    def _detect_auxiliary_verbs(self, doc, used_tokens):
        """åŠ©å‹•è©æ¤œå‡º"""
        subslots = {}
        
        aux_tokens = [token for token in doc if token.dep_ == "aux" and token.i not in used_tokens]
        if aux_tokens:
            aux_text = ' '.join([token.text for token in aux_tokens])
            subslots['sub-aux'] = {
                'text': aux_text,
                'tokens': [token.text for token in aux_tokens],
                'token_indices': [token.i for token in aux_tokens]
            }
            for token in aux_tokens:
                used_tokens.add(token.i)
        
        return subslots
    
    def _extract_phrase(self, token):
        """ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰å¥ã‚’æŠ½å‡º"""
        # ä¾å­˜é–¢ä¿‚ãƒ™ãƒ¼ã‚¹ã®å¥æŠ½å‡º
        phrase_tokens = [token]
        for child in token.children:
            if child.dep_ in ["det", "amod", "compound", "prep", "pobj"]:
                phrase_tokens.append(child)
                # å‰ç½®è©å¥ã®å ´åˆã€ãã®ç›®çš„èªã‚‚å«ã‚ã‚‹
                if child.dep_ == "prep":
                    for grandchild in child.children:
                        if grandchild.dep_ == "pobj":
                            phrase_tokens.append(grandchild)
        
        # èªé †ã§ã‚½ãƒ¼ãƒˆ
        phrase_tokens.sort(key=lambda t: t.i)
        return ' '.join([t.text for t in phrase_tokens])
    
    # ================================================================
    # ğŸ¯ ã‚¹ãƒ­ãƒƒãƒˆå°‚ç”¨å¼·åŒ–ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤
    # ================================================================
    
    def _enhance_subject_patterns(self, doc, subslots):
        """Så°‚ç”¨: ä¸»èªç‰¹æœ‰ãƒ‘ã‚¿ãƒ¼ãƒ³å¼·åŒ–"""
        # é–¢ä¿‚ä»£åè©ç¯€ã®ä¸»èª
        for token in doc:
            if token.dep_ == "relcl":
                subj_in_rel = None
                for child in token.children:
                    if child.dep_ == "nsubj":
                        subj_in_rel = child
                        break
                
                if subj_in_rel and 'sub-s' not in subslots:
                    subslots['sub-s'] = {
                        'text': subj_in_rel.text,
                        'tokens': [subj_in_rel.text],
                        'token_indices': [subj_in_rel.i]
                    }
                    print(f"âœ… Så°‚ç”¨å¼·åŒ–: é–¢ä¿‚ç¯€ä¸»èª '{subj_in_rel.text}'")
    
    def _enhance_indirect_object_patterns(self, doc, subslots):
        """O2å°‚ç”¨: é–“æ¥ç›®çš„èªç‰¹æœ‰ãƒ‘ã‚¿ãƒ¼ãƒ³å¼·åŒ–"""
        # to/forå¥ã®ç›®çš„èª
        for token in doc:
            if token.text.lower() in ['to', 'for'] and token.dep_ == "prep":
                for child in token.children:
                    if child.dep_ == "pobj":
                        if 'sub-o2' not in subslots:
                            subslots['sub-o2'] = {
                                'text': child.text,
                                'tokens': [child.text],
                                'token_indices': [child.i]
                            }
                            print(f"âœ… O2å°‚ç”¨å¼·åŒ–: to/forå¥ '{child.text}'")
    
    def _enhance_complement_patterns(self, doc, subslots, complement_type):
        """C1/C2å°‚ç”¨: è£œèªç‰¹æœ‰ãƒ‘ã‚¿ãƒ¼ãƒ³å¼·åŒ–"""
        # beå‹•è©ã®è£œèª
        for token in doc:
            if token.lemma_ == "be":
                for child in token.children:
                    if child.dep_ in ["acomp", "attr"]:
                        slot_key = 'sub-c1' if complement_type == "C1" else 'sub-c2'
                        if slot_key not in subslots:
                            subslots[slot_key] = {
                                'text': child.text,
                                'tokens': [child.text],
                                'token_indices': [child.i]
                            }
                            print(f"âœ… {complement_type}å°‚ç”¨å¼·åŒ–: beè£œèª '{child.text}'")
    
    def _enhance_modifier_patterns(self, doc, subslots, modifier_type):
        """M1/M2/M3å°‚ç”¨: ä¿®é£¾èªç‰¹æœ‰ãƒ‘ã‚¿ãƒ¼ãƒ³å¼·åŒ–"""
        # æ™‚é–“ãƒ»å ´æ‰€ãƒ»æ§˜æ…‹å‰¯è©ã®ç‰¹åˆ¥å‡¦ç†
        time_advs = {'today', 'yesterday', 'tomorrow', 'now', 'then', 'always', 'never'}
        place_advs = {'here', 'there', 'everywhere', 'nowhere', 'outside', 'inside'}
        manner_advs = {'quickly', 'slowly', 'carefully', 'suddenly', 'quietly'}
        
        for token in doc:
            if token.text.lower() in time_advs | place_advs | manner_advs:
                slot_key = f'sub-{modifier_type.lower()}'
                if slot_key not in subslots:
                    subslots[slot_key] = {
                        'text': token.text,
                        'tokens': [token.text],
                        'token_indices': [token.i]
                    }
                    print(f"âœ… {modifier_type}å°‚ç”¨å¼·åŒ–: å‰¯è© '{token.text}'")

def test_universal_system():
    """å…¨ã‚¹ãƒ­ãƒƒãƒˆå¯¾å¿œã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª Universal Subslot System ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 60)
    
    generator = UniversalSubslotGenerator()
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ï¼ˆå„ã‚¹ãƒ­ãƒƒãƒˆç”¨ï¼‰
    test_cases = [
        # S (Subject) ãƒ†ã‚¹ãƒˆ
        ("S", "The intelligent student"),
        ("S", "My best friend who lives in Tokyo"),
        
        # O1 (å®Œæˆæ¸ˆã¿) ãƒ†ã‚¹ãƒˆ
        ("O1", "the beautiful red car"),
        ("O1", "that he is studying hard"),
        
        # O2 (Object 2) ãƒ†ã‚¹ãƒˆ
        ("O2", "to his mother"),
        ("O2", "for the children"),
        
        # C1 (Complement 1) ãƒ†ã‚¹ãƒˆ
        ("C1", "very happy"),
        ("C1", "a good teacher"),
        
        # C2 (Complement 2) ãƒ†ã‚¹ãƒˆ
        ("C2", "extremely difficult"),
        ("C2", "the best solution"),
        
        # M1, M2, M3 (Modifier) ãƒ†ã‚¹ãƒˆ
        ("M1", "in the morning"),
        ("M2", "very carefully"),
        ("M3", "under the bridge")
    ]
    
    results = {}
    
    for slot_name, sentence in test_cases:
        print(f"\n{'='*50}")
        print(f"ğŸ¯ ãƒ†ã‚¹ãƒˆ: {slot_name}ã‚¹ãƒ­ãƒƒãƒˆ")
        print(f"ğŸ“ å…¥åŠ›: '{sentence}'")
        
        subslots = generator.generate_subslots_for_slot(slot_name, sentence)
        results[f"{slot_name}: {sentence}"] = subslots
        
        print(f"ğŸ“Š æ¤œå‡ºã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ•°: {len(subslots)}")
        for sub_type, sub_data in subslots.items():
            print(f"   {sub_type}: '{sub_data['text']}'")
    
    print(f"\n{'='*60}")
    print("ğŸ‰ Universal System ãƒ†ã‚¹ãƒˆå®Œäº†")
    print(f"ğŸ“Š ç·ãƒ†ã‚¹ãƒˆæ•°: {len(test_cases)}")
    print(f"ğŸ“Š å…¨ã‚¹ãƒ­ãƒƒãƒˆå¯¾å¿œ: {', '.join(generator.target_slots)}")
    
    return results

if __name__ == "__main__":
    test_universal_system()
