#!/usr/bin/env python3
"""å…¨ç¯€ã‚¿ã‚¤ãƒ—ã®æ­£ã—ã„ç½®æ›æ–¹æ³•ã‚’åˆ†æ"""

import spacy

def analyze_all_clause_types():
    """å„ç¯€ã‚¿ã‚¤ãƒ—ã®æ­£ã—ã„ç½®æ›æ–¹æ³•ã‚’è©³ç´°åˆ†æ"""
    
    nlp_spacy = spacy.load("en_core_web_sm")
    
    test_cases = [
        {
            'sentence': "I think that he is smart.",
            'expected_replacement': "I think something.",
            'description': "è£œæ–‡ç¯€(ccomp) - ç¯€ã®ã¿ç½®æ›"
        },
        {
            'sentence': "Being a teacher, she knows students well.",
            'expected_replacement': "somehow, she knows students well.",
            'description': "å‰¯è©ç¯€(advcl) - ç¯€ã®ã¿ç½®æ›ã€ä¸»ç¯€ä¿æŒ"
        },
        {
            'sentence': "The book that I read yesterday was interesting.",
            'expected_replacement': "Something was interesting.",
            'description': "é–¢ä¿‚ç¯€(relcl) - ä¿®é£¾ã•ã‚Œã‚‹åè©å¥å…¨ä½“ç½®æ›"
        },
        {
            'sentence': "Having finished the work, she went home.",
            'expected_replacement': "somehow, she went home.",
            'description': "å‰¯è©ç¯€(advcl) - ç¯€ã®ã¿ç½®æ›ã€ä¸»ç¯€ä¿æŒ"
        },
        {
            'sentence': "If I were rich, I would travel around the world.",
            'expected_replacement': "somehow, I would travel around the world.",
            'description': "å‰¯è©ç¯€(advcl) - ç¯€ã®ã¿ç½®æ›ã€ä¸»ç¯€ä¿æŒ"
        }
    ]
    
    print("ğŸ” å…¨ç¯€ã‚¿ã‚¤ãƒ—ã®ç½®æ›æ–¹æ³•åˆ†æ")
    print("=" * 70)
    
    for i, case in enumerate(test_cases):
        print(f"\nğŸ“ Case {i+1}: {case['sentence']}")
        print(f"ğŸ“‹ {case['description']}")
        print(f"âœ… æœŸå¾…çµæœ: '{case['expected_replacement']}'")
        
        doc = nlp_spacy(case['sentence'])
        
        # å®Ÿéš›ã®ç¯€æ§‹é€ ã‚’åˆ†æ
        for token in doc:
            if token.dep_ in ['ccomp', 'xcomp', 'advcl', 'acl', 'relcl']:
                clause_tokens = list(token.subtree)
                clause_text = ' '.join([t.text for t in clause_tokens])
                
                print(f"ğŸ” æ¤œå‡º: {token.dep_} = '{clause_text}'")
                print(f"   ğŸ“ Head: {token.head.text} (idx: {token.head.i})")
                print(f"   ğŸ“ Range: {min(t.i for t in clause_tokens)} - {max(t.i for t in clause_tokens) + 1}")
                
                # ç½®æ›æ–¹æ³•ã®åˆ¤å®š
                if token.dep_ == 'relcl':
                    # é–¢ä¿‚ç¯€ï¼šä¿®é£¾ã•ã‚Œã‚‹åè©å¥å…¨ä½“ã‚’ç‰¹å®š
                    head_token = token.head
                    noun_phrase_start = head_token.i
                    for chunk in doc.noun_chunks:
                        if head_token.i >= chunk.start and head_token.i < chunk.end:
                            noun_phrase_start = chunk.start
                            print(f"   ğŸ¯ åè©å¥å…¨ä½“ç½®æ›: {noun_phrase_start} - {max(t.i for t in clause_tokens) + 1}")
                            break
                else:
                    # ãã®ä»–ï¼šç¯€ã®ã¿ç½®æ›
                    print(f"   ğŸ¯ ç¯€ã®ã¿ç½®æ›: {min(t.i for t in clause_tokens)} - {max(t.i for t in clause_tokens) + 1}")
        
        print("-" * 50)

def implement_universal_replacement():
    """æ±ç”¨çš„ç½®æ›ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£…"""
    print("\nğŸ› ï¸ æ±ç”¨çš„ç½®æ›ãƒ­ã‚¸ãƒƒã‚¯å®Ÿè£…æŒ‡é‡:")
    print("1. relcl â†’ ä¿®é£¾ã•ã‚Œã‚‹åè©å¥å…¨ä½“ + é–¢ä¿‚ç¯€ â†’ ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼")
    print("2. ccomp/xcomp â†’ ç¯€ã®ã¿ â†’ ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼") 
    print("3. advcl â†’ ç¯€ã®ã¿ â†’ ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼")
    print("4. acl â†’ ç¯€ã®ã¿ â†’ ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼")
    print("5. è¤‡æ•°ç¯€ â†’ å¾Œã‚ã‹ã‚‰é †æ¬¡ç½®æ›ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¶­æŒï¼‰")

if __name__ == "__main__":
    analyze_all_clause_types()
    implement_universal_replacement()
