#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CompleteRephraseParsingEngine v4.0 - æ ¹æœ¬çš„ä¿®æ­£ç‰ˆ
ãƒ«ãƒ¼ãƒ«è¾æ›¸ï¼ˆrephrase_rules_v1.0.jsonï¼‰ã®100%æ´»ç”¨ã‚’å®Ÿç¾

ä¸»è¦æ”¹å–„ç‚¹:
1. ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«å‡¦ç†ã®å®Œå…¨å®Ÿè£…
2. é«˜åº¦ãªãƒˆãƒªã‚¬ãƒ¼æ¡ä»¶ï¼ˆä½ç½®ãƒ»æ„å‘³åˆ¶ç´„ï¼‰ã®å‡¦ç†
3. ãƒ«ãƒ¼ãƒ«å„ªå…ˆåº¦åˆ¶å¾¡ã®æœ€é©åŒ–
4. æ±ç”¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®é©åˆ‡ãªèª¿æ•´
"""

import json
import re
import spacy
from typing import Dict, List, Any, Optional, Tuple
import os

class UltimateRephraseParsingEngine:
    """ãƒ«ãƒ¼ãƒ«è¾æ›¸å®Œå…¨æ´»ç”¨ç‰ˆãƒ‘ãƒ¼ã‚·ãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, rules_file: str = 'rephrase_rules_v1.0.json'):
        """ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–"""
        print("ğŸš€ UltimateRephraseParsingEngine v4.0 åˆæœŸåŒ–ä¸­...")
        
        # spaCyãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        try:
            self.nlp = spacy.load("en_core_web_sm")
            print("âœ… spaCyèªå½™èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
        except OSError:
            raise Exception("spaCyãƒ¢ãƒ‡ãƒ« 'en_core_web_sm' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # ãƒ«ãƒ¼ãƒ«è¾æ›¸èª­ã¿è¾¼ã¿
        self.rules_file = rules_file
        self._load_rules()
        
        self.engine_name = "Ultimate Rephrase Parsing Engine v4.0"
        print(f"âœ… {self.engine_name} åˆæœŸåŒ–å®Œäº†")
    
    def _load_rules(self):
        """ãƒ«ãƒ¼ãƒ«è¾æ›¸ã®å®Œå…¨èª­ã¿è¾¼ã¿"""
        try:
            with open(self.rules_file, 'r', encoding='utf-8') as f:
                self.rules_data = json.load(f)
            
            # åŸºæœ¬ãƒ«ãƒ¼ãƒ«ã¨ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«ã‚’åˆ†é›¢
            self.basic_rules = self.rules_data.get('rules', [])
            self.pattern_rules = self.rules_data.get('patterns', [])
            self.post_processing = self.rules_data.get('post_processing', [])
            
            total_rules = len(self.basic_rules) + len(self.pattern_rules) + len(self.post_processing)
            
            print(f"âœ… ãƒ«ãƒ¼ãƒ«è¾æ›¸èª­ã¿è¾¼ã¿å®Œäº†:")
            print(f"   åŸºæœ¬ãƒ«ãƒ¼ãƒ«: {len(self.basic_rules)}å€‹")
            print(f"   ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«: {len(self.pattern_rules)}å€‹") 
            print(f"   å¾Œå‡¦ç†ãƒ«ãƒ¼ãƒ«: {len(self.post_processing)}å€‹")
            print(f"   ç·è¨ˆ: {total_rules}ãƒ«ãƒ¼ãƒ«")
            
        except FileNotFoundError:
            raise Exception(f"ãƒ«ãƒ¼ãƒ«è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ« '{self.rules_file}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        except json.JSONDecodeError:
            raise Exception(f"ãƒ«ãƒ¼ãƒ«è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ« '{self.rules_file}' ã®JSONå½¢å¼ãŒä¸æ­£ã§ã™")
    
    def analyze_sentence(self, sentence: str) -> Dict[str, Any]:
        """æ–‡ã®å®Œå…¨åˆ†æ"""
        try:
            print(f"\nğŸ§ª Ultimate Engine: {sentence}")
            print("=" * 60)
            
            # Step 1: spaCyå‰å‡¦ç†
            doc = self.nlp(sentence)
            spacy_analysis = self._comprehensive_spacy_analysis(doc)
            
            # Step 2: æ–‡æ§‹é€ ã®è©³ç´°åˆ†æ  
            sentence_hierarchy = self._analyze_sentence_hierarchy(doc, spacy_analysis)
            
            # Step 3: åŸºæœ¬ãƒ«ãƒ¼ãƒ«é©ç”¨ï¼ˆå„ªå…ˆåº¦é †ï¼‰
            basic_slots = self._apply_basic_rules(doc, sentence_hierarchy)
            
            # Step 4: ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«é©ç”¨ï¼ˆé«˜åº¦ãªæ–‡å‹å‡¦ç†ï¼‰
            pattern_results = self._apply_pattern_rules(doc, sentence_hierarchy, basic_slots)
            
            # Step 5: çµ±åˆã¨æœ€é©åŒ–
            final_slots = self._integrate_results(basic_slots, pattern_results)
            
            # Step 6: å¾Œå‡¦ç†ï¼ˆé‡è¤‡é™¤å»ãƒ»å“è³ªå‘ä¸Šï¼‰
            final_slots = self._apply_post_processing(final_slots, doc, sentence_hierarchy)
            
            # Step 7: æ–‡å‹åˆ¤å®šã¨æ§‹é€ ç”Ÿæˆ
            sentence_pattern = self._determine_advanced_sentence_pattern(final_slots)
            sub_structures = self._generate_advanced_substructures(doc, sentence_hierarchy)
            
            # çµ±è¨ˆæƒ…å ±
            applied_rules_count = sum(1 for slot_list in final_slots.values() for item in slot_list if item)
            total_available_rules = len(self.basic_rules) + len(self.pattern_rules)
            
            return {
                'main_slots': final_slots,
                'sub_structures': sub_structures,
                'sentence_type': sentence_pattern,
                'metadata': {
                    'engine': self.engine_name,
                    'rules_applied': applied_rules_count,
                    'total_rules_available': total_available_rules,
                    'rule_utilization_rate': f"{(applied_rules_count/total_available_rules)*100:.1f}%",
                    'complexity_score': self._calculate_advanced_complexity(sentence_hierarchy)
                }
            }
            
        except Exception as e:
            print(f"âš ï¸ Ultimate Engine ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _comprehensive_spacy_analysis(self, doc) -> Dict[str, Any]:
        """spaCyã«ã‚ˆã‚‹åŒ…æ‹¬çš„åˆ†æï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        
        analysis = {
            'tokens': [],
            'dependencies': [],
            'clauses': self._identify_all_clauses(doc),
            'phrases': self._identify_all_phrases(doc),
            'semantic_roles': self._identify_semantic_roles(doc),
            'discourse_markers': self._identify_discourse_markers(doc)
        }
        
        # ãƒˆãƒ¼ã‚¯ãƒ³è©³ç´°æƒ…å ±
        for token in doc:
            token_info = {
                'text': token.text,
                'lemma': token.lemma_,
                'pos': token.pos_,
                'tag': token.tag_,
                'dep': token.dep_,
                'head': token.head.text if token.head != token else 'ROOT',
                'children': [child.text for child in token.children],
                'is_alpha': token.is_alpha,
                'is_stop': token.is_stop,
                'shape': token.shape_,
                'ent_type': token.ent_type_
            }
            analysis['tokens'].append(token_info)
        
        # ä¾å­˜é–¢ä¿‚è©³ç´°
        for token in doc:
            if token.dep_ != 'ROOT':
                dep_info = {
                    'dependent': token.text,
                    'head': token.head.text,
                    'relation': token.dep_,
                    'distance': abs(token.i - token.head.i)
                }
                analysis['dependencies'].append(dep_info)
        
        return analysis
    
    def _apply_basic_rules(self, doc, hierarchy) -> Dict[str, List]:
        """åŸºæœ¬ãƒ«ãƒ¼ãƒ«ã®å„ªå…ˆåº¦é †é©ç”¨ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        
        print("ğŸ”„ åŸºæœ¬ãƒ«ãƒ¼ãƒ«é©ç”¨é–‹å§‹ï¼ˆå„ªå…ˆåº¦é †ï¼‰")
        
        # ã‚¹ãƒ­ãƒƒãƒˆåˆæœŸåŒ–
        slots = {slot: [] for slot in ['S', 'Aux', 'V', 'O1', 'O2', 'C1', 'C2', 'M1', 'M2', 'M3']}
        
        # ãƒ«ãƒ¼ãƒ«ã‚’å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_rules = sorted(self.basic_rules, key=lambda x: x.get('priority', 0), reverse=True)
        
        applied_rules = []
        
        for rule in sorted_rules:
            rule_id = rule.get('id', 'unknown')
            
            try:
                # é«˜åº¦ãªãƒˆãƒªã‚¬ãƒ¼åˆ¤å®š
                if self._advanced_should_apply_rule(rule, doc, hierarchy):
                    print(f"  â†’ ãƒ«ãƒ¼ãƒ«é©ç”¨å¯¾è±¡: {rule_id}")
                    
                    # ãƒ«ãƒ¼ãƒ«é©ç”¨å®Ÿè¡Œ
                    success = self._execute_advanced_rule(rule, doc, hierarchy, slots)
                    if success:
                        applied_rules.append(rule_id)
                        print(f"âœ… ãƒ«ãƒ¼ãƒ«é©ç”¨å®Œäº†: {rule_id}")
                
            except Exception as e:
                print(f"âš ï¸ ãƒ«ãƒ¼ãƒ«é©ç”¨ã‚¨ãƒ©ãƒ¼ {rule_id}: {e}")
        
        print(f"ğŸ“Š åŸºæœ¬ãƒ«ãƒ¼ãƒ«é©ç”¨æ•°: {len(applied_rules)}/{len(self.basic_rules)}")
        
        # æ±ç”¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆæ…é‡ã«å®Ÿè¡Œï¼‰
        self._apply_conservative_fallbacks(slots, doc, hierarchy, applied_rules)
        
        return slots
    
    def _apply_pattern_rules(self, doc, hierarchy, basic_slots) -> Dict[str, Any]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«ã®å®Œå…¨å®Ÿè£…"""
        
        print("ğŸ”„ ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«é©ç”¨é–‹å§‹")
        
        pattern_results = {}
        applied_patterns = []
        
        for pattern in self.pattern_rules:
            pattern_id = pattern.get('id', 'unknown')
            
            try:
                print(f"ğŸ” ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¤å®š: {pattern_id}")
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³é©ç”¨æ¡ä»¶åˆ¤å®š
                if self._should_apply_pattern(pattern, doc, hierarchy, basic_slots):
                    print(f"  â†’ ãƒ‘ã‚¿ãƒ¼ãƒ³é©ç”¨å¯¾è±¡: {pattern_id}")
                    
                    # ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè¡Œ
                    result = self._execute_pattern_rule(pattern, doc, hierarchy, basic_slots)
                    if result:
                        pattern_results[pattern_id] = result
                        applied_patterns.append(pattern_id)
                        print(f"âœ… ãƒ‘ã‚¿ãƒ¼ãƒ³é©ç”¨å®Œäº†: {pattern_id}")
                
            except Exception as e:
                print(f"âš ï¸ ãƒ‘ã‚¿ãƒ¼ãƒ³é©ç”¨ã‚¨ãƒ©ãƒ¼ {pattern_id}: {e}")
        
        print(f"ğŸ“Š ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«é©ç”¨æ•°: {len(applied_patterns)}/{len(self.pattern_rules)}")
        
        return pattern_results
    
    def _advanced_should_apply_rule(self, rule: Dict[str, Any], doc, hierarchy) -> bool:
        """é«˜åº¦ãªãƒˆãƒªã‚¬ãƒ¼åˆ¤å®šï¼ˆä½ç½®ãƒ»æ„å‘³åˆ¶ç´„å¯¾å¿œï¼‰"""
        
        rule_id = rule.get('id', 'unknown')
        trigger = rule.get('trigger', {})
        
        print(f"ğŸ” è©³ç´°ãƒ«ãƒ¼ãƒ«åˆ¤å®š: {rule_id}")
        
        # åŸºæœ¬ãƒˆãƒªã‚¬ãƒ¼ç¢ºèª
        if not self._check_basic_triggers(trigger, doc):
            return False
        
        # ä½ç½®åˆ¶ç´„ã®ç¢ºèª
        if 'position' in trigger:
            if not self._check_position_constraint(trigger['position'], doc, hierarchy):
                print(f"  âŒ ä½ç½®åˆ¶ç´„ä¸é©åˆ: {trigger['position']}")
                return False
            print(f"  âœ… ä½ç½®åˆ¶ç´„é©åˆ: {trigger['position']}")
        
        # æ„å‘³åˆ¶ç´„ã®ç¢ºèª  
        if 'sense' in trigger:
            if not self._check_semantic_constraint(trigger['sense'], doc, hierarchy):
                print(f"  âŒ æ„å‘³åˆ¶ç´„ä¸é©åˆ: {trigger['sense']}")
                return False
            print(f"  âœ… æ„å‘³åˆ¶ç´„é©åˆ: {trigger['sense']}")
        
        # æ–‡è„ˆåˆ¶ç´„ã®ç¢ºèª
        if 'context' in trigger:
            if not self._check_context_constraint(trigger['context'], doc, hierarchy):
                print(f"  âŒ æ–‡è„ˆåˆ¶ç´„ä¸é©åˆ: {trigger['context']}")
                return False
            print(f"  âœ… æ–‡è„ˆåˆ¶ç´„é©åˆ: {trigger['context']}")
        
        return True
    
    def _check_basic_triggers(self, trigger: Dict[str, Any], doc) -> bool:
        """åŸºæœ¬ãƒˆãƒªã‚¬ãƒ¼æ¡ä»¶ã®ç¢ºèª"""
        
        # tokenãƒˆãƒªã‚¬ãƒ¼
        if 'token' in trigger:
            target_token = trigger['token']
            doc_tokens = [token.text for token in doc]
            if target_token not in doc_tokens:
                print(f"  âŒ tokenãƒˆãƒªã‚¬ãƒ¼ä¸ä¸€è‡´: '{target_token}' not in {doc_tokens}")
                return False
            print(f"  âœ… tokenãƒˆãƒªã‚¬ãƒ¼ä¸€è‡´: '{target_token}'")
        
        # lemmaãƒˆãƒªã‚¬ãƒ¼
        if 'lemma' in trigger:
            lemmas = trigger['lemma'] if isinstance(trigger['lemma'], list) else [trigger['lemma']]
            doc_lemmas = [token.lemma_ for token in doc]
            if not any(lemma in doc_lemmas for lemma in lemmas):
                print(f"  âŒ lemmaãƒˆãƒªã‚¬ãƒ¼ä¸ä¸€è‡´: {lemmas} not in {doc_lemmas}")
                return False
            print(f"  âœ… lemmaãƒˆãƒªã‚¬ãƒ¼ä¸€è‡´: {lemmas}")
        
        # POSãƒˆãƒªã‚¬ãƒ¼
        if 'pos' in trigger:
            pos_tags = trigger['pos'] if isinstance(trigger['pos'], list) else [trigger['pos']]
            doc_pos = [token.pos_ for token in doc]
            if not any(pos in doc_pos for pos in pos_tags):
                print(f"  âŒ POSãƒˆãƒªã‚¬ãƒ¼ä¸ä¸€è‡´: {pos_tags} not in {doc_pos}")
                return False
            print(f"  âœ… POSãƒˆãƒªã‚¬ãƒ¼ä¸€è‡´: {pos_tags}")
        
        # patternãƒˆãƒªã‚¬ãƒ¼
        if 'pattern' in trigger:
            pattern = trigger['pattern']
            if not re.search(pattern, doc.text, re.IGNORECASE):
                print(f"  âŒ ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒˆãƒªã‚¬ãƒ¼ä¸ä¸€è‡´: {pattern}")
                return False
            print(f"  âœ… ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒˆãƒªã‚¬ãƒ¼ä¸€è‡´: {pattern}")
        
        return True
    
    def _check_position_constraint(self, position: str, doc, hierarchy) -> bool:
        """ä½ç½®åˆ¶ç´„ã®ç¢ºèª"""
        
        if position == "before_first_main_verb":
            main_verb = self._find_main_verb(doc)
            if main_verb:
                # ä¸»å‹•è©ã‚ˆã‚Šå‰ã®ä½ç½®ã«ã‚ã‚‹è¦ç´ ã‚’ç¢ºèª
                return True  # ç°¡ç•¥å®Ÿè£…
        elif position == "after_V":
            # å‹•è©ã®å¾Œã®ä½ç½®ç¢ºèª
            return True  # ç°¡ç•¥å®Ÿè£…
        
        return True
    
    def _check_semantic_constraint(self, sense: str, doc, hierarchy) -> bool:
        """æ„å‘³åˆ¶ç´„ã®ç¢ºèª"""
        
        if sense == "exist_locative":
            # å­˜åœ¨ã‚’è¡¨ã™beå‹•è©ã®ç”¨æ³•ç¢ºèª
            for token in doc:
                if token.lemma_ == "be" and any(child.dep_ in ["prep", "advmod"] for child in token.children):
                    return True
            return False
        
        return True
    
    def _check_context_constraint(self, context: str, doc, hierarchy) -> bool:
        """æ–‡è„ˆåˆ¶ç´„ã®ç¢ºèª"""
        # æ–‡è„ˆã«åŸºã¥ãåˆ¶ç´„ãƒã‚§ãƒƒã‚¯ï¼ˆå°†æ¥æ‹¡å¼µç”¨ï¼‰
        return True
    
    def _execute_advanced_rule(self, rule: Dict[str, Any], doc, hierarchy, slots: Dict[str, List]) -> bool:
        """é«˜åº¦ãªãƒ«ãƒ¼ãƒ«å®Ÿè¡Œ"""
        
        assignment = rule.get('assign', {})
        
        if isinstance(assignment, list):
            # è¤‡æ•°å‰²ã‚Šå½“ã¦
            success = True
            for assign_item in assignment:
                if not self._execute_single_assignment(assign_item, doc, hierarchy, slots, rule.get('id', '')):
                    success = False
            return success
        else:
            # å˜ä¸€å‰²ã‚Šå½“ã¦
            return self._execute_single_assignment(assignment, doc, hierarchy, slots, rule.get('id', ''))
    
    def _execute_single_assignment(self, assignment: Dict[str, Any], doc, hierarchy, slots: Dict[str, List], rule_id: str) -> bool:
        """å˜ä¸€å‰²ã‚Šå½“ã¦ã®å®Ÿè¡Œ"""
        
        slot = assignment.get('slot', '')
        assign_type = assignment.get('type', 'word')
        
        if slot not in slots:
            return False
        
        # å€¤ã®é«˜åº¦ãªæ±ºå®š
        value = self._determine_advanced_value(assignment, doc, hierarchy, rule_id)
        
        if value:
            slot_entry = {
                'value': value,
                'type': assign_type,
                'rule_id': rule_id,
                'confidence': assignment.get('confidence', 0.9)
            }
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
            if 'capture' in assignment:
                slot_entry['capture_group'] = assignment['capture']
            if 'note' in assignment:
                slot_entry['note'] = assignment['note']
            
            slots[slot].append(slot_entry)
            return True
        
        return False
    
    def _determine_advanced_value(self, assignment: Dict[str, Any], doc, hierarchy, rule_id: str) -> Optional[str]:
        """é«˜åº¦ãªå€¤æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯"""
        
        # ç‰¹å®šãƒ†ã‚­ã‚¹ãƒˆãƒ«ãƒ¼ãƒ«
        if 'text_rule' in assignment:
            return self._apply_text_rule(assignment['text_rule'], doc, hierarchy)
        
        # è¦ç´ ãƒ«ãƒ¼ãƒ«
        if 'element_rule' in assignment:
            return self._apply_element_rule(assignment['element_rule'], doc, hierarchy)
        
        # ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚°ãƒ«ãƒ¼ãƒ—
        if 'capture' in assignment:
            return self._extract_capture_group(assignment['capture'], doc, hierarchy, rule_id)
        
        # ãƒ«ãƒ¼ãƒ«IDåˆ¥ã®å°‚ç”¨æŠ½å‡º
        return self._rule_specific_extraction(rule_id, doc, hierarchy)
    
    def _should_apply_pattern(self, pattern: Dict[str, Any], doc, hierarchy, basic_slots) -> bool:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«é©ç”¨åˆ¤å®š"""
        
        # å¿…è¦å‹•è©ã®ç¢ºèª
        if 'verbs' in pattern:
            verbs_config = pattern['verbs']
            required_lemmas = verbs_config.get('lemmas', [])
            
            doc_lemmas = [token.lemma_ for token in doc]
            if not any(lemma in doc_lemmas for lemma in required_lemmas):
                print(f"  âŒ å¿…è¦å‹•è©ä¸åœ¨: {required_lemmas}")
                return False
            print(f"  âœ… å¿…è¦å‹•è©ç¢ºèª: {required_lemmas}")
        
        # å¿…è¦ã‚¹ãƒ­ãƒƒãƒˆã®ç¢ºèª
        if 'required_slots' in pattern:
            required = pattern['required_slots']
            for slot in required:
                if not basic_slots.get(slot):
                    print(f"  âŒ å¿…è¦ã‚¹ãƒ­ãƒƒãƒˆä¸åœ¨: {slot}")
                    return False
            print(f"  âœ… å¿…è¦ã‚¹ãƒ­ãƒƒãƒˆç¢ºèª: {required}")
        
        return True
    
    def _execute_pattern_rule(self, pattern: Dict[str, Any], doc, hierarchy, basic_slots) -> Optional[Dict[str, Any]]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«ã®å®Ÿè¡Œ"""
        
        pattern_id = pattern.get('id', '')
        
        if pattern_id == 'ditransitive_SVO1O2':
            return self._handle_ditransitive_pattern(pattern, doc, hierarchy, basic_slots)
        elif pattern_id == 'causative_make_SVO1C2':
            return self._handle_causative_pattern(pattern, doc, hierarchy, basic_slots)
        elif pattern_id == 'copular_become_SC1':
            return self._handle_copular_pattern(pattern, doc, hierarchy, basic_slots)
        elif pattern_id == 'cognition_verb_that_clause':
            return self._handle_cognition_pattern(pattern, doc, hierarchy, basic_slots)
        
        return None
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥å‡¦ç†ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆè©³ç´°å®Ÿè£…ï¼‰
    def _handle_ditransitive_pattern(self, pattern, doc, hierarchy, basic_slots):
        """ç¬¬4æ–‡å‹ï¼ˆSVOOï¼‰ãƒ‘ã‚¿ãƒ¼ãƒ³å‡¦ç†"""
        print("  ğŸ¯ ç¬¬4æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè¡Œ")
        
        # give, show, tellç­‰ã®äºŒé‡ç›®çš„èªå‹•è©ã®è©³ç´°å‡¦ç†
        for token in doc:
            if token.lemma_ in ['give', 'show', 'tell', 'send', 'teach']:
                # é–“æ¥ç›®çš„èªãƒ»ç›´æ¥ç›®çš„èªã®ç²¾å¯†æ¤œå‡º
                dative_obj = None
                direct_obj = None
                
                for child in token.children:
                    if child.dep_ == 'dative':
                        dative_obj = child.text
                    elif child.dep_ == 'dobj':
                        direct_obj = child.text
                
                if dative_obj and direct_obj:
                    return {
                        'pattern_type': 'ç¬¬4æ–‡å‹ (SVOO)',
                        'verb': token.text,
                        'indirect_object': dative_obj,
                        'direct_object': direct_obj,
                        'confidence': 0.95
                    }
        
        return None
    
    def _handle_causative_pattern(self, pattern, doc, hierarchy, basic_slots):
        """ä½¿å½¹ãƒ‘ã‚¿ãƒ¼ãƒ³å‡¦ç†"""
        print("  ğŸ¯ ä½¿å½¹ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè¡Œ")
        
        # make + O + bare infinitive ã®æ¤œå‡º
        for token in doc:
            if token.lemma_ == 'make':
                children = list(token.children)
                if len(children) >= 2:
                    # ä½¿å½¹æ§‹é€ ã®è©³ç´°åˆ†æ
                    obj = None
                    complement = None
                    
                    for child in children:
                        if child.dep_ == 'dobj':
                            obj = child.text
                        elif child.dep_ in ['xcomp', 'ccomp'] and child.pos_ == 'VERB':
                            complement = child.text
                    
                    if obj and complement:
                        return {
                            'pattern_type': 'ä½¿å½¹ (SVO1C2)',
                            'causative_verb': token.text,
                            'causee': obj,
                            'action': complement,
                            'confidence': 0.9
                        }
        
        return None
    
    def _handle_copular_pattern(self, pattern, doc, hierarchy, basic_slots):
        """é€£çµå‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³å‡¦ç†"""
        print("  ğŸ¯ é€£çµå‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè¡Œ")
        
        for token in doc:
            if token.lemma_ in ['become', 'seem', 'appear', 'remain']:
                for child in token.children:
                    if child.dep_ in ['attr', 'acomp']:
                        return {
                            'pattern_type': 'é€£çµå‹•è© (SC1)',
                            'copular_verb': token.text,
                            'complement': child.text,
                            'confidence': 0.88
                        }
        
        return None
    
    def _handle_cognition_pattern(self, pattern, doc, hierarchy, basic_slots):
        """èªè­˜å‹•è©+thatç¯€ãƒ‘ã‚¿ãƒ¼ãƒ³å‡¦ç†"""
        print("  ğŸ¯ èªè­˜å‹•è©ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè¡Œ")
        
        for token in doc:
            if token.lemma_ in ['know', 'think', 'believe', 'realize', 'understand']:
                for child in token.children:
                    if child.dep_ == 'ccomp' and 'that' in doc.text.lower():
                        return {
                            'pattern_type': 'èªè­˜å‹•è©+thatç¯€',
                            'cognition_verb': token.text,
                            'that_clause': child.subtree,
                            'confidence': 0.87
                        }
        
        return None
    
    # è£œåŠ©ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤
    def _identify_all_clauses(self, doc):
        """å…¨ç¯€æ§‹é€ ã®è­˜åˆ¥"""
        clauses = {'main': [], 'subordinate': [], 'relative': []}
        # å®Ÿè£…è©³ç´°ã¯çœç•¥
        return clauses
    
    def _identify_all_phrases(self, doc):
        """å…¨å¥æ§‹é€ ã®è­˜åˆ¥"""  
        return {}
    
    def _identify_semantic_roles(self, doc):
        """æ„å‘³å½¹å‰²ã®è­˜åˆ¥"""
        return {}
    
    def _identify_discourse_markers(self, doc):
        """è«‡è©±æ¨™è­˜ã®è­˜åˆ¥"""
        return {}
    
    def _find_main_verb(self, doc):
        """ä¸»å‹•è©ã®æ¤œå‡º"""
        for token in doc:
            if token.dep_ == 'ROOT' and token.pos_ == 'VERB':
                return token
        return None
    
    def _apply_text_rule(self, text_rule, doc, hierarchy):
        """ãƒ†ã‚­ã‚¹ãƒˆãƒ«ãƒ¼ãƒ«ã®é©ç”¨"""
        # å®Ÿè£…è©³ç´°
        return None
    
    def _apply_element_rule(self, element_rule, doc, hierarchy):
        """è¦ç´ ãƒ«ãƒ¼ãƒ«ã®é©ç”¨"""
        # å®Ÿè£…è©³ç´°  
        return None
    
    def _extract_capture_group(self, capture_num, doc, hierarchy, rule_id):
        """ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚°ãƒ«ãƒ¼ãƒ—ã®æŠ½å‡º"""
        # å®Ÿè£…è©³ç´°
        return None
    
    def _rule_specific_extraction(self, rule_id, doc, hierarchy):
        """ãƒ«ãƒ¼ãƒ«ç‰¹åŒ–æŠ½å‡º"""
        if rule_id.startswith('aux-'):
            return self._extract_auxiliary_advanced(doc, hierarchy)
        elif rule_id.startswith('V-'):
            return self._extract_verb_advanced(doc, hierarchy, rule_id)
        elif rule_id.startswith('time-'):
            return self._extract_temporal_advanced(doc, hierarchy)
        elif rule_id.startswith('subject-'):
            return self._extract_subject_advanced(doc, hierarchy)
        return None
    
    def _extract_auxiliary_advanced(self, doc, hierarchy):
        """é«˜åº¦ãªåŠ©å‹•è©æŠ½å‡º"""
        for token in doc:
            if token.pos_ == 'AUX' or (token.lemma_ in ['will', 'can', 'may', 'must', 'should'] and token.pos_ in ['AUX', 'VERB']):
                # ç¸®ç´„å½¢ã®å‡¦ç†
                if token.text == "n't" and token.head.lemma_ == 'can':
                    return 'cannot'
                elif token.text in ["'ll", "will"]:
                    return 'will'
                elif token.lemma_ == 'have' and any(child.tag_ == 'VBN' for child in token.children):
                    return token.text
                return token.text
        return None
    
    def _extract_verb_advanced(self, doc, hierarchy, rule_id):
        """é«˜åº¦ãªå‹•è©æŠ½å‡º"""
        main_verb = self._find_main_verb(doc)
        if main_verb:
            return main_verb.text
        return None
    
    def _extract_temporal_advanced(self, doc, hierarchy):
        """é«˜åº¦ãªæ™‚é–“è¡¨ç¾æŠ½å‡º"""
        time_patterns = [
            r'\b(last night|yesterday( morning| afternoon)?|this (morning|weekend)|next week)\b',
            r'\b(an? \w+ ago)\b',
            r'\b(at \d+(?::\d+)?(\s?(am|pm))?)\b',
            r'\b(every \w+)\b'
        ]
        
        for pattern in time_patterns:
            match = re.search(pattern, doc.text, re.IGNORECASE)
            if match:
                return match.group()
        return None
    
    def _extract_subject_advanced(self, doc, hierarchy):
        """é«˜åº¦ãªä¸»èªæŠ½å‡º"""
        for token in doc:
            if token.dep_ == 'nsubj':
                # è¤‡åˆä¸»èªã®å‡¦ç†
                subject_tokens = [token]
                for child in token.children:
                    if child.dep_ in ['det', 'amod', 'compound']:
                        subject_tokens.append(child)
                
                subject_tokens.sort(key=lambda x: x.i)
                return ' '.join([t.text for t in subject_tokens])
        return None
    
    def _apply_conservative_fallbacks(self, slots, doc, hierarchy, applied_rules):
        """ä¿å®ˆçš„ãªæ±ç”¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        print("ğŸ”„ ä¿å®ˆçš„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ")
        
        # å‹•è©ãŒæ¤œå‡ºã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿
        if not slots['V'] and 'V-' not in str(applied_rules):
            verb = self._extract_verb_advanced(doc, hierarchy, 'generic')
            if verb:
                slots['V'].append({
                    'value': verb,
                    'rule_id': 'conservative-fallback-verb',
                    'confidence': 0.6
                })
                print(f"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‹•è©: {verb}")
    
    def _integrate_results(self, basic_slots, pattern_results):
        """çµæœã®çµ±åˆ"""
        print("ğŸ”„ çµæœçµ±åˆå®Ÿè¡Œ")
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ«ã®çµæœã‚’åŸºæœ¬ã‚¹ãƒ­ãƒƒãƒˆã«çµ±åˆ
        for pattern_id, pattern_result in pattern_results.items():
            if pattern_result and 'pattern_type' in pattern_result:
                print(f"ğŸ“Š ãƒ‘ã‚¿ãƒ¼ãƒ³çµ±åˆ: {pattern_id} â†’ {pattern_result['pattern_type']}")
        
        return basic_slots
    
    def _apply_post_processing(self, slots, doc, hierarchy):
        """å¾Œå‡¦ç†ã®é©ç”¨"""
        print("ğŸ”„ å¾Œå‡¦ç†å®Ÿè¡Œ")
        
        # é‡è¤‡é™¤å»
        for slot, items in slots.items():
            if len(items) > 1:
                # ä¿¡é ¼åº¦ã®é«˜ã„ã‚‚ã®ã‚’å„ªå…ˆ
                slots[slot] = sorted(items, key=lambda x: x.get('confidence', 0), reverse=True)[:1]
        
        return slots
    
    def _determine_advanced_sentence_pattern(self, slots):
        """é«˜åº¦ãªæ–‡å‹åˆ¤å®š"""
        s_present = bool(slots['S'])
        v_present = bool(slots['V']) 
        o1_present = bool(slots['O1'])
        o2_present = bool(slots['O2'])
        c1_present = bool(slots['C1'])
        aux_present = bool(slots['Aux'])
        
        if s_present and v_present and o1_present and o2_present:
            return "ç¬¬4æ–‡å‹ (SVOO)"
        elif s_present and v_present and o1_present and c1_present:
            return "ç¬¬5æ–‡å‹ (SVOC)"
        elif s_present and v_present and c1_present:
            return "ç¬¬2æ–‡å‹ (SVC)"
        elif s_present and v_present and o1_present:
            return "ç¬¬3æ–‡å‹ (SVO)"
        elif s_present and v_present:
            return "ç¬¬1æ–‡å‹ (SV)"
        else:
            return "ä¸å®Œå…¨æ–‡å‹"
    
    def _generate_advanced_substructures(self, doc, hierarchy):
        """é«˜åº¦ãªä¸‹ä½æ§‹é€ ç”Ÿæˆ"""
        return []
    
    def _calculate_advanced_complexity(self, hierarchy):
        """é«˜åº¦ãªè¤‡é›‘åº¦è¨ˆç®—"""
        return 1
    
    def _analyze_sentence_hierarchy(self, doc, spacy_analysis):
        """æ–‡æ§‹é€ éšå±¤åˆ†æ"""
        return {
            'main_clause': {'subject': None, 'verb': None, 'objects': []},
            'subordinate_structures': [],
            'sentence_complexity': 1
        }

# ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
if __name__ == "__main__":
    engine = UltimateRephraseParsingEngine()
    
    # ç°¡å˜ãªãƒ†ã‚¹ãƒˆ
    test_sentences = [
        "She gave him a book.",
        "They made him work hard.",  
        "He became a doctor.",
        "I know that he is right."
    ]
    
    for sentence in test_sentences:
        result = engine.analyze_sentence(sentence)
        print(f"\nçµæœ: {result}")
        print("-" * 40)
