#!/usr/bin/env python3
"""
æ­£å¼ãƒ†ã‚¹ãƒˆæ‰‹é †å®Ÿè¡Œï¼ˆå‹•çš„ç‰ˆï¼‰
- final_54_test_data.jsonã‹ã‚‰å‹•çš„ã«ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿
- æ–‡æ³•é …ç›®åˆ¥é¸æŠæ©Ÿèƒ½ä»˜ãï¼ˆå®Ÿè£…ã—ã¦ã„ãªã„æ–‡æ³•ã®éƒ¨åˆ†ã‚’é™¤å¤–å¯èƒ½ï¼‰
"""

from dynamic_grammar_mapper import DynamicGrammarMapper
from central_controller import CentralController
import json
import os
import argparse
from datetime import datetime
import argparse

def select_test_cases(test_cases, test_selection):
    """
    ãƒ†ã‚¹ãƒˆé¸æŠæ–‡å­—åˆ—ã«åŸºã¥ã„ã¦ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’é¸æŠ
    test_selectionä¾‹:
    - "1,2,3": ID 1,2,3
    - "1-5": ID 1ã‹ã‚‰5ã¾ã§
    - "basic": åŸºæœ¬5æ–‡å‹
    - "relation": é–¢ä¿‚ç¯€
    - "1,3-5,8": è¤‡åˆæŒ‡å®š
    """
    selected_ids = set()
    
    if test_selection.lower() == "basic":
        # åŸºæœ¬5æ–‡å‹ã®ãƒ†ã‚¹ãƒˆIDï¼ˆ17ä»¶ï¼‰
        selected_ids = {1, 2, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69}
    elif test_selection.lower() == "relation":
        # é–¢ä¿‚ç¯€ã®ãƒ†ã‚¹ãƒˆID
        selected_ids = {3, 4, 5, 6, 7, 8, 12, 13, 14, 34, 35, 36}
    elif test_selection.lower() == "passive":
        # å—å‹•æ…‹ã®ãƒ†ã‚¹ãƒˆID
        selected_ids = {9, 10, 11, 21, 22, 23, 24}
    else:
        # æ•°å€¤æŒ‡å®šã®è§£æ
        parts = test_selection.split(',')
        for part in parts:
            part = part.strip()
            if '-' in part:
                # ç¯„å›²æŒ‡å®š (ä¾‹: "3-5")
                start, end = map(int, part.split('-'))
                selected_ids.update(range(start, end + 1))
            else:
                # å˜ä¸€ID (ä¾‹: "1")
                selected_ids.add(int(part))
    
    # é¸æŠã•ã‚ŒãŸIDã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’è¿”ã™
    return [case for case in test_cases if case['id'] in selected_ids]

def load_test_cases():
    """final_54_test_data.jsonã‹ã‚‰ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€ãƒªã‚¹ãƒˆå½¢å¼ã§è¿”ã™"""
    test_file = "final_test_system/final_54_test_data.json"
    
    if not os.path.exists(test_file):
        raise FileNotFoundError(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {test_file}")
    
    with open(test_file, 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    
    # è¾æ›¸å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆå½¢å¼ã«å¤‰æ›
    test_cases = []
    for test_id, test_case in test_data['data'].items():
        test_case['id'] = int(test_id)  # IDã‚’æ•´æ•°ã¨ã—ã¦è¿½åŠ 
        test_cases.append(test_case)
    
    return test_cases

def classify_grammar_type(sentence):
    """æ–‡æ³•ã‚¿ã‚¤ãƒ—ã‚’è‡ªå‹•åˆ†é¡"""
    sentence_lower = sentence.lower()
    
    # é–¢ä¿‚ç¯€ã®åˆ¤å®š
    if any(word in sentence_lower for word in ['who', 'whose', 'which', 'that']):
        if 'whose' in sentence_lower:
            return 'relative_whose'
        return 'relative_clause'
    
    # å—å‹•æ…‹ã®åˆ¤å®š
    passive_patterns = [
        'is ', 'are ', 'was ', 'were ', 'been ', 'being '
    ]
    if any(pattern in sentence_lower for pattern in passive_patterns):
        # ã•ã‚‰ã«è©³ç´°ãªå—å‹•æ…‹åˆ¤å®šãŒå¿…è¦ãªå ´åˆã¯ã“ã“ã§
        if any(word in sentence_lower for word in ['by ']):
            return 'passive_voice'
    
    # è¤‡åˆæ™‚åˆ¶ã®åˆ¤å®š
    if any(aux in sentence_lower for aux in ['have ', 'has ', 'had ', 'will ', 'would ', 'can ', 'could ', 'may ', 'might ', 'should ', 'must ']):
        return 'auxiliary_complex'
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯åŸºæœ¬5æ–‡å‹
    return 'basic_five_pattern'

def filter_tests_by_grammar(test_data, grammar_types=None):
    """æ–‡æ³•ã‚¿ã‚¤ãƒ—ã§ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    if grammar_types is None:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šåŸºæœ¬5æ–‡å‹ + é–¢ä¿‚ç¯€ã®ã¿ï¼ˆå—å‹•æ…‹ã¨è¤‡åˆæ™‚åˆ¶ã‚’é™¤å¤–ï¼‰
        grammar_types = ['basic_five_pattern', 'relative_clause', 'relative_whose']
    
    filtered_tests = {}
    for test_id, test_case in test_data['data'].items():
        sentence = test_case['sentence']
        grammar_type = classify_grammar_type(sentence)
        
        if grammar_type in grammar_types:
            filtered_tests[test_id] = test_case
    
    return filtered_tests

def run_official_test(grammar_types=None):
    """æ­£å¼ãƒ†ã‚¹ãƒˆæ‰‹é †ã®å®Ÿè¡Œï¼ˆå‹•çš„ç‰ˆï¼‰"""
def run_official_test(grammar_types=None):
    """æ­£å¼ãƒ†ã‚¹ãƒˆæ‰‹é †ã®å®Ÿè¡Œï¼ˆå‹•çš„ç‰ˆï¼‰"""
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    try:
        test_cases = load_test_cases()
        print(f"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(test_cases)}ä»¶ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹")
    except FileNotFoundError as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return None
    
    # æ–‡æ³•ã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆé¸æŠã•ã‚ŒãŸã‚±ãƒ¼ã‚¹ã®ã¿ï¼‰
    if grammar_types:
        filtered_cases = []
        for case in test_cases:
            grammar_type = classify_grammar_type(case['sentence'])
            if grammar_type in grammar_types:
                filtered_cases.append(case)
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šåŸºæœ¬5æ–‡å‹ + é–¢ä¿‚ç¯€
        basic_cases = select_test_cases(test_cases, "basic")
        relation_cases = select_test_cases(test_cases, "relation")
        filtered_cases = basic_cases + relation_cases
    print(f"ğŸ“‹ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ: {len(filtered_cases)}ä»¶ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å®Ÿè¡Œ")
    
    # æ–°ã—ã„é–¢æ•°ã‚’ä½¿ç”¨
    return run_official_test_with_selected_cases(filtered_cases)

def run_official_test_with_selected_cases(selected_cases):
    """é¸æŠã•ã‚ŒãŸãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã§æ­£å¼ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    print("âœ… spaCyå‹•çš„æ–‡æ³•èªè­˜ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    # DynamicGrammarMapperã‚’åˆæœŸåŒ–ã—ã¦ã‹ã‚‰Central Controllerã«æ¸¡ã™
    mapper = DynamicGrammarMapper()
    controller = CentralController(mapper)
    print("ğŸ¯ Central ControlleråˆæœŸåŒ–: Phase 2: Precision Enhancement Controller")
    print("ğŸ”¥ Phase 1.0 ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†: 4å€‹ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–")
    print("   ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼: basic_five_pattern, relative_clause, passive_voice, auxiliary_complex")
    
    print(f"\n=== æ­£å¼ãƒ†ã‚¹ãƒˆæ‰‹é †å®Ÿè¡Œï¼ˆé¸æŠã•ã‚ŒãŸãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ {len(selected_cases)}ä»¶ï¼‰===\n")
    
    results = {
        "timestamp": datetime.now().isoformat(),
        "total_tests": len(selected_cases),
        "results": {}
    }
    
    grammar_counts = {}
    
    for test_case in selected_cases:
        test_id = test_case['id']
        sentence = test_case["sentence"]
        expected = test_case["expected"]
        
        # æ–‡æ³•ã‚¿ã‚¤ãƒ—ã®åˆ¤å®š
        grammar_type = classify_grammar_type(sentence)
        grammar_counts[grammar_type] = grammar_counts.get(grammar_type, 0) + 1
        
        print(f"ãƒ†ã‚¹ãƒˆ {test_id} ({grammar_type}): {sentence}")
        
        try:
            # Central Controllerã§åˆ†æå®Ÿè¡Œ
            result = controller.analyze_sentence(sentence)
            
            # compare_results.pyãŒæœŸå¾…ã™ã‚‹å½¢å¼ã§ä¿å­˜
            results["results"][test_id] = {
                "sentence": sentence,
                "expected": expected,
                "analysis_result": result,
                "test_type": grammar_type,
                "status": "success"
            }
            
            print(f"æœŸå¾…å€¤: {expected}")
            print(f"å®Ÿéš›å€¤: {result.get('slots', {})}")  # main_slots -> slots
            if expected.get('sub_slots'):
                print(f"ã‚µãƒ–æœŸå¾…: {expected.get('sub_slots', {})}")
                print(f"ã‚µãƒ–å®Ÿéš›: {result.get('sub_slots', {})}")
                print(f"ğŸ” DEBUG: å®Œå…¨ãªresultæ§‹é€ : {result}")  # ãƒ‡ãƒãƒƒã‚°ç”¨
            
        except Exception as e:
            print(f"å‹•çš„æ–‡æ³•è§£æã‚¨ãƒ©ãƒ¼: {e}")
            results["results"][test_id] = {
                "sentence": sentence,
                "expected": expected,
                "analysis_result": {},
                "test_type": grammar_type,
                "status": "error",
                "error": str(e)
            }
            print(f"æœŸå¾…å€¤: {expected}")
            print(f"å®Ÿéš›å€¤: {{}}")
            if expected.get('sub_slots'):
                print(f"ã‚µãƒ–æœŸå¾…: {expected.get('sub_slots', {})}")
                print(f"ã‚µãƒ–å®Ÿéš›: {{}}")
        
        print("-" * 60)
    
    print(f"\n=== ãƒ†ã‚¹ãƒˆæ¦‚è¦ ===")
    for grammar_type, count in grammar_counts.items():
        print(f"{grammar_type}ãƒ†ã‚¹ãƒˆ: {count}ä»¶")
    print(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {len(selected_cases)}ä»¶")
    
    # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_file = "official_test_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"çµæœã‚’ä¿å­˜: {output_file}")
    return output_file

def main():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œç”¨ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description='æ­£å¼ãƒ†ã‚¹ãƒˆæ‰‹é †å®Ÿè¡Œï¼ˆå‹•çš„ç‰ˆï¼‰')
    parser.add_argument('--tests', '-t', 
                       type=str,
                       help='å®Ÿè¡Œã™ã‚‹ãƒ†ã‚¹ãƒˆç•ªå·ï¼ˆä¾‹: "1,2,3-5,8" ã¾ãŸã¯ "basic" ã¾ãŸã¯ "relation" ã¾ãŸã¯ "passive"ï¼‰')
    parser.add_argument('--all', action='store_true', help='å…¨ã¦ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å®Ÿè¡Œ')
    
    args = parser.parse_args()
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰
    test_cases = load_test_cases()
    
    if args.all:
        # å…¨ã¦ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å®Ÿè¡Œ
        selected_cases = test_cases
        print("ğŸ”¥ å…¨ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å®Ÿè¡Œã—ã¾ã™")
    elif args.tests:
        # æŒ‡å®šã•ã‚ŒãŸãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å®Ÿè¡Œ
        selected_cases = select_test_cases(test_cases, args.tests)
        print(f"ğŸ¯ é¸æŠã•ã‚ŒãŸãƒ†ã‚¹ãƒˆ: {args.tests}")
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: åŸºæœ¬5æ–‡å‹ + é–¢ä¿‚ç¯€ + å—å‹•æ…‹ (24ä»¶)
        basic_cases = select_test_cases(test_cases, "basic")
        relation_cases = select_test_cases(test_cases, "relation") 
        passive_cases = select_test_cases(test_cases, "passive")
        selected_cases = basic_cases + relation_cases + passive_cases
        print("ğŸ¯ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå®Ÿè¡Œ: åŸºæœ¬5æ–‡å‹ + é–¢ä¿‚ç¯€ + å—å‹•æ…‹ (24ä»¶)")
    
    run_official_test_with_selected_cases(selected_cases)

if __name__ == "__main__":
    main()
