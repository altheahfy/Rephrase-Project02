#!/usr/bin/env python3
"""
åŒ…æ‹¬çš„æ–‡æ³•è§£æã‚·ã‚¹ãƒ†ãƒ  - ç¯€ä»¥å¤–ã®æ–‡æ³•è¦ç´ ã‚‚æ¤œå‡º

ç¾åœ¨ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã¯è¦‹é€ƒã•ã‚Œã¦ã„ã‚‹æ–‡æ³•è¦ç´ ï¼š
1. å€’ç½®æ§‹é€  (Inversion)
2. æ™‚åˆ¶ãƒ»ç›¸ (Tense/Aspect)
3. å¼·èª¿æ§‹é€  (Emphasis)
4. å¦å®šæ§‹é€  (Negation patterns)
5. æ¡ä»¶æ§‹é€  (Conditional patterns)
"""

import spacy
import stanza
from hierarchical_grammar_detector_v4 import HierarchicalGrammarDetector

class ComprehensiveGrammarDetector:
    def __init__(self):
        print("ğŸ”§ Comprehensive Grammar Detector åˆæœŸåŒ–ä¸­...")
        self.nlp_spacy = spacy.load("en_core_web_sm")
        stanza.download('en', quiet=True)
        self.nlp_stanza = stanza.Pipeline('en', quiet=True)
        self.v4_detector = HierarchicalGrammarDetector()
    
    def analyze_comprehensive_grammar(self, sentence):
        """åŒ…æ‹¬çš„æ–‡æ³•è§£æ"""
        print(f"\nğŸ” åŒ…æ‹¬çš„è§£æ: {sentence}")
        print("=" * 60)
        
        doc = self.nlp_spacy(sentence)
        
        result = {
            'sentence': sentence,
            'basic_pattern': None,
            'special_structures': [],
            'tense_aspect': None,
            'emphasis': [],
            'negation': [],
            'word_order': 'normal',
            'complexity_score': 0
        }
        
        # 1. åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºï¼ˆV4ä½¿ç”¨ï¼‰
        try:
            v4_result = self.v4_detector.detect_hierarchical_grammar(sentence)
            result['basic_pattern'] = v4_result.main_pattern
        except:
            result['basic_pattern'] = 'unknown'
        
        # 2. èªé †åˆ†æï¼ˆå€’ç½®æ¤œå‡ºï¼‰
        result['word_order'] = self._detect_word_order(doc)
        
        # 3. æ™‚åˆ¶ãƒ»ç›¸åˆ†æ
        result['tense_aspect'] = self._detect_tense_aspect(doc)
        
        # 4. å¼·èª¿æ§‹é€ æ¤œå‡º
        result['emphasis'] = self._detect_emphasis(doc)
        
        # 5. å¦å®šæ§‹é€ æ¤œå‡º
        result['negation'] = self._detect_negation(doc)
        
        # 6. ç‰¹æ®Šæ§‹é€ æ¤œå‡º
        result['special_structures'] = self._detect_special_structures(doc)
        
        # 7. è¤‡é›‘åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
        result['complexity_score'] = self._calculate_complexity(result)
        
        self._print_analysis(result)
        return result
    
    def _detect_word_order(self, doc):
        """èªé †ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º"""
        # ROOTå‹•è©ã‚’æ¢ã™
        root_verb = None
        for token in doc:
            if token.dep_ == "ROOT" and token.pos_ == "VERB":
                root_verb = token
                break
        
        if not root_verb:
            return 'no_verb'
        
        # ä¸»èªã®ä½ç½®ãƒã‚§ãƒƒã‚¯
        subject_before_verb = False
        aux_before_subject = False
        
        for token in doc:
            if token.dep_ == "nsubj":
                if token.i < root_verb.i:
                    subject_before_verb = True
                break
        
        # åŠ©å‹•è©ãŒä¸»èªã‚ˆã‚Šå‰ã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        for token in doc:
            if token.pos_ == "AUX" and token.dep_ == "aux":
                subj_tokens = [t for t in doc if t.dep_ == "nsubj"]
                if subj_tokens and token.i < subj_tokens[0].i:
                    aux_before_subject = True
                break
        
        # æ–‡é ­ã®å‰¯è©ã‚„å¦å®šèªãƒã‚§ãƒƒã‚¯
        first_meaningful = None
        for token in doc:
            if token.pos_ not in ["PUNCT", "SPACE"]:
                first_meaningful = token
                break
        
        if first_meaningful:
            # å¦å®šå‰¯è©ã§å§‹ã¾ã‚‹å€’ç½®
            if first_meaningful.text.lower() in ['never', 'rarely', 'seldom', 'hardly', 'scarcely', 'little', 'not', 'nowhere']:
                if aux_before_subject:
                    return 'negative_inversion'
            
            # æ¡ä»¶ç¯€ã®å€’ç½® (Had I known...)
            if first_meaningful.pos_ == "AUX" and first_meaningful.text.lower() in ['had', 'were', 'should', 'could']:
                return 'conditional_inversion'
            
            # onlyæ§‹é€ ã®å€’ç½®
            if first_meaningful.text.lower() == 'only':
                return 'only_inversion'
        
        return 'normal' if subject_before_verb else 'inverted'
    
    def _detect_tense_aspect(self, doc):
        """æ™‚åˆ¶ãƒ»ç›¸ã®æ¤œå‡º"""
        aux_verbs = []
        main_verbs = []
        
        for token in doc:
            if token.pos_ == "AUX":
                aux_verbs.append(token.text.lower())
            elif token.pos_ == "VERB":
                main_verbs.append((token.text, token.tag_))
        
        # å®Œäº†æ™‚åˆ¶æ¤œå‡º
        if 'have' in aux_verbs or 'has' in aux_verbs or 'had' in aux_verbs:
            past_participles = [v for v, tag in main_verbs if tag in ['VBN']]
            if past_participles:
                if 'had' in aux_verbs:
                    return 'past_perfect'
                elif 'have' in aux_verbs or 'has' in aux_verbs:
                    return 'present_perfect'
        
        # é€²è¡Œæ™‚åˆ¶æ¤œå‡º
        if 'be' in [token.lemma_.lower() for token in doc if token.pos_ == "AUX"]:
            present_participles = [v for v, tag in main_verbs if tag in ['VBG']]
            if present_participles:
                return 'progressive'
        
        # æœªæ¥æ™‚åˆ¶æ¤œå‡º
        if 'will' in aux_verbs or 'shall' in aux_verbs:
            return 'future'
        
        # æ¡ä»¶æ³•æ¤œå‡º
        if 'would' in aux_verbs or 'could' in aux_verbs or 'should' in aux_verbs:
            return 'conditional'
        
        return 'simple'
    
    def _detect_emphasis(self, doc):
        """å¼·èª¿æ§‹é€ æ¤œå‡º"""
        emphasis = []
        
        # å¼·èª¿å‰¯è©
        emphasis_adverbs = ['very', 'extremely', 'absolutely', 'completely', 'totally', 'really', 'quite', 'rather', 'indeed', 'certainly']
        for token in doc:
            if token.text.lower() in emphasis_adverbs:
                emphasis.append(f"emphasis_adverb: {token.text}")
        
        # Doå¼·èª¿
        for token in doc:
            if token.text.lower() == 'do' and token.pos_ == 'AUX' and token.dep_ != 'aux':
                emphasis.append("do_emphasis")
        
        # It is ... thatæ§‹æ–‡
        tokens_text = [t.text.lower() for t in doc]
        if 'it' in tokens_text and 'is' in tokens_text and 'that' in tokens_text:
            emphasis.append("cleft_sentence")
        
        return emphasis
    
    def _detect_negation(self, doc):
        """å¦å®šæ§‹é€ æ¤œå‡º"""
        negation = []
        
        # åŸºæœ¬å¦å®š
        for token in doc:
            if token.dep_ == "neg":
                negation.append(f"simple_negation: {token.text}")
        
        # å¦å®šå‰¯è©
        neg_adverbs = ['never', 'rarely', 'seldom', 'hardly', 'scarcely', 'barely']
        for token in doc:
            if token.text.lower() in neg_adverbs:
                negation.append(f"negative_adverb: {token.text}")
        
        # å¦å®šå¥
        neg_phrases = ['not only', 'by no means', 'under no circumstances', 'in no way']
        sentence_lower = doc.text.lower()
        for phrase in neg_phrases:
            if phrase in sentence_lower:
                negation.append(f"negative_phrase: {phrase}")
        
        return negation
    
    def _detect_special_structures(self, doc):
        """ç‰¹æ®Šæ§‹é€ æ¤œå‡º"""
        structures = []
        
        # çœç•¥æ§‹é€ 
        if len([t for t in doc if t.pos_ == "VERB"]) == 0:
            structures.append("ellipsis")
        
        # æ„Ÿå˜†æ§‹é€ 
        if any(token.text in ['What', 'How'] for token in doc):
            structures.append("exclamation")
        
        # ç–‘å•æ§‹é€ 
        if doc.text.endswith('?'):
            structures.append("question")
        
        # å‘½ä»¤æ§‹é€ 
        if doc[0].pos_ == "VERB" and len([t for t in doc if t.dep_ == "nsubj"]) == 0:
            structures.append("imperative")
        
        return structures
    
    def _calculate_complexity(self, result):
        """è¤‡é›‘åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score = 0
        
        # èªé †ã®è¤‡é›‘ã•
        if result['word_order'] != 'normal':
            score += 2
        
        # æ™‚åˆ¶ã®è¤‡é›‘ã•
        if result['tense_aspect'] in ['present_perfect', 'past_perfect', 'conditional']:
            score += 2
        elif result['tense_aspect'] in ['progressive', 'future']:
            score += 1
        
        # å¼·èª¿ãƒ»å¦å®š
        score += len(result['emphasis']) * 1
        score += len(result['negation']) * 1
        
        # ç‰¹æ®Šæ§‹é€ 
        score += len(result['special_structures']) * 1
        
        return score
    
    def _print_analysis(self, result):
        """åˆ†æçµæœå‡ºåŠ›"""
        print(f"ğŸ“Š åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³: {result['basic_pattern']}")
        print(f"ğŸ“Š èªé †: {result['word_order']}")
        print(f"ğŸ“Š æ™‚åˆ¶ãƒ»ç›¸: {result['tense_aspect']}")
        
        if result['emphasis']:
            print(f"ğŸ“Š å¼·èª¿: {', '.join(result['emphasis'])}")
        
        if result['negation']:
            print(f"ğŸ“Š å¦å®š: {', '.join(result['negation'])}")
        
        if result['special_structures']:
            print(f"ğŸ“Š ç‰¹æ®Šæ§‹é€ : {', '.join(result['special_structures'])}")
        
        print(f"ğŸ“Š è¤‡é›‘åº¦ã‚¹ã‚³ã‚¢: {result['complexity_score']}")

def main():
    detector = ComprehensiveGrammarDetector()
    
    # ãƒ†ã‚¹ãƒˆæ–‡
    test_sentences = [
        "Never have I seen such a beautiful sunset.",
        "Little did I know what would happen.",
        "Had I known earlier, I would have acted.",
        "Not only is he smart, but he is also kind.",
        "Rarely do we see such dedication.",
        "Under no circumstances should you do this.",
        "I have been working here for five years.",
        "She is extremely talented.",
        "What a beautiful day it is!",
        "Do tell me the truth.",
    ]
    
    results = []
    for sentence in test_sentences:
        result = detector.analyze_comprehensive_grammar(sentence)
        results.append(result)
    
    print("\n" + "="*80)
    print("ğŸ“ˆ ç·åˆè©•ä¾¡")
    print("="*80)
    
    # è¤‡é›‘åº¦åˆ¥åˆ†é¡
    simple_sentences = [r for r in results if r['complexity_score'] <= 1]
    moderate_sentences = [r for r in results if 1 < r['complexity_score'] <= 3]
    complex_sentences = [r for r in results if r['complexity_score'] > 3]
    
    print(f"ğŸŸ¢ ã‚·ãƒ³ãƒ—ãƒ« (ã‚¹ã‚³ã‚¢â‰¤1): {len(simple_sentences)}")
    print(f"ğŸŸ¡ ä¸­ç¨‹åº¦ (ã‚¹ã‚³ã‚¢2-3): {len(moderate_sentences)}")  
    print(f"ğŸ”´ è¤‡é›‘ (ã‚¹ã‚³ã‚¢>3): {len(complex_sentences)}")
    
    # ç¾åœ¨ã®V4/V5.1ã§ã¯è¦‹é€ƒã•ã‚Œã‚‹æ§‹é€ 
    missed_structures = 0
    for result in results:
        if (result['word_order'] != 'normal' or 
            result['tense_aspect'] not in ['simple'] or
            result['emphasis'] or result['negation']):
            missed_structures += 1
    
    print(f"\nğŸš¨ ç¾åœ¨ã®ã‚·ã‚¹ãƒ†ãƒ ã§è¦‹é€ƒã•ã‚Œã‚‹è¤‡é›‘æ§‹é€ : {missed_structures}/{len(results)} ({missed_structures/len(results)*100:.1f}%)")

if __name__ == "__main__":
    main()
