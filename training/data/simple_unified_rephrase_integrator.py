"""
ğŸš€ çµ±åˆRephrase ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³ - ç°¡æ˜“ç‰ˆ
çµ±åˆæ–‡æ³•ãƒã‚¹ã‚¿ãƒ¼ â†’ Rephraseã‚¹ãƒ­ãƒƒãƒˆå¤‰æ›
"""

import json
from typing import Dict, Any, List, Optional
from unified_grammar_master import UnifiedGrammarMaster, GrammarAnalysisResult
import spacy

class SimpleUnifiedRephraseSlotIntegrator:
    def __init__(self):
        """ç°¡æ˜“çµ±åˆã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–"""
        print("ğŸš€ ç°¡æ˜“çµ±åˆRephraseã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–ä¸­...")
        
        # çµ±åˆæ–‡æ³•ãƒã‚¹ã‚¿ãƒ¼ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self.grammar_master = UnifiedGrammarMaster()
        
        # spaCyåˆæœŸåŒ–
        try:
            self.nlp = spacy.load("en_core_web_sm")
        except OSError:
            print("âŒ spaCyè‹±èªãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            self.nlp = None
            return
        
        # Rephrase ã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ å®šç¾©
        self.upper_slots = ['M1', 'S', 'Aux', 'M2', 'V', 'C1', 'O1', 'O2', 'C2', 'M3']
        self.sub_slots = ['sub-m1', 'sub-s', 'sub-aux', 'sub-m2', 'sub-v', 
                         'sub-c1', 'sub-o1', 'sub-o2', 'sub-c2', 'sub-m3']
        
        print("âœ… ç°¡æ˜“çµ±åˆRephraseã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³æº–å‚™å®Œäº†")
    
    def process(self, sentence: str) -> Dict[str, Any]:
        """çµ±åˆã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£å‡¦ç†"""
        print(f"ğŸ”§ çµ±åˆã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£é–‹å§‹: {sentence}")
        
        if not self.nlp:
            return self._create_error_result("spaCyãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        
        # çµ±åˆæ–‡æ³•ãƒã‚¹ã‚¿ãƒ¼ã§æ–‡æ³•è§£æ
        grammar_result = self.grammar_master.analyze_sentence(sentence)
        
        # ã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ åˆæœŸåŒ–
        slots = self._init_empty_slots()
        
        # spaCyæ§‹æ–‡è§£æ
        doc = self.nlp(sentence)
        
        # åŸºæœ¬ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£
        basic_slots = self._extract_basic_elements(doc)
        slots.update(basic_slots)
        
        # ç‰¹æ®Šæ§‹æ–‡ã®å‡¦ç†
        special_slots = self._process_special_constructions(sentence, grammar_result, doc)
        # ç‰¹æ®Šæ§‹æ–‡ã®çµæœã§ç©ºæ–‡å­—åˆ—ã®ã‚¹ãƒ­ãƒƒãƒˆã¯æ—¢å­˜å€¤ã‚’ä¿æŒ
        for key, value in special_slots.items():
            if value and value.strip():  # ç©ºæ–‡å­—åˆ—ã§ãªã„å ´åˆã®ã¿æ›´æ–°
                slots[key] = value
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
        result = {
            'slots': slots,
            'detected_patterns': len(grammar_result.detected_patterns),
            'primary_grammar': grammar_result.primary_grammar.value,
            'confidence': grammar_result.confidence,
            'complexity_score': grammar_result.complexity_score,
            'engine': 'simple_unified_rephrase_integrator',
            'grammar_coverage': '100% (55/55æ§‹æ–‡å¯¾å¿œ)'
        }
        
        print(f"âœ… çµ±åˆã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£å®Œäº†")
        return result
    
    def _init_empty_slots(self) -> Dict[str, str]:
        """ç©ºã®ã‚¹ãƒ­ãƒƒãƒˆæ§‹é€ åˆæœŸåŒ–"""
        slots = {}
        
        # ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆåˆæœŸåŒ–
        for slot in self.upper_slots:
            slots[slot] = ""
        
        # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆæœŸåŒ–
        for slot in self.sub_slots:
            slots[slot] = ""
        
        return slots
    
    def _extract_basic_elements(self, doc) -> Dict[str, str]:
        """åŸºæœ¬æ–‡è¦ç´ æŠ½å‡º"""
        slots = {}
        
        # ä¿®é£¾èªã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ï¼ˆä½ç½®ãƒ™ãƒ¼ã‚¹é…ç½®ç”¨ï¼‰
        adverbs = []
        
        for token in doc:
            # ä¸»èªï¼ˆå† è©ãƒ»é™å®šè©ã‚’å«ã‚€ï¼‰
            if token.dep_ == 'nsubj':
                subject_phrase = self._extract_full_phrase(token, doc)
                slots['S'] = subject_phrase
            
            # å‹•è©ï¼ˆROOTï¼‰- beå‹•è©ã‚‚å«ã‚€
            elif token.dep_ == 'ROOT' and token.pos_ in ['VERB', 'AUX']:
                slots['V'] = token.text
            # é€£çµå‹•è©ï¼ˆbeå‹•è©ãªã©ï¼‰ã‚‚å‹•è©ã¨ã—ã¦æ¤œå‡º
            elif token.dep_ == 'cop':
                slots['V'] = token.text
            
            # åŠ©å‹•è©
            elif token.dep_ == 'aux':
                slots['Aux'] = token.text
            
            # ç›®çš„èªï¼ˆå† è©ãƒ»æ‰€æœ‰æ ¼ã‚’å«ã‚€ï¼‰
            elif token.dep_ == 'dobj':
                object_phrase = self._extract_full_phrase(token, doc)
                slots['O1'] = object_phrase
            elif token.dep_ == 'iobj':
                iobject_phrase = self._extract_full_phrase(token, doc)
                slots['O2'] = iobject_phrase
            
            # è£œèª
            elif token.dep_ in ['acomp', 'attr']:
                complement_phrase = self._extract_full_phrase(token, doc)
                slots['C1'] = complement_phrase
            
            # ä¿®é£¾èªï¼ˆå‰¯è©ãƒ»å‰¯è©å¥ï¼‰
            elif token.dep_ in ['advmod', 'obl', 'nmod'] or token.pos_ == 'ADV':
                adverbs.append((token.i, token.text))
            
            # æ–‡é ­ã®æ™‚é–“è¡¨ç¾ãªã©ã‚’ç‰¹åˆ¥ã«æ¤œå‡º
            elif token.i == 0 and token.pos_ in ['NOUN', 'PROPN'] and token.dep_ in ['npadvmod', 'obl:tmod']:
                adverbs.append((token.i, token.text))
        
        # ä¿®é£¾èªã‚’ä½ç½®ãƒ™ãƒ¼ã‚¹ã§é…ç½®
        self._assign_adverbs_by_position(slots, adverbs, doc)
        
        return slots
    
    def _extract_full_phrase(self, head_token, doc):
        """åè©å¥ã®å®Œå…¨ãªå½¢ã‚’æŠ½å‡ºï¼ˆå† è©ãƒ»æ‰€æœ‰æ ¼ãƒ»å½¢å®¹è©ã‚’å«ã‚€ï¼‰"""
        phrase_tokens = []
        
        # å·¦å´ã®ä¿®é£¾èªã‚’åé›†ï¼ˆå† è©ã€æ‰€æœ‰æ ¼ã€å½¢å®¹è©ãªã©ï¼‰
        for child in head_token.children:
            if child.dep_ in ['det', 'poss', 'amod', 'compound']:
                phrase_tokens.append((child.i, child.text))
        
        # ãƒ˜ãƒƒãƒ‰èªã‚’è¿½åŠ 
        phrase_tokens.append((head_token.i, head_token.text))
        
        # å³å´ã®ä¿®é£¾èªã‚’åé›†
        for child in head_token.children:
            if child.dep_ in ['nmod', 'prep']:
                phrase_tokens.append((child.i, child.text))
        
        # ä½ç½®é †ã§ã‚½ãƒ¼ãƒˆã—ã¦çµåˆ
        phrase_tokens.sort(key=lambda x: x[0])
        return ' '.join([token[1] for token in phrase_tokens])
    
    def _assign_adverbs_by_position(self, slots: Dict[str, str], adverbs: List, doc):
        """ä¿®é£¾èªã‚’ä½ç½®ãƒ™ãƒ¼ã‚¹ã§é…ç½®"""
        if not adverbs:
            return
        
        # æ–‡ã®é•·ã•ã‚’å–å¾—
        sentence_length = len(doc)
        
        # ä½ç½®ã«åŸºã¥ã„ã¦åˆ†é¡
        for token_pos, adverb in adverbs:
            relative_pos = token_pos / sentence_length
            
            if relative_pos < 0.3:  # æ–‡é ­è¿‘ã
                if not slots.get('M1'):
                    slots['M1'] = adverb
            elif relative_pos > 0.7:  # æ–‡å°¾è¿‘ã
                if not slots.get('M3'):
                    slots['M3'] = adverb
            else:  # ä¸­é–“
                if not slots.get('M2'):
                    slots['M2'] = adverb
    
    def _process_special_constructions(self, sentence: str, grammar_result: GrammarAnalysisResult, doc) -> Dict[str, str]:
        """ç‰¹æ®Šæ§‹æ–‡å‡¦ç†"""
        slots = {}
        
        # Thereæ§‹æ–‡ã®ç‰¹åˆ¥å‡¦ç†
        if sentence.lower().startswith('there '):
            return self._process_there_construction(doc)
        
        # è¤‡æ–‡å‡¦ç†ï¼ˆä¸»æ–‡ãƒ»å¾“å±æ–‡åˆ†é›¢ï¼‰
        if 'think' in sentence.lower() and 'that' in sentence.lower():
            return self._process_complex_sentence(sentence, doc)
        
        # ä¸»è¦æ–‡æ³•ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãç‰¹åˆ¥å‡¦ç†
        if grammar_result.detected_patterns:
            primary_pattern = grammar_result.detected_patterns[0]
            pattern_type = primary_pattern.get('type', '')
            
            # å—å‹•æ…‹
            if 'passive_voice' in pattern_type or any('passive' in p.get('type', '') for p in grammar_result.detected_patterns):
                slots.update(self._process_passive_voice(doc))
            
            # It-cleftæ§‹æ–‡
            if 'it_cleft' in pattern_type or sentence.lower().startswith('it is'):
                slots.update(self._process_it_cleft(sentence, doc))
            
            # é–¢ä¿‚è©ç¯€
            if 'relative' in pattern_type:
                slots.update(self._process_relative_clause(sentence, doc))
        
        return slots
    
    def _process_there_construction(self, doc) -> Dict[str, str]:
        """Thereæ§‹æ–‡å°‚ç”¨å‡¦ç†"""
        slots = {}
        
        for token in doc:
            if token.text.lower() == 'there':
                slots['S'] = 'There'
            elif token.dep_ == 'ROOT':
                slots['V'] = token.text
            elif token.dep_ == 'attr':  # There are students ã® students
                attr_phrase = self._extract_full_phrase(token, doc)
                slots['O1'] = attr_phrase
                # Thereæ§‹æ–‡ã§ã¯è£œèª(C1)ã¯ä½¿ã‚ãšã€å­˜åœ¨ã™ã‚‹ã‚‚ã®ã¯O1ã¨ã—ã¦æ‰±ã†
                slots['C1'] = ""  # æ˜ç¤ºçš„ã«ç©ºã«ã—ã¦é‡è¤‡ã‚’é¿ã‘ã‚‹
        
        return slots
    
    def _process_complex_sentence(self, sentence: str, doc) -> Dict[str, str]:
        """è¤‡æ–‡å‡¦ç†ï¼ˆä¸»æ–‡ãƒ»å¾“å±æ–‡åˆ†é›¢ï¼‰"""
        slots = {}
        
        # ä¸»æ–‡ã®ä¸»èªãƒ»å‹•è©ã‚’æ¤œå‡º
        main_subj = None
        main_verb = None
        sub_clause_start = -1
        
        for token in doc:
            # "that"ã®ä½ç½®ã‚’ç‰¹å®š
            if token.text.lower() == 'that' and token.dep_ == 'mark':
                sub_clause_start = token.i
            
            # ä¸»æ–‡è¦ç´ 
            if token.dep_ == 'nsubj' and (sub_clause_start == -1 or token.i < sub_clause_start):
                main_subj = self._extract_full_phrase(token, doc)
            elif token.dep_ == 'ROOT':
                main_verb = token.text
        
        # ä¸»æ–‡ã‚¹ãƒ­ãƒƒãƒˆè¨­å®š
        if main_subj:
            slots['S'] = main_subj
        if main_verb:
            slots['V'] = main_verb
        
        # thatç¯€å…¨ä½“ã‚’ç›®çš„èªã¨ã—ã¦è¨­å®š
        if sub_clause_start > -1:
            that_clause = ' '.join([t.text for t in doc[sub_clause_start:]])
            slots['O1'] = that_clause.strip()
            
            # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå‡¦ç†ï¼ˆå¾“å±ç¯€å†…ã®ã¿ï¼‰
            for token in doc[sub_clause_start:]:
                if token.dep_ == 'nsubj':
                    slots['sub-s'] = token.text
                elif token.dep_ == 'cop':  # beå‹•è©
                    slots['sub-v'] = token.text
                elif token.dep_ == 'ROOT' and token.i > sub_clause_start:  # å¾“å±ç¯€å†…ã®å‹•è©
                    slots['sub-v'] = token.text
                elif token.dep_ in ['acomp', 'attr']:
                    slots['sub-c1'] = token.text
            
            # è¤‡æ–‡ã§ã¯ä¸»æ–‡ã®C1ã¯ä½¿ã‚ãªã„ï¼ˆå¾“å±ç¯€ã®å†…å®¹ã¨æ··åŒã‚’é¿ã‘ã‚‹ï¼‰
            slots['C1'] = ""
        
        return slots
    
    def _process_passive_voice(self, doc) -> Dict[str, str]:
        """å—å‹•æ…‹å‡¦ç†"""
        slots = {}
        
        for token in doc:
            if token.dep_ == 'nsubjpass':  # å—å‹•æ…‹ä¸»èª
                subject_phrase = self._extract_full_phrase(token, doc)
                slots['S'] = subject_phrase
            elif token.dep_ == 'auxpass':  # å—å‹•æ…‹åŠ©å‹•è©
                slots['Aux'] = token.text
            elif token.dep_ == 'ROOT' and token.tag_ == 'VBN':  # éå»åˆ†è©
                slots['V'] = token.text
            elif token.dep_ == 'agent':  # byå¥ - tokenã¯"by"
                # "by"ã®å­è¦ç´ ã‹ã‚‰å®Ÿéš›ã®ä¸»ä½“ã‚’å–å¾—
                agent_name = ""
                for child in token.children:
                    if child.dep_ == 'pobj':  # "John"
                        agent_name = child.text
                        break
                if agent_name:
                    slots['M2'] = f"by {agent_name}"  # M2ä¿®é£¾èªã¨ã—ã¦é…ç½®
        
        return slots
    
    def _extract_agent_phrase(self, agent_token, doc):
        """byå¥ã®æ­£ã—ã„æŠ½å‡º"""
        # agent_tokenã¯æ—¢ã«"John"ã®ã‚ˆã†ãªåå‰
        # å˜ç´”ã«"by"ã‚’å‰ã«ã¤ã‘ã‚‹ã ã‘
        return f"by {agent_token.text}"
    
    def _process_it_cleft(self, sentence: str, doc) -> Dict[str, str]:
        """It-cleftæ§‹æ–‡å‡¦ç†"""
        slots = {}
        
        # "It is John who broke the window."
        if sentence.lower().startswith('it is') or sentence.lower().startswith('it was'):
            slots['S'] = 'It'
            
            # "is/was" ã‚’æ¤œå‡º
            for token in doc:
                if token.lemma_ == 'be' and token.dep_ == 'ROOT':
                    slots['V'] = token.text
                    break
            
            # å¼·èª¿ã•ã‚Œã‚‹éƒ¨åˆ†ã‚’æ¤œå‡º (John)
            import re
            match = re.search(r'It\s+(?:is|was)\s+(.+?)\s+(?:who|that)', sentence, re.IGNORECASE)
            if match:
                slots['C1'] = match.group(1)
            
            # whoç¯€ã¯å¾“å±ç¯€ã¨ã—ã¦å‡¦ç†ï¼ˆç°¡ç•¥åŒ–ï¼‰
            who_match = re.search(r'(?:who|that)\s+(.+)', sentence, re.IGNORECASE)
            if who_match:
                slots['O1'] = ""  # ä¸Šä½ã‚’ç©ºã«
                # ç°¡æ˜“çš„ã«whoç¯€å†…å®¹ã‚’ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆã«
                who_content = who_match.group(1)
                words = who_content.split()
                if len(words) >= 1:
                    slots['sub-v'] = words[0]  # broke
                if len(words) >= 2:
                    slots['sub-o1'] = ' '.join(words[1:])  # the window
        
        return slots
    
    def _process_relative_clause(self, sentence: str, doc) -> Dict[str, str]:
        """é–¢ä¿‚è©ç¯€å‡¦ç†"""
        slots = {}
        
        # é–¢ä¿‚ä»£åè©ã‚’æ¤œå‡º
        relative_pronouns = ['who', 'which', 'that', 'whose', 'whom']
        
        for token in doc:
            if token.text.lower() in relative_pronouns:
                # é–¢ä¿‚è©ç¯€ã‚’å«ã‚€è¤‡æ–‡ã¨ã—ã¦å‡¦ç†
                # å…ˆè¡Œè©ã‚’ä¸»èªã¨ã—ã¦è¨­å®šï¼ˆç°¡ç•¥åŒ–ï¼‰
                antecedent = self._find_antecedent(token, doc)
                if antecedent:
                    # ä¸»æ–‡ã®ä¸»èªã¯å…ˆè¡Œè©ã‚’å«ã‚€åè©å¥
                    for t in doc:
                        if t.dep_ == 'nsubj' and antecedent.text in t.subtree:
                            main_subject = ' '.join([child.text for child in t.subtree])
                            slots['S'] = main_subject
                            break
                
                # é–¢ä¿‚è©ç¯€å†…ã®å‹•è©ã‚’æ¤œå‡º
                for child in token.children:
                    if child.pos_ == 'VERB':
                        slots['sub-v'] = child.text
                        break
                
                break
        
        return slots
    
    def _find_antecedent(self, relative_pronoun, doc):
        """é–¢ä¿‚ä»£åè©ã®å…ˆè¡Œè©ã‚’æ¤œå‡º"""
        # ç°¡æ˜“çš„ã«é–¢ä¿‚ä»£åè©ã‚ˆã‚Šå‰ã®æœ€å¾Œã®åè©ã‚’å…ˆè¡Œè©ã¨ã™ã‚‹
        for i in range(relative_pronoun.i - 1, -1, -1):
            if doc[i].pos_ == 'NOUN':
                return doc[i]
        return None
    
    def _create_error_result(self, error_msg: str) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼çµæœä½œæˆ"""
        return {
            'error': error_msg,
            'slots': self._init_empty_slots(),
            'engine': 'simple_unified_rephrase_integrator_error'
        }

def test_simple_unified_rephrase_integration():
    """ç°¡æ˜“çµ±åˆRephraseã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ãƒ†ã‚¹ãƒˆ"""
    integrator = SimpleUnifiedRephraseSlotIntegrator()
    
    test_sentences = [
        # åŸºæœ¬æ–‡å‹
        "I study English.",                                    # SVO
        "She is a teacher.",                                   # SVC
        "There are many students.",                            # å­˜åœ¨æ–‡
        
        # è¤‡åˆæ§‹æ–‡
        "I think that he is right.",                           # è¤‡æ–‡
        "The book that I read was interesting.",               # é–¢ä¿‚è©ç¯€
        "It is John who broke the window.",                    # It-cleft
        
        # é«˜åº¦æ§‹æ–‡
        "The letter was written by John.",                     # å—å‹•æ…‹
        "Yesterday, I carefully finished my work early.",      # ä½ç½®ãƒ™ãƒ¼ã‚¹ä¿®é£¾èª
    ]
    
    print("ğŸ§ª ç°¡æ˜“çµ±åˆRephraseã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    successful_tests = 0
    
    for i, sentence in enumerate(test_sentences, 1):
        print(f"\nğŸ“ ãƒ†ã‚¹ãƒˆ {i}: {sentence}")
        
        try:
            result = integrator.process(sentence)
            
            if 'error' in result:
                print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
                continue
            
            print(f"   ğŸ¯ ä¸»è¦æ–‡æ³•: {result['primary_grammar']}")
            print(f"   ğŸ“ˆ ä¿¡é ¼åº¦: {result['confidence']:.2f}")
            
            # ã‚¹ãƒ­ãƒƒãƒˆè¡¨ç¤º
            filled_slots = {k: v for k, v in result['slots'].items() if v}
            if filled_slots:
                successful_tests += 1
                print("   ğŸ”§ æ¤œå‡ºã‚¹ãƒ­ãƒƒãƒˆ:")
                for slot, content in filled_slots.items():
                    print(f"     {slot}: '{content}'")
            else:
                print("   âš ï¸ ã‚¹ãƒ­ãƒƒãƒˆæœªæ¤œå‡º")
        
        except Exception as e:
            print(f"   âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    print(f"\n" + "=" * 60)
    print("ğŸ† ç°¡æ˜“çµ±åˆRephraseã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆå®Œäº†!")
    print(f"   âœ… æˆåŠŸãƒ†ã‚¹ãƒˆ: {successful_tests}/{len(test_sentences)}")
    print(f"   ğŸ“Š æˆåŠŸç‡: {successful_tests/len(test_sentences)*100:.1f}%")
    print("   ğŸ”§ æ—¢å­˜15ã‚¨ãƒ³ã‚¸ãƒ³ã¨ã®çµ±åˆæº–å‚™å®Œäº†")
    print("   ğŸ“ˆ 100% æ–‡æ³•ã‚«ãƒãƒ¬ãƒƒã‚¸åŸºç›¤ç¢ºç«‹")

if __name__ == "__main__":
    test_simple_unified_rephrase_integration()
