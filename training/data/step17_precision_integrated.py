#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Step 17: ç²¾å¯†ãƒ­ã‚¸ãƒƒã‚¯çµ±åˆç‰ˆã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 

Step15ã‚’ãƒ™ãƒ¼ã‚¹ã«å€‹åˆ¥ã‚·ã‚¹ãƒ†ãƒ ï¼ˆStep10-13ï¼‰ã®å„ªç§€ãªãƒ­ã‚¸ãƒƒã‚¯ã‚’çµ±åˆ
- Step12ã®Sé–¢ä¿‚ä»£åè©å‡¦ç† â†’ Så¼·åŒ–
- Step10ã®C1è£œèªå‡¦ç† â†’ è£œèªæ¤œå‡ºå¼·åŒ–  
- Step11ã®C2è£œèªå‡¦ç† â†’ thatç¯€å‡¦ç†å¼·åŒ–
"""

import spacy
import json
import traceback
from collections import OrderedDict

class PrecisionIntegratedSubslotGenerator:
    """ç²¾å¯†ãƒ­ã‚¸ãƒƒã‚¯çµ±åˆç‰ˆ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        print("ğŸš€ Precision Integrated Subslot Generator èµ·å‹•é–‹å§‹...")
        try:
            self.nlp = spacy.load("en_core_web_sm")
            print("âœ… spaCy 'en_core_web_sm' èª­ã¿è¾¼ã¿å®Œäº†")
        except IOError:
            print("âŒ spaCyè‹±èªãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            raise
        
        # 10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå®šç¾©
        self.subslot_types = [
            'sub-s', 'sub-v', 'sub-o1', 'sub-o2', 
            'sub-c1', 'sub-c2', 'sub-m1', 'sub-m2', 'sub-m3', 'sub-aux'
        ]
        
        # å…¨10ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆï¼ˆRephraseå®Œå…¨ä½“ï¼‰
        self.all_upper_slots = ['M1', 'S', 'Aux', 'M2', 'V', 'C1', 'O1', 'O2', 'C2', 'M3']
        
        # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå¯¾è±¡8ã‚¹ãƒ­ãƒƒãƒˆï¼ˆAuxã€Vé™¤ãï¼‰
        self.target_slots = ['M1', 'S', 'M2', 'C1', 'O1', 'O2', 'C2', 'M3']
        
        # å˜ä¸€æ©Ÿèƒ½ã‚¹ãƒ­ãƒƒãƒˆï¼ˆã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç„¡ã—ï¼‰
        self.single_slots = ['Aux', 'V']
        
        print(f"ğŸ¯ å…¨10ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆ: {', '.join(self.all_upper_slots)}")
        print(f"ğŸ”§ ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå¯¾è±¡8ã‚¹ãƒ­ãƒƒãƒˆ: {', '.join(self.target_slots)}")
        print(f"âš¡ å˜ä¸€æ©Ÿèƒ½ã‚¹ãƒ­ãƒƒãƒˆ: {', '.join(self.single_slots)}")
        print(f"ğŸ§© 10ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆä½“ç³»: {', '.join(self.subslot_types)}")
        
    def generate_subslots_for_slot(self, slot_name, sentence):
        """æŒ‡å®šã‚¹ãƒ­ãƒƒãƒˆã®ç²¾å¯†ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆ"""
        print(f"\nğŸ¯ {slot_name}ã‚¹ãƒ­ãƒƒãƒˆ ç²¾å¯†ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆé–‹å§‹: '{sentence}'")
        
        # å˜ä¸€æ©Ÿèƒ½ã‚¹ãƒ­ãƒƒãƒˆï¼ˆAux, Vï¼‰ã®å‡¦ç†
        if slot_name in self.single_slots:
            print(f"âš¡ {slot_name}ã¯å˜ä¸€æ©Ÿèƒ½ã‚¹ãƒ­ãƒƒãƒˆï¼ˆã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç„¡ã—ï¼‰")
            return {
                'slot_phrase': sentence,
                'slot_type': 'single',
                'message': f'{slot_name}ã‚¹ãƒ­ãƒƒãƒˆï¼šã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆåˆ†è§£ä¸è¦'
            }
        
        # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå¯¾è±¡ã‚¹ãƒ­ãƒƒãƒˆã®å‡¦ç†
        if slot_name not in self.target_slots:
            print(f"âŒ éå¯¾è±¡ã‚¹ãƒ­ãƒƒãƒˆ: {slot_name}")
            return {}
        
        try:
            doc = self.nlp(sentence)
            print(f"ğŸ“ è§£æå¯¾è±¡: {[(token.text, token.dep_, token.pos_) for token in doc]}")
            
            # Step15ãƒ™ãƒ¼ã‚¹åŒ…æ‹¬æ¤œå‡º
            base_subslots = self._comprehensive_subslot_detection(doc)
            
            # å€‹åˆ¥ã‚·ã‚¹ãƒ†ãƒ ç²¾å¯†ãƒ­ã‚¸ãƒƒã‚¯é©ç”¨
            if slot_name == 'S':
                enhanced = self._apply_step12_s_precision(doc, base_subslots, sentence)
            elif slot_name == 'C1':
                enhanced = self._apply_step10_c1_precision(doc, base_subslots, sentence) 
            elif slot_name == 'C2':
                enhanced = self._apply_step11_c2_precision(doc, base_subslots, sentence)
            else:
                # ä»–ã®ã‚¹ãƒ­ãƒƒãƒˆï¼ˆM1, M2, O1, O2, M3ï¼‰ã¯Step15ãƒ™ãƒ¼ã‚¹ + è»½å¾®å¼·åŒ–
                enhanced = self._apply_general_enhancements(doc, base_subslots, slot_name)
            
            print(f"ğŸ”§ ç²¾å¯†çµ±åˆå¾Œã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ•°: {len(enhanced)}")
            return enhanced
            
        except Exception as e:
            print(f"âŒ {slot_name}ã‚¹ãƒ­ãƒƒãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            traceback.print_exc()
            return {}
    
    # ================================================================
    # Step15ãƒ™ãƒ¼ã‚¹åŒ…æ‹¬æ¤œå‡ºï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
    # ================================================================
    
    def _comprehensive_subslot_detection(self, doc):
        """Step15ãƒ™ãƒ¼ã‚¹åŒ…æ‹¬çš„ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡º"""
        subslots = {}
        used_tokens = set()
        
        # Step15ã®åŸºæœ¬æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ã‚’é©ç”¨
        basic_slots = self._enhanced_basic_detection(doc, used_tokens)
        subslots.update(basic_slots)
        
        complement_result = self._enhanced_complement_detection(doc, used_tokens)
        subslots.update(complement_result)
        
        modifier_result = self._enhanced_modifier_detection(doc, used_tokens)
        subslots.update(modifier_result)
        
        aux_result = self._enhanced_auxiliary_detection(doc, used_tokens)
        subslots.update(aux_result)
        
        return subslots
    
    def _enhanced_basic_detection(self, doc, used_tokens):
        """åŸºæœ¬è¦ç´ æ¤œå‡ºï¼ˆStep15ãƒ™ãƒ¼ã‚¹ï¼‰"""
        subslots = {}
        
        # ä¸»èªæ¤œå‡º
        subjects_found = []
        for token in doc:
            if token.dep_ in ["nsubj", "nsubjpass", "csubj"] and token.i not in used_tokens:
                subjects_found.append(token)
        
        if subjects_found:
            subject = subjects_found[0]
            subslots['sub-s'] = {
                'text': subject.text,
                'tokens': [subject.text],
                'token_indices': [subject.i]
            }
            used_tokens.add(subject.i)
            print(f"âœ… åŸºæœ¬ä¸»èªæ¤œå‡º: '{subject.text}'")
        
        # å‹•è©æ¤œå‡º
        verbs_found = []
        for token in doc:
            if token.pos_ in ["VERB"] and token.i not in used_tokens:
                verbs_found.append(token)
        
        if verbs_found:
            verb = verbs_found[0]
            subslots['sub-v'] = {
                'text': verb.text,
                'tokens': [verb.text],
                'token_indices': [verb.i]
            }
            used_tokens.add(verb.i)
            print(f"âœ… åŸºæœ¬å‹•è©æ¤œå‡º: '{verb.text}'")
        
        return subslots
    
    def _enhanced_complement_detection(self, doc, used_tokens):
        """è£œèªæ¤œå‡ºï¼ˆStep15ãƒ™ãƒ¼ã‚¹ï¼‰"""
        subslots = {}
        
        for token in doc:
            if token.dep_ in ["acomp", "attr", "pcomp"] and token.i not in used_tokens:
                if 'sub-c1' not in subslots:
                    subslots['sub-c1'] = {
                        'text': token.text,
                        'tokens': [token.text],
                        'token_indices': [token.i]
                    }
                    used_tokens.add(token.i)
                    print(f"âœ… åŸºæœ¬è£œèªæ¤œå‡º: sub-c1='{token.text}'")
                    break
        
        return subslots
    
    def _enhanced_modifier_detection(self, doc, used_tokens):
        """ä¿®é£¾èªæ¤œå‡ºï¼ˆStep15ãƒ™ãƒ¼ã‚¹ï¼‰"""
        subslots = {}
        modifier_slots = ['sub-m1', 'sub-m2', 'sub-m3']
        modifier_count = 0
        
        for token in doc:
            if (token.dep_ in ["advmod", "amod", "npadvmod"] or token.pos_ in ["ADJ", "ADV"]) and token.i not in used_tokens:
                if modifier_count < 3:
                    slot_name = modifier_slots[modifier_count]
                    subslots[slot_name] = {
                        'text': token.text,
                        'tokens': [token.text],
                        'token_indices': [token.i]
                    }
                    used_tokens.add(token.i)
                    print(f"âœ… åŸºæœ¬ä¿®é£¾èªæ¤œå‡º: {slot_name}='{token.text}'")
                    modifier_count += 1
        
        return subslots
    
    def _enhanced_auxiliary_detection(self, doc, used_tokens):
        """åŠ©å‹•è©æ¤œå‡ºï¼ˆStep15ãƒ™ãƒ¼ã‚¹ï¼‰"""
        subslots = {}
        aux_tokens = []
        
        for token in doc:
            if (token.dep_ in ["aux", "auxpass"] or token.pos_ == "AUX") and token.i not in used_tokens:
                aux_tokens.append(token)
        
        if aux_tokens:
            aux_text = ' '.join([t.text for t in aux_tokens])
            subslots['sub-aux'] = {
                'text': aux_text,
                'tokens': [t.text for t in aux_tokens],
                'token_indices': [t.i for t in aux_tokens]
            }
            for t in aux_tokens:
                used_tokens.add(t.i)
            print(f"âœ… åŸºæœ¬åŠ©å‹•è©æ¤œå‡º: '{aux_text}'")
        
        return subslots
    
    # ================================================================
    # å€‹åˆ¥ã‚·ã‚¹ãƒ†ãƒ ç²¾å¯†ãƒ­ã‚¸ãƒƒã‚¯çµ±åˆ
    # ================================================================
    
    def _apply_step12_s_precision(self, doc, base_subslots, sentence):
        """Step12ã®Sä¸»èªç²¾å¯†ãƒ­ã‚¸ãƒƒã‚¯é©ç”¨"""
        print("ğŸ¯ Step12 Sä¸»èªç²¾å¯†ãƒ­ã‚¸ãƒƒã‚¯é©ç”¨")
        enhanced = base_subslots.copy()
        
        # Step12ã®é–¢ä¿‚ä»£åè©ç¯€å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã‚’å†ç¾
        relcl_verb = None
        for token in doc:
            if token.dep_ == "relcl":
                relcl_verb = token
                break
        
        if relcl_verb:
            print("ğŸ” é–¢ä¿‚ä»£åè©ç¯€æ¤œå‡º")
            # Step12ãƒ­ã‚¸ãƒƒã‚¯: "the woman who" ã®ã‚ˆã†ãªæ§‹é€ ã‚’æ­£ç¢ºã«æŠ½å‡º
            head_noun = relcl_verb.head
            if head_noun:
                # ä¸»èªåè©å¥ã®ç¯„å›²ç‰¹å®š
                noun_phrase_start = None
                for child in head_noun.children:
                    if child.dep_ == "det" and child.i < head_noun.i:
                        noun_phrase_start = child.i
                        break
                
                if noun_phrase_start is None:
                    noun_phrase_start = head_noun.i
                
                # é–¢ä¿‚ä»£åè©ã‚’æ¢ã™
                rel_pronoun = None
                for child in relcl_verb.children:
                    if child.dep_ in ["nsubj", "dobj"] and child.pos_ == "PRON":
                        rel_pronoun = child
                        break
                
                if rel_pronoun:
                    # Step12ã®æˆåŠŸãƒ­ã‚¸ãƒƒã‚¯: "the woman who"
                    tokens = list(doc)
                    sub_s_tokens = tokens[noun_phrase_start:head_noun.i+1] + [rel_pronoun]
                    sub_s_tokens = sorted(sub_s_tokens, key=lambda x: x.i)
                    
                    enhanced['sub-s'] = {
                        'text': ' '.join([t.text for t in sub_s_tokens]),
                        'tokens': [t.text for t in sub_s_tokens],
                        'token_indices': [t.i for t in sub_s_tokens]
                    }
                    print(f"âœ… Step12ç²¾å¯†ä¸»èª: '{enhanced['sub-s']['text']}'")
                
                # é–¢ä¿‚ç¯€å†…å‹•è©
                enhanced['sub-v'] = {
                    'text': relcl_verb.text,
                    'tokens': [relcl_verb.text],
                    'token_indices': [relcl_verb.i]
                }
                print(f"âœ… Step12é–¢ä¿‚ç¯€å‹•è©: '{relcl_verb.text}'")
                
                # é–¢ä¿‚ç¯€å†…è£œèªæ¤œå‡ºï¼ˆindecisiveå¯¾å¿œï¼‰
                for child in relcl_verb.children:
                    if child.dep_ in ["acomp", "oprd", "attr"]:
                        enhanced['sub-c1'] = {
                            'text': child.text,
                            'tokens': [child.text],
                            'token_indices': [child.i]
                        }
                        print(f"âœ… Step12é–¢ä¿‚ç¯€è£œèª: sub-c1='{child.text}'")
                        break
        
        return enhanced
    
    def _apply_step10_c1_precision(self, doc, base_subslots, sentence):
        """Step10ã®C1è£œèªç²¾å¯†ãƒ­ã‚¸ãƒƒã‚¯é©ç”¨"""
        print("ğŸ¯ Step10 C1è£œèªç²¾å¯†ãƒ­ã‚¸ãƒƒã‚¯é©ç”¨")
        enhanced = base_subslots.copy()
        
        # Step10ã®è£œèªå‡¦ç†å¼·åŒ–
        # å½¢å®¹è©è£œèªã®è©³ç´°æ¤œå‡º
        for token in doc:
            if token.pos_ == "ADJ" and token.dep_ in ["acomp", "attr", "oprd", "ROOT"]:
                enhanced['sub-c1'] = {
                    'text': token.text,
                    'tokens': [token.text],
                    'token_indices': [token.i]
                }
                print(f"âœ… Step10ç²¾å¯†è£œèª: sub-c1='{token.text}'")
                
                # ä¿®é£¾èªã‚‚æ¤œå‡º
                for child in token.children:
                    if child.dep_ == "advmod":
                        enhanced['sub-m1'] = {
                            'text': child.text,
                            'tokens': [child.text],
                            'token_indices': [child.i]
                        }
                        print(f"âœ… Step10è£œèªä¿®é£¾: sub-m1='{child.text}'")
                break
        
        return enhanced
    
    def _apply_step11_c2_precision(self, doc, base_subslots, sentence):
        """Step11ã®C2è£œèªç²¾å¯†ãƒ­ã‚¸ãƒƒã‚¯é©ç”¨"""
        print("ğŸ¯ Step11 C2è£œèªç²¾å¯†ãƒ­ã‚¸ãƒƒã‚¯é©ç”¨")
        enhanced = base_subslots.copy()
        
        # thatç¯€è£œèªå‡¦ç†
        that_token = None
        for token in doc:
            if token.text.lower() == "that":
                that_token = token
                break
        
        if that_token:
            print("ğŸ” thatç¯€æ¤œå‡º")
            # Step11ãƒ­ã‚¸ãƒƒã‚¯é©ç”¨
            # å®Ÿè£…è©³ç´°ã¯å¿…è¦ã«å¿œã˜ã¦è¿½åŠ 
        
        return enhanced
    
    def _apply_general_enhancements(self, doc, base_subslots, slot_name):
        """ä»–ã‚¹ãƒ­ãƒƒãƒˆç”¨ä¸€èˆ¬å¼·åŒ–"""
        print(f"ğŸ¯ {slot_name}ã‚¹ãƒ­ãƒƒãƒˆ ä¸€èˆ¬å¼·åŒ–é©ç”¨")
        return base_subslots

def precision_integration_test():
    """ç²¾å¯†çµ±åˆãƒ†ã‚¹ãƒˆ - Rephraseå®Œå…¨ç‰ˆ"""
    print("ğŸ† Rephraseå®Œå…¨ç‰ˆç²¾å¯†çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 80)
    
    generator = PrecisionIntegratedSubslotGenerator()
    
    # Rephraseå®Œå…¨æ§‹é€ ãƒ†ã‚¹ãƒˆ
    test_cases = [
        # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆå¯¾è±¡8ã‚¹ãƒ­ãƒƒãƒˆ
        ("M1", "this morning"),
        ("S", "the woman who seemed indecisive"), 
        ("M2", "although it was emotionally hard"),
        ("C1", "very experienced"),
        ("O1", "that he had been trying to avoid Tom"),
        ("O2", "to avoid Tom"),
        ("C2", "confident that he will succeed"),
        ("M3", "because he was afraid of hurting her feelings"),
        # å˜ä¸€æ©Ÿèƒ½2ã‚¹ãƒ­ãƒƒãƒˆ
        ("Aux", "had"),
        ("V", "known")
    ]
    
    results = {}
    total_score = 0
    subslot_count = 0
    single_count = 0
    
    for slot_name, phrase in test_cases:
        print(f"\n{'='*60}")
        print(f"ğŸ§ª {slot_name}ã‚¹ãƒ­ãƒƒãƒˆå®Œå…¨ç‰ˆãƒ†ã‚¹ãƒˆ")
        print(f"ğŸ“ ãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ã‚º: '{phrase}'")
        
        result = generator.generate_subslots_for_slot(slot_name, phrase)
        
        if isinstance(result, dict) and 'slot_type' in result and result['slot_type'] == 'single':
            print(f"âš¡ å˜ä¸€æ©Ÿèƒ½ã‚¹ãƒ­ãƒƒãƒˆ: {result['message']}")
            single_count += 1
        else:
            print(f"ğŸ“Š ç²¾å¯†åˆ†è§£çµæœ: {len(result)}")
            for sub_type, sub_data in result.items():
                print(f"   âœ… {sub_type}: '{sub_data['text']}'")
            subslot_count += len(result)
        
        results[slot_name] = {
            'phrase': phrase,
            'result': result,
            'count': len(result) if not (isinstance(result, dict) and 'slot_type' in result) else 1
        }
        total_score += 1  # æˆåŠŸã‚¹ãƒ­ãƒƒãƒˆæ•°
    
    print(f"\n{'='*80}")
    print("ğŸ¯ Rephraseå®Œå…¨ç‰ˆæœ€çµ‚çµæœ")
    print(f"{'='*80}")
    print(f"å‡¦ç†ã‚¹ãƒ­ãƒƒãƒˆæ•°: {total_score}/10 (å®Œå…¨)")
    print(f"ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆç”Ÿæˆæ•°: {subslot_count}")
    print(f"å˜ä¸€æ©Ÿèƒ½ã‚¹ãƒ­ãƒƒãƒˆæ•°: {single_count}")
    print(f"å¹³å‡ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ•°ï¼ˆå¯¾è±¡8ã‚¹ãƒ­ãƒƒãƒˆï¼‰: {subslot_count/8:.1f}")
    
    return results

if __name__ == "__main__":
    precision_integration_test()
