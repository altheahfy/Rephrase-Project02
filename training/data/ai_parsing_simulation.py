"""
å®Ÿéš›ã®AIæ–‡æ³•åˆ†è§£å‡¦ç†ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
ã©ã®ã‚ˆã†ã«ãƒ«ãƒ¼ãƒ«è¾æ›¸ã‚’ä½¿ã£ã¦åˆ¤æ–­ã—ã¦ã„ã‚‹ã‹
"""

import json
import re
from typing import List, Dict, Tuple

class RuleBasedParser:
    """ãƒ«ãƒ¼ãƒ«è¾æ›¸ã‚’ä½¿ã£ãŸæ–‡æ³•åˆ†è§£ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, rules_file: str):
        """ãƒ«ãƒ¼ãƒ«è¾æ›¸èª­ã¿è¾¼ã¿"""
        with open(rules_file, 'r', encoding='utf-8') as f:
            self.rules_data = json.load(f)
        
        self.rules = self.rules_data['rules']
        self.slot_order = self.rules_data['slot_order']
        
        print(f"ğŸ“š ãƒ«ãƒ¼ãƒ«è¾æ›¸èª­ã¿è¾¼ã¿å®Œäº†: {len(self.rules)}å€‹ã®ãƒ«ãƒ¼ãƒ«")
    
    def tokenize(self, sentence: str) -> List[str]:
        """æ–‡ã‚’å˜èªã«åˆ†è§£"""
        # ç®€å˜ãªåˆ†è§£ï¼ˆå®Ÿéš›ã¯ã‚ˆã‚Šè¤‡é›‘ï¼‰
        sentence = sentence.replace("'", "'")  # ç‰¹æ®Šæ–‡å­—æ­£è¦åŒ–
        tokens = re.findall(r"\b\w+'\w+|\b\w+|[.,!?]", sentence)
        return tokens
    
    def find_matching_rules(self, token: str, context: List[str]) -> List[Dict]:
        """ãƒˆãƒ¼ã‚¯ãƒ³ã«ãƒãƒƒãƒã™ã‚‹ãƒ«ãƒ¼ãƒ«ã‚’æ¤œç´¢"""
        matching_rules = []
        
        for rule in self.rules:
            if self.rule_matches(rule, token, context):
                matching_rules.append(rule)
        
        # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
        matching_rules.sort(key=lambda r: r.get('priority', 0), reverse=True)
        return matching_rules
    
    def rule_matches(self, rule: Dict, token: str, context: List[str]) -> bool:
        """ãƒ«ãƒ¼ãƒ«ãŒãƒˆãƒ¼ã‚¯ãƒ³ã«ãƒãƒƒãƒã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        trigger = rule.get('trigger', {})
        
        # ç›´æ¥ãƒˆãƒ¼ã‚¯ãƒ³ãƒãƒƒãƒ
        if 'token' in trigger:
            if trigger['token'].lower() == token.lower():
                return True
        
        # èªå¹¹ãƒãƒƒãƒ
        if 'lemma' in trigger:
            if token.lower() in [l.lower() for l in trigger['lemma']]:
                return True
        
        # å½¢æ…‹ãƒãƒƒãƒ
        if 'form' in trigger:
            if token.lower() in [f.lower() for f in trigger['form']]:
                return True
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒï¼ˆç°¡æ˜“ç‰ˆï¼‰
        if 'pattern' in trigger:
            if re.search(trigger['pattern'], token.lower()):
                return True
                
        return False
    
    def apply_heuristics(self, token: str, position: int, tokens: List[str]) -> Tuple[str, str]:
        """ãƒ«ãƒ¼ãƒ«ã«ãƒãƒƒãƒã—ãªã„å ´åˆã®ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯åˆ¤æ–­"""
        
        # ä»£åè©åˆ¤å®š
        pronouns_s = ['i', 'you', 'he', 'she', 'it', 'we', 'they']
        pronouns_o = ['me', 'him', 'her', 'us', 'them']
        
        if token.lower() in pronouns_s:
            return 'S', 'word'
        elif token.lower() in pronouns_o:
            return 'O1', 'word'
        
        # ä½ç½®ãƒ™ãƒ¼ã‚¹åˆ¤å®š
        if position == 0:  # æ–‡é ­
            return 'S', 'word'
        
        # å‹•è©çš„è¦ç´ ã®åˆ¤å®šï¼ˆç°¡æ˜“ï¼‰
        verb_endings = ['ed', 'ing', 's']
        if any(token.lower().endswith(end) for end in verb_endings):
            return 'V', 'word'
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        return 'O1', 'word'
    
    def parse_sentence(self, sentence: str) -> List[Dict]:
        """æ–‡ã‚’åˆ†è§£ã—ã¦ã‚¹ãƒ­ãƒƒãƒˆåˆ†é¡"""
        
        print(f"\nğŸ” '{sentence}' ã®åˆ†è§£ãƒ—ãƒ­ã‚»ã‚¹:")
        print("-" * 40)
        
        tokens = self.tokenize(sentence)
        print(f"1ï¸âƒ£ ãƒˆãƒ¼ã‚¯ãƒ³åˆ†è§£: {tokens}")
        
        results = []
        
        for i, token in enumerate(tokens):
            if token in ['.', ',', '!', '?']:  # å¥èª­ç‚¹ã‚¹ã‚­ãƒƒãƒ—
                continue
                
            print(f"\n2ï¸âƒ£ '{token}' ã®åˆ†é¡:")
            
            # ãƒ«ãƒ¼ãƒ«ãƒãƒƒãƒãƒ³ã‚°
            matching_rules = self.find_matching_rules(token, tokens)
            
            if matching_rules:
                # æœ€å„ªå…ˆãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨
                best_rule = matching_rules[0]
                slot = best_rule['assign']['slot']
                phrase_type = best_rule['assign']['type']
                print(f"   ğŸ“‹ ãƒ«ãƒ¼ãƒ«é©ç”¨: {best_rule['id']} â†’ {slot}({phrase_type})")
            else:
                # ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯åˆ¤æ–­
                slot, phrase_type = self.apply_heuristics(token, i, tokens)
                print(f"   ğŸ§  ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯åˆ¤æ–­: â†’ {slot}({phrase_type})")
            
            results.append({
                'token': token,
                'slot': slot,
                'phrase_type': phrase_type,
                'position': i
            })
        
        return results
    
    def demonstrate_parsing(self):
        """åˆ†è§£ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        
        test_sentences = [
            "I can't afford it.",
            "Where did you get it?",
            "She got married with a bald man."
        ]
        
        for sentence in test_sentences:
            results = self.parse_sentence(sentence)
            
            print(f"\n3ï¸âƒ£ æœ€çµ‚çµæœ:")
            for result in results:
                print(f"   {result['slot']}: '{result['token']}' ({result['phrase_type']})")
            
            print("=" * 50)

def main():
    print("ğŸš€ Rephraseãƒ«ãƒ¼ãƒ«è¾æ›¸ã‚·ã‚¹ãƒ†ãƒ å®Ÿæ¼”")
    print("=" * 50)
    
    # ãƒ‘ãƒ¼ã‚µãƒ¼åˆæœŸåŒ–
    parser = RuleBasedParser('rephrase_rules_v1.0.json')
    
    # ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
    parser.demonstrate_parsing()
    
    print("\nğŸ’¡ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ:")
    print("1. ãƒ«ãƒ¼ãƒ«è¾æ›¸ã«ã‚ˆã‚‹è‡ªå‹•åˆ†é¡ãŒåŸºæœ¬")
    print("2. ãƒ«ãƒ¼ãƒ«ãŒãªã„å ´åˆã¯ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ï¼ˆçµŒé¨“çš„åˆ¤æ–­ï¼‰")
    print("3. å„ªå…ˆåº¦ã«ã‚ˆã‚‹ç«¶åˆè§£æ±º")
    print("4. æ–‡è„ˆæƒ…å ±ã®æ´»ç”¨")
    print("5. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«ã‚ˆã‚‹ç¶™ç¶šæ”¹å–„")
    
    print("\nğŸ”„ æ”¹å–„ã‚µã‚¤ã‚¯ãƒ«:")
    print("ã‚¨ãƒ©ãƒ¼ç™ºè¦‹ â†’ ãƒ«ãƒ¼ãƒ«è¿½åŠ /ä¿®æ­£ â†’ å†ãƒ†ã‚¹ãƒˆ â†’ ç²¾åº¦å‘ä¸Š")

if __name__ == "__main__":
    main()
