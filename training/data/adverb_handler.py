#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AdverbHandler: å‰¯è©ãƒ»ä¿®é£¾èªå‡¦ç†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
spaCyå“è©åˆ†æãƒ™ãƒ¼ã‚¹ï¼ˆãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç¦æ­¢ï¼‰
è²¬ä»»åˆ†æ‹…åŸå‰‡ã«åŸºã¥ãå°‚é–€ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
"""

import spacy
from typing import Dict, Any, List, Tuple

class AdverbHandler:
    """å‰¯è©ãƒ»ä¿®é£¾èªå‡¦ç†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆspaCy POSåˆ¤å®šãƒ™ãƒ¼ã‚¹ï¼‰"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.name = "AdverbHandler"
        self.version = "spaCy_v1.0"
        self.nlp = spacy.load('en_core_web_sm')  # spaCyå“è©åˆ¤å®šç”¨
    
    def process(self, text: str) -> Dict[str, Any]:
        """
        å‰¯è©ãƒ»ä¿®é£¾èªå‡¦ç†ãƒ¡ã‚¤ãƒ³
        
        Args:
            text: å‡¦ç†å¯¾è±¡ã®è‹±èªæ–‡
            
        Returns:
            Dict: å‡¦ç†çµæœ
        """
        try:
            # æ–‡å…¨ä½“ã‚’spaCyã§è§£æ
            doc = self.nlp(text)
            
            # å‹•è©ã¨ä¿®é£¾èªã®ãƒšã‚¢ã‚’ç‰¹å®š
            verb_modifier_pairs = self._identify_verb_modifier_pairs(doc)
            
            if not verb_modifier_pairs:
                # ä¿®é£¾èªãŒãªã„å ´åˆã‚‚æˆåŠŸã¨ã—ã¦æ‰±ã†ï¼ˆå…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãã®ã¾ã¾è¿”ã™ï¼‰
                return {
                    'success': True,
                    'separated_text': text,
                    'modifiers': {},
                    'verb_positions': {},
                    'modifier_slots': {}
                }
            
            # ä¿®é£¾èªã‚’åˆ†é›¢ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã¨ä¿®é£¾èªæƒ…å ±ã‚’è¿”ã™
            result = self._separate_modifiers(doc, verb_modifier_pairs)
            
            return {
                'success': True,
                'separated_text': result['separated_text'],
                'modifiers': result['modifiers'],
                'verb_positions': result['verb_positions'],
                'modifier_slots': self._assign_modifier_slots(result['modifiers'], verb_modifier_pairs, doc)
            }
            
        except Exception as e:
            return {'success': False, 'error': f'å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}'}
    
    def _identify_verb_modifier_pairs(self, doc) -> List[Dict]:
        """å‹•è©ã¨ä¿®é£¾èªã®ãƒšã‚¢ã‚’ç‰¹å®š"""
        pairs = []
        
        for i, token in enumerate(doc):
            # å‹•è©ã‚’è¦‹ã¤ã‘ã‚‹
            if token.pos_ in ['VERB', 'AUX']:
                verb_info = {
                    'verb_idx': i,
                    'verb_text': token.text,
                    'verb_lemma': token.lemma_,
                    'modifiers': []
                }
                
                # å‹•è©ã®å¾Œç¶šä¿®é£¾èªã‚’åé›†
                modifiers = self._collect_verb_modifiers(doc, i)
                if modifiers:
                    verb_info['modifiers'] = modifiers
                    pairs.append(verb_info)
        
        return pairs
    
    def _collect_verb_modifiers(self, doc, verb_idx: int) -> List[Dict]:
        """å‹•è©ã®ä¿®é£¾èªã‚’åé›†ï¼ˆå‰å¾Œä¸¡æ–¹å‘ã‹ã‚‰ï¼‰- å°‚é–€åˆ†æ‹…å‹ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æï¼ˆå—å‹•æ…‹å¯¾å¿œï¼‰"""
        modifiers = []
        
        # ğŸ¯ åè©ç¯€å¢ƒç•Œã‚’å…ˆã«ãƒã‚§ãƒƒã‚¯ï¼ˆthatç¯€ã€whç¯€ã€whetherç¯€ã€ifç¯€ï¼‰
        noun_clause_boundaries = self._detect_noun_clause_boundaries(doc)
        main_clause_verb_idx = self._find_main_clause_verb(doc)
        
        # ç¾åœ¨ã®å‹•è©ãŒä¸»æ–‡ã®å‹•è©ã§ãªã„å ´åˆã€ä¿®é£¾èªåˆ†é›¢ã‚’ã‚¹ã‚­ãƒƒãƒ—
        if verb_idx != main_clause_verb_idx:
            print(f"ğŸ”§ åè©ç¯€å†…å‹•è©æ¤œå‡º: verb_idx={verb_idx}, main_verb_idx={main_clause_verb_idx} â†’ ä¿®é£¾èªåˆ†é›¢ã‚¹ã‚­ãƒƒãƒ—")
            return modifiers
        
        # ğŸ¯ å—å‹•æ…‹æ¤œå‡º: ä¸»å‹•è©ã‚’ç‰¹å®š
        main_verb_idx = self._find_main_verb_for_modifiers(doc, verb_idx)
        effective_verb_idx = main_verb_idx if main_verb_idx != verb_idx else verb_idx
        print(f"ğŸ”§ ä¿®é£¾èªåŸºæº–å‹•è©: verb_idx={verb_idx}, effective_verb_idx={effective_verb_idx}")
        
        # æ–‡é ­æ™‚é–“è¡¨ç¾ã®ç‰¹åˆ¥å‡¦ç†ï¼ˆã€ŒEvery morningã€ãªã©ã®è¤‡åˆè¡¨ç¾ï¼‰
        if verb_idx > 1:  # å‹•è©ãŒè¤‡åˆè¡¨ç¾ã®å¾Œã«ä½ç½®ã™ã‚‹å ´åˆ
            # npadvmodã¨ã—ã¦åˆ†æã•ã‚Œã‚‹æ™‚é–“è¡¨ç¾ã‚’æ¤œç´¢
            for i in range(min(verb_idx, 3)):  # æ–‡é ­ã‹ã‚‰3èªç¨‹åº¦ã‚’ãƒã‚§ãƒƒã‚¯
                token = doc[i]
                if token.dep_ == 'npadvmod' and token.head.i == verb_idx:
                    # æ™‚é–“è¡¨ç¾ã®é–‹å§‹ä½ç½®ã‚’ç‰¹å®šï¼ˆæ±ºå®šè©ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼‰
                    start_idx = i
                    if i > 0 and doc[i-1].dep_ == 'det' and doc[i-1].head.i == i:
                        start_idx = i - 1  # æ±ºå®šè©ã‹ã‚‰é–‹å§‹
                    
                    # æ™‚é–“è¡¨ç¾ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
                    time_tokens = []
                    for j in range(start_idx, i + 1):
                        if doc[j].pos_ not in ['PUNCT']:  # å¥èª­ç‚¹ã‚’é™¤ã
                            time_tokens.append(doc[j].text)
                    
                    time_text = ' '.join(time_tokens)
                    
                    modifier_info = {
                        'text': time_text,
                        'pos': token.pos_,
                        'tag': token.tag_,
                        'idx': start_idx,
                        'type': 'temporal',
                        'position': 'sentence-initial',
                        'method': 'dependency_analysis'
                    }
                    modifiers.append(modifier_info)
                    print(f"ğŸ” æ–‡é ­æ™‚é–“è¡¨ç¾æ¤œå‡º: {time_text} (ä¾å­˜é–¢ä¿‚: {token.dep_})")
                    break  # æœ€åˆã®æ™‚é–“è¡¨ç¾ã®ã¿å‡¦ç†
        
        # æ–‡é ­å‰¯è©ã®ç‰¹åˆ¥å‡¦ç†ï¼ˆã€ŒActuallyã€ãªã©ã®æ–‡å‰¯è©ï¼‰
        if verb_idx > 0:  # å‹•è©ãŒæ–‡é ­ã§ãªã„å ´åˆ
            first_token = doc[0]
            # æ–‡é ­å‰¯è©ã‚’æ¤œå‡ºï¼ˆadvmod ã¾ãŸã¯ ä¸€èˆ¬çš„ãªæ–‡å‰¯è©ï¼‰
            sentence_adverbs = ['actually', 'honestly', 'frankly', 'clearly', 'obviously', 'certainly', 'definitely', 'unfortunately', 'fortunately', 'hopefully']
            
            is_sentence_adverb = (
                (first_token.dep_ == 'advmod' and first_token.head.i == verb_idx and first_token.pos_ == 'ADV') or
                (first_token.text.lower() in sentence_adverbs and first_token.pos_ == 'ADV')
            )
            
            if is_sentence_adverb and not any(mod for mod in modifiers if mod['idx'] == 0):
                modifier_info = {
                    'text': first_token.text,
                    'pos': first_token.pos_,
                    'tag': first_token.tag_,
                    'idx': 0,
                    'type': 'sentence_adverb',
                    'position': 'sentence-initial',
                    'method': 'dependency_analysis'
                }
                modifiers.append(modifier_info)
                print(f"ğŸ” æ–‡é ­å‰¯è©æ¤œå‡º: {first_token.text} (ä¾å­˜é–¢ä¿‚: {first_token.dep_})")
        
        
        # æ–‡é ­å‰¯è©ã®ç‰¹åˆ¥å‡¦ç†ï¼ˆActually, Fortunatelyç­‰ï¼‰
        if verb_idx > 0:
            first_token = doc[0]
            # æ–‡é ­å‰¯è©ã¨ã—ã¦æ˜ç¤ºçš„ã«æ¤œå‡º
            sentence_adverbs = ['actually', 'fortunately', 'unfortunately', 'honestly', 'basically', 'obviously', 'clearly', 'frankly', 'seriously', 'literally']
            
            if (first_token.text.lower() in sentence_adverbs and 
                first_token.pos_ == 'ADV' and
                first_token.dep_ == 'advmod'):
                
                modifier_info = {
                    'text': first_token.text,
                    'pos': first_token.pos_,
                    'tag': first_token.tag_,
                    'idx': 0,
                    'type': 'sentence_adverb',
                    'position': 'sentence-initial',
                    'method': 'dependency_analysis'
                }
                modifiers.append(modifier_info)
                print(f"ğŸ” æ–‡é ­å‰¯è©æ¤œå‡º: {first_token.text} (ä¾å­˜é–¢ä¿‚: {first_token.dep_})")
        
        
        # Part 1: å‹•è©ã®å‰ã«ã‚ã‚‹ä¿®é£¾èªã‚’æ¤œç´¢ï¼ˆè¤‡åˆä¿®é£¾èªå¯¾å¿œãƒ»å—å‹•æ…‹å¯¾å¿œï¼‰
        pre_verb_modifiers = []
        for i in range(effective_verb_idx - 1, -1, -1):  # effective_verb_idx ã‚’ä½¿ç”¨
            token = doc[i]
            
            # æ—¢ã«å‡¦ç†æ¸ˆã¿ã®æ–‡é ­æ™‚é–“è¡¨ç¾ã¯ã‚¹ã‚­ãƒƒãƒ—
            if any(mod for mod in modifiers if mod['idx'] <= i <= mod['idx'] + len(mod['text'].split()) - 1):
                continue
            
            # ä¿®é£¾èªã¨ã—ã¦è­˜åˆ¥ï¼ˆã“ã®å‹•è©ã‚’ä¿®é£¾ã—ã¦ã„ã‚‹ã‹ç¢ºèªãƒ»å—å‹•æ…‹å¯¾å¿œï¼‰
            if self._is_modifier(token):
                # å—å‹•æ…‹ã®å ´åˆ: å‰¯è©ãŒä¸»å‹•è©ã‚’ä¿®é£¾ã™ã‚‹å ´åˆã‚’ç‰¹åˆ¥å‡¦ç†
                is_passive_adverb = (
                    effective_verb_idx != verb_idx and  # å—å‹•æ…‹
                    token.pos_ == 'ADV' and 
                    effective_verb_idx < len(doc) and
                    (token.head.i == effective_verb_idx or token.head.i == verb_idx)
                )
                
                if token.head.i == verb_idx or is_passive_adverb:
                    # å—å‹•æ…‹ã®å ´åˆã€ä½ç½®ã‚’ä¸»å‹•è©åŸºæº–ã§å†è¨ˆç®—
                    position_type = 'pre-verb' if i < effective_verb_idx else 'post-verb'
                    
                    modifier_info = {
                        'text': token.text,
                        'pos': token.pos_,
                        'tag': token.tag_,
                        'idx': i,
                        'type': self._classify_modifier_type(token),
                        'position': position_type,  # ä¸»å‹•è©åŸºæº–ã®ä½ç½®
                        'method': 'pos_analysis_passive_aware',  # å—å‹•æ…‹å¯¾å¿œæ‰‹æ³•
                        'effective_verb_idx': effective_verb_idx  # ãƒ‡ãƒãƒƒã‚°ç”¨
                    }
                    pre_verb_modifiers.append(modifier_info)
                    print(f"ğŸ” å—å‹•æ…‹å‰¯è©æ¤œå‡º: {token.text} (ä½ç½®: {position_type}, åŸºæº–å‹•è©idx: {effective_verb_idx})")
            
            # ä¸»èªã«é”ã—ãŸã‚‰åœæ­¢ï¼ˆå‹•è©å‰ä¿®é£¾èªã®ç¯„å›²ã‚’åˆ¶é™ï¼‰
            if token.dep_ in ['nsubj', 'nsubjpass']:
                break
                break
        
        # è¤‡åˆä¿®é£¾èªã®çµåˆå‡¦ç†ï¼ˆä¾‹: "very carefully"ï¼‰
        pre_verb_modifiers = self._merge_compound_modifiers(doc, pre_verb_modifiers)
        modifiers.extend(pre_verb_modifiers)
        
        # Part 2: å‹•è©ã®ç›´å¾Œã‹ã‚‰æ–‡æœ«ã¾ã§ï¼ˆã¾ãŸã¯æ¬¡ã®ä¸»è¦è¦ç´ ã¾ã§ï¼‰ã‚’æ¤œç´¢
        i = verb_idx + 1
        while i < len(doc):
            token = doc[i]
            
            # å¥èª­ç‚¹ã§åœæ­¢
            if token.pos_ == 'PUNCT':
                break
            
            # æ¬¡ã®å‹•è©ã§åœæ­¢ï¼ˆä¸»ç¯€ã®å‹•è©ãªã©ï¼‰
            if token.pos_ in ['VERB', 'AUX'] and self._is_main_clause_verb(doc, i):
                break
            
            # æ™‚é–“è¡¨ç¾ã®ç‰¹åˆ¥å‡¦ç†ï¼ˆ"every day", "last week", "next month"ãªã©ï¼‰
            if token.text.lower() in ['every', 'each', 'last', 'next', 'this', 'that'] and i + 1 < len(doc):
                next_token = doc[i + 1]
                # æ™‚é–“åè©ã‚’ãƒã‚§ãƒƒã‚¯
                time_nouns = ['day', 'week', 'month', 'year', 'morning', 'afternoon', 'evening', 'night', 'time', 'moment', 
                             'summer', 'winter', 'spring', 'autumn', 'fall', 'season', 'today', 'tomorrow', 'yesterday']
                if next_token.text.lower() in time_nouns:
                    time_phrase = f"{token.text} {next_token.text}"
                    modifier_info = {
                        'text': time_phrase,
                        'pos': 'ADV',  # æ™‚é–“å‰¯è©å¥ã¨ã—ã¦æ‰±ã†
                        'tag': 'RB',
                        'idx': i,
                        'type': 'temporal_phrase',
                        'phrase_end': i + 1,
                        'position': 'post-verb',
                        'method': 'compound_detection'
                    }
                    modifiers.append(modifier_info)
                    print(f"ğŸ” æ™‚é–“å‰¯è©å¥æ¤œå‡º: {time_phrase}")
                    # å‰ç½®è©å¥ã®æ®‹ã‚Šã®éƒ¨åˆ†ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    i = i + 2
                    continue
            
            # å˜ä½“æ™‚é–“åè©ã®å‡¦ç†ï¼ˆ"day"ãŒå˜ç‹¬ã§ç¾ã‚Œã‚‹å ´åˆã‚‚ï¼‰
            if (token.text.lower() in ['day', 'week', 'month', 'year', 'morning', 'afternoon', 'evening', 'night'] and 
                i > 0 and doc[i-1].text.lower() in ['every', 'each', 'last', 'next', 'this', 'that']):
                # å‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¨åˆã‚ã›ã¦æ™‚é–“è¡¨ç¾ã¨ã—ã¦å‡¦ç†
                prev_token = doc[i-1]
                time_phrase = f"{prev_token.text} {token.text}"
                
                # æ—¢ã«å‡¦ç†æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
                already_processed = any(mod for mod in modifiers if mod['idx'] == i-1)
                if not already_processed:
                    modifier_info = {
                        'text': time_phrase,
                        'pos': 'ADV',  # æ™‚é–“å‰¯è©å¥ã¨ã—ã¦æ‰±ã†
                        'tag': 'RB',
                        'idx': i-1,
                        'type': 'temporal_phrase',
                        'phrase_end': i,
                        'position': 'post-verb',
                        'method': 'retroactive_compound_detection'
                    }
                    modifiers.append(modifier_info)
                    print(f"ğŸ” é¡åŠæ™‚é–“å‰¯è©å¥æ¤œå‡º: {time_phrase}")
                    continue
            
            # ä¿®é£¾èªã¨ã—ã¦è­˜åˆ¥ï¼ˆä¿å®ˆçš„åˆ¤å®šï¼‰
            if self._is_modifier(token):
                # å‰ç½®è©å¥ã®å ´åˆã¯å…¨ä½“ã‚’ãƒã‚§ãƒƒã‚¯
                if token.pos_ == 'ADP':
                    prep_phrase = self._get_prepositional_phrase(doc, i)
                    if prep_phrase['is_modifiable']:
                        modifier_info = {
                            'text': prep_phrase['text'],
                            'pos': token.pos_,
                            'tag': token.tag_,
                            'idx': i,
                            'type': 'prepositional_phrase',
                            'phrase_end': prep_phrase['end_idx'],
                            'position': 'post-verb'  # å‹•è©å¾Œä¿®é£¾èª
                        }
                        modifiers.append(modifier_info)
                        # å‰ç½®è©å¥ã®æ®‹ã‚Šã®éƒ¨åˆ†ã‚’ã‚¹ã‚­ãƒƒãƒ—
                        i = prep_phrase['end_idx'] + 1
                        continue
                else:
                    modifier_info = {
                        'text': token.text,
                        'pos': token.pos_,
                        'tag': token.tag_,
                        'idx': i,
                        'type': self._classify_modifier_type(token),
                        'position': 'post-verb'  # å‹•è©å¾Œä¿®é£¾èª
                    }
                    modifiers.append(modifier_info)
            
            i += 1
        
        # è¤‡åˆä¿®é£¾èªã®çµåˆå‡¦ç†ï¼ˆpost-verbä¿®é£¾èªã«ã‚‚é©ç”¨ï¼‰
        modifiers = self._merge_compound_modifiers(doc, modifiers)
        
        return modifiers
    
    def _merge_compound_modifiers(self, doc, modifiers: List[Dict]) -> List[Dict]:
        """é©åˆ‡ãªä¿®é£¾èªã®ã¿ã‚’çµåˆï¼ˆä¾‹: "very carefully" â†’ 1ã¤ã®ä¿®é£¾èªï¼‰"""
        if len(modifiers) <= 1:
            return modifiers
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é †ã§ã‚½ãƒ¼ãƒˆ
        modifiers.sort(key=lambda x: x['idx'])
        
        merged = []
        i = 0
        
        while i < len(modifiers):
            current = modifiers[i]
            
            # æ¬¡ã®ä¿®é£¾èªã¨éš£æ¥ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if i + 1 < len(modifiers):
                next_mod = modifiers[i + 1]
                
                # éš£æ¥ã—ã¦ã„ã‚‹ï¼ˆé–“ã«1ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§è¨±å®¹ï¼‰
                if next_mod['idx'] - current['idx'] <= 2:
                    # çµåˆå¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆå³æ ¼ãªæ¡ä»¶ï¼‰
                    if self._can_merge_modifiers(doc, current, next_mod):
                        # è¤‡åˆä¿®é£¾èªã¨ã—ã¦çµåˆ
                        start_idx = current['idx']
                        end_idx = next_mod['idx']
                        
                        # çµåˆãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
                        compound_text = ' '.join([doc[j].text for j in range(start_idx, end_idx + 1)])
                        
                        merged_modifier = {
                            'text': compound_text,
                            'pos': next_mod['pos'],  # ãƒ¡ã‚¤ãƒ³ã®ä¿®é£¾èªã®POSã‚’ä½¿ç”¨
                            'tag': next_mod['tag'],
                            'idx': start_idx,
                            'type': next_mod['type'],
                            'position': current['position'],
                            'method': 'compound_merge'
                        }
                        
                        merged.append(merged_modifier)
                        i += 2  # ä¸¡æ–¹ã®ä¿®é£¾èªã‚’ã‚¹ã‚­ãƒƒãƒ—
                        continue
            
            # çµåˆã—ãªã„å ´åˆã¯ãã®ã¾ã¾è¿½åŠ 
            merged.append(current)
            i += 1
        
        return merged
    
    def _can_merge_modifiers(self, doc, first_mod: Dict, second_mod: Dict) -> bool:
        """2ã¤ã®ä¿®é£¾èªãŒçµåˆå¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆå³æ ¼ãªæ¡ä»¶ï¼‰"""
        first_token = doc[first_mod['idx']]
        second_token = doc[second_mod['idx']]
        
        # å‰ç½®è©å¥ã¯ä»–ã®ä¿®é£¾èªã¨çµåˆã—ãªã„
        if first_mod['type'] == 'prepositional_phrase' or second_mod['type'] == 'prepositional_phrase':
            return False
        
        # ç¨‹åº¦å‰¯è© + å‰¯è© ã®çµ„ã¿åˆã‚ã›
        degree_adverbs = ['very', 'quite', 'rather', 'extremely', 'incredibly', 'really', 'truly', 'highly', 'perfectly', 'completely']
        
        if (first_token.text.lower() in degree_adverbs and 
            first_token.pos_ == 'ADV' and 
            second_token.pos_ == 'ADV'):
            return True
        
        # æ™‚é–“è¡¨ç¾ã®çµåˆï¼ˆlast week, next month, etc.ï¼‰
        time_determiners = ['last', 'next', 'this', 'every']
        time_nouns = ['week', 'month', 'year', 'day', 'morning', 'afternoon', 'evening', 'night']
        
        if (first_token.text.lower() in time_determiners and 
            second_token.text.lower() in time_nouns and
            second_mod['idx'] - first_mod['idx'] == 1):  # å³å¯†ã«éš£æ¥
            return True
        
        # æ³¨æ„: å‰¯è© + and + å‰¯è© ã¯çµåˆã—ãªã„ï¼ˆåˆ†é›¢ã—ã¦M-slotã«å€‹åˆ¥ã«å‰²ã‚Šå½“ã¦ã‚‹ï¼‰
        
        return False

    def _is_modifier(self, token) -> bool:
        """ãƒˆãƒ¼ã‚¯ãƒ³ãŒä¿®é£¾èªã‹ã©ã†ã‹åˆ¤å®šï¼ˆspaCyä¾å­˜é–¢ä¿‚ãƒ™ãƒ¼ã‚¹ï¼‰"""
        # å‰¯è©ã¯åŸºæœ¬çš„ã«ä¿®é£¾èªã¨ã—ã¦æ‰±ã†ï¼ˆ5æ–‡å‹ã®æ ¸å¿ƒè¦ç´ ã§ã¯ãªã„ï¼‰
        if token.pos_ == 'ADV':
            # ãŸã ã—ã€æ–‡æ³•çš„ã«å¿…é ˆã®å¦å®šå‰¯è©ã®ã¿é™¤å¤–
            essential_adverbs = ['not', "n't", 'never']
            return token.text.lower() not in essential_adverbs
        
        # å‰ç½®è©å¥ã®åˆ¤å®šï¼šspaCyã®ä¾å­˜é–¢ä¿‚ã‚’ä½¿ç”¨
        if token.pos_ == 'ADP':
            # å‰ç½®è©ãŒå‹•è©ã‚’ä¿®é£¾ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆä¾å­˜é–¢ä¿‚ prepï¼‰
            if token.dep_ == 'prep' and token.head.pos_ == 'VERB':
                return True
            # ã¾ãŸã¯å‰ç½®è©ãŒå‹•è©ã«å¯¾ã—ã¦å‰¯è©çš„ã«æ©Ÿèƒ½ã—ã¦ã„ã‚‹å ´åˆ
            if token.dep_ in ['prep', 'advmod'] and token.head.pos_ in ['VERB', 'ADJ', 'ADV']:
                return True
            # 5æ–‡å‹ã®å¿…é ˆè¦ç´ ï¼ˆtoä¸å®šè©ãªã©ï¼‰ã¯é™¤å¤–
            essential_prep_patterns = [
                'to',  # toä¸å®šè©ï¼ˆãŸã ã—å‰¯è©çš„ç”¨æ³•ã¯å«ã‚ã‚‹ï¼‰
            ]
            if token.text.lower() in essential_prep_patterns:
                # toä¸å®šè©ã®å ´åˆã€å‰¯è©çš„ç”¨æ³•ãªã‚‰ä¿®é£¾èªã¨ã—ã¦æ‰±ã†
                if token.text.lower() == 'to' and token.dep_ == 'aux':
                    return False  # ä¸å®šè©ã®to
                # ãã®ä»–ã®å‰¯è©çš„ãªtoã¯ä¿®é£¾èª
                return True
            return True  # ãã®ä»–ã®å‰ç½®è©ã¯åŸºæœ¬çš„ã«ä¿®é£¾èª
        
        # æ˜ç¢ºãªæ™‚é–“ãƒ»å ´æ‰€å‰¯è©ï¼ˆå ´æ‰€å‰¯è©here/thereã¯ä¿®é£¾èªã¨ã—ã¦æ‰±ã†ï¼‰
        if token.pos_ in ['NOUN', 'PROPN']:
            temporal_locative = ['yesterday', 'today', 'tomorrow', 'here', 'there', 'week', 'month', 'year', 'morning', 'afternoon', 'evening', 'night']
            return token.text.lower() in temporal_locative
        
        # å½¢å®¹è©ãŒå‰¯è©çš„ã«ä½¿ã‚ã‚Œã¦ã„ã‚‹å ´åˆï¼ˆæ™‚é–“è¡¨ç¾ãªã©ï¼‰
        if token.pos_ == 'ADJ':
            time_adjectives = ['last', 'next', 'daily', 'weekly', 'monthly', 'yearly']
            return token.text.lower() in time_adjectives
        
        # å ´æ‰€å‰¯è©here/thereã¯ä¿®é£¾èªã¨ã—ã¦æ‰±ã†
        if token.pos_ == 'ADV' and token.text.lower() in ['here', 'there']:
            return True
        
        return False
    
    def _is_adverbial_noun(self, token) -> bool:
        """å‰¯è©çš„ãªåè©ã‹ã©ã†ã‹åˆ¤å®šï¼ˆå ´æ‰€ãƒ»æ™‚é–“ãªã©ï¼‰"""
        # å ´æ‰€ãƒ»æ™‚é–“ã‚’è¡¨ã™ä¸€èˆ¬çš„ãªèª
        adverbial_patterns = [
            'here', 'there', 'everywhere', 'somewhere',
            'today', 'yesterday', 'tomorrow', 'now',
            'home', 'abroad', 'upstairs', 'downtown'
        ]
        
        return token.text.lower() in adverbial_patterns
    
    def _get_prepositional_phrase(self, doc, prep_idx: int) -> Dict:
        """å‰ç½®è©å¥å…¨ä½“ã‚’å–å¾—ã—ã€åˆ†é›¢å¯èƒ½ã‹ã©ã†ã‹åˆ¤å®šï¼ˆspaCyä¾å­˜é–¢ä¿‚ãƒ™ãƒ¼ã‚¹ï¼‰"""
        prep_token = doc[prep_idx]
        phrase_tokens = [prep_token.text]
        end_idx = prep_idx
        
        # spaCyã®ä¾å­˜é–¢ä¿‚ã‚’ä½¿ã£ã¦å‰ç½®è©å¥ã®ç¯„å›²ã‚’ç‰¹å®š
        # å‰ç½®è©ã®å­è¦ç´ ï¼ˆé€šå¸¸ã¯pobj = prepositional objectï¼‰ã‚’æ¢ã™
        phrase_indices = {prep_idx}
        
        def add_subtree(token):
            """ãƒˆãƒ¼ã‚¯ãƒ³ã¨ãã®å­è¦ç´ ã‚’å†å¸°çš„ã«è¿½åŠ """
            phrase_indices.add(token.i)
            for child in token.children:
                add_subtree(child)
        
        # å‰ç½®è©ã®å­è¦ç´ ï¼ˆç›®çš„èªã¨ãã®ä¿®é£¾èªï¼‰ã‚’å–å¾—
        for child in prep_token.children:
            if child.dep_ == 'pobj':  # prepositional object
                add_subtree(child)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚½ãƒ¼ãƒˆã—ã¦é€£ç¶šçš„ãªå¥ã‚’ä½œæˆ
        sorted_indices = sorted(phrase_indices)
        
        # é€£ç¶šã—ã¦ã„ã‚‹ç¯„å›²ã®ã¿ã‚’å–å¾—ï¼ˆé€”åˆ‡ã‚Œã‚‹å ´åˆã¯å‰ç½®è©å¥ã®çµ‚äº†ï¼‰
        continuous_indices = [prep_idx]
        for idx in sorted_indices:
            if idx > prep_idx and (not continuous_indices or idx == continuous_indices[-1] + 1):
                continuous_indices.append(idx)
            elif idx > prep_idx:
                break  # é€£ç¶šæ€§ãŒé€”åˆ‡ã‚ŒãŸ
        
        # ãƒ•ãƒ¬ãƒ¼ã‚ºãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
        phrase_tokens = [doc[i].text for i in continuous_indices]
        end_idx = continuous_indices[-1]
        phrase_text = ' '.join(phrase_tokens)
        
        # å‰ç½®è©å¥ãŒä¿®é£¾èªã¨ã—ã¦åˆ†é›¢å¯èƒ½ã‹ã©ã†ã‹åˆ¤å®š
        is_modifiable = self._is_prepositional_phrase_modifiable_by_dependency(prep_token)
        
        return {
            'text': phrase_text,
            'end_idx': end_idx,
            'is_modifiable': is_modifiable
        }
    
    def _is_prepositional_phrase_modifiable_by_dependency(self, prep_token) -> bool:
        """spaCyã®ä¾å­˜é–¢ä¿‚ã«åŸºã¥ã„ã¦å‰ç½®è©å¥ãŒä¿®é£¾èªã¨ã—ã¦åˆ†é›¢å¯èƒ½ã‹ã©ã†ã‹åˆ¤å®š"""
        # å‰ç½®è©ãŒå‹•è©ã‚’ä¿®é£¾ã—ã¦ã„ã‚‹å ´åˆï¼ˆdep_=prepï¼‰ã¯é€šå¸¸ä¿®é£¾èª
        if prep_token.dep_ == 'prep' and prep_token.head.pos_ == 'VERB':
            return True
        
        # å‰¯è©çš„ä¿®é£¾ï¼ˆadvmodï¼‰ã®å ´åˆã‚‚ä¿®é£¾èª
        if prep_token.dep_ == 'advmod':
            return True
        
        # agentå¥ï¼ˆå—å‹•æ–‡ã®byå¥ï¼‰ã¯ä¿®é£¾èª
        if prep_token.dep_ == 'agent':
            return True
        
        # 5æ–‡å‹ã®å¿…é ˆè¦ç´ ã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
        # - dobj, iobj, attr, acomp ãªã©ã®å ´åˆã¯å¿…é ˆè¦ç´ ãªã®ã§åˆ†é›¢ä¸å¯
        essential_deps = ['dobj', 'iobj', 'attr', 'acomp', 'xcomp', 'ccomp']
        if prep_token.dep_ in essential_deps:
            return False
        
        # ãã®ä»–ã¯åŸºæœ¬çš„ã«ä¿®é£¾èªã¨ã—ã¦æ‰±ã†
        return True
    
    def _classify_modifier_type(self, token) -> str:
        """ä¿®é£¾èªã®ç¨®é¡ã‚’åˆ†é¡"""
        if token.pos_ == 'ADV':
            return 'adverb'
        elif token.pos_ == 'ADP':
            return 'prepositional'
        elif self._is_adverbial_noun(token):
            return 'adverbial_noun'
        else:
            return 'other'
    
    def _is_main_clause_verb(self, doc, verb_idx: int) -> bool:
        """ä¸»ç¯€ã®å‹•è©ã‹ã©ã†ã‹åˆ¤å®šï¼ˆé–¢ä¿‚ç¯€å†…ã®å‹•è©ã¨åŒºåˆ¥ï¼‰"""
        # ç°¡æ˜“åˆ¤å®šï¼šé–¢ä¿‚ä»£åè©ã‚ˆã‚Šå¾Œã«ã‚ã‚‹å‹•è©ã¯é–¢ä¿‚ç¯€ã€
        # ãã‚Œã‚ˆã‚Šå‰ã¾ãŸã¯é–¢ä¿‚ä»£åè©ãŒãªã„å ´åˆã¯ä¸»ç¯€
        relative_pronouns = ['who', 'which', 'that', 'whom', 'whose']
        
        # é–¢ä¿‚ä»£åè©ã®ä½ç½®ã‚’ç‰¹å®š
        rel_pronoun_idx = None
        for i, token in enumerate(doc):
            if token.text.lower() in relative_pronouns:
                rel_pronoun_idx = i
                break
        
        # é–¢ä¿‚ä»£åè©ãŒãªã„å ´åˆã€ä¸»ç¯€ã®å‹•è©
        if rel_pronoun_idx is None:
            return True
        
        # é–¢ä¿‚ä»£åè©ã‚ˆã‚Šå¾Œã®æœ€åˆã®å‹•è©ã¯é–¢ä¿‚ç¯€ã€ãã®å¾Œã¯ä¸»ç¯€
        if verb_idx > rel_pronoun_idx:
            # é–¢ä¿‚ç¯€å†…ã®æœ€åˆã®å‹•è©ã‚’ãƒã‚§ãƒƒã‚¯
            first_rel_verb_idx = None
            for i in range(rel_pronoun_idx + 1, len(doc)):
                if doc[i].pos_ in ['VERB', 'AUX']:
                    first_rel_verb_idx = i
                    break
            
            # é–¢ä¿‚ç¯€å†…ã®æœ€åˆã®å‹•è©ã§ã¯ãªã„å ´åˆã€ä¸»ç¯€ã®å‹•è©
            return verb_idx != first_rel_verb_idx
        
        return True
    
    def _separate_modifiers(self, doc, verb_modifier_pairs: List[Dict]) -> Dict:
        """ä¿®é£¾èªã‚’åˆ†é›¢ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã¨ä¿®é£¾èªæƒ…å ±ã‚’ç”Ÿæˆ"""
        separated_tokens = []
        modifiers_info = {}
        verb_positions = {}
        
        modifier_indices = set()
        
        # ä¿®é£¾èªã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åé›†
        for pair in verb_modifier_pairs:
            verb_idx = pair['verb_idx']
            verb_text = pair['verb_text']
            
            # å‹•è©ä½ç½®ã‚’è¨˜éŒ²
            verb_positions[verb_idx] = {
                'original_text': verb_text,
                'modifiers': []
            }
            
            for modifier in pair['modifiers']:
                modifier_idx = modifier['idx']
                modifier_text = modifier['text']
                
                # çµåˆã•ã‚ŒãŸä¿®é£¾èªã®å ´åˆã€ã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åé›†
                if modifier.get('method') == 'compound_merge':
                    # çµåˆã•ã‚ŒãŸä¿®é£¾èªã®ã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‰Šé™¤å¯¾è±¡ã«ã™ã‚‹
                    phrase_parts = modifier_text.split()
                    current_idx = modifier_idx
                    for part in phrase_parts:
                        if current_idx < len(doc):
                            modifier_indices.add(current_idx)
                            current_idx += 1
                elif modifier['type'] == 'prepositional_phrase':
                    # å‰ç½®è©å¥ã®ã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‰Šé™¤å¯¾è±¡ã«ã™ã‚‹
                    phrase_parts = modifier_text.split()
                    current_idx = modifier_idx
                    for part in phrase_parts:
                        if current_idx < len(doc) and doc[current_idx].text == part:
                            modifier_indices.add(current_idx)
                            current_idx += 1
                elif modifier['type'] == 'temporal_phrase':
                    # æ™‚é–“å‰¯è©å¥ã®ã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‰Šé™¤å¯¾è±¡ã«ã™ã‚‹ï¼ˆ"every day"ãªã©ï¼‰
                    phrase_parts = modifier_text.split()
                    current_idx = modifier_idx
                    for part in phrase_parts:
                        if current_idx < len(doc) and doc[current_idx].text.lower() == part.lower():
                            modifier_indices.add(current_idx)
                            current_idx += 1
                            modifier_indices.add(current_idx)
                            current_idx += 1
                elif ' ' in modifier_text:
                    # è¤‡æ•°èªã®ä¿®é£¾èªï¼ˆæ™‚é–“è¡¨ç¾ãªã©ã‚’å«ã‚€ï¼‰
                    phrase_parts = modifier_text.split()
                    current_idx = modifier_idx
                    for part in phrase_parts:
                        if current_idx < len(doc) and doc[current_idx].text == part:
                            modifier_indices.add(current_idx)
                            current_idx += 1
                else:
                    # å˜ä¸€èªã®ä¿®é£¾èª
                    modifier_indices.add(modifier_idx)
                
                # ä¿®é£¾èªæƒ…å ±ã‚’è¨˜éŒ²
                if verb_idx not in modifiers_info:
                    modifiers_info[verb_idx] = []
                
                modifiers_info[verb_idx].append({
                    'text': modifier['text'],
                    'type': modifier['type'],
                    'pos': modifier['pos'],
                    'idx': modifier['idx']  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æƒ…å ±ã‚’ä¿æŒ
                })
                
                verb_positions[verb_idx]['modifiers'].append(modifier['text'])
        
        # å‰¯è©é–“ã®æ¥ç¶šè©ã€Œandã€ã‚’å‰Šé™¤å¯¾è±¡ã«è¿½åŠ 
        for i in range(len(doc) - 2):
            if (i in modifier_indices and  # æœ€åˆã®å‰¯è©
                i + 1 < len(doc) and doc[i + 1].text.lower() == 'and' and
                i + 2 in modifier_indices and  # æ¬¡ã®å‰¯è©
                doc[i].pos_ == 'ADV' and doc[i + 2].pos_ == 'ADV'):
                modifier_indices.add(i + 1)  # ã€Œandã€ã‚’å‰Šé™¤å¯¾è±¡ã«è¿½åŠ 
        
        # ä¿®é£¾èªã‚’é™¤ã„ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
        for i, token in enumerate(doc):
            if i not in modifier_indices:
                separated_tokens.append(token.text)
        
        separated_text = ' '.join(separated_tokens)
        
        return {
            'separated_text': separated_text,
            'modifiers': modifiers_info,
            'verb_positions': verb_positions
        }
    
    def _assign_modifier_slots(self, modifiers_info: Dict, verb_modifier_pairs: List[Dict], doc) -> Dict[str, str]:
        """
        REPHRASE_SLOT_STRUCTURE_MANDATORY_REFERENCE.mdä»•æ§˜ã«å¾“ã£ã¦ä¿®é£¾èªã‚’Mã‚¹ãƒ­ãƒƒãƒˆã«é…ç½®
        
        ã€å‰å¾Œåˆ†æ•£é…ç½®ãƒ«ãƒ¼ãƒ«ã€‘ï¼ˆ2025å¹´8æœˆç¢ºå®šç‰ˆï¼‰:
        1å€‹ã®ã¿ â†’ M2ï¼ˆä½ç½®ç„¡é–¢ä¿‚ï¼‰
        2å€‹ã®å ´åˆï¼š
        - å‰ã«1ã¤ã€å¾Œã«1ã¤ â†’ M1ï¼ˆå‰ï¼‰, M3ï¼ˆå¾Œï¼‰, M2ã¯ç©º
        - å‰ã®ã¿2ã¤ â†’ M1, M2
        - å¾Œã®ã¿2ã¤ â†’ M2, M3
        3å€‹ â†’ M1, M2, M3ï¼ˆä½ç½®é †ï¼‰
        """
        modifier_slots = {}
        
        if not modifiers_info:
            return modifier_slots
        
        # å…¨ä¿®é£¾èªã‚’åé›†ï¼ˆé‡è¤‡é™¤å»ä»˜ãï¼‰
        all_modifiers = []
        seen_modifiers = set()  # é‡è¤‡é˜²æ­¢
        
        for verb_idx, modifier_list in modifiers_info.items():
            for modifier_info in modifier_list:
                modifier_text = modifier_info['text']
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                if modifier_text in seen_modifiers:
                    continue
                seen_modifiers.add(modifier_text)
                
                # å‹•è©ä½ç½®ã‚’å–å¾—
                verb_position = verb_idx
                modifier_position = modifier_info.get('idx', 0)
                
                all_modifiers.append({
                    'text': modifier_text,
                    'verb_idx': verb_idx,
                    'modifier_idx': modifier_position,
                    'position_type': 'pre-verb' if modifier_position < verb_position else 'post-verb'
                })
        
        # ä¿®é£¾èªã‚’æ–‡ä¸­ã®ä½ç½®é †ã§ã‚½ãƒ¼ãƒˆ
        all_modifiers.sort(key=lambda x: x['modifier_idx'])
        
        modifier_count = len(all_modifiers)
        
        if modifier_count == 1:
            # ğŸ”¥ REPHRASEä»•æ§˜ï¼š1å€‹ã®ã¿ â†’ å¿…ãšM2ã«é…ç½®ï¼ˆä½ç½®ç„¡é–¢ä¿‚ï¼‰
            modifier = all_modifiers[0]
            modifier_slots['M2'] = modifier['text']
            
        elif modifier_count == 2:
            # ç‰¹åˆ¥ã‚±ãƒ¼ã‚¹: ã€Œå‰¯è© and å‰¯è©ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å…ˆã«ãƒã‚§ãƒƒã‚¯
            if (all_modifiers[1]['modifier_idx'] - all_modifiers[0]['modifier_idx'] == 2):
                # é–“ã«ã€Œandã€ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                and_idx = all_modifiers[0]['modifier_idx'] + 1
                if and_idx < len(doc) and doc[and_idx].text.lower() == 'and':
                    # ã€Œquicklyã€ã¨ã€Œand carefullyã€ã¨ã—ã¦åˆ†å‰²
                    modifier_slots['M2'] = all_modifiers[0]['text']
                    modifier_slots['M3'] = f"and {all_modifiers[1]['text']}"
                    print(f"ğŸ”§ ã€Œå‰¯è© and å‰¯è©ã€ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º: M2='{all_modifiers[0]['text']}', M3='and {all_modifiers[1]['text']}'")
                    return modifier_slots
                    
            # 2å€‹ã®å ´åˆï¼šè·é›¢ãƒ™ãƒ¼ã‚¹ãƒ«ãƒ¼ãƒ«é©ç”¨
            
            # å‹•è©ä½ç½®ã‚’å–å¾—ï¼ˆæœ€åˆã®å‹•è©ã‚’ä½¿ç”¨ï¼‰
            verb_idx = all_modifiers[0]['verb_idx']
            
            # å„ä¿®é£¾èªã¨å‹•è©ã®è·é›¢ã‚’è¨ˆç®—
            modifier1 = all_modifiers[0]
            modifier2 = all_modifiers[1]
            
            distance1 = abs(modifier1['modifier_idx'] - verb_idx)
            distance2 = abs(modifier2['modifier_idx'] - verb_idx)
            
            # ğŸ”§ Agentå¥ï¼ˆbyå¥ï¼‰ã®ç‰¹åˆ¥å‡¦ç†
            by_clause_modifiers = []
            regular_modifiers = []
            
            for modifier in all_modifiers:
                if modifier['text'].lower().startswith('by '):
                    by_clause_modifiers.append(modifier)
                else:
                    regular_modifiers.append(modifier)
            
            # Agentå¥ãŒã‚ã‚‹å ´åˆã®ç‰¹åˆ¥é…ç½®: M2ï¼ˆå‰¯è©ï¼‰, M3ï¼ˆbyå¥ï¼‰
            if len(by_clause_modifiers) == 1 and len(regular_modifiers) == 1:
                modifier_slots['M2'] = regular_modifiers[0]['text']
                modifier_slots['M3'] = by_clause_modifiers[0]['text']
                return modifier_slots
            
            # è·é›¢ãƒ™ãƒ¼ã‚¹é…ç½®: å‹•è©ã«è¿‘ã„æ–¹ãŒM2ã€é ã„æ–¹ãŒM1/M3
            if distance1 <= distance2:
                # modifier1ãŒå‹•è©ã«è¿‘ã„
                closer_modifier = modifier1
                farther_modifier = modifier2
            else:
                # modifier2ãŒå‹•è©ã«è¿‘ã„
                closer_modifier = modifier2
                farther_modifier = modifier1
            
            # å‹•è©ã«è¿‘ã„ä¿®é£¾èªã¯å¸¸ã«M2
            modifier_slots['M2'] = closer_modifier['text']
            
            # é ã„ä¿®é£¾èªã¯ä½ç½®ã«ã‚ˆã£ã¦M1ã¾ãŸã¯M3
            if farther_modifier['modifier_idx'] < verb_idx:
                modifier_slots['M1'] = farther_modifier['text']  # å‹•è©ã‚ˆã‚Šå‰
            else:
                modifier_slots['M3'] = farther_modifier['text']  # å‹•è©ã‚ˆã‚Šå¾Œ
            
        elif modifier_count == 3:
            # 3å€‹ â†’ M1, M2, M3ï¼ˆä½ç½®é †ï¼‰
            modifier_slots['M1'] = all_modifiers[0]['text']
            modifier_slots['M2'] = all_modifiers[1]['text']
            modifier_slots['M3'] = all_modifiers[2]['text']
        
        return modifier_slots
    
    def _get_verb_positions(self, verb_modifier_pairs: List[Dict]) -> List[int]:
        """å‹•è©ã®ä½ç½®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        positions = []
        for pair in verb_modifier_pairs:
            positions.append(pair['verb_idx'])
        return positions
    
    def _find_main_verb_for_modifiers(self, doc, verb_idx: int) -> int:
        """
        ä¿®é£¾èªå‡¦ç†ç”¨ã®ä¸»å‹•è©ã‚’æ¤œå‡ºï¼ˆå—å‹•æ…‹å¯¾å¿œï¼‰
        
        Args:
            doc: spaCy Doc ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            verb_idx: ç¾åœ¨ã®å‹•è©ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            
        Returns:
            int: ä¸»å‹•è©ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        """
        # å—å‹•æ…‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
        verb_token = doc[verb_idx]
        
        # beå‹•è©ã®å ´åˆã€æ¬¡ã®éå»åˆ†è©ã‚’æ¢ã™
        if verb_token.lemma_ == 'be' or verb_token.text.lower() in ['am', 'is', 'are', 'was', 'were', 'being']:
            for i in range(verb_idx + 1, len(doc)):
                next_token = doc[i]
                # å‰¯è©ã¯ã‚¹ã‚­ãƒƒãƒ—
                if next_token.pos_ == 'ADV':
                    continue
                # éå»åˆ†è©ã‚’ç™ºè¦‹
                if next_token.pos_ == 'VERB' and next_token.tag_ == 'VBN':
                    print(f"ğŸ”§ å—å‹•æ…‹ä¸»å‹•è©æ¤œå‡º: beå‹•è©idx={verb_idx} â†’ ä¸»å‹•è©idx={i} ({next_token.text})")
                    return i
                # ä»–ã®å“è©ã«é”ã—ãŸã‚‰åœæ­¢
                if next_token.pos_ in ['NOUN', 'PRON', 'PROPN', 'ADP']:
                    break
        
        # é€šå¸¸ã®å‹•è©ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
        return verb_idx

    def _detect_noun_clause_boundaries(self, doc) -> List[Dict]:
        """åè©ç¯€ã®å¢ƒç•Œã‚’æ¤œå‡º"""
        boundaries = []
        
        for i, token in enumerate(doc):
            # thatç¯€ã®æ¤œå‡º
            if token.text.lower() == 'that' and token.dep_ == 'mark':
                boundaries.append({
                    'type': 'that_clause',
                    'start': i,
                    'connector': 'that'
                })
            
            # whç¯€ã®æ¤œå‡º
            elif token.text.lower() in ['what', 'who', 'whom', 'whose', 'which', 'where', 'when', 'why', 'how']:
                if token.dep_ in ['nsubj', 'dobj', 'pobj', 'advmod']:
                    boundaries.append({
                        'type': 'wh_clause',
                        'start': i,
                        'connector': token.text.lower()
                    })
            
            # whether/ifç¯€ã®æ¤œå‡º
            elif token.text.lower() in ['whether', 'if'] and token.dep_ == 'mark':
                boundaries.append({
                    'type': 'whether_if_clause',
                    'start': i,
                    'connector': token.text.lower()
                })
        
        return boundaries
    
    def _find_main_clause_verb(self, doc) -> int:
        """ä¸»æ–‡ã®å‹•è©ä½ç½®ã‚’ç‰¹å®šï¼ˆåŠ©å‹•è©æ–‡å¯¾å¿œï¼‰"""
        # æœ€åˆã«ROOTã‚’æ¢ã™
        root_idx = -1
        for i, token in enumerate(doc):
            if token.dep_ == 'ROOT':
                root_idx = i
                break
        
        if root_idx == -1:
            return -1
        
        # åŠ©å‹•è©æ–‡ã®å ´åˆã€å®Ÿéš›ã®ä¸»å‹•è©ã‚’æ¢ã™
        root_token = doc[root_idx]
        
        # "used to", "going to" ãªã©ã®åŠ©å‹•è©æ§‹é€ ã‚’ç‰¹å®š
        if (root_token.text.lower() in ['used', 'going'] and 
            root_idx + 1 < len(doc) and 
            doc[root_idx + 1].text.lower() == 'to'):
            
            # æ¬¡ã®å‹•è©ï¼ˆxcompï¼‰ã‚’ä¸»å‹•è©ã¨ã—ã¦æ‰±ã†
            for i in range(root_idx + 2, len(doc)):
                token = doc[i]
                if token.dep_ == 'xcomp' and token.pos_ == 'VERB':
                    print(f"ğŸ”§ åŠ©å‹•è©æ–‡ä¸»å‹•è©æ¤œå‡º: {root_token.text} to {token.text} â†’ ä¸»å‹•è©ã¯ {token.text} (idx: {i})")
                    return i
        
        # ãã®ä»–ã®åŠ©å‹•è©ã®å ´åˆã‚‚ç¢ºèª
        if root_token.pos_ in ['AUX', 'VERB']:
            # xcompã¾ãŸã¯vcompãŒä¸»å‹•è©ã®å¯èƒ½æ€§
            for i in range(root_idx + 1, len(doc)):
                token = doc[i]
                if token.dep_ in ['xcomp', 'ccomp'] and token.pos_ == 'VERB':
                    print(f"ğŸ”§ åŠ©å‹•è©æ–‡ä¸»å‹•è©æ¤œå‡º: {root_token.text} + {token.text} â†’ ä¸»å‹•è©ã¯ {token.text} (idx: {i})")
                    return i
        
        return root_idx
