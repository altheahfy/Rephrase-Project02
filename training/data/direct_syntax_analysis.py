#!/usr/bin/env python3
"""ç›´æ¥Stanza/spaCyã‚’ä½¿ç”¨ã—ãŸæ§‹æ–‡è§£æèª¿æŸ»"""

import stanza
import spacy

def direct_syntactic_analysis():
    """Stanzaã¨spaCyã‚’ç›´æ¥ä½¿ç”¨ã—ãŸæ§‹æ–‡è§£æ"""
    
    print("ğŸ” Direct Stanza/spaCy æ§‹æ–‡è§£æèª¿æŸ»")
    print("=" * 60)
    
    # åˆæœŸåŒ–
    try:
        print("ğŸš€ NLPãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ä¸­...")
        nlp_stanza = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse', verbose=False)
        nlp_spacy = spacy.load("en_core_web_sm")
        print("âœ… åˆæœŸåŒ–å®Œäº†")
    except Exception as e:
        print(f"âŒ åˆæœŸåŒ–å¤±æ•—: {e}")
        return
    
    test_sentences = [
        "I think that he is smart.",           # thatç¯€
        "Being a teacher, she knows well.",    # åˆ†è©æ§‹æ–‡  
        "The book that I read was good.",      # é–¢ä¿‚ä»£åè©
        "If I were rich, I would travel.",     # ä»®å®šæ³•
    ]
    
    for sentence in test_sentences:
        print(f"\nğŸ“ åˆ†ææ–‡: \"{sentence}\"")
        print("-" * 50)
        
        try:
            # Stanzaè§£æ
            stanza_doc = nlp_stanza(sentence)
            
            print("ğŸ”µ Stanzaä¾å­˜æ§‹é€ :")
            for sent in stanza_doc.sentences:
                for word in sent.words:
                    if word.head != 0:  # rootã§ãªã„å ´åˆ
                        head_word = sent.words[word.head-1].text
                        print(f"  {word.text}({word.upos}) --{word.deprel}--> {head_word}")
                    else:
                        print(f"  {word.text}({word.upos}) [ROOT]")
            
            # spaCyè§£æ
            spacy_doc = nlp_spacy(sentence)
            
            print("\nğŸŸ¢ spaCyåè©å¥ãƒ»å¥æ§‹é€ :")
            for chunk in spacy_doc.noun_chunks:
                print(f"  NP: '{chunk.text}' (root: {chunk.root.text})")
                
            print("\nğŸŸ¢ spaCyé‡è¦ä¾å­˜é–¢ä¿‚:")
            for token in spacy_doc:
                if token.dep_ in ['nsubj', 'dobj', 'ccomp', 'xcomp', 'advcl', 'acl', 'relcl', 'mark']:
                    print(f"  {token.text}({token.pos_}) --{token.dep_}--> {token.head.text}")
                    
            # ç¯€æ§‹é€ æ¤œå‡º
            print("\nğŸ¯ æ¤œå‡ºå¯èƒ½ãªç¯€æ§‹é€ :")
            clauses = []
            for token in spacy_doc:
                if token.dep_ in ['ccomp', 'xcomp', 'advcl', 'acl', 'relcl']:
                    clause_tokens = [t for t in token.subtree]
                    clause_text = ' '.join([t.text for t in clause_tokens])
                    clauses.append({
                        'type': token.dep_,
                        'text': clause_text,
                        'root': token.text,
                        'has_subject': any(t.dep_ == 'nsubj' for t in clause_tokens),
                        'has_verb': any(t.pos_ == 'VERB' for t in clause_tokens)
                    })
            
            for clause in clauses:
                sv_status = "SV" if clause['has_subject'] and clause['has_verb'] else "phrase"
                print(f"  {clause['type']}: '{clause['text']}' ({sv_status})")
                    
        except Exception as e:
            print(f"âŒ è§£æã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    direct_syntactic_analysis()
