#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RelativeClauseHandler: Phase 2 é–¢ä¿‚ç¯€å‡¦ç†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
spaCyå“è©åˆ¤å®šãƒ™ãƒ¼ã‚¹ï¼ˆãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç¦æ­¢ï¼‰
Legacy ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å‚è€ƒã«ã—ãŸæ­£è¦è¡¨ç¾ + spaCy POSåˆ¤å®š
"""

import re
import spacy
from typing import Dict, Any, Tuple

class RelativeClauseHandler:
    """é–¢ä¿‚ç¯€å‡¦ç†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆå”åŠ›ã‚¢ãƒ—ãƒ­ãƒ¼ãƒç‰ˆï¼‰"""
    
    def __init__(self, collaborators=None):
        """
        åˆæœŸåŒ–
        
        Args:
            collaborators: å”åŠ›è€…ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¾æ›¸
                - 'adverb': AdverbHandlerï¼ˆä¿®é£¾èªåˆ†é›¢ï¼‰
                - 'five_pattern': BasicFivePatternHandlerï¼ˆ5æ–‡å‹åˆ†æï¼‰
                - 'passive': PassiveVoiceHandlerï¼ˆå—å‹•æ…‹ç†è§£ï¼‰
        """
        self.name = "RelativeClauseHandler"
        self.version = "cooperation_v1.0"
        self.nlp = spacy.load('en_core_web_sm')  # spaCyå“è©åˆ¤å®šç”¨
        
        # å”åŠ›è€…ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŸã¡ï¼ˆDependency Injectionï¼‰
        if collaborators:
            self.adverb_handler = collaborators.get('adverb') or collaborators.get('AdverbHandler')
            self.five_pattern_handler = collaborators.get('five_pattern') or collaborators.get('FivePatternHandler')
            self.passive_handler = collaborators.get('passive') or collaborators.get('PassiveHandler')
        else:
            self.adverb_handler = None
            self.five_pattern_handler = None
            self.passive_handler = None
    
    def process(self, text: str, original_text: str = None) -> Dict[str, Any]:
        """
        é–¢ä¿‚ç¯€å‡¦ç†ãƒ¡ã‚¤ãƒ³ï¼ˆå”åŠ›ã‚¢ãƒ—ãƒ­ãƒ¼ãƒç‰ˆï¼‰
        
        Args:
            text: å‡¦ç†å¯¾è±¡ã®è‹±èªæ–‡ï¼ˆä¿®é£¾èªåˆ†é›¢æ¸ˆã¿å¯èƒ½æ€§ã‚ã‚Šï¼‰
            original_text: ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¿®é£¾èªæƒ…å ±ä¿æŒç”¨ï¼‰
            
        Returns:
            Dict: å‡¦ç†çµæœ
        """
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ†ã‚­ã‚¹ãƒˆã®æ±ºå®š
        self.original_text = original_text if original_text else text
        
        # æ›–æ˜§èªå¥è§£æ±ºãƒ•ãƒ©ã‚°ã®åˆæœŸåŒ–
        self._verb_override = None
        
        try:
            print(f"ğŸ” é–¢ä¿‚ç¯€å‡¦ç†é–‹å§‹: '{text}'")
            
            # æ›–æ˜§èªå¥è§£æ±ºã®å®Ÿè¡Œ
            resolved_text = self._resolve_ambiguous_words(text)
            
            # åŸºæœ¬çš„ãªé–¢ä¿‚ä»£åè©æ¤œå‡ºï¼ˆå„ªå…ˆé †ä½é †ï¼‰
            if ' whose ' in resolved_text.lower():
                print(f"ğŸ¯ whoseæ¤œå‡º")
                return self._process_whose(resolved_text)
            elif ' whom ' in resolved_text.lower():
                return self._process_whom(resolved_text)
            elif ' who ' in resolved_text.lower():
                return self._process_who(resolved_text)
            elif ' which ' in resolved_text.lower():
                return self._process_which(resolved_text)
            elif ' that ' in resolved_text.lower():
                return self._process_that(resolved_text)
            elif ' where ' in resolved_text.lower():
                return self._process_relative_adverb(resolved_text, 'where')
            elif ' when ' in resolved_text.lower():
                return self._process_relative_adverb(resolved_text, 'when')
            elif ' why ' in resolved_text.lower():
                return self._process_relative_adverb(resolved_text, 'why')
            elif ' how ' in text.lower():
                return self._process_relative_adverb(text, 'how')
            else:
                print(f"âš ï¸ é–¢ä¿‚ç¯€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: '{text}'")
                return {'success': False, 'error': 'é–¢ä¿‚ç¯€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ'}
                
        except Exception as e:
            return {'success': False, 'error': f'å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}'}
    
    def _resolve_ambiguous_words(self, text: str) -> str:
        """æ›–æ˜§èªå¥è§£æ±º: å‹•è©/åè©ã®æ›–æ˜§æ€§ã‚’äººé–“çš„æ‰‹æ³•ã§è§£æ±º"""
        doc = self.nlp(text)
        
        # æ›–æ˜§èªå¥ã®å€™è£œãƒªã‚¹ãƒˆ
        ambiguous_patterns = {
            'works': [('VERB', '3rd person singular'), ('NOUN', 'plural')],
            'lives': [('VERB', '3rd person singular'), ('NOUN', 'plural')], 
            'loves': [('VERB', '3rd person singular'), ('NOUN', 'plural')],
            'runs': [('VERB', '3rd person singular'), ('NOUN', 'plural')],
            'calls': [('VERB', '3rd person singular'), ('NOUN', 'plural')]
        }
        
        print(f"ğŸ” æ›–æ˜§èªå¥è§£æ±ºé–‹å§‹: '{text}'")
        
        for token in doc:
            if token.text.lower() in ambiguous_patterns:
                print(f"âš ï¸ æ›–æ˜§èªå¥ç™ºè¦‹: '{token.text}' - spaCyåˆ¤å®š: {token.pos_}")
                
                # é–¢ä¿‚ç¯€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
                rel_pronoun_pos = None
                for i, t in enumerate(doc):
                    if t.text.lower() in ['that', 'who', 'whose', 'which']:
                        rel_pronoun_pos = i
                        break
                
                if rel_pronoun_pos is not None:
                    # é–¢ä¿‚ç¯€å¾Œã®æœ€åˆã®å€™è£œèªã‚’å‹•è©ã¨ã—ã¦è©¦è¡Œ
                    if token.i > rel_pronoun_pos:
                        print(f"ğŸ“ é–¢ä¿‚ç¯€å¾Œã®èªå¥: '{token.text}' â†’ å‹•è©å€™è£œã¨ã—ã¦åˆ¤å®š")
                        # ã“ã®å ´åˆã€å‹•è©ã¨ã—ã¦æ‰±ã†ã®ãŒæ–‡æ³•çš„ã«æ­£ã—ã„
                        return self._apply_verb_interpretation(text, token.text, token.i)
        
        return text
    
    def _apply_verb_interpretation(self, text: str, ambiguous_word: str, position: int) -> str:
        """æ›–æ˜§èªå¥ã‚’å‹•è©ã¨ã—ã¦è§£é‡ˆã—ã¦æ–‡æ§‹é€ ã‚’ä¿®æ­£"""
        print(f"ğŸ”§ å‹•è©è§£é‡ˆé©ç”¨: '{ambiguous_word}' at position {position}")
        
        # ã“ã®æƒ…å ±ã‚’å¾Œç¶šå‡¦ç†ã§ä½¿ç”¨ã™ã‚‹ãŸã‚ã®ãƒ•ãƒ©ã‚°ã‚’è¨­å®š
        self._verb_override = {
            'word': ambiguous_word,
            'position': position,
            'interpretation': 'VERB'
        }
        
        return text

    def _apply_verb_override_to_analysis(self, analysis: Dict, text: str) -> Dict:
        """æ›–æ˜§èªå¥ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã‚’åˆ†æçµæœã«é©ç”¨"""
        if not self._verb_override:
            return analysis
            
        doc = self.nlp(text)
        override_word = self._verb_override['word']
        override_pos = self._verb_override['position']
        
        print(f"ğŸ”§ åˆ†æä¿®æ­£: '{override_word}' at {override_pos} ã‚’å‹•è©ã¨ã—ã¦è§£é‡ˆ")
        
        # æ–‡æ§‹é€ ã‚’æ‰‹å‹•ã§ä¿®æ­£
        # Case 64: "The machine that was properly maintained works efficiently every day."
        # works (position 6) ã‚’ä¸»ç¯€ã®å‹•è©ã¨ã—ã¦è¨­å®š
        
        # é–¢ä¿‚ç¯€ã®çµ‚äº†ä½ç½®ã‚’ "maintained" ã§è¨­å®š
        rel_end = None
        main_verb_pos = None
        
        for i, token in enumerate(doc):
            if token.text == override_word and i == override_pos:
                main_verb_pos = i
                # ãã®å‰ã®å‹•è©ã‚’é–¢ä¿‚ç¯€ã®å‹•è©ã¨ã™ã‚‹
                for j in range(i-1, -1, -1):
                    if doc[j].pos_ == 'VERB' or doc[j].text in ['was', 'were', 'is', 'are']:
                        rel_end = j
                        break
                break
        
        if main_verb_pos is not None and rel_end is not None:
            print(f"ğŸ“ æ§‹é€ ä¿®æ­£: é–¢ä¿‚ç¯€çµ‚äº†={rel_end}, ä¸»ç¯€é–‹å§‹={main_verb_pos}")
            analysis['main_clause_start'] = main_verb_pos
            analysis['main_verb'] = override_word
            analysis['relative_clause_end'] = rel_end
            
        return analysis

    def _process_who(self, text: str) -> Dict[str, Any]:
        """whoé–¢ä¿‚ç¯€å‡¦ç†ï¼ˆå”åŠ›ã‚¢ãƒ—ãƒ­ãƒ¼ãƒç‰ˆï¼‰"""
        
        # spaCyæ–‡è„ˆè§£æã§é–¢ä¿‚ç¯€ã‚’åˆ†æï¼ˆå”åŠ›è€…æƒ…å ±ã‚’å«ã‚€ï¼‰
        analysis = self._analyze_relative_clause(text, 'who')
        if not analysis['success']:
            return analysis
        
        antecedent = analysis['antecedent']
        rel_verb = analysis['relative_verb']
        
        # ä¿®é£¾èªæƒ…å ±ï¼ˆå”åŠ›è€… AdverbHandler ã®çµæœã‚’æ´»ç”¨ï¼‰
        modifiers_info = analysis.get('modifiers', {})
        print(f"ğŸ” DEBUG: å—ä¿¡ã—ãŸmodifiers_info = {modifiers_info}")
        
        # ğŸ¯ ä¿®é£¾èªã®ä½ç½®åˆ†é›¢: é–¢ä¿‚ç¯€å†… vs ä¸»ç¯€
        rel_modifiers = {}
        main_modifiers = {}
        
        if modifiers_info:
            # é–¢ä¿‚ç¯€å¢ƒç•Œã‚’å–å¾—
            rel_boundary = analysis.get('relative_clause_end', len(text.split()))
            doc = analysis.get('doc')
            
            # ä¿®é£¾èªã®ä½ç½®ã‚’åˆ¤å®š
            if doc:
                for slot, modifier_text in modifiers_info.items():
                    # ä¿®é£¾èªã®ä½ç½®ã‚’ç‰¹å®š
                    modifier_pos = None
                    for i, token in enumerate(doc):
                        if modifier_text.lower() in token.text.lower():
                            modifier_pos = i
                            break
                    
                    # ä½ç½®ã«åŸºã¥ã„ã¦åˆ†é›¢
                    if modifier_pos is not None and modifier_pos < rel_boundary:
                        rel_modifiers[slot] = modifier_text
                        print(f"ğŸ” DEBUG: é–¢ä¿‚ç¯€å†…ä¿®é£¾èª {slot} = {modifier_text}")
                    else:
                        main_modifiers[slot] = modifier_text
                        print(f"ğŸ” DEBUG: ä¸»ç¯€ä¿®é£¾èª {slot} = {modifier_text}")
        
        sub_m2 = rel_modifiers.get('M2', "")
        sub_m3 = rel_modifiers.get('M3', "")
        
        # å—å‹•æ…‹æƒ…å ±ï¼ˆå”åŠ›è€… PassiveVoiceHandler ã®çµæœã‚’æ´»ç”¨ï¼‰
        passive_info = analysis.get('passive_analysis', {})
        is_passive = passive_info.get('is_passive', False) if passive_info else False
        
        # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ§‹ç¯‰ï¼ˆå—å‹•æ…‹è€ƒæ…®ï¼‰
        if is_passive and passive_info:
            # å—å‹•æ…‹ã®å ´åˆ: Aux + V ã«åˆ†é›¢
            sub_slots = {
                'sub-s': f"{antecedent} who",
                'sub-aux': passive_info.get('aux', ''),  # beå‹•è©
                'sub-v': passive_info.get('verb', ''),   # éå»åˆ†è©
                '_parent_slot': 'S'
            }
        else:
            # é€šå¸¸ã®å ´åˆ
            sub_slots = {
                'sub-s': f"{antecedent} who",
                'sub-v': rel_verb,  # å‹•è©ã®ã¿
                '_parent_slot': 'S'  # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
            }
        
        # ä¿®é£¾èªãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
        if sub_m2:
            sub_slots['sub-m2'] = sub_m2
        if sub_m3:
            sub_slots['sub-m3'] = sub_m3
        
        # ä¸»ç¯€ã‚’æ§‹ç¯‰
        main_clause_start = analysis.get('main_clause_start')
        main_clause = ""
        if main_clause_start is not None:
            doc = analysis['doc']
            main_tokens = [token.text for token in doc[main_clause_start:]]
            main_clause = " ".join(main_tokens)
        
        return {
            'success': True,
            'main_slots': {'S': ''},  # è¨­è¨ˆä»•æ§˜æ›¸æº–æ‹ : ä¸»èªã‚¹ãƒ­ãƒƒãƒˆç©ºæ–‡å­—åˆ—
            'sub_slots': sub_slots,
            'pattern_type': 'who_subject',
            'relative_pronoun': 'who',
            'antecedent': antecedent,
            'main_continuation': main_clause.strip(),
            'main_modifiers': main_modifiers,  # ä¸»ç¯€ç”¨ä¿®é£¾èªã‚’è¿½åŠ 
            'spacy_analysis': {
                'relative_verb_pos': analysis['relative_verb_pos'],
                'relative_verb_lemma': analysis['relative_verb_lemma']
            },
            'cooperation_details': {
                'passive_analysis': passive_info,
                'modifiers_analysis': modifiers_info
            }
        }
    
    def _extract_relative_clause_text_original(self, text: str, relative_pronoun: str) -> str:
        """ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é–¢ä¿‚ç¯€éƒ¨åˆ†ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆä¿®é£¾èªè¾¼ã¿ï¼‰"""
        try:
            doc = self.nlp(text)
            
            rel_start = None
            rel_end = len(doc)
            
            # Step 1: é–¢ä¿‚ä»£åè©ã®ä½ç½®ã‚’ç‰¹å®š
            for i, token in enumerate(doc):
                if token.text.lower() == relative_pronoun.lower():
                    rel_start = i
                    break
            
            if rel_start is None:
                return text
            
            # Step 2: æ–‡å…¨ä½“ã®ãƒ¡ã‚¤ãƒ³å‹•è©ï¼ˆçœŸã®ROOTï¼‰ã‚’ç‰¹å®š
            main_root_idx = None
            for i, token in enumerate(doc):
                if token.dep_ == 'ROOT':
                    main_root_idx = i
                    break
            
            # Step 3: é–¢ä¿‚ç¯€ã®çµ‚äº†ä½ç½®ã‚’æ±ºå®š
            # whose ã®å ´åˆã¯ç‰¹åˆ¥å‡¦ç†ï¼šè¨­è¨ˆä»•æ§˜æ›¸ã®äººé–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜æº–æ‹ 
            if relative_pronoun.lower() == 'whose':
                print(f"ğŸ” whoseå¢ƒç•Œèªè­˜ï¼ˆè¨­è¨ˆä»•æ§˜æ›¸æº–æ‹ ï¼‰")
                
                # "The man whose car is red lives here." ã®ä¾‹ã«å¾“ã£ã¦å‡¦ç†
                whose_noun = None
                rel_verb = None
                boundary_pos = rel_start + 1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¢ƒç•Œ
                
                # whoseç›´å¾Œã®åè©ã‚’ç‰¹å®š
                if rel_start + 1 < len(doc):
                    whose_noun = doc[rel_start + 1].text
                    print(f"ğŸ“ whoseåè©: '{whose_noun}'")
                
                # é–¢ä¿‚ç¯€å†…ã®å‹•è©ã¨ãã®å¾Œç¶šè¦ç´ ã‚’åˆ†æ
                for i in range(rel_start + 2, len(doc)):
                    token = doc[i]
                    
                    # é–¢ä¿‚ç¯€å†…ã®å‹•è©ã‚’ç‰¹å®š
                    if token.pos_ in ['VERB', 'AUX'] and rel_verb is None:
                        rel_verb = token.text
                        print(f"ğŸ“ é–¢ä¿‚ç¯€å‹•è©: '{rel_verb}'")
                        continue
                    
                    # å‹•è©ã®å¾Œã®å½¢å®¹è©ã§é–¢ä¿‚ç¯€çµ‚äº†ï¼ˆè¨­è¨ˆä»•æ§˜æ›¸ä¾‹: "redã§é–¢ä¿‚ç¯€çµ‚äº†"ï¼‰
                    if rel_verb and token.pos_ == 'ADJ':
                        print(f"ğŸ¯ å½¢å®¹è© '{token.text}' ã§é–¢ä¿‚ç¯€çµ‚äº†ï¼ˆè¨­è¨ˆä»•æ§˜æ›¸æº–æ‹ ï¼‰")
                        boundary_pos = i + 1
                        break
                    
                    # ROOTå‹•è©ã«åˆ°é”ã—ãŸã‚‰ä¸»ç¯€é–‹å§‹
                    if token.dep_ == 'ROOT':
                        print(f"ğŸ” ROOTå‹•è© '{token.text}' ã§ä¸»ç¯€é–‹å§‹")
                        boundary_pos = i
                        break
                    
                    # ğŸ¯ åŠ©å‹•è©ï¼ˆis, are, was, wereï¼‰ã§ä¸»ç¯€é–‹å§‹ï¼ˆwhoseæ§‹é€ ç‰¹åŒ–ï¼‰
                    if token.pos_ == 'AUX' and token.text.lower() in ['is', 'are', 'was', 'were']:
                        print(f"ğŸ” åŠ©å‹•è© '{token.text}' ã§ä¸»ç¯€é–‹å§‹ï¼ˆwhoseç‰¹åŒ–ï¼‰")
                        boundary_pos = i
                        break
                
                rel_end = boundary_pos
                print(f"ğŸ“Š whoseé–¢ä¿‚ç¯€å¢ƒç•Œ: {rel_start} â†’ {rel_end}")
                
                # é–¢ä¿‚ç¯€ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆwhoseä»¥é™ã®éƒ¨åˆ†ã®ã¿ï¼‰
                if rel_start + 1 < rel_end:
                    clause_tokens = doc[rel_start + 1:rel_end]  # whoseã¯é™¤å¤–
                    extracted = ' '.join([t.text for t in clause_tokens])
                    print(f"ğŸ“Š whoseé–¢ä¿‚ç¯€æŠ½å‡º: '{extracted}'")
                    return extracted
                else:
                    return ""
            # ãã®ä»–ã®é–¢ä¿‚ä»£åè©ã®å ´åˆ
            elif main_root_idx is not None and main_root_idx > rel_start:
                rel_end = main_root_idx
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å“è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã§åˆ¤å®š
                for i in range(rel_start + 1, len(doc)):
                    token = doc[i]
                    # ä¸»èªçš„èªå¥ï¼ˆåè©ï¼‹å‹•è©ï¼‰ã«é­é‡ã—ãŸã‚‰é–¢ä¿‚ç¯€çµ‚äº†
                    if (token.pos_ in ['NOUN', 'PROPN'] and 
                        i + 1 < len(doc) and 
                        doc[i + 1].pos_ in ['VERB', 'AUX']):
                        rel_end = i
                        break
            
            # Step 4: é–¢ä¿‚ç¯€ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
            if rel_start is not None:
                clause_tokens = doc[rel_start:rel_end]
                extracted = ' '.join([t.text for t in clause_tokens])
                return extracted
            
            return text
            
        except Exception as e:
            return text

    def _extract_relative_clause_text(self, text: str, relative_pronoun: str) -> str:
        """é–¢ä¿‚ç¯€éƒ¨åˆ†ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆä¿®é£¾èªè¾¼ã¿ï¼‰"""
        try:
            doc = self.nlp(text)
            
            rel_start = None
            rel_end = len(doc)
            
            # Step 1: é–¢ä¿‚ä»£åè©ã®ä½ç½®ã‚’ç‰¹å®š
            for i, token in enumerate(doc):
                if token.text.lower() == relative_pronoun.lower():
                    rel_start = i
                    break
            
            if rel_start is None:
                return text
            
            # Step 2: æ–‡å…¨ä½“ã®ãƒ¡ã‚¤ãƒ³å‹•è©ï¼ˆçœŸã®ROOTï¼‰ã‚’ç‰¹å®š
            main_root_idx = None
            for i, token in enumerate(doc):
                if token.dep_ == 'ROOT':
                    main_root_idx = i
                    break
            
            # Step 3: é–¢ä¿‚ç¯€ã®çµ‚äº†ä½ç½®ã‚’æ±ºå®šï¼ˆå°‚é–€åˆ†æ‹…å‹ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æï¼‰
            # ä¾å­˜é–¢ä¿‚ã§æ­£ç¢ºãªä¸»ç¯€å‹•è©ã‚’æ¤œå‡ºï¼ˆè¤‡æ–‡æ§‹é€ ã®ãŸã‚ï¼‰
            if main_root_idx is not None and main_root_idx > rel_start:
                rel_end = main_root_idx
                print(f"ğŸ” _extract_relative_clause_text: ä¾å­˜é–¢ä¿‚ã«ã‚ˆã‚‹å¢ƒç•Œæ¤œå‡º = {rel_end} (ä¸»å‹•è©: {doc[main_root_idx].text})")
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å“è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã§åˆ¤å®š
                rel_end = len(doc) - 1  # æœ€å¾Œã¾ã§é–¢ä¿‚ç¯€ã¨ã—ã¦æ‰±ã†
                for i in range(rel_start + 1, len(doc)):
                    token = doc[i]
                    # beå‹•è©ã‚„ãƒ¡ã‚¤ãƒ³å‹•è©ã‚’æ¤œå‡ºã—ãŸã‚‰é–¢ä¿‚ç¯€çµ‚äº†
                    if token.pos_ in ['VERB', 'AUX'] and token.text.lower() in ['is', 'are', 'was', 'were']:
                        rel_end = i
                        print(f"ğŸ” _extract_relative_clause_text: å“è©ã«ã‚ˆã‚‹å¢ƒç•Œæ¤œå‡º = {rel_end} ({token.text})")
                        break
            
            # Step 4: é–¢ä¿‚ç¯€ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
            if rel_start is not None:
                clause_tokens = doc[rel_start:rel_end]
                extracted = ' '.join([t.text for t in clause_tokens])
                print(f"[DEBUG] é–¢ä¿‚ç¯€æŠ½å‡º: '{text}' â†’ '{extracted}'")
                return extracted
            
            return text
            
        except Exception as e:
            print(f"[DEBUG] é–¢ä¿‚ç¯€æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return text
            return text

    def _analyze_relative_clause(self, text: str, relative_pronoun: str) -> Dict[str, Any]:
        """spaCyæ–‡è„ˆè§£æã«ã‚ˆã‚‹é–¢ä¿‚ç¯€åˆ†æï¼ˆå”åŠ›ã‚¢ãƒ—ãƒ­ãƒ¼ãƒç‰ˆï¼‰"""
        try:
            # Step 1: ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é–¢ä¿‚ç¯€éƒ¨åˆ†ã‚’æŠ½å‡ºï¼ˆä¿®é£¾èªè¾¼ã¿ï¼‰
            original_clause_text = self._extract_relative_clause_text_original(
                getattr(self, 'original_text', text), relative_pronoun
            )
            
            # Step 2: å”åŠ›è€…ï¼ˆå‰¯è©ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼‰ã¨é€£æºï¼šä¿®é£¾èªåˆ†é›¢
            cleaned_clause = original_clause_text
            modifiers = {}
            
            if self.adverb_handler and original_clause_text:
                adverb_result = self.adverb_handler.process(original_clause_text)
                
                if adverb_result.get('success'):
                    cleaned_clause = adverb_result.get('separated_text', original_clause_text)
                    
                    # ğŸ¯ AdverbHandlerãŒç›´æ¥æä¾›ã™ã‚‹modifier_slotsã‚’ä½¿ç”¨ï¼ˆæœ€é©åŒ–ï¼‰
                    modifier_slots = adverb_result.get('modifier_slots', {})
                    if modifier_slots:
                        modifiers.update(modifier_slots)
                        print(f"ğŸ¯ å”åŠ›è€…ã‹ã‚‰ä¿®é£¾èªå–å¾—: {modifier_slots}")
                    
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ—§å¼ã®å¤‰æ›å‡¦ç†
                    if not modifiers:
                        raw_modifiers = adverb_result.get('modifiers', {})
                        if raw_modifiers:
                            # ä½ç½®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚­ãƒ¼ã‹ã‚‰ä¿®é£¾èªãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦M2ã«çµ±åˆ
                            modifier_texts = []
                            for pos_idx, modifier_list in raw_modifiers.items():
                                if isinstance(modifier_list, list):
                                    for modifier_info in modifier_list:
                                        if isinstance(modifier_info, dict) and 'text' in modifier_info:
                                            modifier_texts.append(modifier_info['text'])
                            
                            # M2ã‚­ãƒ¼ã¨ã—ã¦çµ±åˆ
                            if modifier_texts:
                                modifiers['M2'] = ' '.join(modifier_texts)
            
            # Step 3: å”åŠ›è€…ï¼ˆ5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼‰ã¨é€£æºï¼šæ§‹é€ åˆ†æ
            structure_analysis = None
            if self.five_pattern_handler and cleaned_clause:
                structure_result = self.five_pattern_handler.process(cleaned_clause)
                if structure_result.get('success'):
                    structure_analysis = structure_result
            
            # Step 3.5: å”åŠ›è€…ï¼ˆå—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼‰ã¨é€£æºï¼šå—å‹•æ…‹æ¤œå‡º
            passive_analysis = None
            if self.passive_handler and cleaned_clause:
                passive_result = self.passive_handler.process(cleaned_clause)
                if passive_result:
                    passive_analysis = passive_result
            
            # Step 4: æ–‡å…¨ä½“ã‚’spaCyã§è§£æï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ»è©³ç´°æƒ…å ±ç”¨ï¼‰
            doc = self.nlp(text)
            
            # ğŸ¯ Step 4.5: spaCyèª¤åˆ¤å®šå¯¾å‡¦æ³•ï¼ˆè¨­è¨ˆä»•æ§˜æ›¸æº–æ‹ ï¼‰
            doc = self._apply_spacy_fallback(doc, text)
            
            # Step 5: é–¢ä¿‚ä»£åè©ã®ä½ç½®ã‚’ç‰¹å®š
            rel_pronoun_idx = None
            for i, token in enumerate(doc):
                if token.text.lower() == relative_pronoun.lower():
                    rel_pronoun_idx = i
                    break
            
            if rel_pronoun_idx is None:
                return {'success': False, 'error': f'{relative_pronoun}ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}
            
            # Step 6: é–¢ä¿‚ç¯€å†…ã®å‹•è©ã‚’ç‰¹å®šï¼ˆå”åŠ›è€…ã®çµæœã‚’å„ªå…ˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æœ‰ã‚Šï¼‰
            rel_verb_token = None
            if structure_analysis and structure_analysis.get('slots', {}).get('V'):
                # å”åŠ›è€…ã‹ã‚‰ã®5æ–‡å‹åˆ†æçµæœã‚’ä½¿ç”¨
                rel_verb = structure_analysis['slots']['V']
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: spaCyç›´æ¥åˆ†æ
                for i in range(rel_pronoun_idx + 1, len(doc)):
                    token = doc[i]
                    if token.pos_ in ['VERB', 'AUX']:
                        rel_verb_token = token
                        rel_verb = token.text
                        break
                    # ä¸»ç¯€ã®å‹•è©ã«é”ã—ãŸã‚‰åœæ­¢
                    if token.dep_ == 'ROOT':
                        break
            
            if not rel_verb_token and not rel_verb:
                return {'success': False, 'error': 'é–¢ä¿‚ç¯€å†…ã«å‹•è©ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}
            
            # Step 7: å…ˆè¡Œè©ã‚’ç‰¹å®š
            antecedent_tokens = []
            for i in range(rel_pronoun_idx):
                antecedent_tokens.append(doc[i])
            
            # Step 8: ä¸»ç¯€éƒ¨åˆ†ã‚’ç‰¹å®šï¼ˆå‹•è©å‰ã®ä¿®é£¾èªã‚‚å«ã‚ã‚‹ï¼‰
            main_clause_start = None
            root_verb_idx = None
            
            # ã¾ãšROOTå‹•è©ã‚’ç‰¹å®š
            for i in range(rel_pronoun_idx + 1, len(doc)):
                if doc[i].dep_ == 'ROOT':
                    root_verb_idx = i
                    break
            
            if root_verb_idx is not None:
                # ğŸ¯ ç²¾å¯†ãªå¢ƒç•Œåˆ¤å®š: é–¢ä¿‚ç¯€ã®ç¯„å›²ã‚’æ­£ç¢ºã«ç‰¹å®š
                rel_clause_end = rel_pronoun_idx + 1
                
                # relclå‹•è©ã‚’ç‰¹å®š
                rel_verb_idx = None
                for i, token in enumerate(doc):
                    if token.dep_ == 'relcl' and i > rel_pronoun_idx:
                        rel_verb_idx = i
                        break
                
                if rel_verb_idx is not None:
                    # é–¢ä¿‚ç¯€å†…ã®å…¨è¦ç´ ã‚’å«ã‚ã‚‹ï¼ˆä¾å­˜é–¢ä¿‚ã«åŸºã¥ãï¼‰
                    max_rel_idx = rel_verb_idx
                    for i in range(rel_verb_idx + 1, len(doc)):
                        token = doc[i]
                        # é–¢ä¿‚ç¯€å‹•è©ã«ç›´æ¥ã¾ãŸã¯é–“æ¥çš„ã«ä¾å­˜ã™ã‚‹è¦ç´ 
                        if (token.head.i == rel_verb_idx or 
                            (token.head.i > rel_pronoun_idx and token.head.i <= max_rel_idx)):
                            max_rel_idx = i
                        else:
                            break
                    rel_clause_end = max_rel_idx + 1
                
                # ğŸ¯ ä¸»ç¯€é–‹å§‹ä½ç½®: é–¢ä¿‚ç¯€çµ‚äº†å¾Œã‹ã¤ROOTå‹•è©å‘¨è¾ºã®å‹•è©/åŠ©å‹•è©
                main_clause_start = None
                
                # é–¢ä¿‚ç¯€çµ‚äº†å¾Œã‹ã‚‰ROOTå‹•è©ã¾ã§ã®ç¯„å›²ã§å‹•è©/åŠ©å‹•è©ã‚’æ¢ã™
                for i in range(rel_clause_end, len(doc)):
                    if doc[i].pos_ in ['VERB', 'AUX'] and i >= root_verb_idx - 1:
                        main_clause_start = i
                        print(f"ğŸ” ä¸»ç¯€é–‹å§‹ä½ç½®æ±ºå®š: é–¢ä¿‚ç¯€çµ‚äº†å¾Œã®å‹•è©/åŠ©å‹•è© {i} ({doc[i].text}) ã‹ã‚‰é–‹å§‹")
                        break
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ROOTå‹•è©ã‹ã‚‰é–‹å§‹
                if main_clause_start is None:
                    main_clause_start = root_verb_idx
                    print(f"ğŸ” ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ROOTå‹•è© {root_verb_idx} ({doc[root_verb_idx].text}) ã‹ã‚‰é–‹å§‹")
            
            result = {
                'success': True,
                'antecedent': ' '.join([t.text for t in antecedent_tokens]).strip(),
                'relative_verb': rel_verb,
                'relative_verb_pos': rel_verb_token.pos_ if rel_verb_token else 'VERB',
                'relative_verb_lemma': rel_verb_token.lemma_ if rel_verb_token else rel_verb,
                'main_clause_start': main_clause_start,
                'doc': doc,
                'modifiers': modifiers,  # å”åŠ›è€…ã‹ã‚‰ã®ä¿®é£¾èªæƒ…å ±
                'structure_analysis': structure_analysis,  # å”åŠ›è€…ã‹ã‚‰ã®5æ–‡å‹åˆ†æ
                'passive_analysis': passive_analysis  # å”åŠ›è€…ã‹ã‚‰ã®å—å‹•æ…‹åˆ†æ
            }
            
            return result
            
        except Exception as e:
            return {'success': False, 'error': f'spaCyè§£æã‚¨ãƒ©ãƒ¼: {str(e)}'}
    
    def _apply_spacy_fallback(self, doc, text: str):
        """
        spaCyèª¤åˆ¤å®šå¯¾å‡¦æ³•ï¼ˆè¨­è¨ˆä»•æ§˜æ›¸æº–æ‹ ï¼‰
        
        è¨­è¨ˆä»•æ§˜æ›¸ä¾‹2:
        â†’ spaCyèª¤åˆ¤å®š: livesã‚’åè©lifeè¤‡æ•°å½¢ã¨åˆ¤å®š
        â†’ ã‚·ã‚¹ãƒ†ãƒ è­¦æˆ’: æ›–æ˜§èªå¥ã¨ã—ã¦2é¸æŠè‚¢ã‚’æº–å‚™
        â†’ ç¬¬1å€™è£œ: lives_åè© â†’ ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã‚¼ãƒ­ã§æ–‡æ³•ç ´ç¶»
        â†’ ç¬¬2å€™è£œ: lives_å‹•è© â†’ redã§é–¢ä¿‚ç¯€çµ‚äº† â†’ lives_V, here_M2ã§æ–‡æˆç«‹
        â†’ åˆ¤æ–­: ç¬¬2å€™è£œãŒæ­£ã—ã„
        """
        # æ›–æ˜§èªå¥ã®æ¤œå‡ºï¼ˆè¤‡æ•°ã®å“è©è§£é‡ˆãŒå¯èƒ½ãªèªï¼‰
        ambiguous_words = []
        
        for i, token in enumerate(doc):
            # lives ã®ç‰¹åˆ¥å‡¦ç†
            if token.text.lower() == 'lives':
                if token.pos_ == 'NOUN' and token.tag_ == 'NNS':
                    print(f"ğŸš¨ spaCyèª¤åˆ¤å®šæ¤œå‡º: '{token.text}' ã‚’ {token.pos_} ã¨ã—ã¦åˆ¤å®š")
                    ambiguous_words.append({
                        'index': i,
                        'text': token.text,
                        'original_pos': token.pos_,
                        'alternative_pos': 'VERB',
                        'alternative_tag': 'VBZ',
                        'reason': 'è¨­è¨ˆä»•æ§˜æ›¸ä¾‹2å¯¾å¿œ'
                    })
        
        # ä»£æ›¿è§£é‡ˆã®æ¤œè¨¼
        if ambiguous_words:
            for ambiguous in ambiguous_words:
                if self._validate_alternative_interpretation(doc, ambiguous, text):
                    print(f"âœ… ä»£æ›¿è§£é‡ˆæ¡ç”¨: '{ambiguous['text']}' â†’ {ambiguous['alternative_pos']}")
                    # å®Ÿéš›ã®ä¿®æ­£ã¯æ§‹é€ è§£æã§åæ˜ 
                    return self._create_corrected_doc(doc, ambiguous, text)
        
        return doc
    
    def _validate_alternative_interpretation(self, doc, ambiguous: dict, text: str) -> bool:
        """
        ä»£æ›¿è§£é‡ˆã®æ–‡æ³•çš„å¦¥å½“æ€§æ¤œè¨¼
        
        è¨­è¨ˆä»•æ§˜æ›¸: "ç¬¬1å€™è£œ: lives_åè© â†’ ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã‚¼ãƒ­ã§æ–‡æ³•ç ´ç¶»"
                   "ç¬¬2å€™è£œ: lives_å‹•è© â†’ redã§é–¢ä¿‚ç¯€çµ‚äº† â†’ lives_V, here_M2ã§æ–‡æˆç«‹"
        """
        word_idx = ambiguous['index']
        
        # lives ã‚’å‹•è©ã¨ã—ã¦è§£é‡ˆã—ãŸå ´åˆã®æ–‡æ³•ãƒã‚§ãƒƒã‚¯
        if ambiguous['text'].lower() == 'lives' and ambiguous['alternative_pos'] == 'VERB':
            # hereãŒä¿®é£¾èªã¨ã—ã¦é©åˆ‡ã«é…ç½®ã§ãã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            next_tokens = [token.text for token in doc[word_idx+1:]]
            if 'here' in next_tokens:
                print(f"ğŸ” æ–‡æ³•æ¤œè¨¼: lives_VERB + here_M2 ã§æ–‡æ§‹é€ æˆç«‹")
                return True
        
        return False
    
    def _create_corrected_doc(self, doc, ambiguous: dict, text: str):
        """ä¿®æ­£ã•ã‚ŒãŸæ–‡æ§‹é€ ã‚’åæ˜ ã—ãŸdocã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ"""
        # ç¾åœ¨ã®å®Ÿè£…ã§ã¯å…ƒã®docã‚’ãã®ã¾ã¾è¿”ã—ã€
        # æ§‹é€ è§£ææ™‚ã«ä¿®æ­£ã‚’é©ç”¨
        return doc
    
    def _process_which(self, text: str) -> Dict[str, Any]:
        """whiché–¢ä¿‚ç¯€å‡¦ç†ï¼ˆå”åŠ›ã‚¢ãƒ—ãƒ­ãƒ¼ãƒç‰ˆï¼‰"""
        
        # spaCyæ–‡è„ˆè§£æã§é–¢ä¿‚ç¯€ã‚’åˆ†æï¼ˆå”åŠ›è€…æƒ…å ±ã‚’å«ã‚€ï¼‰
        analysis = self._analyze_relative_clause(text, 'which')
        if not analysis['success']:
            return analysis
        
        antecedent = analysis['antecedent']
        rel_verb = analysis['relative_verb']
        
        # ä¿®é£¾èªæƒ…å ±ï¼ˆå”åŠ›è€… AdverbHandler ã®çµæœã‚’æ´»ç”¨ï¼‰
        modifiers_info = analysis.get('modifiers', {})
        sub_m2 = ""
        sub_m3 = ""
        
        # å”åŠ›è€…ã‹ã‚‰ä¿®é£¾èªæƒ…å ±ã‚’å–å¾—ï¼ˆM2, M3å¯¾å¿œï¼‰
        if modifiers_info and 'M2' in modifiers_info:
            sub_m2 = modifiers_info['M2']
        if modifiers_info and 'M3' in modifiers_info:
            sub_m3 = modifiers_info['M3']
        
        # whichã¯ä¸»èªãƒ»ç›®çš„èªã‚’æ–‡è„ˆã§åˆ¤å®š
        doc = analysis['doc']  # _analyze_relative_clauseã‹ã‚‰å–å¾—
        which_idx = None
        for i, token in enumerate(doc):
            if token.text.lower() == 'which':
                which_idx = i
                break
        
        is_subject = True
        if which_idx is not None and which_idx + 1 < len(doc):
            next_token = doc[which_idx + 1]
            if next_token.pos_ in ['PRON', 'NOUN', 'PROPN']:
                is_subject = False  # which + åè© = ç›®çš„æ ¼
        
        # å—å‹•æ…‹æƒ…å ±ï¼ˆå”åŠ›è€… PassiveVoiceHandler ã®çµæœã‚’æ´»ç”¨ï¼‰
        passive_info = analysis.get('passive_analysis', {})
        is_passive = passive_info.get('is_passive', False) if passive_info else False
        
        # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ§‹ç¯‰ï¼ˆå—å‹•æ…‹è€ƒæ…®ï¼‰
        if is_subject:
            if is_passive and passive_info:
                # å—å‹•æ…‹ã®å ´åˆ: Aux + V ã«åˆ†é›¢
                sub_slots = {
                    'sub-s': f"{antecedent} which",
                    'sub-aux': passive_info.get('aux', ''),  # beå‹•è©
                    'sub-v': passive_info.get('verb', ''),   # éå»åˆ†è©
                    '_parent_slot': 'S'
                }
            else:
                # é€šå¸¸ã®å ´åˆ
                sub_slots = {
                    'sub-s': f"{antecedent} which",
                    'sub-v': rel_verb,
                    '_parent_slot': 'S'
                }
        else:
            # ç›®çš„æ ¼whichã®å ´åˆã€é–¢ä¿‚ç¯€å†…ã®ä¸»èªã‚’ç‰¹å®š
            rel_subject = ""
            if which_idx is not None:
                for i in range(which_idx + 1, len(doc)):
                    if doc[i].pos_ in ['PRON', 'NOUN', 'PROPN'] and doc[i].text != rel_verb:
                        rel_subject = doc[i].text
                        break
            
            if is_passive:
                # å—å‹•æ…‹ã®å ´åˆ: Aux + V ã«åˆ†é›¢
                sub_slots = {
                    'sub-o1': f"{antecedent} which",
                    'sub-s': rel_subject,
                    'sub-aux': passive_info.get('aux', ''),  # beå‹•è©
                    'sub-v': passive_info.get('verb', ''),   # éå»åˆ†è©
                    '_parent_slot': 'S'
                }
            else:
                # é€šå¸¸ã®å ´åˆ
                sub_slots = {
                    'sub-o1': f"{antecedent} which",
                    'sub-s': rel_subject,
                    'sub-v': rel_verb,
                    '_parent_slot': 'S'
                }
        
        # ä¿®é£¾èªãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
        if sub_m2:
            sub_slots['sub-m2'] = sub_m2
        if sub_m3:
            sub_slots['sub-m3'] = sub_m3
        
        # ä¸»ç¯€ã‚’æ§‹ç¯‰
        main_clause_start = analysis.get('main_clause_start')
        main_clause = ""
        if main_clause_start is not None:
            doc = analysis['doc']
            main_tokens = [token.text for token in doc[main_clause_start:]]
            main_clause = " ".join(main_tokens)

        return {
            'success': True,
            'main_slots': {'S': ''},
            'sub_slots': sub_slots,
            'pattern_type': 'which_object' if not is_subject else 'which_subject',
            'relative_pronoun': 'which',
            'antecedent': antecedent,
            'main_continuation': main_clause.strip(),
            'spacy_analysis': {
                'relative_verb_pos': analysis['relative_verb_pos'],
                'relative_verb_lemma': analysis['relative_verb_lemma']
            }
        }
    
    def _process_that(self, text: str) -> Dict[str, Any]:
        """thaté–¢ä¿‚ç¯€å‡¦ç†ï¼ˆspaCyæ–‡è„ˆè§£æãƒ™ãƒ¼ã‚¹ï¼‰"""
        
        # spaCyæ–‡è„ˆè§£æã§é–¢ä¿‚ç¯€ã‚’åˆ†æ
        analysis = self._analyze_relative_clause(text, 'that')
        if not analysis or not analysis.get('success'):
            print(f"âš ï¸ _analyze_relative_clauseå¤±æ•—: {analysis}")
            return analysis if analysis else {'success': False, 'error': 'analysis is None'}
        
        # æ›–æ˜§èªå¥ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰å‡¦ç†
        if hasattr(self, '_verb_override') and self._verb_override:
            print(f"ğŸ”§ æ›–æ˜§èªå¥ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰é©ç”¨: {self._verb_override}")
            analysis = self._apply_verb_override_to_analysis(analysis, text)
        
        doc = analysis['doc']
        antecedent = analysis['antecedent']
        rel_verb = analysis['relative_verb']
        main_clause_start = analysis['main_clause_start']
        
        # é–¢ä¿‚ç¯€ã®ä¿®é£¾èªã‚’ç‰¹å®š
        rel_verb_idx = None
        for i, token in enumerate(doc):
            if token.text == rel_verb:
                rel_verb_idx = i
                break
        
        # é–¢ä¿‚ç¯€éƒ¨åˆ†ã®å®Œå…¨ãªå‹•è©å¥ã‚’æ§‹ç¯‰
        rel_verb_phrase = rel_verb
        
        # ä¿®é£¾èªæƒ…å ±ï¼ˆå”åŠ›è€… AdverbHandler ã®çµæœã‚’æ´»ç”¨ï¼‰
        modifiers_info = analysis.get('modifiers', {})
        print(f"ğŸ” DEBUG _process_that: å—ä¿¡ã—ãŸmodifiers_info = {modifiers_info}")
        sub_m2 = ""
        sub_m3 = ""
        
        # å”åŠ›è€…ã‹ã‚‰ä¿®é£¾èªæƒ…å ±ã‚’å–å¾—ï¼ˆM2, M3å¯¾å¿œï¼‰
        if modifiers_info and 'M2' in modifiers_info:
            sub_m2 = modifiers_info['M2']
            print(f"ğŸ” DEBUG _process_that: sub_m2è¨­å®š = {sub_m2}")
        if modifiers_info and 'M3' in modifiers_info:
            sub_m3 = modifiers_info['M3']
            print(f"ğŸ” DEBUG _process_that: sub_m3è¨­å®š = {sub_m3}")
        
        # ä¸»ç¯€ã‚’æ§‹ç¯‰
        main_clause = ""
        if main_clause_start is not None:
            main_tokens = [token.text for token in doc[main_clause_start:]]
            main_clause = " ".join(main_tokens)
        
        # thatã¯ä¸»èªãƒ»ç›®çš„èªã‚’æ–‡è„ˆã§åˆ¤å®š
        # ç°¡ç•¥åˆ¤å®šï¼šthatç›´å¾Œã«å‹•è©ãŒã‚ã‚Œã°ä¸»èªã€åè©ãŒã‚ã‚Œã°ç›®çš„èª
        is_subject = True
        that_idx = None
        for i, token in enumerate(doc):
            if token.text.lower() == 'that':
                that_idx = i
                break
        
        if that_idx is not None and that_idx + 1 < len(doc):
            next_token = doc[that_idx + 1]
            if next_token.pos_ in ['PRON', 'NOUN', 'PROPN']:
                is_subject = False  # that + åè© = ç›®çš„æ ¼
        
        # å—å‹•æ…‹æƒ…å ±ï¼ˆå”åŠ›è€… PassiveVoiceHandler ã®çµæœã‚’æ´»ç”¨ï¼‰
        passive_info = analysis.get('passive_analysis') or {}
        is_passive = passive_info.get('is_passive', False) if passive_info else False
        
        # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ§‹ç¯‰ï¼ˆå—å‹•æ…‹è€ƒæ…®ï¼‰
        if is_subject:
            if is_passive:
                # å—å‹•æ…‹ã®å ´åˆ: Aux + V ã«åˆ†é›¢
                sub_slots = {
                    'sub-s': f"{antecedent} that",
                    'sub-aux': passive_info.get('aux', ''),  # beå‹•è©
                    'sub-v': passive_info.get('verb', ''),   # éå»åˆ†è©
                    '_parent_slot': 'S'
                }
            else:
                # é€šå¸¸ã®å ´åˆ
                sub_slots = {
                    'sub-s': f"{antecedent} that",
                    'sub-v': rel_verb,
                    '_parent_slot': 'S'
                }
        else:
            # ç›®çš„æ ¼thatã®å ´åˆã€é–¢ä¿‚ç¯€å†…ã®ä¸»èªã‚’ç‰¹å®š
            rel_subject = ""
            if that_idx is not None:
                for i in range(that_idx + 1, len(doc)):
                    if doc[i].pos_ in ['PRON', 'NOUN', 'PROPN'] and doc[i].text != rel_verb:
                        rel_subject = doc[i].text
                        break
            
            if is_passive:
                # å—å‹•æ…‹ã®å ´åˆ: Aux + V ã«åˆ†é›¢
                sub_slots = {
                    'sub-o1': f"{antecedent} that",
                    'sub-s': rel_subject,
                    'sub-aux': passive_info.get('aux', ''),  # beå‹•è©
                    'sub-v': passive_info.get('verb', ''),   # éå»åˆ†è©
                    '_parent_slot': 'S'
                }
            else:
                # é€šå¸¸ã®å ´åˆ
                sub_slots = {
                    'sub-o1': f"{antecedent} that",
                    'sub-s': rel_subject,
                    'sub-v': rel_verb,
                    '_parent_slot': 'S'
                }
        
        # ä¿®é£¾èªãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
        if sub_m2:
            sub_slots['sub-m2'] = sub_m2
        if sub_m3:
            sub_slots['sub-m3'] = sub_m3
        
        return {
            'success': True,
            'main_slots': {'S': ''},
            'sub_slots': sub_slots,
            'pattern_type': 'that_subject' if is_subject else 'that_object',
            'relative_pronoun': 'that',
            'antecedent': antecedent,
            'main_continuation': main_clause.strip(),
            'spacy_analysis': {
                'relative_verb_pos': analysis['relative_verb_pos'],
                'relative_verb_lemma': analysis['relative_verb_lemma']
            }
        }

    def _process_whom(self, text: str) -> Dict[str, Any]:
        """whomé–¢ä¿‚ç¯€å‡¦ç†ï¼ˆspaCyæ–‡è„ˆè§£æãƒ™ãƒ¼ã‚¹ï¼‰"""
        
        # spaCyæ–‡è„ˆè§£æã§é–¢ä¿‚ç¯€ã‚’åˆ†æ
        analysis = self._analyze_relative_clause(text, 'whom')
        if not analysis['success']:
            return analysis
        
        doc = analysis['doc']
        antecedent = analysis['antecedent']
        rel_verb = analysis['relative_verb']
        main_clause_start = analysis['main_clause_start']
        
        # whomã¯ç›®çš„æ ¼ãªã®ã§ã€é–¢ä¿‚ç¯€å†…ã«ä¸»èªãŒå¿…è¦
        # "The man whom I met" -> I ãŒä¸»èªã€met ãŒå‹•è©
        rel_verb_idx = None
        for i, token in enumerate(doc):
            if token.text == rel_verb:
                rel_verb_idx = i
                break
        
        # é–¢ä¿‚ç¯€å†…ã®ä¸»èªã‚’ç‰¹å®š
        rel_subject = ""
        if rel_verb_idx is not None:
            for i in range(rel_verb_idx):
                if doc[i].text.lower() == 'whom':
                    # whomã®å¾Œã®æœ€åˆã®åè©/ä»£åè©ãŒä¸»èª
                    for j in range(i + 1, rel_verb_idx):
                        if doc[j].pos_ in ['PRON', 'NOUN', 'PROPN']:
                            rel_subject = doc[j].text
                            break
                    break
        
        # ä¸»ç¯€ã‚’æ§‹ç¯‰
        main_clause = ""
        if main_clause_start is not None:
            main_tokens = [token.text for token in doc[main_clause_start:]]
            main_clause = " ".join(main_tokens)
        
        return {
            'success': True,
            'main_slots': {'S': ''},
            'sub_slots': {
                'sub-o1': f"{antecedent} whom",  # whomã¯ç›®çš„æ ¼
                'sub-s': rel_subject,
                'sub-v': rel_verb,
                '_parent_slot': 'S'
            },
            'pattern_type': 'whom_object',
            'relative_pronoun': 'whom',
            'antecedent': antecedent,
            'main_continuation': main_clause.strip(),
            'spacy_analysis': {
                'relative_verb_pos': analysis['relative_verb_pos'],
                'relative_verb_lemma': analysis['relative_verb_lemma']
            }
        }

    def _process_whose(self, text: str) -> Dict[str, Any]:
        """whoseé–¢ä¿‚ç¯€å‡¦ç†ï¼ˆå”åŠ›ã‚¢ãƒ—ãƒ­ãƒ¼ãƒç‰ˆï¼‰"""
        
        print(f"ğŸ” whoseå‡¦ç†é–‹å§‹: '{text}'")
        
        # spaCyæ–‡è„ˆè§£æã§é–¢ä¿‚ç¯€ã‚’åˆ†æï¼ˆå”åŠ›è€…æƒ…å ±ã‚’å«ã‚€ï¼‰
        analysis = self._analyze_relative_clause(text, 'whose')
        if not analysis['success']:
            print(f"âš ï¸ whoseè§£æå¤±æ•—: {analysis}")
            return analysis
        
        # whoseã¯ç‰¹æ®Šãªã®ã§å°‚ç”¨è§£æã‚‚ä½µç”¨
        doc = self.nlp(text)
        whose_info = self._analyze_whose_structure(doc)
        if not whose_info['success']:
            print(f"âš ï¸ whoseè§£æå¤±æ•—: {whose_info}")
            return whose_info
        
        antecedent = whose_info['antecedent']
        rel_verb = whose_info['relative_verb']
        whose_noun = whose_info['whose_noun']
        main_verb_idx = whose_info['main_verb_idx']
        whose_idx = whose_info.get('whose_idx')  # whoseä½ç½®ã‚’å–å¾—
        
        # ä¸»ç¯€ã‚’æ§‹ç¯‰
        main_clause = ""
        if main_verb_idx is not None:
            main_tokens = [token.text for token in doc[main_verb_idx:]]
            main_clause = " ".join(main_tokens)
        
        # æ§‹é€ åˆ†æçµæœï¼ˆå”åŠ›è€… BasicFivePatternHandler ã®çµæœã‚’æ´»ç”¨ï¼‰
        structure_analysis = analysis.get('structure_analysis', {})
        structure_slots = structure_analysis.get('slots', {}) if structure_analysis else {}
        
        # ğŸ¯ whoseæ§‹é€ ã®æ–‡è„ˆåˆ¤å®šï¼ˆä¸»èªå‹ vs ç›®çš„èªå‹ï¼‰
        # whose + åè© + äººç§°ä»£åè©ï¼ˆI, you, he, she, etc.) â†’ ç›®çš„èªå‹
        # whose + åè© + å‹•è© â†’ ä¸»èªå‹
        doc = analysis['doc']
        whose_type = 'subject'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ä¸»èªå‹
        
        # whoseä½ç½®ã‹ã‚‰åˆ†æ
        if whose_idx is not None and whose_idx + 2 < len(doc):
            # whose + åè© + æ¬¡ã®èªã‚’ç¢ºèª
            next_after_noun = doc[whose_idx + 2]
            if next_after_noun.pos_ == 'PRON' and next_after_noun.text.lower() in ['i', 'you', 'he', 'she', 'we', 'they']:
                whose_type = 'object'  # ç›®çš„èªå‹
                print(f"ğŸ¯ whoseç›®çš„èªå‹æ¤œå‡º: {next_after_noun.text}")
        
        # ğŸ¯ whoseå‹ã«å¿œã˜ãŸã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ§‹ç¯‰
        # å—å‹•æ…‹æƒ…å ±ï¼ˆå”åŠ›è€… PassiveVoiceHandler ã®çµæœã‚’æ´»ç”¨ï¼‰
        passive_info = analysis.get('passive_analysis', {})
        is_passive = passive_info.get('is_passive', False) if passive_info else False
        
        if whose_type == 'object':
            # ç›®çš„èªå‹: whoseå¥ + ä¸»èª + å‹•è©
            rel_subject = ""
            for i in range(whose_idx + 2, len(doc)):
                token = doc[i]
                if token.pos_ == 'PRON' and token.text.lower() in ['i', 'you', 'he', 'she', 'we', 'they']:
                    rel_subject = token.text
                    print(f"ğŸ¯ é–¢ä¿‚ç¯€ä¸»èªæ¤œå‡º: '{rel_subject}'")
                    break
            
            if is_passive and passive_info:
                # å—å‹•æ…‹ã®å ´åˆ: Aux + V ã«åˆ†é›¢
                sub_slots = {
                    'sub-o1': f"{antecedent} whose {whose_noun}",  # ç›®çš„èª
                    'sub-s': rel_subject,  # ä¸»èª
                    'sub-aux': passive_info.get('aux', ''),  # beå‹•è©
                    'sub-v': passive_info.get('verb', ''),   # éå»åˆ†è©
                    '_parent_slot': 'S'
                }
            else:
                # é€šå¸¸ã®å ´åˆ
                sub_slots = {
                    'sub-o1': f"{antecedent} whose {whose_noun}",  # ç›®çš„èª
                    'sub-s': rel_subject,  # ä¸»èª
                    'sub-v': rel_verb,  # å‹•è©
                    '_parent_slot': 'S'
                }
        else:
            # ä¸»èªå‹: whoseå¥ãŒä¸»èª
            if is_passive and passive_info:
                # å—å‹•æ…‹ã®å ´åˆ: Aux + V ã«åˆ†é›¢
                sub_slots = {
                    'sub-s': f"{antecedent} whose {whose_noun}",  # ä¸»èª
                    'sub-aux': passive_info.get('aux', ''),  # beå‹•è©
                    'sub-v': passive_info.get('verb', ''),   # éå»åˆ†è©
                    '_parent_slot': 'S'
                }
            else:
                # é€šå¸¸ã®å ´åˆ
                sub_slots = {
                    'sub-s': f"{antecedent} whose {whose_noun}",  # ä¸»èª
                    'sub-v': rel_verb,  # å‹•è©
                '_parent_slot': 'S'
            }
        
        # ğŸ¯ å”åŠ›è€…ã‹ã‚‰ã®æ§‹é€ æƒ…å ±ã‚’çµ±åˆï¼ˆè£œèªãƒ»ç›®çš„èªãªã©ï¼‰
        if structure_slots:
            # C1ï¼ˆè£œèªï¼‰ãŒã‚ã‚‹å ´åˆ
            if 'C1' in structure_slots:
                sub_slots['sub-c1'] = structure_slots['C1']
                print(f"ğŸ¯ æ§‹é€ åˆ†æã‹ã‚‰è£œèªå–å¾—: sub-c1 = '{structure_slots['C1']}'")
            
            # O1ï¼ˆç›®çš„èªï¼‰ãŒã‚ã‚‹å ´åˆï¼ˆä¸»èªå‹ã®å ´åˆã®ã¿ï¼‰
            if whose_type == 'subject' and 'O1' in structure_slots:
                sub_slots['sub-o1'] = structure_slots['O1']
                print(f"ğŸ¯ æ§‹é€ åˆ†æã‹ã‚‰ç›®çš„èªå–å¾—: sub-o1 = '{structure_slots['O1']}'")
        
        # é–¢ä¿‚ç¯€å†…ã®å‰¯è©ä¿®é£¾èªã‚’æ¤œå‡ºï¼ˆwhoseç›®çš„èªå‹ã®å ´åˆã¯é™¤å¤–ï¼‰
        is_object_type = (whose_type == 'object')
        modifiers_info = analysis.get('modifiers', {})
        if modifiers_info and 'M2' in modifiers_info and not is_object_type:
            # whoseä¸»èªå‹ã®å ´åˆã®ã¿ä¿®é£¾èªã‚’è¿½åŠ 
            sub_slots['sub-m2'] = modifiers_info['M2']
            print(f"ğŸ¯ å”åŠ›è€…ã‹ã‚‰ä¿®é£¾èªå–å¾—: sub-m2 = '{modifiers_info['M2']}'")
        elif is_object_type and modifiers_info and 'M2' in modifiers_info:
            print(f"ğŸ¯ whoseç›®çš„èªå‹ã§ã¯ä¿®é£¾èªé™¤å¤–: '{modifiers_info['M2']}' ã‚’ç„¡è¦–")
        
        result = {
            'success': True,
            'main_slots': {'S': ''},
            'sub_slots': sub_slots,
            'pattern_type': 'whose_possessive',
            'relative_pronoun': 'whose',
            'antecedent': antecedent,
            'main_continuation': main_clause.strip(),
            'spacy_analysis': {
                'relative_verb_pos': 'VERB',
                'relative_verb_lemma': rel_verb
            }
        }
        print(f"ğŸ¯ whoseå‡¦ç†å®Œäº†: {result}")
        return result

    def _analyze_whose_structure(self, doc) -> Dict[str, Any]:
        """whoseæ§‹é€ å°‚ç”¨è§£æ"""
        try:
            print(f"ğŸ” whoseæ§‹é€ è§£æé–‹å§‹")
            # Step 1: whoseä½ç½®ã‚’ç‰¹å®š
            whose_idx = None
            for i, token in enumerate(doc):
                if token.text.lower() == 'whose':
                    whose_idx = i
                    break
            
            if whose_idx is None:
                print(f"âš ï¸ whoseãŒè¦‹ã¤ã‹ã‚‰ãªã„")
                return {'success': False, 'error': 'whose not found'}
            
            print(f"ğŸ¯ whoseä½ç½®: {whose_idx}")
            
            # Step 2: å…ˆè¡Œè©ã‚’ç‰¹å®šï¼ˆwhoseã‚ˆã‚Šå‰ï¼‰
            antecedent_tokens = []
            for i in range(whose_idx):
                antecedent_tokens.append(doc[i].text)
            antecedent = " ".join(antecedent_tokens).strip()
            print(f"ğŸ¯ å…ˆè¡Œè©: '{antecedent}'")
            
            # Step 3: whose + åè©ã‚’ç‰¹å®š
            whose_noun = ""
            if whose_idx + 1 < len(doc):
                whose_noun = doc[whose_idx + 1].text
            print(f"ğŸ¯ whoseåè©: '{whose_noun}'")
            
            # Step 4: é–¢ä¿‚ç¯€å†…ã®å‹•è©ã‚’ç‰¹å®šï¼ˆwhose + åè©ã®å¾Œã®æœ€åˆã®å‹•è©ï¼‰
            rel_verb = ""
            rel_verb_idx = None
            for i in range(whose_idx + 2, len(doc)):
                token = doc[i]
                if token.pos_ in ['VERB', 'AUX']:
                    rel_verb = token.text
                    rel_verb_idx = i
                    print(f"ğŸ¯ é–¢ä¿‚ç¯€å‹•è©: '{rel_verb}' at {i}")
                    break
            
            # Step 5: ä¸»ç¯€å‹•è©ã‚’ç‰¹å®šï¼ˆé–¢ä¿‚ç¯€å¾Œã®æœ€åˆã®å‹•è©ï¼‰
            main_verb_idx = None
            if rel_verb_idx is not None:
                # é–¢ä¿‚ç¯€å‹•è©ã®å¾Œã‹ã‚‰ä¸»ç¯€å‹•è©ã‚’æ¢ã™
                for i in range(rel_verb_idx + 1, len(doc)):
                    token = doc[i]
                    
                    # é€šå¸¸ã®å‹•è©æ¤œå‡º
                    if token.pos_ in ['VERB', 'AUX'] and token.dep_ != 'relcl':
                        main_verb_idx = i
                        print(f"ğŸ¯ ä¸»ç¯€å‹•è©: '{token.text}' at {i}")
                        break
                    
                    # spaCyèª¤åˆ¤å®šä¿®æ­£: å‹•è©çš„ãªå˜èªãŒåè©ã¨ã—ã¦åˆ¤å®šã•ã‚Œã‚‹å ´åˆ
                    if token.pos_ == 'NOUN' and token.text.lower() in ['lives', 'works', 'runs', 'goes', 'comes', 'stays']:
                        main_verb_idx = i
                        print(f"ğŸ¯ ä¸»ç¯€å‹•è©(ä¿®æ­£): '{token.text}' at {i}")
                        break
                    
                    # å½¢å®¹è©ã®å¾Œã«ç¶šãèªå¥ã§å‹•è©ã‚’æ¢ã™
                    if i > 0 and doc[i-1].pos_ == 'ADJ' and token.pos_ in ['NOUN'] and token.text.lower() in ['lives', 'works']:
                        main_verb_idx = i
                        print(f"ğŸ¯ ä¸»ç¯€å‹•è©(ADJå¾Œ): '{token.text}' at {i}")
                        break
            
            result = {
                'success': True,
                'antecedent': antecedent,
                'whose_noun': whose_noun,
                'relative_verb': rel_verb,
                'main_verb_idx': main_verb_idx,
                'whose_idx': whose_idx  # whoseä½ç½®ã‚’è¿½åŠ 
            }
            print(f"ğŸ¯ whoseè§£æçµæœ: {result}")
            return result
            
        except Exception as e:
            print(f"âŒ whoseè§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'success': False, 'error': f'whoseè§£æã‚¨ãƒ©ãƒ¼: {str(e)}'}

    def _process_relative_adverb(self, text: str, relative_adverb: str) -> Dict[str, Any]:
        """é–¢ä¿‚å‰¯è©å‡¦ç†ï¼ˆwhere, when, why, howï¼‰"""
        
        # spaCyæ–‡è„ˆè§£æã§é–¢ä¿‚ç¯€ã‚’åˆ†æ
        analysis = self._analyze_relative_clause(text, relative_adverb)
        if not analysis['success']:
            return analysis
        
        antecedent = analysis['antecedent']
        rel_verb = analysis['relative_verb']
        
        # ä¿®é£¾èªæƒ…å ±ï¼ˆå”åŠ›è€… AdverbHandler ã®çµæœã‚’æ´»ç”¨ï¼‰
        modifiers_info = analysis.get('modifiers', {})
        sub_m2 = ""
        
        # å”åŠ›è€…ã‹ã‚‰ä¿®é£¾èªæƒ…å ±ã‚’å–å¾—
        if modifiers_info and 'M2' in modifiers_info:
            sub_m2 = modifiers_info['M2']
        
        # ä¸»ç¯€ã‚’æ§‹ç¯‰
        main_clause_start = analysis.get('main_clause_start')
        main_clause = ""
        if main_clause_start is not None:
            doc = analysis['doc']
            main_tokens = [token.text for token in doc[main_clause_start:]]
            main_clause = " ".join(main_tokens)
        
        # é–¢ä¿‚å‰¯è©ã«å¿œã˜ãŸã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ§‹ç¯‰
        if relative_adverb in ['where']:
            # where: å ´æ‰€ã®ä¿®é£¾
            sub_slots = {
                'sub-m2': f"{antecedent} {relative_adverb}",  # å ´æ‰€ä¿®é£¾ã¨ã—ã¦
                'sub-s': self._extract_relative_subject(analysis, relative_adverb),
                'sub-v': rel_verb,
                '_parent_slot': 'S'
            }
        elif relative_adverb in ['when']:
            # when: æ™‚é–“ã®ä¿®é£¾
            sub_slots = {
                'sub-m1': f"{antecedent} {relative_adverb}",  # æ™‚é–“ä¿®é£¾ã¨ã—ã¦
                'sub-s': self._extract_relative_subject(analysis, relative_adverb),
                'sub-v': rel_verb,
                '_parent_slot': 'S'
            }
        elif relative_adverb in ['why']:
            # why: ç†ç”±ã®ä¿®é£¾
            sub_slots = {
                'sub-m3': f"{antecedent} {relative_adverb}",  # ç†ç”±ä¿®é£¾ã¨ã—ã¦
                'sub-s': self._extract_relative_subject(analysis, relative_adverb),
                'sub-v': rel_verb,
                '_parent_slot': 'S'
            }
        elif relative_adverb in ['how']:
            # how: æ–¹æ³•ã®ä¿®é£¾
            sub_slots = {
                'sub-m2': f"{antecedent} {relative_adverb}",  # æ–¹æ³•ä¿®é£¾ã¨ã—ã¦
                'sub-s': self._extract_relative_subject(analysis, relative_adverb),
                'sub-v': rel_verb,
                '_parent_slot': 'S'
            }
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            sub_slots = {
                'sub-m2': f"{antecedent} {relative_adverb}",
                'sub-s': self._extract_relative_subject(analysis, relative_adverb),
                'sub-v': rel_verb,
                '_parent_slot': 'S'
            }
        
        # ä¿®é£¾èªãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
        if sub_m2 and 'sub-m2' not in sub_slots:
            sub_slots['sub-m2'] = sub_m2
        
        return {
            'success': True,
            'main_slots': {'S': ''},
            'sub_slots': sub_slots,
            'pattern_type': f'{relative_adverb}_adverb',
            'relative_pronoun': relative_adverb,
            'antecedent': antecedent,
            'main_continuation': main_clause.strip(),
            'spacy_analysis': {
                'relative_verb_pos': analysis['relative_verb_pos'],
                'relative_verb_lemma': analysis['relative_verb_lemma']
            }
        }

    def _extract_relative_subject(self, analysis: Dict, relative_adverb: str) -> str:
        """é–¢ä¿‚å‰¯è©ç¯€å†…ã®ä¸»èªã‚’æŠ½å‡º"""
        doc = analysis['doc']
        
        # é–¢ä¿‚å‰¯è©ã®ä½ç½®ã‚’ç‰¹å®š
        adverb_idx = None
        for i, token in enumerate(doc):
            if token.text.lower() == relative_adverb.lower():
                adverb_idx = i
                break
        
        if adverb_idx is None:
            return ""
        
        # é–¢ä¿‚å‰¯è©ç›´å¾Œã‹ã‚‰ä¸»èªã‚’æ¢ã™
        for i in range(adverb_idx + 1, len(doc)):
            token = doc[i]
            if token.dep_ == 'ROOT':
                break
            if token.pos_ in ['NOUN', 'PRON', 'PROPN']:
                return token.text
        
        return ""
