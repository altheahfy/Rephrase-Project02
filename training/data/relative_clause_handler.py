#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RelativeClauseHandler: Phase 2 é–¢ä¿‚ç¯€å‡¦ç†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
spaCyå“è©åˆ¤å®šãƒ™ãƒ¼ã‚¹ï¼ˆãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç¦æ­¢ï¼‰
Legacy ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å‚è€ƒã«ã—ãŸæ­£è¦è¡¨ç¾ + spaCy POSåˆ¤å®š
"""

import re
import spacy
from typing import Dict, Any, Tuple

class RelativeClauseHandler:
    """é–¢ä¿‚ç¯€å‡¦ç†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆå”åŠ›ã‚¢ãƒ—ãƒ­ãƒ¼ãƒç‰ˆï¼‰"""
    
    def __init__(self, collaborators=None):
        """
        åˆæœŸåŒ–
        
        Args:
            collaborators: å”åŠ›è€…ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¾æ›¸
                - 'adverb': AdverbHandlerï¼ˆä¿®é£¾èªåˆ†é›¢ï¼‰
                - 'five_pattern': BasicFivePatternHandlerï¼ˆ5æ–‡å‹åˆ†æï¼‰
                - 'passive': PassiveVoiceHandlerï¼ˆå—å‹•æ…‹ç†è§£ï¼‰
        """
        self.name = "RelativeClauseHandler"
        self.version = "cooperation_v1.0"
        self.nlp = spacy.load('en_core_web_sm')  # spaCyå“è©åˆ¤å®šç”¨
        
        # å”åŠ›è€…ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŸã¡ï¼ˆDependency Injectionï¼‰
        if collaborators:
            self.adverb_handler = collaborators.get('adverb') or collaborators.get('AdverbHandler')
            self.five_pattern_handler = collaborators.get('five_pattern') or collaborators.get('FivePatternHandler')
            self.passive_handler = collaborators.get('passive') or collaborators.get('PassiveHandler')
        else:
            self.adverb_handler = None
            self.five_pattern_handler = None
            self.passive_handler = None
    
    def process(self, text: str, original_text: str = None) -> Dict[str, Any]:
        """
        é–¢ä¿‚ç¯€å‡¦ç†ãƒ¡ã‚¤ãƒ³ï¼ˆå”åŠ›ã‚¢ãƒ—ãƒ­ãƒ¼ãƒç‰ˆï¼‰
        
        Args:
            text: å‡¦ç†å¯¾è±¡ã®è‹±èªæ–‡ï¼ˆä¿®é£¾èªåˆ†é›¢æ¸ˆã¿å¯èƒ½æ€§ã‚ã‚Šï¼‰
            original_text: ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¿®é£¾èªæƒ…å ±ä¿æŒç”¨ï¼‰
            
        Returns:
            Dict: å‡¦ç†çµæœ
        """
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ†ã‚­ã‚¹ãƒˆã®æ±ºå®š
        self.original_text = original_text if original_text else text
        
        try:
            print(f"ğŸ” é–¢ä¿‚ç¯€å‡¦ç†é–‹å§‹: '{text}'")
            # åŸºæœ¬çš„ãªé–¢ä¿‚ä»£åè©æ¤œå‡ºï¼ˆå„ªå…ˆé †ä½é †ï¼‰
            if ' whose ' in text.lower():
                print(f"ğŸ¯ whoseæ¤œå‡º")
                return self._process_whose(text)
            elif ' whom ' in text.lower():
                return self._process_whom(text)
            elif ' who ' in text.lower():
                return self._process_who(text)
            elif ' which ' in text.lower():
                return self._process_which(text)
            elif ' that ' in text.lower():
                return self._process_that(text)
            elif ' where ' in text.lower():
                return self._process_relative_adverb(text, 'where')
            elif ' when ' in text.lower():
                return self._process_relative_adverb(text, 'when')
            elif ' why ' in text.lower():
                return self._process_relative_adverb(text, 'why')
            elif ' how ' in text.lower():
                return self._process_relative_adverb(text, 'how')
            else:
                print(f"âš ï¸ é–¢ä¿‚ç¯€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: '{text}'")
                return {'success': False, 'error': 'é–¢ä¿‚ç¯€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ'}
                
        except Exception as e:
            return {'success': False, 'error': f'å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}'}
    
    def _process_who(self, text: str) -> Dict[str, Any]:
        """whoé–¢ä¿‚ç¯€å‡¦ç†ï¼ˆå”åŠ›ã‚¢ãƒ—ãƒ­ãƒ¼ãƒç‰ˆï¼‰"""
        
        # spaCyæ–‡è„ˆè§£æã§é–¢ä¿‚ç¯€ã‚’åˆ†æï¼ˆå”åŠ›è€…æƒ…å ±ã‚’å«ã‚€ï¼‰
        analysis = self._analyze_relative_clause(text, 'who')
        if not analysis['success']:
            return analysis
        
        antecedent = analysis['antecedent']
        rel_verb = analysis['relative_verb']
        
        # ä¿®é£¾èªæƒ…å ±ï¼ˆå”åŠ›è€… AdverbHandler ã®çµæœã‚’æ´»ç”¨ï¼‰
        modifiers_info = analysis.get('modifiers', {})
        sub_m2 = ""
        
        # å”åŠ›è€…ã‹ã‚‰ä¿®é£¾èªæƒ…å ±ã‚’å–å¾—
        if modifiers_info and 'M2' in modifiers_info:
            sub_m2 = modifiers_info['M2']
        
        # å—å‹•æ…‹æƒ…å ±ï¼ˆå”åŠ›è€… PassiveVoiceHandler ã®çµæœã‚’æ´»ç”¨ï¼‰
        passive_info = analysis.get('passive_analysis', {})
        is_passive = passive_info.get('is_passive', False)
        
        # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ§‹ç¯‰ï¼ˆå—å‹•æ…‹è€ƒæ…®ï¼‰
        if is_passive:
            # å—å‹•æ…‹ã®å ´åˆ: Aux + V ã«åˆ†é›¢
            sub_slots = {
                'sub-s': f"{antecedent} who",
                'sub-aux': passive_info.get('aux', ''),  # beå‹•è©
                'sub-v': passive_info.get('verb', ''),   # éå»åˆ†è©
                '_parent_slot': 'S'
            }
        else:
            # é€šå¸¸ã®å ´åˆ
            sub_slots = {
                'sub-s': f"{antecedent} who",
                'sub-v': rel_verb,  # å‹•è©ã®ã¿
                '_parent_slot': 'S'  # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
            }
        
        # ä¿®é£¾èªãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
        if sub_m2:
            sub_slots['sub-m2'] = sub_m2
        
        # ä¸»ç¯€ã‚’æ§‹ç¯‰
        main_clause_start = analysis.get('main_clause_start')
        main_clause = ""
        if main_clause_start is not None:
            doc = analysis['doc']
            main_tokens = [token.text for token in doc[main_clause_start:]]
            main_clause = " ".join(main_tokens)
        
        return {
            'success': True,
            'main_slots': {'S': ''},  # è¨­è¨ˆä»•æ§˜æ›¸æº–æ‹ : ä¸»èªã‚¹ãƒ­ãƒƒãƒˆç©ºæ–‡å­—åˆ—
            'sub_slots': sub_slots,
            'pattern_type': 'who_subject',
            'relative_pronoun': 'who',
            'antecedent': antecedent,
            'main_continuation': main_clause.strip(),
            'spacy_analysis': {
                'relative_verb_pos': analysis['relative_verb_pos'],
                'relative_verb_lemma': analysis['relative_verb_lemma']
            },
            'cooperation_details': {
                'passive_analysis': passive_info,
                'modifiers_analysis': modifiers_info
            }
        }
    
    def _extract_relative_clause_text_original(self, text: str, relative_pronoun: str) -> str:
        """ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é–¢ä¿‚ç¯€éƒ¨åˆ†ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆä¿®é£¾èªè¾¼ã¿ï¼‰"""
        try:
            doc = self.nlp(text)
            
            rel_start = None
            rel_end = len(doc)
            
            # Step 1: é–¢ä¿‚ä»£åè©ã®ä½ç½®ã‚’ç‰¹å®š
            for i, token in enumerate(doc):
                if token.text.lower() == relative_pronoun.lower():
                    rel_start = i
                    break
            
            if rel_start is None:
                return text
            
            # Step 2: æ–‡å…¨ä½“ã®ãƒ¡ã‚¤ãƒ³å‹•è©ï¼ˆçœŸã®ROOTï¼‰ã‚’ç‰¹å®š
            main_root_idx = None
            for i, token in enumerate(doc):
                if token.dep_ == 'ROOT':
                    main_root_idx = i
                    break
            
            # Step 3: é–¢ä¿‚ç¯€ã®çµ‚äº†ä½ç½®ã‚’æ±ºå®š
            # - é–¢ä¿‚ä»£åè©ä»¥é™ã§ä¸»ç¯€å‹•è©ã‚ˆã‚Šå‰ã¾ã§
            if main_root_idx is not None and main_root_idx > rel_start:
                rel_end = main_root_idx
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å“è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã§åˆ¤å®š
                for i in range(rel_start + 1, len(doc)):
                    token = doc[i]
                    # ä¸»èªçš„èªå¥ï¼ˆåè©ï¼‹å‹•è©ï¼‰ã«é­é‡ã—ãŸã‚‰é–¢ä¿‚ç¯€çµ‚äº†
                    if (token.pos_ in ['NOUN', 'PROPN'] and 
                        i + 1 < len(doc) and 
                        doc[i + 1].pos_ in ['VERB', 'AUX']):
                        rel_end = i
                        break
            
            # Step 4: é–¢ä¿‚ç¯€ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
            if rel_start is not None:
                clause_tokens = doc[rel_start:rel_end]
                extracted = ' '.join([t.text for t in clause_tokens])
                return extracted
            
            return text
            
        except Exception as e:
            return text

    def _extract_relative_clause_text(self, text: str, relative_pronoun: str) -> str:
        """é–¢ä¿‚ç¯€éƒ¨åˆ†ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆä¿®é£¾èªè¾¼ã¿ï¼‰"""
        try:
            doc = self.nlp(text)
            
            rel_start = None
            rel_end = len(doc)
            
            # Step 1: é–¢ä¿‚ä»£åè©ã®ä½ç½®ã‚’ç‰¹å®š
            for i, token in enumerate(doc):
                if token.text.lower() == relative_pronoun.lower():
                    rel_start = i
                    break
            
            if rel_start is None:
                return text
            
            # Step 2: æ–‡å…¨ä½“ã®ãƒ¡ã‚¤ãƒ³å‹•è©ï¼ˆçœŸã®ROOTï¼‰ã‚’ç‰¹å®š
            main_root_idx = None
            for i, token in enumerate(doc):
                if token.dep_ == 'ROOT':
                    main_root_idx = i
                    break
            
            # Step 3: é–¢ä¿‚ç¯€ã®çµ‚äº†ä½ç½®ã‚’æ±ºå®š
            # - é–¢ä¿‚ä»£åè©ä»¥é™ã§ä¸»ç¯€å‹•è©ã‚ˆã‚Šå‰ã¾ã§
            if main_root_idx is not None and main_root_idx > rel_start:
                rel_end = main_root_idx
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å“è©ãƒ‘ã‚¿ãƒ¼ãƒ³ã§åˆ¤å®š
                for i in range(rel_start + 1, len(doc)):
                    token = doc[i]
                    # ä¸»èªçš„èªå¥ï¼ˆåè©ï¼‹å‹•è©ï¼‰ã«é­é‡ã—ãŸã‚‰é–¢ä¿‚ç¯€çµ‚äº†
                    if (token.pos_ in ['NOUN', 'PROPN'] and 
                        i + 1 < len(doc) and 
                        doc[i + 1].pos_ in ['VERB', 'AUX']):
                        rel_end = i
                        break
            
            # Step 4: é–¢ä¿‚ç¯€ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
            if rel_start is not None:
                clause_tokens = doc[rel_start:rel_end]
                extracted = ' '.join([t.text for t in clause_tokens])
                print(f"[DEBUG] é–¢ä¿‚ç¯€æŠ½å‡º: '{text}' â†’ '{extracted}'")
                return extracted
            
            return text
            
        except Exception as e:
            print(f"[DEBUG] é–¢ä¿‚ç¯€æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return text
            return text

    def _analyze_relative_clause(self, text: str, relative_pronoun: str) -> Dict[str, Any]:
        """spaCyæ–‡è„ˆè§£æã«ã‚ˆã‚‹é–¢ä¿‚ç¯€åˆ†æï¼ˆå”åŠ›ã‚¢ãƒ—ãƒ­ãƒ¼ãƒç‰ˆï¼‰"""
        try:
            # Step 1: ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é–¢ä¿‚ç¯€éƒ¨åˆ†ã‚’æŠ½å‡ºï¼ˆä¿®é£¾èªè¾¼ã¿ï¼‰
            original_clause_text = self._extract_relative_clause_text_original(
                getattr(self, 'original_text', text), relative_pronoun
            )
            
            # Step 2: å”åŠ›è€…ï¼ˆå‰¯è©ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼‰ã¨é€£æºï¼šä¿®é£¾èªåˆ†é›¢
            cleaned_clause = original_clause_text
            modifiers = {}
            
            if self.adverb_handler and original_clause_text:
                adverb_result = self.adverb_handler.process(original_clause_text)
                
                if adverb_result.get('success'):
                    cleaned_clause = adverb_result.get('separated_text', original_clause_text)
                    
                    # AdverbHandlerã®çµæœã‚’5æ–‡å‹ã‚·ã‚¹ãƒ†ãƒ å½¢å¼ã«å¤‰æ›
                    raw_modifiers = adverb_result.get('modifiers', {})
                    
                    if raw_modifiers:
                        # ä½ç½®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚­ãƒ¼ã‹ã‚‰ä¿®é£¾èªãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦M2ã«çµ±åˆ
                        modifier_texts = []
                        for pos_idx, modifier_list in raw_modifiers.items():
                            if isinstance(modifier_list, list):
                                for modifier_info in modifier_list:
                                    if isinstance(modifier_info, dict) and 'text' in modifier_info:
                                        modifier_texts.append(modifier_info['text'])
                        
                        # M2ã‚­ãƒ¼ã¨ã—ã¦çµ±åˆ
                        if modifier_texts:
                            modifiers['M2'] = ' '.join(modifier_texts)
            
            # Step 3: å”åŠ›è€…ï¼ˆ5æ–‡å‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼‰ã¨é€£æºï¼šæ§‹é€ åˆ†æ
            structure_analysis = None
            if self.five_pattern_handler and cleaned_clause:
                structure_result = self.five_pattern_handler.process(cleaned_clause)
                if structure_result.get('success'):
                    structure_analysis = structure_result
            
            # Step 3.5: å”åŠ›è€…ï¼ˆå—å‹•æ…‹ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼‰ã¨é€£æºï¼šå—å‹•æ…‹æ¤œå‡º
            passive_analysis = None
            if self.passive_handler and cleaned_clause:
                passive_result = self.passive_handler.process(cleaned_clause)
                if passive_result:
                    passive_analysis = passive_result
            
            # Step 4: æ–‡å…¨ä½“ã‚’spaCyã§è§£æï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ»è©³ç´°æƒ…å ±ç”¨ï¼‰
            doc = self.nlp(text)
            
            # ğŸ¯ Step 4.5: spaCyèª¤åˆ¤å®šå¯¾å‡¦æ³•ï¼ˆè¨­è¨ˆä»•æ§˜æ›¸æº–æ‹ ï¼‰
            doc = self._apply_spacy_fallback(doc, text)
            
            # Step 5: é–¢ä¿‚ä»£åè©ã®ä½ç½®ã‚’ç‰¹å®š
            rel_pronoun_idx = None
            for i, token in enumerate(doc):
                if token.text.lower() == relative_pronoun.lower():
                    rel_pronoun_idx = i
                    break
            
            if rel_pronoun_idx is None:
                return {'success': False, 'error': f'{relative_pronoun}ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}
            
            # Step 6: é–¢ä¿‚ç¯€å†…ã®å‹•è©ã‚’ç‰¹å®šï¼ˆå”åŠ›è€…ã®çµæœã‚’å„ªå…ˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æœ‰ã‚Šï¼‰
            rel_verb_token = None
            if structure_analysis and structure_analysis.get('slots', {}).get('V'):
                # å”åŠ›è€…ã‹ã‚‰ã®5æ–‡å‹åˆ†æçµæœã‚’ä½¿ç”¨
                rel_verb = structure_analysis['slots']['V']
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: spaCyç›´æ¥åˆ†æ
                for i in range(rel_pronoun_idx + 1, len(doc)):
                    token = doc[i]
                    if token.pos_ in ['VERB', 'AUX']:
                        rel_verb_token = token
                        rel_verb = token.text
                        break
                    # ä¸»ç¯€ã®å‹•è©ã«é”ã—ãŸã‚‰åœæ­¢
                    if token.dep_ == 'ROOT':
                        break
            
            if not rel_verb_token and not rel_verb:
                return {'success': False, 'error': 'é–¢ä¿‚ç¯€å†…ã«å‹•è©ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}
            
            # Step 7: å…ˆè¡Œè©ã‚’ç‰¹å®š
            antecedent_tokens = []
            for i in range(rel_pronoun_idx):
                antecedent_tokens.append(doc[i])
            
            # Step 8: ä¸»ç¯€éƒ¨åˆ†ã‚’ç‰¹å®šï¼ˆå‹•è©å‰ã®ä¿®é£¾èªã‚‚å«ã‚ã‚‹ï¼‰
            main_clause_start = None
            root_verb_idx = None
            
            # ã¾ãšROOTå‹•è©ã‚’ç‰¹å®š
            for i in range(rel_pronoun_idx + 1, len(doc)):
                if doc[i].dep_ == 'ROOT':
                    root_verb_idx = i
                    break
            
            if root_verb_idx is not None:
                # ROOTå‹•è©ã®å‰ã«ã‚ã‚‹ä¸»ç¯€ä¿®é£¾èªã‚’æ¢ã™
                # é–¢ä¿‚ç¯€ã®çµ‚äº†ä½ç½®ã‹ã‚‰ ROOTå‹•è©ã®é–“ã«ã‚ã‚‹ä¿®é£¾èªã‚’å«ã‚ã‚‹
                rel_clause_end = rel_pronoun_idx + 1
                
                # é–¢ä¿‚ç¯€ã®çµ‚äº†ä½ç½®ã‚’ç‰¹å®šï¼ˆé–¢ä¿‚ç¯€å†…ã®æœ€å¾Œã®è¦ç´ ï¼‰
                for i in range(rel_pronoun_idx + 1, root_verb_idx):
                    token = doc[i]
                    # é–¢ä¿‚ç¯€å†…ã®è¦ç´ ï¼ˆé–¢ä¿‚ä»£åè©ã«ä¾å­˜ï¼‰ã‹ãƒã‚§ãƒƒã‚¯
                    if token.head.i <= rel_pronoun_idx or token.head.i >= root_verb_idx:
                        rel_clause_end = i + 1
                        break
                
                # é–¢ä¿‚ç¯€çµ‚äº†å¾Œã‹ã‚‰ ROOTå‹•è©ã®é–“ã®ä¿®é£¾èªã‚’å«ã‚ã‚‹
                main_clause_start = rel_clause_end
                for i in range(rel_clause_end, root_verb_idx):
                    token = doc[i]
                    # ä¸»ç¯€ã®ä¿®é£¾èªï¼ˆROOTå‹•è©ã‚’ä¿®é£¾ï¼‰ãªã‚‰ä¸»ç¯€é–‹å§‹ä½ç½®ã‚’æ›´æ–°
                    if token.head.i == root_verb_idx and token.pos_ == 'ADV':
                        main_clause_start = i
                        break
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ROOTå‹•è©ã‹ã‚‰é–‹å§‹
                if main_clause_start == rel_clause_end and main_clause_start == root_verb_idx:
                    main_clause_start = root_verb_idx
            
            result = {
                'success': True,
                'antecedent': ' '.join([t.text for t in antecedent_tokens]).strip(),
                'relative_verb': rel_verb,
                'relative_verb_pos': rel_verb_token.pos_ if rel_verb_token else 'VERB',
                'relative_verb_lemma': rel_verb_token.lemma_ if rel_verb_token else rel_verb,
                'main_clause_start': main_clause_start,
                'doc': doc,
                'modifiers': modifiers,  # å”åŠ›è€…ã‹ã‚‰ã®ä¿®é£¾èªæƒ…å ±
                'structure_analysis': structure_analysis  # å”åŠ›è€…ã‹ã‚‰ã®5æ–‡å‹åˆ†æ
            }
            
            return result
            
        except Exception as e:
            return {'success': False, 'error': f'spaCyè§£æã‚¨ãƒ©ãƒ¼: {str(e)}'}
    
    def _apply_spacy_fallback(self, doc, text: str):
        """
        spaCyèª¤åˆ¤å®šå¯¾å‡¦æ³•ï¼ˆè¨­è¨ˆä»•æ§˜æ›¸æº–æ‹ ï¼‰
        
        è¨­è¨ˆä»•æ§˜æ›¸ä¾‹2:
        â†’ spaCyèª¤åˆ¤å®š: livesã‚’åè©lifeè¤‡æ•°å½¢ã¨åˆ¤å®š
        â†’ ã‚·ã‚¹ãƒ†ãƒ è­¦æˆ’: æ›–æ˜§èªå¥ã¨ã—ã¦2é¸æŠè‚¢ã‚’æº–å‚™
        â†’ ç¬¬1å€™è£œ: lives_åè© â†’ ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã‚¼ãƒ­ã§æ–‡æ³•ç ´ç¶»
        â†’ ç¬¬2å€™è£œ: lives_å‹•è© â†’ redã§é–¢ä¿‚ç¯€çµ‚äº† â†’ lives_V, here_M2ã§æ–‡æˆç«‹
        â†’ åˆ¤æ–­: ç¬¬2å€™è£œãŒæ­£ã—ã„
        """
        # æ›–æ˜§èªå¥ã®æ¤œå‡ºï¼ˆè¤‡æ•°ã®å“è©è§£é‡ˆãŒå¯èƒ½ãªèªï¼‰
        ambiguous_words = []
        
        for i, token in enumerate(doc):
            # lives ã®ç‰¹åˆ¥å‡¦ç†
            if token.text.lower() == 'lives':
                if token.pos_ == 'NOUN' and token.tag_ == 'NNS':
                    print(f"ğŸš¨ spaCyèª¤åˆ¤å®šæ¤œå‡º: '{token.text}' ã‚’ {token.pos_} ã¨ã—ã¦åˆ¤å®š")
                    ambiguous_words.append({
                        'index': i,
                        'text': token.text,
                        'original_pos': token.pos_,
                        'alternative_pos': 'VERB',
                        'alternative_tag': 'VBZ',
                        'reason': 'è¨­è¨ˆä»•æ§˜æ›¸ä¾‹2å¯¾å¿œ'
                    })
        
        # ä»£æ›¿è§£é‡ˆã®æ¤œè¨¼
        if ambiguous_words:
            for ambiguous in ambiguous_words:
                if self._validate_alternative_interpretation(doc, ambiguous, text):
                    print(f"âœ… ä»£æ›¿è§£é‡ˆæ¡ç”¨: '{ambiguous['text']}' â†’ {ambiguous['alternative_pos']}")
                    # å®Ÿéš›ã®ä¿®æ­£ã¯æ§‹é€ è§£æã§åæ˜ 
                    return self._create_corrected_doc(doc, ambiguous, text)
        
        return doc
    
    def _validate_alternative_interpretation(self, doc, ambiguous: dict, text: str) -> bool:
        """
        ä»£æ›¿è§£é‡ˆã®æ–‡æ³•çš„å¦¥å½“æ€§æ¤œè¨¼
        
        è¨­è¨ˆä»•æ§˜æ›¸: "ç¬¬1å€™è£œ: lives_åè© â†’ ä¸Šä½ã‚¹ãƒ­ãƒƒãƒˆã‚¼ãƒ­ã§æ–‡æ³•ç ´ç¶»"
                   "ç¬¬2å€™è£œ: lives_å‹•è© â†’ redã§é–¢ä¿‚ç¯€çµ‚äº† â†’ lives_V, here_M2ã§æ–‡æˆç«‹"
        """
        word_idx = ambiguous['index']
        
        # lives ã‚’å‹•è©ã¨ã—ã¦è§£é‡ˆã—ãŸå ´åˆã®æ–‡æ³•ãƒã‚§ãƒƒã‚¯
        if ambiguous['text'].lower() == 'lives' and ambiguous['alternative_pos'] == 'VERB':
            # hereãŒä¿®é£¾èªã¨ã—ã¦é©åˆ‡ã«é…ç½®ã§ãã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            next_tokens = [token.text for token in doc[word_idx+1:]]
            if 'here' in next_tokens:
                print(f"ğŸ” æ–‡æ³•æ¤œè¨¼: lives_VERB + here_M2 ã§æ–‡æ§‹é€ æˆç«‹")
                return True
        
        return False
    
    def _create_corrected_doc(self, doc, ambiguous: dict, text: str):
        """ä¿®æ­£ã•ã‚ŒãŸæ–‡æ§‹é€ ã‚’åæ˜ ã—ãŸdocã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ"""
        # ç¾åœ¨ã®å®Ÿè£…ã§ã¯å…ƒã®docã‚’ãã®ã¾ã¾è¿”ã—ã€
        # æ§‹é€ è§£ææ™‚ã«ä¿®æ­£ã‚’é©ç”¨
        return doc
    
    def _process_which(self, text: str) -> Dict[str, Any]:
        """whiché–¢ä¿‚ç¯€å‡¦ç†ï¼ˆå”åŠ›ã‚¢ãƒ—ãƒ­ãƒ¼ãƒç‰ˆï¼‰"""
        
        # spaCyæ–‡è„ˆè§£æã§é–¢ä¿‚ç¯€ã‚’åˆ†æï¼ˆå”åŠ›è€…æƒ…å ±ã‚’å«ã‚€ï¼‰
        analysis = self._analyze_relative_clause(text, 'which')
        if not analysis['success']:
            return analysis
        
        antecedent = analysis['antecedent']
        rel_verb = analysis['relative_verb']
        
        # ä¿®é£¾èªæƒ…å ±ï¼ˆå”åŠ›è€… AdverbHandler ã®çµæœã‚’æ´»ç”¨ï¼‰
        modifiers_info = analysis.get('modifiers', {})
        sub_m2 = ""
        
        # å”åŠ›è€…ã‹ã‚‰ä¿®é£¾èªæƒ…å ±ã‚’å–å¾—
        if modifiers_info and 'M2' in modifiers_info:
            sub_m2 = modifiers_info['M2']
        
        # whichã¯ä¸»èªãƒ»ç›®çš„èªã‚’æ–‡è„ˆã§åˆ¤å®š
        doc = analysis['doc']  # _analyze_relative_clauseã‹ã‚‰å–å¾—
        which_idx = None
        for i, token in enumerate(doc):
            if token.text.lower() == 'which':
                which_idx = i
                break
        
        is_subject = True
        if which_idx is not None and which_idx + 1 < len(doc):
            next_token = doc[which_idx + 1]
            if next_token.pos_ in ['PRON', 'NOUN', 'PROPN']:
                is_subject = False  # which + åè© = ç›®çš„æ ¼
        
        # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ§‹ç¯‰
        if is_subject:
            sub_slots = {
                'sub-s': f"{antecedent} which",
                'sub-v': rel_verb,
                '_parent_slot': 'S'
            }
        else:
            # ç›®çš„æ ¼whichã®å ´åˆã€é–¢ä¿‚ç¯€å†…ã®ä¸»èªã‚’ç‰¹å®š
            rel_subject = ""
            if which_idx is not None:
                for i in range(which_idx + 1, len(doc)):
                    if doc[i].pos_ in ['PRON', 'NOUN', 'PROPN'] and doc[i].text != rel_verb:
                        rel_subject = doc[i].text
                        break
            
            sub_slots = {
                'sub-o1': f"{antecedent} which",
                'sub-s': rel_subject,
                'sub-v': rel_verb,
                '_parent_slot': 'S'
            }
        
        # ä¿®é£¾èªãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
        if sub_m2:
            sub_slots['sub-m2'] = sub_m2
        
        # ä¸»ç¯€ã‚’æ§‹ç¯‰
        main_clause_start = analysis.get('main_clause_start')
        main_clause = ""
        if main_clause_start is not None:
            doc = analysis['doc']
            main_tokens = [token.text for token in doc[main_clause_start:]]
            main_clause = " ".join(main_tokens)

        return {
            'success': True,
            'main_slots': {'S': ''},
            'sub_slots': sub_slots,
            'pattern_type': 'which_object' if not is_subject else 'which_subject',
            'relative_pronoun': 'which',
            'antecedent': antecedent,
            'main_continuation': main_clause.strip(),
            'spacy_analysis': {
                'relative_verb_pos': analysis['relative_verb_pos'],
                'relative_verb_lemma': analysis['relative_verb_lemma']
            }
        }
    
    def _process_that(self, text: str) -> Dict[str, Any]:
        """thaté–¢ä¿‚ç¯€å‡¦ç†ï¼ˆspaCyæ–‡è„ˆè§£æãƒ™ãƒ¼ã‚¹ï¼‰"""
        
        # spaCyæ–‡è„ˆè§£æã§é–¢ä¿‚ç¯€ã‚’åˆ†æ
        analysis = self._analyze_relative_clause(text, 'that')
        if not analysis['success']:
            return analysis
        
        doc = analysis['doc']
        antecedent = analysis['antecedent']
        rel_verb = analysis['relative_verb']
        main_clause_start = analysis['main_clause_start']
        
        # é–¢ä¿‚ç¯€ã®ä¿®é£¾èªã‚’ç‰¹å®š
        rel_verb_idx = None
        for i, token in enumerate(doc):
            if token.text == rel_verb:
                rel_verb_idx = i
                break
        
        # é–¢ä¿‚ç¯€éƒ¨åˆ†ã®å®Œå…¨ãªå‹•è©å¥ã‚’æ§‹ç¯‰
        rel_verb_phrase = rel_verb
        rel_modifiers = []  # ä¿®é£¾èªã‚’åˆ¥é€”è¨˜éŒ²
        
        if rel_verb_idx is not None:
            for i in range(rel_verb_idx + 1, len(doc)):
                if doc[i].dep_ == 'ROOT':
                    break
                if doc[i].head.i == rel_verb_idx:
                    rel_modifiers.append(doc[i].text)
        
        # ä¿®é£¾èªãŒã‚ã‚‹å ´åˆã¯sub-m2ã«è¨­å®š
        sub_m2 = " ".join(rel_modifiers) if rel_modifiers else ""
        
        # ä¸»ç¯€ã‚’æ§‹ç¯‰
        main_clause = ""
        if main_clause_start is not None:
            main_tokens = [token.text for token in doc[main_clause_start:]]
            main_clause = " ".join(main_tokens)
        
        # thatã¯ä¸»èªãƒ»ç›®çš„èªã‚’æ–‡è„ˆã§åˆ¤å®š
        # ç°¡ç•¥åˆ¤å®šï¼šthatç›´å¾Œã«å‹•è©ãŒã‚ã‚Œã°ä¸»èªã€åè©ãŒã‚ã‚Œã°ç›®çš„èª
        is_subject = True
        that_idx = None
        for i, token in enumerate(doc):
            if token.text.lower() == 'that':
                that_idx = i
                break
        
        if that_idx is not None and that_idx + 1 < len(doc):
            next_token = doc[that_idx + 1]
            if next_token.pos_ in ['PRON', 'NOUN', 'PROPN']:
                is_subject = False  # that + åè© = ç›®çš„æ ¼
        
        # ã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ§‹ç¯‰
        if is_subject:
            sub_slots = {
                'sub-s': f"{antecedent} that",
                'sub-v': rel_verb,
                '_parent_slot': 'S'
            }
        else:
            # ç›®çš„æ ¼thatã®å ´åˆã€é–¢ä¿‚ç¯€å†…ã®ä¸»èªã‚’ç‰¹å®š
            rel_subject = ""
            if that_idx is not None:
                for i in range(that_idx + 1, len(doc)):
                    if doc[i].pos_ in ['PRON', 'NOUN', 'PROPN'] and doc[i].text != rel_verb:
                        rel_subject = doc[i].text
                        break
            
            sub_slots = {
                'sub-o1': f"{antecedent} that",
                'sub-s': rel_subject,
                'sub-v': rel_verb,
                '_parent_slot': 'S'
            }
        
        # ä¿®é£¾èªãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
        if sub_m2:
            sub_slots['sub-m2'] = sub_m2
        
        return {
            'success': True,
            'main_slots': {'S': ''},
            'sub_slots': sub_slots,
            'pattern_type': 'that_subject' if is_subject else 'that_object',
            'relative_pronoun': 'that',
            'antecedent': antecedent,
            'main_continuation': main_clause.strip(),
            'spacy_analysis': {
                'relative_verb_pos': analysis['relative_verb_pos'],
                'relative_verb_lemma': analysis['relative_verb_lemma']
            }
        }

    def _process_whom(self, text: str) -> Dict[str, Any]:
        """whomé–¢ä¿‚ç¯€å‡¦ç†ï¼ˆspaCyæ–‡è„ˆè§£æãƒ™ãƒ¼ã‚¹ï¼‰"""
        
        # spaCyæ–‡è„ˆè§£æã§é–¢ä¿‚ç¯€ã‚’åˆ†æ
        analysis = self._analyze_relative_clause(text, 'whom')
        if not analysis['success']:
            return analysis
        
        doc = analysis['doc']
        antecedent = analysis['antecedent']
        rel_verb = analysis['relative_verb']
        main_clause_start = analysis['main_clause_start']
        
        # whomã¯ç›®çš„æ ¼ãªã®ã§ã€é–¢ä¿‚ç¯€å†…ã«ä¸»èªãŒå¿…è¦
        # "The man whom I met" -> I ãŒä¸»èªã€met ãŒå‹•è©
        rel_verb_idx = None
        for i, token in enumerate(doc):
            if token.text == rel_verb:
                rel_verb_idx = i
                break
        
        # é–¢ä¿‚ç¯€å†…ã®ä¸»èªã‚’ç‰¹å®š
        rel_subject = ""
        if rel_verb_idx is not None:
            for i in range(rel_verb_idx):
                if doc[i].text.lower() == 'whom':
                    # whomã®å¾Œã®æœ€åˆã®åè©/ä»£åè©ãŒä¸»èª
                    for j in range(i + 1, rel_verb_idx):
                        if doc[j].pos_ in ['PRON', 'NOUN', 'PROPN']:
                            rel_subject = doc[j].text
                            break
                    break
        
        # ä¸»ç¯€ã‚’æ§‹ç¯‰
        main_clause = ""
        if main_clause_start is not None:
            main_tokens = [token.text for token in doc[main_clause_start:]]
            main_clause = " ".join(main_tokens)
        
        return {
            'success': True,
            'main_slots': {'S': ''},
            'sub_slots': {
                'sub-o1': f"{antecedent} whom",  # whomã¯ç›®çš„æ ¼
                'sub-s': rel_subject,
                'sub-v': rel_verb,
                '_parent_slot': 'S'
            },
            'pattern_type': 'whom_object',
            'relative_pronoun': 'whom',
            'antecedent': antecedent,
            'main_continuation': main_clause.strip(),
            'spacy_analysis': {
                'relative_verb_pos': analysis['relative_verb_pos'],
                'relative_verb_lemma': analysis['relative_verb_lemma']
            }
        }

    def _process_whose(self, text: str) -> Dict[str, Any]:
        """whoseé–¢ä¿‚ç¯€å‡¦ç†ï¼ˆç‰¹åŒ–å‹è§£æï¼‰"""
        
        print(f"ğŸ” whoseå‡¦ç†é–‹å§‹: '{text}'")
        
        # whoseã¯ç‰¹æ®Šãªã®ã§å°‚ç”¨è§£æ
        doc = self.nlp(text)
        
        # Step 1: whoseæ§‹é€ ã®è©³ç´°è§£æ
        whose_info = self._analyze_whose_structure(doc)
        if not whose_info['success']:
            print(f"âš ï¸ whoseè§£æå¤±æ•—: {whose_info}")
            return whose_info
        
        antecedent = whose_info['antecedent']
        rel_verb = whose_info['relative_verb']
        whose_noun = whose_info['whose_noun']
        main_verb_idx = whose_info['main_verb_idx']
        whose_idx = whose_info.get('whose_idx')  # whoseä½ç½®ã‚’å–å¾—
        
        # ä¸»ç¯€ã‚’æ§‹ç¯‰
        main_clause = ""
        if main_verb_idx is not None:
            main_tokens = [token.text for token in doc[main_verb_idx:]]
            main_clause = " ".join(main_tokens)
        
        # ğŸ¯ å‰¯è©ä¿®é£¾èªå‡¦ç†ã‚’è¿½åŠ 
        sub_slots = {
            'sub-s': f"{antecedent} whose {whose_noun}",  # ğŸ¯ Rephraseæº–æ‹ : å…ˆè¡Œè©+é–¢ä¿‚ä»£åè©+é–¢ä¿‚ç¯€ä¸»èª
            'sub-v': rel_verb,
            '_parent_slot': 'S'
        }
        
        # é–¢ä¿‚ç¯€å†…ã®å‰¯è©ä¿®é£¾èªã‚’æ¤œå‡º
        if whose_idx is not None and main_verb_idx is not None:
            # whose ... main_verb ã®é–“ã§å‰¯è©ã‚’æ¢ã™
            for i in range(whose_idx, main_verb_idx):
                token = doc[i]
                if token.pos_ == 'ADV':
                    sub_slots['sub-m2'] = token.text
                    print(f"ğŸ¯ é–¢ä¿‚ç¯€å†…å‰¯è©æ¤œå‡º: sub-m2 = '{token.text}'")
                    break
        
        result = {
            'success': True,
            'main_slots': {'S': ''},
            'sub_slots': sub_slots,
            'pattern_type': 'whose_possessive',
            'relative_pronoun': 'whose',
            'antecedent': antecedent,
            'main_continuation': main_clause.strip(),
            'spacy_analysis': {
                'relative_verb_pos': 'VERB',
                'relative_verb_lemma': rel_verb
            }
        }
        print(f"ğŸ¯ whoseå‡¦ç†å®Œäº†: {result}")
        return result

    def _analyze_whose_structure(self, doc) -> Dict[str, Any]:
        """whoseæ§‹é€ å°‚ç”¨è§£æ"""
        try:
            print(f"ğŸ” whoseæ§‹é€ è§£æé–‹å§‹")
            # Step 1: whoseä½ç½®ã‚’ç‰¹å®š
            whose_idx = None
            for i, token in enumerate(doc):
                if token.text.lower() == 'whose':
                    whose_idx = i
                    break
            
            if whose_idx is None:
                print(f"âš ï¸ whoseãŒè¦‹ã¤ã‹ã‚‰ãªã„")
                return {'success': False, 'error': 'whose not found'}
            
            print(f"ğŸ¯ whoseä½ç½®: {whose_idx}")
            
            # Step 2: å…ˆè¡Œè©ã‚’ç‰¹å®šï¼ˆwhoseã‚ˆã‚Šå‰ï¼‰
            antecedent_tokens = []
            for i in range(whose_idx):
                antecedent_tokens.append(doc[i].text)
            antecedent = " ".join(antecedent_tokens).strip()
            print(f"ğŸ¯ å…ˆè¡Œè©: '{antecedent}'")
            
            # Step 3: whose + åè©ã‚’ç‰¹å®š
            whose_noun = ""
            if whose_idx + 1 < len(doc):
                whose_noun = doc[whose_idx + 1].text
            print(f"ğŸ¯ whoseåè©: '{whose_noun}'")
            
            # Step 4: é–¢ä¿‚ç¯€å†…ã®å‹•è©ã‚’ç‰¹å®šï¼ˆwhose + åè©ã®å¾Œã®æœ€åˆã®å‹•è©ï¼‰
            rel_verb = ""
            rel_verb_idx = None
            for i in range(whose_idx + 2, len(doc)):
                token = doc[i]
                if token.pos_ in ['VERB', 'AUX']:
                    rel_verb = token.text
                    rel_verb_idx = i
                    print(f"ğŸ¯ é–¢ä¿‚ç¯€å‹•è©: '{rel_verb}' at {i}")
                    break
            
            # Step 5: ä¸»ç¯€å‹•è©ã‚’ç‰¹å®šï¼ˆé–¢ä¿‚ç¯€å¾Œã®æœ€åˆã®å‹•è©ï¼‰
            main_verb_idx = None
            if rel_verb_idx is not None:
                # é–¢ä¿‚ç¯€å‹•è©ã®å¾Œã‹ã‚‰ä¸»ç¯€å‹•è©ã‚’æ¢ã™
                for i in range(rel_verb_idx + 1, len(doc)):
                    token = doc[i]
                    
                    # é€šå¸¸ã®å‹•è©æ¤œå‡º
                    if token.pos_ in ['VERB', 'AUX'] and token.dep_ != 'relcl':
                        main_verb_idx = i
                        print(f"ğŸ¯ ä¸»ç¯€å‹•è©: '{token.text}' at {i}")
                        break
                    
                    # spaCyèª¤åˆ¤å®šä¿®æ­£: å‹•è©çš„ãªå˜èªãŒåè©ã¨ã—ã¦åˆ¤å®šã•ã‚Œã‚‹å ´åˆ
                    if token.pos_ == 'NOUN' and token.text.lower() in ['lives', 'works', 'runs', 'goes', 'comes', 'stays']:
                        main_verb_idx = i
                        print(f"ğŸ¯ ä¸»ç¯€å‹•è©(ä¿®æ­£): '{token.text}' at {i}")
                        break
                    
                    # å½¢å®¹è©ã®å¾Œã«ç¶šãèªå¥ã§å‹•è©ã‚’æ¢ã™
                    if i > 0 and doc[i-1].pos_ == 'ADJ' and token.pos_ in ['NOUN'] and token.text.lower() in ['lives', 'works']:
                        main_verb_idx = i
                        print(f"ğŸ¯ ä¸»ç¯€å‹•è©(ADJå¾Œ): '{token.text}' at {i}")
                        break
            
            result = {
                'success': True,
                'antecedent': antecedent,
                'whose_noun': whose_noun,
                'relative_verb': rel_verb,
                'main_verb_idx': main_verb_idx,
                'whose_idx': whose_idx  # whoseä½ç½®ã‚’è¿½åŠ 
            }
            print(f"ğŸ¯ whoseè§£æçµæœ: {result}")
            return result
            
        except Exception as e:
            print(f"âŒ whoseè§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'success': False, 'error': f'whoseè§£æã‚¨ãƒ©ãƒ¼: {str(e)}'}

    def _process_relative_adverb(self, text: str, relative_adverb: str) -> Dict[str, Any]:
        """é–¢ä¿‚å‰¯è©å‡¦ç†ï¼ˆwhere, when, why, howï¼‰"""
        
        # spaCyæ–‡è„ˆè§£æã§é–¢ä¿‚ç¯€ã‚’åˆ†æ
        analysis = self._analyze_relative_clause(text, relative_adverb)
        if not analysis['success']:
            return analysis
        
        antecedent = analysis['antecedent']
        rel_verb = analysis['relative_verb']
        
        # ä¿®é£¾èªæƒ…å ±ï¼ˆå”åŠ›è€… AdverbHandler ã®çµæœã‚’æ´»ç”¨ï¼‰
        modifiers_info = analysis.get('modifiers', {})
        sub_m2 = ""
        
        # å”åŠ›è€…ã‹ã‚‰ä¿®é£¾èªæƒ…å ±ã‚’å–å¾—
        if modifiers_info and 'M2' in modifiers_info:
            sub_m2 = modifiers_info['M2']
        
        # ä¸»ç¯€ã‚’æ§‹ç¯‰
        main_clause_start = analysis.get('main_clause_start')
        main_clause = ""
        if main_clause_start is not None:
            doc = analysis['doc']
            main_tokens = [token.text for token in doc[main_clause_start:]]
            main_clause = " ".join(main_tokens)
        
        # é–¢ä¿‚å‰¯è©ã«å¿œã˜ãŸã‚µãƒ–ã‚¹ãƒ­ãƒƒãƒˆæ§‹ç¯‰
        if relative_adverb in ['where']:
            # where: å ´æ‰€ã®ä¿®é£¾
            sub_slots = {
                'sub-m2': f"{antecedent} {relative_adverb}",  # å ´æ‰€ä¿®é£¾ã¨ã—ã¦
                'sub-s': self._extract_relative_subject(analysis, relative_adverb),
                'sub-v': rel_verb,
                '_parent_slot': 'S'
            }
        elif relative_adverb in ['when']:
            # when: æ™‚é–“ã®ä¿®é£¾
            sub_slots = {
                'sub-m1': f"{antecedent} {relative_adverb}",  # æ™‚é–“ä¿®é£¾ã¨ã—ã¦
                'sub-s': self._extract_relative_subject(analysis, relative_adverb),
                'sub-v': rel_verb,
                '_parent_slot': 'S'
            }
        elif relative_adverb in ['why']:
            # why: ç†ç”±ã®ä¿®é£¾
            sub_slots = {
                'sub-m3': f"{antecedent} {relative_adverb}",  # ç†ç”±ä¿®é£¾ã¨ã—ã¦
                'sub-s': self._extract_relative_subject(analysis, relative_adverb),
                'sub-v': rel_verb,
                '_parent_slot': 'S'
            }
        elif relative_adverb in ['how']:
            # how: æ–¹æ³•ã®ä¿®é£¾
            sub_slots = {
                'sub-m2': f"{antecedent} {relative_adverb}",  # æ–¹æ³•ä¿®é£¾ã¨ã—ã¦
                'sub-s': self._extract_relative_subject(analysis, relative_adverb),
                'sub-v': rel_verb,
                '_parent_slot': 'S'
            }
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            sub_slots = {
                'sub-m2': f"{antecedent} {relative_adverb}",
                'sub-s': self._extract_relative_subject(analysis, relative_adverb),
                'sub-v': rel_verb,
                '_parent_slot': 'S'
            }
        
        # ä¿®é£¾èªãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
        if sub_m2 and 'sub-m2' not in sub_slots:
            sub_slots['sub-m2'] = sub_m2
        
        return {
            'success': True,
            'main_slots': {'S': ''},
            'sub_slots': sub_slots,
            'pattern_type': f'{relative_adverb}_adverb',
            'relative_pronoun': relative_adverb,
            'antecedent': antecedent,
            'main_continuation': main_clause.strip(),
            'spacy_analysis': {
                'relative_verb_pos': analysis['relative_verb_pos'],
                'relative_verb_lemma': analysis['relative_verb_lemma']
            }
        }

    def _extract_relative_subject(self, analysis: Dict, relative_adverb: str) -> str:
        """é–¢ä¿‚å‰¯è©ç¯€å†…ã®ä¸»èªã‚’æŠ½å‡º"""
        doc = analysis['doc']
        
        # é–¢ä¿‚å‰¯è©ã®ä½ç½®ã‚’ç‰¹å®š
        adverb_idx = None
        for i, token in enumerate(doc):
            if token.text.lower() == relative_adverb.lower():
                adverb_idx = i
                break
        
        if adverb_idx is None:
            return ""
        
        # é–¢ä¿‚å‰¯è©ç›´å¾Œã‹ã‚‰ä¸»èªã‚’æ¢ã™
        for i in range(adverb_idx + 1, len(doc)):
            token = doc[i]
            if token.dep_ == 'ROOT':
                break
            if token.pos_ in ['NOUN', 'PRON', 'PROPN']:
                return token.text
        
        return ""
