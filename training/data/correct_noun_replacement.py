#!/usr/bin/env python3
"""æ­£ã—ã„åè©å¥å…¨ä½“ç½®æ›ã®å®Ÿè£…"""

import spacy

def correct_noun_phrase_replacement():
    """é–¢ä¿‚ç¯€ã‚’å«ã‚€åè©å¥å…¨ä½“ã®æ­£ã—ã„ç½®æ›"""
    
    nlp_spacy = spacy.load("en_core_web_sm")
    
    test_sentence = "The book that I read yesterday was interesting."
    
    print(f"ğŸ” åˆ†ææ–‡: {test_sentence}")
    print("=" * 50)
    
    doc = nlp_spacy(test_sentence)
    
    # é–¢ä¿‚ç¯€ã¨ãã®ä¿®é£¾å¯¾è±¡ã‚’ç‰¹å®š
    relcl_info = None
    for token in doc:
        if token.dep_ == 'relcl':
            # é–¢ä¿‚ç¯€ã®ç¯„å›²
            relcl_tokens = list(token.subtree)
            relcl_start = min(t.i for t in relcl_tokens)
            relcl_end = max(t.i for t in relcl_tokens) + 1
            
            # ä¿®é£¾ã•ã‚Œã‚‹åè©ã®ä½ç½®
            head_token = token.head
            
            # åè©å¥ã®é–‹å§‹ä½ç½®ã‚’ç‰¹å®šï¼ˆå† è©ç­‰ã‚’å«ã‚€ï¼‰
            noun_phrase_start = head_token.i
            for chunk in doc.noun_chunks:
                if head_token.i >= chunk.start and head_token.i < chunk.end:
                    noun_phrase_start = chunk.start
                    break
            
            relcl_info = {
                'noun_start': noun_phrase_start,
                'noun_head_idx': head_token.i,
                'relcl_start': relcl_start, 
                'relcl_end': relcl_end,
                'full_start': noun_phrase_start,
                'full_end': relcl_end
            }
            
            print(f"ğŸ“ ä¿®é£¾ã•ã‚Œã‚‹åè©ä½ç½®: {head_token.i} ('{head_token.text}')")
            print(f"ğŸ“ åè©å¥é–‹å§‹: {noun_phrase_start}")
            print(f"ğŸ“ é–¢ä¿‚ç¯€ç¯„å›²: {relcl_start}-{relcl_end}")
            print(f"ğŸ“ å…¨ä½“ç½®æ›ç¯„å›²: {noun_phrase_start}-{relcl_end}")
            
            break
    
    if relcl_info:
        # æ­£ã—ã„ç½®æ›å®Ÿè¡Œ
        result_tokens = []
        i = 0
        while i < len(doc):
            if i == relcl_info['full_start']:
                result_tokens.append('Something')
                i = relcl_info['full_end']  # åè©å¥+é–¢ä¿‚ç¯€å…¨ä½“ã‚’ã‚¹ã‚­ãƒƒãƒ—
            else:
                result_tokens.append(doc[i].text)
                i += 1
        
        result = ' '.join(result_tokens)
        print(f"\nâœ… æ­£ã—ã„ç½®æ›çµæœ: '{result}'")
        
        # V4ã§ãƒ†ã‚¹ãƒˆ
        from hierarchical_grammar_detector_v4 import HierarchicalGrammarDetectorV4
        detector = HierarchicalGrammarDetectorV4()
        
        try:
            grammar_result = detector.detect_hierarchical_grammar(result)
            pattern = grammar_result.main_clause.grammatical_pattern.value if grammar_result.main_clause else 'Unknown'
            print(f"ğŸ¯ æ–‡æ³•ãƒ‘ã‚¿ãƒ¼ãƒ³: {pattern}")
        except Exception as e:
            print(f"âŒ æ–‡æ³•è§£æã‚¨ãƒ©ãƒ¼: {e}")
    
    else:
        print("âŒ é–¢ä¿‚ç¯€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

if __name__ == "__main__":
    correct_noun_phrase_replacement()
