"""
æ–°æ—§ã‚·ã‚¹ãƒ†ãƒ æ¯”è¼ƒãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ (Phase 6a)
æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨æ–°ä¸­å¤®ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã®ä¸¦è¡Œé‹ç”¨ãƒ»æ¯”è¼ƒæ¤œè¨¼
"""

import sys
import time
from typing import Dict, List, Any
from central_controller import CentralController
from central_controller_v2 import CentralControllerV2


class SystemComparison:
    """æ–°æ—§ã‚·ã‚¹ãƒ†ãƒ ã®æ¯”è¼ƒæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        print("ğŸ”¬ ã‚·ã‚¹ãƒ†ãƒ æ¯”è¼ƒæ¤œè¨¼ åˆæœŸåŒ–é–‹å§‹")
        
        # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        try:
            self.legacy_controller = CentralController()
            print("âœ… æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"âŒ æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.legacy_controller = None
        
        # æ–°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        try:
            self.v2_controller = CentralControllerV2()
            print("âœ… æ–°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            print(f"âŒ æ–°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            self.v2_controller = None
            
        self.test_results = []
    
    def run_comparison_test(self, text: str) -> Dict[str, Any]:
        """å˜ä¸€ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®æ¯”è¼ƒå®Ÿè¡Œ"""
        
        print(f"\nğŸ§ª æ¯”è¼ƒãƒ†ã‚¹ãƒˆå®Ÿè¡Œ: '{text}'")
        print("=" * 60)
        
        # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã§ã®å‡¦ç†
        legacy_result = None
        legacy_time = None
        legacy_error = None
        
        if self.legacy_controller:
            try:
                start_time = time.time()
                legacy_result = self.legacy_controller.analyze_grammar_structure(text)
                legacy_time = time.time() - start_time
                print(f"ğŸ“Š æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ çµæœ: {legacy_result} (å®Ÿè¡Œæ™‚é–“: {legacy_time:.3f}s)")
            except Exception as e:
                legacy_error = str(e)
                print(f"âŒ æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æ–°ã‚·ã‚¹ãƒ†ãƒ ã§ã®å‡¦ç†
        v2_result = None
        v2_time = None
        v2_error = None
        
        if self.v2_controller:
            try:
                start_time = time.time()
                v2_analysis = self.v2_controller.analyze_grammar_structure_v2(text)
                v2_result = v2_analysis['legacy_format']
                v2_time = time.time() - start_time
                print(f"ğŸ†• æ–°ã‚·ã‚¹ãƒ†ãƒ çµæœ: {v2_result} (å®Ÿè¡Œæ™‚é–“: {v2_time:.3f}s)")
                print(f"   ä¿¡é ¼åº¦: {v2_analysis['v2_result'].confidence_score:.2f}")
                print(f"   ä½¿ç”¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼: {list(v2_analysis['v2_result'].handler_reports.keys())}")
            except Exception as e:
                v2_error = str(e)
                print(f"âŒ æ–°ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æ¯”è¼ƒåˆ†æ
        comparison_result = self._analyze_comparison(
            text, legacy_result, v2_result, legacy_time, v2_time, 
            legacy_error, v2_error, v2_analysis if 'v2_analysis' in locals() else None
        )
        
        self.test_results.append(comparison_result)
        return comparison_result
    
    def _analyze_comparison(self, text: str, legacy_result: List[str], v2_result: List[str], 
                           legacy_time: float, v2_time: float, legacy_error: str, v2_error: str,
                           v2_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """æ¯”è¼ƒçµæœã®è©³ç´°åˆ†æ"""
        
        # çµæœä¸€è‡´æ€§åˆ†æ
        results_match = False
        if legacy_result is not None and v2_result is not None:
            # ã‚½ãƒ¼ãƒˆã—ã¦é †åºã«ä¾å­˜ã—ãªã„æ¯”è¼ƒ
            legacy_sorted = sorted(legacy_result) if legacy_result else []
            v2_sorted = sorted(v2_result) if v2_result else []
            results_match = legacy_sorted == v2_sorted
        
        # å·®åˆ†åˆ†æ
        differences = {}
        if legacy_result is not None and v2_result is not None:
            legacy_set = set(legacy_result) if legacy_result else set()
            v2_set = set(v2_result) if v2_result else set()
            
            differences = {
                'v2_extra': list(v2_set - legacy_set),
                'legacy_extra': list(legacy_set - v2_set),
                'common': list(v2_set & legacy_set)
            }
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        performance_analysis = {}
        if legacy_time is not None and v2_time is not None:
            performance_analysis = {
                'legacy_faster': legacy_time < v2_time,
                'time_difference': abs(v2_time - legacy_time),
                'performance_ratio': v2_time / legacy_time if legacy_time > 0 else None
            }
        
        # å“è³ªè©•ä¾¡
        quality_assessment = {
            'both_successful': legacy_error is None and v2_error is None,
            'legacy_error': legacy_error,
            'v2_error': v2_error,
            'results_match': results_match,
            'v2_confidence': v2_analysis['v2_result'].confidence_score if v2_analysis else None
        }
        
        # çµæœè¡¨ç¤º
        print(f"\nğŸ“ˆ æ¯”è¼ƒåˆ†æçµæœ:")
        print(f"   çµæœä¸€è‡´: {'âœ…' if results_match else 'âŒ'} {results_match}")
        if differences:
            print(f"   å·®åˆ†: V2ç‹¬è‡ª={differences['v2_extra']}, æ—¢å­˜ç‹¬è‡ª={differences['legacy_extra']}")
        if performance_analysis:
            print(f"   ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: {'æ—¢å­˜' if performance_analysis['legacy_faster'] else 'V2'}ãŒé«˜é€Ÿ")
        
        return {
            'text': text,
            'timestamp': time.time(),
            'results': {
                'legacy': legacy_result,
                'v2': v2_result
            },
            'timing': {
                'legacy': legacy_time,
                'v2': v2_time
            },
            'errors': {
                'legacy': legacy_error,
                'v2': v2_error
            },
            'analysis': {
                'results_match': results_match,
                'differences': differences,
                'performance': performance_analysis,
                'quality': quality_assessment
            },
            'v2_metadata': v2_analysis['v2_result'] if v2_analysis else None
        }
    
    def run_batch_test(self, test_cases: List[str]) -> Dict[str, Any]:
        """ãƒãƒƒãƒãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
        
        print(f"\nğŸ”„ ãƒãƒƒãƒãƒ†ã‚¹ãƒˆé–‹å§‹ - {len(test_cases)}ä»¶ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹")
        print("=" * 80)
        
        batch_results = []
        
        for i, text in enumerate(test_cases, 1):
            print(f"\nğŸ“ ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ {i}/{len(test_cases)}")
            result = self.run_comparison_test(text)
            batch_results.append(result)
            
            # é€²æ—è¡¨ç¤º
            if i % 5 == 0 or i == len(test_cases):
                print(f"\nâ³ é€²æ—: {i}/{len(test_cases)} ({i/len(test_cases)*100:.1f}%)")
        
        # ãƒãƒƒãƒçµæœã®çµ±è¨ˆåˆ†æ
        batch_summary = self._generate_batch_summary(batch_results)
        
        return {
            'individual_results': batch_results,
            'batch_summary': batch_summary
        }
    
    def _generate_batch_summary(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ãƒãƒƒãƒãƒ†ã‚¹ãƒˆçµæœã®çµ±è¨ˆã‚µãƒãƒªãƒ¼"""
        
        total_tests = len(results)
        successful_tests = len([r for r in results if r['analysis']['quality']['both_successful']])
        matching_results = len([r for r in results if r['analysis']['results_match']])
        
        # ã‚¨ãƒ©ãƒ¼åˆ†æ
        legacy_errors = len([r for r in results if r['errors']['legacy'] is not None])
        v2_errors = len([r for r in results if r['errors']['v2'] is not None])
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        performance_data = [r['analysis']['performance'] for r in results 
                          if r['analysis']['performance']]
        v2_faster_count = len([p for p in performance_data if not p['legacy_faster']])
        
        # ä¿¡é ¼åº¦åˆ†æ
        confidence_scores = [r['v2_metadata'].confidence_score for r in results 
                           if r['v2_metadata']]
        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
        
        summary = {
            'test_statistics': {
                'total_tests': total_tests,
                'successful_tests': successful_tests,
                'success_rate': successful_tests / total_tests if total_tests > 0 else 0,
                'matching_results': matching_results,
                'match_rate': matching_results / total_tests if total_tests > 0 else 0
            },
            'error_analysis': {
                'legacy_errors': legacy_errors,
                'v2_errors': v2_errors,
                'legacy_error_rate': legacy_errors / total_tests if total_tests > 0 else 0,
                'v2_error_rate': v2_errors / total_tests if total_tests > 0 else 0
            },
            'performance_analysis': {
                'total_performance_tests': len(performance_data),
                'v2_faster_count': v2_faster_count,
                'v2_faster_rate': v2_faster_count / len(performance_data) if performance_data else 0
            },
            'quality_analysis': {
                'average_v2_confidence': avg_confidence,
                'high_confidence_tests': len([s for s in confidence_scores if s > 0.7]),
                'low_confidence_tests': len([s for s in confidence_scores if s < 0.5])
            }
        }
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print(f"\nğŸ“Š ãƒãƒƒãƒãƒ†ã‚¹ãƒˆ ã‚µãƒãƒªãƒ¼:")
        print(f"   ç·ãƒ†ã‚¹ãƒˆæ•°: {total_tests}")
        print(f"   æˆåŠŸç‡: {summary['test_statistics']['success_rate']:.1%}")
        print(f"   çµæœä¸€è‡´ç‡: {summary['test_statistics']['match_rate']:.1%}")
        print(f"   V2å¹³å‡ä¿¡é ¼åº¦: {avg_confidence:.2f}")
        print(f"   V2é«˜é€ŸåŒ–ç‡: {summary['performance_analysis']['v2_faster_rate']:.1%}")
        
        return summary


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ æ–°æ—§ã‚·ã‚¹ãƒ†ãƒ æ¯”è¼ƒãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    # æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    comparison = SystemComparison()
    
    # Phase 6a ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ï¼ˆåŸºæœ¬çš„ãªã‚‚ã®ï¼‰
    test_cases = [
        # åŸºæœ¬5æ–‡å‹
        "I love you.",
        "She is beautiful.",
        "He gave me a book.",
        
        # åŠ©å‹•è©
        "I can speak English.",
        "You should study hard.",
        "We must go now.",
        
        # é–¢ä¿‚ç¯€
        "The book that I read was interesting.",
        "The person who called you is here.",
        "This is the house which I bought.",
        
        # è¤‡åˆæ–‡æ³•
        "I wish I could fly.",
        "What did you see yesterday?",
        "The man who can speak Japanese is my teacher."
    ]
    
    # ãƒãƒƒãƒãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    batch_results = comparison.run_batch_test(test_cases)
    
    print(f"\nğŸ¯ Phase 6a ãƒ†ã‚¹ãƒˆå®Œäº†!")
    print(f"   è©³ç´°çµæœã¯ comparison.test_results ã«ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™")


if __name__ == "__main__":
    main()
